//Users/Users/austinjaybecker/Users/austinjaybecker/projects/Users/austinjaybecker/projects/abeck-go-testing/Users/austinjaybecker/projects/abeck-go-testing/auth.goActionActiveAddDashboardCellOptionsAllResourceTypesAuthorizationAuthorizationFilterAuthorizationKindAuthorizationServiceAuthorizationUpdateAuthorizationsResourceTypeAuthorizerAuthorizerV1AxisBackupFilenamePatternBackupServiceBandViewPropertiesBucketBucketFilterBucketIDLengthBucketOperationLogServiceBucketServiceBucketTypeBucketTypeSystemBucketTypeUserBucketUpdateBucketsResourceTypeBuildInfoBuilderConfigCRUDLogCRUDLogSetterCellCellPropertyCellUpdateCheckCheckCreateCheckDefaultPageSizeCheckFilterCheckMaxPageSizeCheckServiceCheckUpdateCheckViewPropertiesChecksResourceTypeChronografErrorCreateMeasurementCredentialsV1DBRPMappingDBRPMappingFilterDBRPMappingFilterV2DBRPMappingServiceDBRPMappingServiceV2DBRPMappingV2DBRPResourceTypeDashboardDashboardFilterDashboardMetaDashboardOperationLogServiceDashboardQueryDashboardServiceDashboardUpdateDashboardsResourceTypeDecimalPlacesDecodeFindOptionsDefaultDashboardFindOptionsDefaultLeaseTTLDefaultOperationLogFindOptionsDefaultPageSizeDefaultSessionLengthDefaultSourceFindOptionsDefaultTaskStatusDefaultVariableFindOptionsDeleteServiceDocumentDocumentMetaDocumentServiceDocumentStoreDocumentsResourceTypeDurationEConflictEEmptyValueEForbiddenEInternalEInvalidEMethodNotAllowedENotFoundENotImplementedETooLargeETooManyRequestsEUnauthorizedEUnavailableEUnprocessableEntityEmptyViewPropertiesEqualErrAuthorizerNotSupportedErrCellNotFoundErrCorruptIDErrCredentialsUnauthorizedErrDashboardNotFoundErrDocumentNotFoundErrFluxParseErrorErrInternalBucketServiceErrorErrInternalOrgServiceErrorErrInternalTaskServiceErrorErrInvalidActionErrInvalidIDErrInvalidIDLengthErrInvalidMappingTypeErrInvalidNotificationEndpointTypeErrInvalidOrgFilterErrInvalidOwnerIDErrInvalidResourceTypeErrInvalidTaskIDErrInvalidUserTypeErrJsonMarshalErrorErrLabelExistsOnResourceErrLabelNameisEmptyErrLabelNotFoundErrNoAcquireErrNoRunsFoundErrNoTelegrafPluginsErrOrgNameisEmptyErrOrgNotFoundErrOutOfBoundsLimitErrPageSizeTooLargeErrPageSizeTooSmallErrQueryErrorErrResourceIDRequiredErrResultIteratorErrorErrRunCanceledErrRunExecutionErrorErrRunKeyNotFoundErrRunNotFoundErrScraperTargetNotFoundErrSecretNotFoundErrSessionExpiredErrSessionNotFoundErrSourceNotFoundErrTaskAlreadyClaimedErrTaskConcurrencyLimitReachedErrTaskNotClaimedErrTaskNotFoundErrTaskOptionParseErrTaskRunAlreadyQueuedErrTaskTimeParseErrTelegrafConfigInvalidOrgIDErrTelegrafConfigNotFoundErrTelegrafPluginNameUnmatchErrUnableToCreateTokenErrUnexpectedTaskBucketErrErrUnsupportTelegrafPluginNameErrUnsupportTelegrafPluginTypeErrUserIDRequiredErrVariableNotFoundErrViewNotFoundErrorErrorCodeErrorMessageErrorOpFindOptionParamsFindOptionsFluxLanguageServiceGaugeViewPropertiesGetBuildInfoHTTPErrorHandlerHeatmapViewPropertiesHistogramViewPropertiesIDIDFromStringIDGeneratorIDLengthInactiveInfiniteRetentionInvalidIDIsActiveKeyValueLogLabelLabelFilterLabelMappingLabelMappingFilterLabelServiceLabelUpdateLabelsResourceTypeLeaseLegendLimitLinePlusSingleStatPropertiesLogLogColumnSettingLogFilterLogViewPropertiesLogViewerColumnLookupServiceManifestManifestEntryManifestKVEntryMappingTypeMarkdownViewPropertiesMarshalViewPropertiesJSONMaxPageSizeMePermissionsMeasurementLengthMemberMemberBucketPermissionMemberPermissionsMonitoringSystemBucketNameMonitoringSystemBucketRetentionMosaicViewPropertiesNewBuilderTagNewErrorNewGlobalPermissionNewPagingLinksNewPermissionNewPermissionAtIDNewResourcePermissionNewTagNopSemaphoreNotEqualNotRegexEqualNotificationEndpointNotificationEndpointFilterNotificationEndpointResourceTypeNotificationEndpointServiceNotificationEndpointUpdateNotificationRuleNotificationRuleCreateNotificationRuleFilterNotificationRuleResourceTypeNotificationRuleStoreNotificationRuleUpdateOnboardingRequestOnboardingResultsOnboardingServiceOpAddDashboardCellOpAddTargetOpCreateAuthorizationOpCreateBucketOpCreateCheckOpCreateDashboardOpCreateLabelOpCreateLabelMappingOpCreateNotificationEndpointOpCreateOrganizationOpCreateSessionOpCreateSourceOpCreateTelegrafConfigOpCreateUserOpCreateVariableOpDefaultSourceOpDeleteAuthorizationOpDeleteBucketOpDeleteCheckOpDeleteDashboardOpDeleteLabelOpDeleteLabelMappingOpDeleteNotificationEndpointOpDeleteOrganizationOpDeleteSourceOpDeleteTelegrafConfigOpDeleteUserOpDeleteVariableOpExpireSessionOpFindAuthorizationByIDOpFindAuthorizationByTokenOpFindAuthorizationsOpFindBucketOpFindBucketByIDOpFindBucketsOpFindCheckOpFindCheckByIDOpFindChecksOpFindDashboardByIDOpFindDashboardsOpFindLabelByIDOpFindLabelMappingOpFindLabelsOpFindNotificationEndpointOpFindNotificationEndpointByIDOpFindNotificationEndpointsOpFindOrganizationOpFindOrganizationByIDOpFindOrganizationsOpFindSessionOpFindSourceByIDOpFindSourcesOpFindTelegrafConfigByIDOpFindTelegrafConfigsOpFindUserOpFindUserByIDOpFindUsersOpFindVariableByIDOpFindVariablesOpGetDashboardCellViewOpGetTargetByIDOpListTargetsOpPutBucketOpPutOrganizationOpRemoveDashboardCellOpRemoveTargetOpRenewSessionOpReplaceDashboardCellsOpReplaceVariableOpUpdateAuthorizationOpUpdateBucketOpUpdateCheckOpUpdateDashboardOpUpdateDashboardCellOpUpdateDashboardCellViewOpUpdateLabelOpUpdateNotificationEndpointOpUpdateOrganizationOpUpdateSourceOpUpdateTargetOpUpdateTelegrafConfigOpUpdateUserOpUpdateVariableOperPermissionsOperationLogEntryOperatorOrgIDLengthOrgMappingTypeOrgResourceTypesOrganizationOrganizationFilterOrganizationOperationLogServiceOrganizationServiceOrganizationUpdateOrgsResourceTypeOwnerOwnerPermissionsPagingFilterPagingLinksParseBucketTypeParseRequestStillQueuedErrorPasswordsServicePermissionPermissionAllowedPermissionSetPredicatePrometheusScraperTypeReadActionReadAllPermissionsReadMeasurementRealTimeGeneratorRegexEqualRenamableFieldRenewSessionTimeRequestStillQueuedErrorResourceResourceTypeRestoreServiceRunRunCanceledRunFailRunFilterRunScheduledRunStartedRunStatusRunSuccessScatterViewPropertiesSchemeV1SchemeV1BasicSchemeV1TokenSchemeV1URLScraperResourceTypeScraperTargetScraperTargetFilterScraperTargetStoreServiceScraperTypeSecretFieldSecretServiceSecretsResourceTypeSelfSourceTypeSemaphoreSessionSessionAuthorizationKindSessionServiceSetBuildInfoSingleStatViewPropertiesSortDashboardsSourceSourceFieldsSourceQuerySourceServiceSourceTypeSourceUpdateSourcesResourceTypeStatusTableOptionsTableViewPropertiesTagTagRuleTaskTaskActiveTaskCreateTaskDefaultPageSizeTaskFilterTaskInactiveTaskMaxPageSizeTaskServiceTaskStatusTaskStatusActiveTaskStatusInactiveTaskSystemTypeTaskUpdateTasksResourceTypeTasksSystemBucketNameTasksSystemBucketRetentionTelegrafConfigTelegrafConfigFilterTelegrafConfigStoreTelegrafsResourceTypeTenantServiceTimeGeneratorTimespanToOperatorTokenGeneratorUnmarshalViewPropertiesJSONUsageUsageFilterUsageMetricUsageQueryRequestBytesUsageQueryRequestCountUsageSeriesUsageServiceUsageValuesUsageWriteRequestBytesUsageWriteRequestCountUserUserFilterUserMappingTypeUserOperationLogServiceUserResourceMappingUserResourceMappingFilterUserResourceMappingServiceUserServiceUserStatusUserTypeUserUpdateUsersResourceTypeV1SourceFieldsV1SourceTypeV2SourceTypeValidScraperTypeVariableVariableArgumentsVariableConstantValuesVariableFilterVariableMapValuesVariableQueryValuesVariableServiceVariableUpdateVariablesResourceTypeViewViewColorViewContentsViewContentsUpdateViewFilterViewPropertiesViewPropertyTypeBandViewPropertyTypeCheckViewPropertyTypeGaugeViewPropertyTypeHeatMapViewPropertyTypeHistogramViewPropertyTypeLogViewerViewPropertyTypeMarkdownViewPropertyTypeMosaicViewPropertyTypeScatterViewPropertyTypeSingleStatViewPropertyTypeSingleStatPlusLineViewPropertyTypeTableViewPropertyTypeXYViewUpdateViewsResourceTypeWithErrorCodeWithErrorErrWithErrorMsgWithErrorOpWriteActionWriteServiceXYViewPropertiesactionsavailableInputPluginsavailableOutputPluginsbucketsbuildInfodecodeInternalErrordecodePluginRawerrEncodefmtRequestStillQueuednewMatchBehaviornopLeasenopSemaphoreopStropStrMapparseMetadatapluginCountsafeParseSourcestrPtrtelegrafConfigDecodetelegrafPluginDecodeunsafeBytesToStringvalidNamectxidtfilteroptaupdpDecodeiDecodeFromStringEncodeValidStringGoStringMarshalTextUnmarshalTextsPtrTypeOrgIDrMatchesmatchesV1matchesV2TimeLocationzonenameoffsetisDSTzoneTranswhenindexisstdisutctxextendcacheStartcacheEndcacheZonegetllookuplookupFirstZonefirstZoneUsedlookupNamewallextlocFormatAppendFormatappendFormatappendFormatRFC3339appendStrictRFC3339nsecsecunixSecaddSecsetLocstripMonosetMonomonoAfterBeforeCompareIsZeroabslocabsDateYearMonthDayWeekdayISOWeekClockHourMinuteSecondNanosecondYearDayAddSubAddDatedateUTCLocalInZoneZoneBoundsUnixUnixMilliUnixMicroUnixNanoMarshalBinaryUnmarshalBinaryGobEncodeGobDecodeMarshalJSONUnmarshalJSONIsDSTTruncateRoundCreatedAtUpdatedAtSetCreatedAtlogSetUpdatedAtTokenDescriptionUserIDPermissionsGetUserIDKindIdentifiercontextfmtinfluxdb"context""fmt"authorization"authorization"errorCodeMsgOpErreunable to create token"unable to create token"invalidjson:"id"`json:"id"`stringjson:"token"`json:"token"`json:"status"`json:"status"`json:"description"`json:"description"`json:"orgID"`json:"orgID"`json:"userID,omitempty"`json:"userID,omitempty"`json:"permissions"`json:"permissions"`json:"status,omitempty"`json:"status,omitempty"`json:"description,omitempty"`json:"description,omitempty"`_nilSprintfpermission %s is not for org id %s"permission %s is not for org id %s"Allowedpsunauthorizedtoken is inactive"token is inactive"boolactiveFindAuthorizationByID"FindAuthorizationByID"FindAuthorizationByToken"FindAuthorizationByToken"FindAuthorizations"FindAuthorizations"CreateAuthorization"CreateAuthorization"UpdateAuthorization"UpdateAuthorization"DeleteAuthorization"DeleteAuthorization"ContextDeadlineDoneValueOrgOffsetSortByDescendingGetLimitfQueryParamsint AuthorizationKind is returned by (*Authorization).Kind(). ErrUnableToCreateToken sanitized error message for all errors when a user cannot create a token Authorization is an authorization. ðŸŽ‰ AuthorizationUpdate is the authorization update request. Valid ensures that the authorization is valid. PermissionSet returns the set of permissions associated with the Authorization. IsActive is a stub for idpe. IsActive returns true if the authorization active. GetUserID returns the user id. Kind returns session and is used for auditing. Identifier returns the authorizations ID and is used for auditing. auth service op AuthorizationService represents a service for managing authorization data. Returns a single authorization by ID. Returns a single authorization by Token. Returns a list of authorizations that match filter and the total count of matching authorizations. Additional options provide pagination & sorting. Creates a new authorization and sets a.Token and a.UserID with the new identifier. UpdateAuthorization updates the status and description if available. Removes a authorization by token. AuthorizationFilter represents a set of filter that restrict the returned results.GetTypeviewPropertiesKeyMarshalerBackfillSecretKeysGetCRUDLogGetDescriptionGetIDGetNameGetOrgIDGetStatusSecretFieldsSetDescriptionSetIDSetNameSetOrgIDSetStatusutResourceIDNamenCreateNotificationEndpointDeleteNotificationEndpointFindNotificationEndpointByIDFindNotificationEndpointsPatchNotificationEndpointUpdateNotificationEndpointdNanosecondsMicrosecondsMillisecondsSecondsMinutesHoursAbsRetentionPeriodValuesAggregateFunctionTypePeriodFillValuesBucketsTagsFunctionsAggregateWindowbKeepAliveReleaseTTLGetDashboardOperationLogTextEditModeRunIDMessageTaskIDScheduledForRunAtStartedAtFinishedAtRequestedAtbtRetentionPolicyNameCloneHexQueriesViewColorsXColumnFillColumnsXDomainXAxisLabelPositionBinCountNoteShowNoteWhenEmptyLegendColorizeRowsLegendOpacityLegendOrientationThresholdvPropertiesValidateTryAcquireVersionCommitExpiresAtExpiredEphemeralAuthCreateSessionExpireSessionFindSessionRenewSessionXYWHcQueryLanguageOrganizationIDCellsMetaOwnerIDTypesAddLogEntryFirstLogEntryForEachLogEntryLastLogEntryOAuthIDuAuthmtmownerPermsmemberPermsToPermissionsCreateUserResourceMappingDeleteUserResourceMappingFindUserResourceMappingsApplyIsEnforcedDigitsPrefixTickPrefixSuffixTickSuffixSchemeUsernameAuthorize2BoundsLegacyBoundsBaseScaleOrientationAxesGeomTimeFormatHoverDimensionGenerateXAxisTicksXTotalTicksXTickStartXTickStepYColumnGenerateYAxisTicksYTotalTicksYTickStartYTickStepUpperColumnMainColumnLowerColumnPrevSelfNextQueryParamPasswordIsOnboardingOnboardInitialUserOnboardUserLabelIDCreateLabelCreateLabelMappingDeleteLabelDeleteLabelMappingFindLabelByIDFindLabelsFindResourceLabelsUpdateLabelNowgSharedSecretMetaURLDefaultRPDefaultURLInsecureSkipVerifyTelegrafRoleCreateSourceDefaultSourceDeleteSourceFindSourceByIDFindSourcesUpdateSourceStartEndStopSelectedArgumentsDatabaseRetentionPolicyBucketIDCompleterScopeArraymonthsnsecsnegativeMulIsPositiveIsNegativeMonthsOnlyNanoOnlyMonthsNormalizeAsValuesFunctionObjectRegexpProgInstInstOpOutArgRuneopMatchRuneMatchRunePosMatchEmptyWidthNumCapskipNopStartCondonePassProgonePassInstEmptyOpexprprogonepassnumSubexpmaxBitStateLensubexpNamesprefixprefixBytesprefixRuneprefixEndmpoolmatchcapprefixCompletecondminInputLenlongesttryBacktrackrebacktrackdoOnePassdoMatchdoExecuteCopyLongestputNumSubexpSubexpNamesSubexpIndexLiteralPrefixMatchReaderMatchStringMatchReplaceAllStringReplaceAllLiteralStringReplaceAllStringFuncreplaceAllReplaceAllReplaceAllLiteralReplaceAllFuncpadallMatchesFindFindIndexFindStringFindStringIndexFindReaderIndexFindSubmatchExpandExpandStringexpandFindSubmatchIndexFindStringSubmatchFindStringSubmatchIndexFindReaderSubmatchIndexFindAllFindAllIndexFindAllStringFindAllStringIndexFindAllSubmatchFindAllSubmatchIndexFindAllStringSubmatchFindAllStringSubmatchIndexSplitRemainderMonoTypefbTablerUOffsetTTableBytesPosIndirectByteVectorVectorLenVectorUnionGetBoolGetByteGetUint8GetUint16GetUint32GetUint64GetInt8GetInt16GetInt32GetInt64GetFloat32GetFloat64GetUOffsetTGetVOffsetTGetSOffsetTGetBoolSlotGetByteSlotGetInt8SlotGetUint8SlotGetInt16SlotGetUint16SlotGetInt32SlotGetUint32SlotGetInt64SlotGetUint64SlotGetFloat32SlotGetFloat64SlotGetVOffsetTSlotMutateBoolMutateByteMutateUint8MutateUint16MutateUint32MutateUint64MutateInt8MutateInt16MutateInt32MutateInt64MutateFloat32MutateFloat64MutateUOffsetTMutateVOffsetTMutateSOffsetTMutateBoolSlotMutateByteSlotMutateInt8SlotMutateUint8SlotMutateInt16SlotMutateUint16SlotMutateInt32SlotMutateUint32SlotMutateInt64SlotMutateUint64SlotMutateFloat32SlotMutateFloat64SlotInittblNatureBasicVarNumNumArgumentsArgumentPipeArgumentSortedArgumentsReturnTypeElemTypeNumPropertiesRecordPropertySortedPropertiesExtendsCanonicalStringgetCanonicalMappingBoolFloatGetIntIsNullLenRangeSetStrTyperUIntCallHasSideEffectAppendSortLocalLookupLocalRangeLookupNestPopReturnSetReturnSizescopeNamesFunctionNamesFunctionSuggestionPackageBaseNodeSourceLocationLineColumnLessIsValidFromBufFileLocErrorsErrsPackageClausenodeexpressionImportDeclarationStringLiteralliteralAsPathStatementNodestmtMetadataImportsBodyFilesSideEffectNodeTypeEvalASTParseClearPrivateDataGenerateFluxGetOwnerIDGetTaskIDSetOwnerIDSetTaskIDIDsCreateDeleteFindByIDFindManyUpdateClusterCompareAndSetPasswordComparePasswordSetPasswordOrganizationNameBucketNameShardIDFileNameLastModifiedRateEveryGetEndpointIDMatchesTagsReaderReadRestoreBucketRestoreKVStoreRestoreShardDeleteSecretGetSecretKeysLoadSecretPatchSecretsPutSecretPutSecretsResponseWriterHeaderhhasDelWritewritesortedKeyValuesWriteSubsetwriteSubsetWriteHeaderHandleHTTPErrorConfigCountPluginstcContentLabelsOrganizationsCreateDocumentFindDocumentFindDocumentsCreateDocumentStoreFindDocumentStoreFluxCronLatestCompletedLatestScheduledLatestSuccessLatestFailureLastRunStatusLastRunErrorEffectiveCronAfterTimeBeforeTimeOptionsDurationLiteralMagnitudeUnitDurationFromConcurrencyRetryClearoEffectiveCronStringUpdateFluxupdateFluxupdateFluxASTCancelRunCreateTaskDeleteTaskFindLogsFindRunByIDFindRunsFindTaskByIDFindTasksForceRunRetryRunUpdateTaskGetUserOperationLogCheckIDBinSizeYDomainYAxisLabelXPrefixXSuffixYPrefixYSuffixUnmarshalTOMLPluginNameTOMLWriterBackupKVStoreBackupShardGetBucketOperationLogSettingsColumnsInternalNameDisplayNameVisibleVerticalTimeAxisWrappingFixFirstColumnFieldOptionsMarshalCreateNotificationRuleDeleteNotificationRuleFindNotificationRuleByIDFindNotificationRulesPatchNotificationRuleUpdateNotificationRuleAllowInsecureAddTargetGetTargetByIDListTargetsRemoveTargetUpdateTargetRequestUserinfousernamepasswordpasswordSetOpaqueHostRawPathOmitHostForceQueryRawQueryFragmentRawFragmentsetPathEscapedPathsetFragmentEscapedFragmentRedactedIsAbsResolveReferenceRequestURIHostnamePortJoinPathReadCloserCloserCloseHasFormFileHeaderMIMEHeaderFilenamecontenttmpfiletmpofftmpsharedOpenfhRemoveAllConnectionStateCertificateSignatureAlgorithmisRSAPSSalgoPublicKeyAlgorithmnatWordmodInversezclearnormmakesetWordsetUint64setaddsubcmpxmulAddWWmontgomerymulsqrmulRangebitLentrailingZeroBitsisPow2shlshrsetBitbitstickyandtruncandNotorxorrandomexpNNexpNNMontgomeryEvenexpNNWindowedexpNNMontgomerybytessetBytessqrtsubMod2NscanutoaitoaconvertWordsqexpWWremdivdivWmodWdivLargedivBasicdivRecursivedivRecursiveStepprobablyPrimeMillerRabinprobablyPrimeLucasnegSignSetInt64SetUint64BitsSetBitsNegMulRangeBinomialQuoRemQuoRemDivModDivModCmpCmpAbsInt64Uint64IsInt64IsUint64SetStringsetFromScannerSetBytesFillBytesBitLenTrailingZeroBitsExpexpSlowexpGCDlehmerGCDRandModInversemodSqrt3Mod4PrimemodSqrt5Mod8PrimemodSqrtTonelliShanksModSqrtLshRshBitSetBitAndAndNotOrXorNotSqrtScanProbablyPrimescaleDenomAttributeTypeAndValueObjectIdentifieroiCountryOrganizationalUnitLocalityProvinceStreetAddressPostalCodeSerialNumberCommonNameExtraNamesFillFromRDNSequenceappendRDNsToRDNSequenceKeyUsageExtensionIdCriticalExtKeyUsageIPIsUnspecifiedipIsLoopbackIsPrivateIsMulticastIsInterfaceLocalMulticastIsLinkLocalMulticastIsLinkLocalUnicastIsGlobalUnicastTo4To16DefaultMaskMaskmatchAddrFamilyIPNetIPMaskContainsNetworkRawRawTBSCertificateRawSubjectPublicKeyInfoRawSubjectRawIssuerSignaturePublicKeyIssuerSubjectNotBeforeNotAfterExtensionsExtraExtensionsUnhandledCriticalExtensionsUnknownExtKeyUsageBasicConstraintsValidIsCAMaxPathLenMaxPathLenZeroSubjectKeyIdAuthorityKeyIdOCSPServerIssuingCertificateURLDNSNamesEmailAddressesIPAddressesURIsPermittedDNSDomainsCriticalPermittedDNSDomainsExcludedDNSDomainsPermittedIPRangesExcludedIPRangesPermittedEmailAddressesExcludedEmailAddressesPermittedURIDomainsExcludedURIDomainsCRLDistributionPointsPolicyIdentifierssystemVerifycheckNameConstraintsisValidVerifybuildChainsVerifyHostnamehasSANExtensionCheckSignatureFromCheckSignaturehasNameConstraintsgetSANExtensionCheckCRLSignatureCreateCRLHandshakeCompleteDidResumeCipherSuiteNegotiatedProtocolNegotiatedProtocolIsMutualServerNamePeerCertificatesVerifiedChainsSignedCertificateTimestampsOCSPResponseTLSUniqueekmExportKeyingMaterialcsResponseStatusCodeProtoProtoMajorProtoMinorContentLengthTransferEncodingUncompressedTrailerTLSCookiesProtoAtLeastcloseBodybodyIsWritableisProtocolSwitchMethodGetBodyPostFormMultipartFormRemoteAddrCancelWithContextUserAgentCookieAddCookieRefererMultipartReadermultipartReaderisH2UpgradeWriteProxyBasicAuthSetBasicAuthParseFormParseMultipartFormFormValuePostFormValueFormFileexpectsContinuewantsHttp10KeepAlivewantsCloseisReplayableoutgoingLengthrequiresHTTP1RawMessageAliasPluginsSymbolColumnsuuCreateUserDeleteUserFindPermissionForUserFindUserFindUserByIDFindUsersUpdateUserCreateOrganizationDeleteOrganizationFindOrganizationFindOrganizationByIDFindOrganizationsUpdateOrganizationCreateBucketDeleteBucketFindBucketFindBucketByIDFindBucketByNameFindBucketsUpdateBucketShadeBelowAddDashboardCellCreateDashboardDeleteDashboardFindDashboardByIDFindDashboardsGetDashboardCellViewRemoveDashboardCellReplaceDashboardCellsUpdateDashboardUpdateDashboardCellUpdateDashboardCellViewYSeriesColumnsFindResourceNameDeleteBucketRangePredicateCreateCheckDeleteCheckFindCheckFindCheckByIDFindChecksPatchCheckUpdateCheckKVCreateTelegrafConfigDeleteTelegrafConfigFindTelegrafConfigByIDFindTelegrafConfigsUpdateTelegrafConfigGetOrganizationOperationLogWriteToCreateVariableDeleteVariableFindVariableByIDFindVariablesReplaceVariableUpdateVariableFindBytrGetUsageByteScannerByteReaderReadByteUnreadByteVerifyOptionsCertPoollazyCertrawSubjectgetCertsum22428byNamelazyCertshaveSumsystemPoollencertfindPotentialParentscontainsAddCertaddCertFuncAppendCertsFromPEMSubjectsDNSNameIntermediatesRootsCurrentTimeKeyUsagesMaxConstraintComparisions_tabrcvValuesLengthSOffsetTVOffsetTbufrdwerrlastBytelastRuneSizeResetresetfillreadErrPeekDiscardReadRuneUnreadRuneBufferedReadSliceReadLinecollectFragmentsReadBytesReadStringwriteBufPartmrdispositiondispositionParamstotalFormNameparseContentDispositionpopulateHeadersbufReadertempDircurrentPartpartsReadnlnlDashBoundarydashBoundaryDashdashBoundaryReadFormreadFormNextPartNextRawPartnextPartisFinalBoundaryisBoundaryDelimiterLineImportsLengthBodyLengthRevokedCertificateRevocationTimeTTypeMutateTTypeTPipeMutatePipeOptionalMutateOptionalTypeOfRuneReaderRDNSequenceRelativeDistinguishedNameSETStructMutateLineMutateColumnkeyValueskeyvaluesheaderSorterkvsSwapParamsErrorsLengthmachinequeueentrythreadinstcappcsparsedenseinputsinputBytesstrstepcanCheckPrefixhasPrefixinputStringinputReaderatEOTposreadernewBytesnewStringnewReaderinitq0q1poolmatchedallocmatchSameSiteDomainExpiresRawExpiresMaxAgeSecureHttpOnlyUnparseddivisorbbbnbitsndigitsInt63SeedSource64srcs64readValreadPosExpFloat64NormFloat64Uint32Int31Int63nInt31nint31nIntnFloat64Float32PermShuffleCertificateListTBSCertificateListRawContentAlgorithmIdentifierRawValueClassIsCompoundFullBytesAlgorithmParametersThisUpdateNextUpdateRevokedCertificatesBitStringBitLengthAtRightAlignTBSCertListSignatureValueHasExpiredcertListClientTraceGotConnInfoConnAddrLocalAddrSetDeadlineSetReadDeadlineSetWriteDeadlineReusedWasIdleIdleTimeDNSStartInfoDNSDoneInfoIPAddrisWildcardopAddrfamilysockaddrtoLocalAddrsCoalescedWroteRequestInfoGetConnGotConnPutIdleConnGotFirstResponseByteGot100ContinueGot1xxResponseDNSStartDNSDoneConnectStartConnectDoneTLSHandshakeStartTLSHandshakeDoneWroteHeaderFieldWroteHeadersWait100ContinueWroteRequestcomposehasNetHooksScanStateSkipSpaceWidthbitStatejobargendjobsvisitedshouldVisitpushinputlazyFlagPropKVTypeMutateVTypeVfbStateFlagPrecisionMutateMagnitudeMutateUnitReaderAtReadAtSeekerSeekWrappedStatementStatementTypeMutateStatementTypeSockaddr_Socklen/Users/austinjaybecker/projects/abeck-go-testing/authorization/Users/austinjaybecker/projects/abeck-go-testing/authorization/error.goAuthHandlerAuthLoggerAuthMetricsAuthedAuthorizationServiceAuthorizationClientServiceErrAuthNotFoundErrFailureGeneratingIDErrInternalServiceErrorErrInvalidAuthIDErrInvalidAuthIDErrorErrTokenAlreadyExistsErrorMaxIDGenerationNNewAuthLoggerNewAuthMetricsNewAuthedAuthorizationServiceNewHTTPAuthHandlerNewServiceNewStoreNotUniqueIDErrorReservedIDsServiceStoreUnexpectedAuthIndexErrorVerifyPermissionsauthBucketauthIndexauthIndexBucketauthIndexKeyauthResponseauthorizationsPredicateFnauthsResponsedecodeAuthorizationdecodeGetAuthorizationsRequestdecodePostAuthorizationRequestdecodeUpdateAuthorizationRequestencodeAuthorizationfilterAuthorizationsFngetAuthorizationsRequestgetAuthorizedUsernewAuthsResponsenewPostAuthorizationRequestpermissionResponsepostAuthorizationRequestprefixAuthorizationresourceResponsetenantServiceuniqueuniqueIDupdateAuthorizationRequestgithub.com/influxdata/influxdb/v2"github.com/influxdata/influxdb/v2"authorization ID is invalid"authorization ID is invalid"not foundauthorization not found"authorization not found"conflictID already exists"ID already exists"internal errorunable to generate valid id"unable to generate valid id"token already exists"token already exists"auth id provided is invalid"auth id provided is invalid"unexpected error retrieving auth index; Err: %v"unexpected error retrieving auth index; Err: %v" ErrInvalidAuthID is used when the Authorization's ID cannot be encoded ErrAuthNotFound is used when the specified auth cannot be found NotUniqueIDError occurs when attempting to create an Authorization with an ID that already belongs to another one ErrFailureGeneratingID occurs ony when the random number generator cannot generate an ID in MaxIDGenerationN times. ErrTokenAlreadyExistsError is used when attempting to create an authorization with a token that already exists ErrInvalidAuthIDError is used when a service was provided an invalid ID. ErrInternalServiceError is used when the error comes from an internal system. UnexpectedAuthIndexError is used when the error comes from an internal system.TxCursorHintCursorHintsCursorPredicateFuncKeyPrefixKeyStartPredicateFnCursorFirstLastCursorOptionCursorConfigCursorDirectionDirectionHintsSkipFirstForwardCursorGetBatchPutREDClientmetricCollectorMetricDescLabelPairXXX_NoUnkeyedLiteralXXX_unrecognizedXXX_sizecacheProtoMessageDescriptorXXX_UnmarshalXXX_MarshalXXX_MergeXXX_SizeXXX_DiscardUnknownGetValuefqNamehelpconstLabelPairsvariableLabelsdimHashGaugeCounterExemplarTimestampNanosXXX_WellKnownTypeGetSecondsGetNanosGetLabelGetTimestampGetExemplarSummaryQuantileGetQuantileSampleCountSampleSumGetSampleCountGetSampleSumUntypedHistogramCumulativeCountUpperBoundGetCumulativeCountGetUpperBoundGetBucketTimestampMsGetGaugeGetCounterGetSummaryGetUntypedGetHistogramGetTimestampMsCollectFnOptsAdditionalPropsCollectorCollectDescribecollectmetricsRecordcollectorsrecauthServiceBackupRestorekvStoreIDGensetupgenerateSafeIDGetAuthorizationByIDGetAuthorizationByTokenListAuthorizationsforEachAuthorizationuniqueAuthTokenstoretokenGeneratorLinkstoInfluxdbAuthsSetDefaultsLoggerCoreEntryLevelCapitalStringunmarshalTextEnabledEntryCallerDefinedPCecFullPathTrimmedPathLoggerNameCallerStackCheckedEntryWriteSyncerSyncCheckWriteActionErrorOutputdirtyshouldcoresceAddCoreShouldFieldFieldTypeIntegerInterfaceAddToEqualsLevelEnablerWithcoredevelopmenterrorOutputaddCalleraddStackcallerSkipSugarNamedWithOptionsDebugInfoWarnDPanicPanicFatalclonecheckloggerFindUserByIDFnFindUserFnFindOrganizationByIDFFindOrganizationFFindBucketByIDFntsRegistererMustRegisterRegisterUnregisterClientOptFnmetricOptsVecOptsCounterFnCounterVecmetricVecmetricMapRWMutexMutexstatesemaLockTryLocklockSlowUnlockunlockSlowInt32noCopyLoadCompareAndSwapwriterSemreaderSemreaderCountreaderWaitRLockrwTryRLockRUnlockrUnlockSlowRLockermetricWithLabelValuesmetricmtxdescnewMetricdeleteByHashWithLabelValuesdeleteByHashWithLabelsgetOrCreateMetricWithLabelValuesgetOrCreateMetricWithLabelsgetMetricWithHashAndLabelValuesgetMetricWithHashAndLabelscurriedLabelValuevaluecurryhashAddhashAddByteDeleteLabelValuescurryWithgetMetricWithLabelValuesgetMetricWithhashLabelValueshashLabelsGetMetricWithLabelValuesGetMetricWithWithLabelValuesCurryWithMustCurryWithHistogramFnHistogramVecHelpLabelNamesnamespaceserviceserviceSuffixcounterMetricshistogramMetricsserviceNameApplySuffixRouterHandlerFuncServeHTTPHandlerRoutesMiddlewaresmwsRoutePatternHandlersSubRoutesRouteParamsKeysRoutePathRouteMethodRoutePatternsURLParamsroutePatternrouteParamsmethodNotAllowedURLParamRoutePatternConnectGroupHandleHandleFuncHeadMethodFuncMethodNotAllowedMountNotFoundPatchPostTraceUseAPIprettyJSONencodeGZIPunmarshalErrFnokErrFnerrFnDecodeJSONDecodeGobdecodeRespondlogErrapiauthSvchandlePostAuthorizationnewAuthResponsenewPermissionsResponsegetNameForResourcehandleGetAuthorizationshandleGetAuthorizationhandleUpdateAuthorizationhandleDeleteAuthorizationClientdoerDoWriteCloserFnWriteCloseraddrdefaultHeaderswriterFnsauthFnrespFnstatusFnPatchJSONPostJSONPutJSONReqbuildURLIncclientreqdecodeFnAcceptContentTypeHeadersRespFnStatusFndoclientOptinsecureSkipVerifyheadersBodyFnObserverObserveObserverVecSugaredLoggerbaseDesugarDebugfInfofWarnfErrorfDPanicfPanicfFatalfDebugwInfowWarnwErrorwDPanicwPanicwFatalwsweetenFieldsdecoderLockerRecordFnObjectEncoderArrayMarshalerArrayEncoderObjectMarshalerMarshalLogObjectPrimitiveArrayEncoderAppendBoolAppendByteStringAppendComplex128AppendComplex64AppendFloat32AppendFloat64AppendIntAppendInt16AppendInt32AppendInt64AppendInt8AppendStringAppendUintAppendUint16AppendUint32AppendUint64AppendUint8AppendUintptrAppendArrayAppendDurationAppendObjectAppendReflectedAppendTimeMarshalLogArrayAddArrayAddBinaryAddBoolAddByteStringAddComplex128AddComplex64AddDurationAddFloat32AddFloat64AddIntAddInt16AddInt32AddInt64AddInt8AddObjectAddReflectedAddStringAddTimeAddUintAddUint16AddUint32AddUint64AddUint8AddUintptrOpenNamespaceOptionapply/Users/austinjaybecker/projects/abeck-go-testing/authorization/http_client.gonewAuthtokenasauthsparamsreserrorshttpc"errors"github.com/influxdata/influxdb/v2/pkg/httpc"github.com/influxdata/influxdb/v2/pkg/httpc"/api/v2/authorizationsappend"id"userID"userID"user"user"orgID"orgID"org"org"0Newnot supported in HTTP authorization service"not supported in HTTP authorization service" AuthorizationClientService connects to Influx via HTTP using tokens to manage authorizations CreateAuthorization creates a new authorization and sets b.ID with the new identifier. FindAuthorizations returns a list of authorizations that match filter and the total count of matching authorizations. FindAuthorizationByToken is not supported by the HTTP authorization service. FindAuthorizationByID finds a single Authorization by its ID against a remote influx server. DeleteAuthorization removes a authorization by id./Users/austinjaybecker/projects/abeck-go-testing/authorization/http_server.goauthpermsresppermokresourceoptsauthIDqpchihttpicontextjsonkithttpmiddlewaretimezapencoding/json"encoding/json"net/http"net/http""time"github.com/go-chi/chi"github.com/go-chi/chi"github.com/go-chi/chi/middleware"github.com/go-chi/chi/middleware"github.com/influxdata/influxdb/v2/context"github.com/influxdata/influxdb/v2/context"github.com/influxdata/influxdb/v2/kit/transport/http"github.com/influxdata/influxdb/v2/kit/transport/http"go.uber.org/zap"go.uber.org/zap"APIOptFnNewAPIWithLogMuxnodeTypendpointsmethodTypendpointhandlerpatternparamKeysnodesnstailSortfindEdge4typlabeltailrexsubrouteschildrenInsertRouteaddChildreplaceChildgetEdgesetEndpointFindRoutefindRouteisLeaffindPatternrouteswalkPoollocallocalSizevictimvictimSizegetSlowpinpinSlowtreemiddlewaresinlineparentnotFoundHandlermethodNotAllowedHandlermxNotFoundHandlerMethodNotAllowedHandlerbuildRouteHandlerhandlerouteHTTPnextRoutePathupdateSubRoutesNewRouterRecovererRequestIDRealIP"/"/{id}"/{id}""/api/v2/authorizations"Auth created "Auth created ""auth"SprintStatusCreated201GetAuthorizerjson:"org"`json:"org"`json:"userID"`json:"userID"`json:"user"`json:"user"`json:"links"`json:"links"`json:"createdAt"`json:"createdAt"`json:"updatedAt"`json:"updatedAt"`Failed to get org"Failed to get org""handler"getAuthorizations"getAuthorizations"Failed to get user"Failed to get user"self"self"/api/v2/authorizations/%s"/api/v2/authorizations/%s"/api/v2/users/%s"/api/v2/users/%s"json:"authorizations"`json:"authorizations"`""authorization must include permissions"authorization must include permissions"org id required"org id required"json:"action"`json:"action"`json:"resource"`json:"resource"`json:"name,omitempty"`json:"name,omitempty"`json:"org,omitempty"`json:"org,omitempty"`orgsusersDecoderdecodeStatescannerendTopparseStateeofpushParseStatepopParseStateerrorContextChanDirStructFieldStructTagtagPkgPathIndexAnonymousIsExportedkrtypetflagnameOfftypeOffsizeptrdatahashalignfieldAlignkindequalgcdataptrToThistextOffuncommonAlignFieldAlignpointerscommonexportedMethodsNumMethodMethodByNamehasNameIsVariadicElemFieldByIndexFieldByNameFieldByNameFuncNumFieldNumInNumOutptrToImplementsAssignableToConvertibleToComparablegcSliceflagromustBemustBeExportedmustBeExportedSlowmustBeAssignablemustBeAssignableSlowpanicNotMapptrpointerpanicNotBoolbytesSlowrunesCanAddrCanSetCallSlicecallCapcapNonSliceCanComplexComplexFieldByIndexErrCanFloatCanIntCanInterfaceInterfaceDataIsNilSetZerolenNonSliceMapIndexMapKeysSetIterKeySetIterValueMapRangeOverflowComplexOverflowFloatOverflowIntOverflowUintPointerRecvrecvSendsendSetBoolsetRunesSetComplexSetFloatSetIntSetLenSetCapSetMapIndexSetUintSetPointerSliceSlice3stringNonStringTryRecvTrySendtypeSlowCanUintUintUnsafeAddrUnsafePointerGrowgrowextendSliceassignToConvertCanConvertFuncuncommonTypepkgPathmcountxcountmoffmethodsFieldStackdataoffopcodesavedErroruseNumberdisallowUnknownFieldsunmarshalreadIndexsaveErroraddErrorContextskipscanNextscanWhilerescanLiteralvalueQuotedarrayobjectconvertNumberliteralStorevalueInterfacearrayInterfaceobjectInterfaceliteralInterfacescanpscannedtokenStatetokenStackUseNumberdecDisallowUnknownFieldsreadValuerefilltokenPrepareForDecodetokenValueAllowedtokenValueEndtokenErrorMorepeekInputOffsetNewDecoderinvalid json structure"invalid json structure"Failed to decode request"Failed to decode request"Failed to create auth response"Failed to create auth response"Auths retrieved "Auths retrieved ""auths"StatusOK200getAuthorization"getAuthorization"Auth retrieved "Auth retrieved "updateAuthorization"updateAuthorization"Auth updated"Auth updated"deleteAuthorization"deleteAuthorization"Auth deleted"Auth deleted""authID"StatusNoContent204 TenantService is used to look up the Organization and User for an Authorization NewHTTPAuthHandler constructs a new http server. handlePostAuthorization is the HTTP handler for the POST /api/v2/authorizations route. In the future, we would like only the service layer to look up the user and org to see if they are valid but for now we need to look up the User and Org here because the API expects the response to have the names of the Org and User TODO(desa): update links to include paging and filter information handleGetAuthorizations is the HTTP handler for the GET /api/v2/authorizations route. If the user or org name was provided, look up the ID first Don't log here, it should already be handled by the service handleUpdateAuthorization is the HTTP handler for the PATCH /api/v2/authorizations/:id route that updates the authorization's status and desc. handleDeleteAuthorization is the HTTP handler for the DELETE /api/v2/authorizations/:id route.MapIterhiterelembptroverflowoldoverflowstartBucketwrappedBbucketcheckBucketinitializediterpoolLocalpoolLocalInternalpoolChainpoolChainEltpoolDequeueefacevalheadTailvalsunpackpackpushHeadpopHeadpopTailnextprevheadprivateshared96methodmtypifntfnisExportedhasTagembeddedreadVarint/Users/austinjaybecker/projects/abeck-go-testing/authorization/middleware_auth.goauthorizergithub.com/influxdata/influxdb/v2/authorizer"github.com/influxdata/influxdb/v2/authorizer"AuthorizeCreateauthorizationsAuthorizeWriteResourceAuthorizeReadAuthorizeReadResourceAuthorizeFindAuthorizationsAuthorizeWriteIsAllowedpermission %s is not allowed"permission %s is not allowed"forbidden TODO: we'll likely want to push this operation into the database eventually since fetching the whole list of data will likely be expensive. VerifyPermissions ensures that an authorization is allowed all of the appropriate permissions./Users/austinjaybecker/projects/abeck-go-testing/authorization/middleware_logging.godurstartmsgcounttook"took"Sincefailed to create authorization"failed to create authorization"authorization create"authorization create"failed to find authorization with ID %v"failed to find authorization with ID %v"auth find by ID"auth find by ID"failed to find authorization with token"failed to find authorization with token"auth find"auth find"failed to find authorizations matching the given filter"failed to find authorizations matching the given filter"authorizations find"authorizations find"failed to update authorization"failed to update authorization"authorization update"authorization update"failed to delete authorization with ID %v"failed to delete authorization with ID %v"authorization delete"authorization delete" NewAuthLogger returns a logging service middleware for the Authorization Service./Users/austinjaybecker/projects/abeck-go-testing/authorization/middleware_metrics.goregprometheusgithub.com/influxdata/influxdb/v2/kit/metric"github.com/influxdata/influxdb/v2/kit/metric"github.com/prometheus/client_golang/prometheus"github.com/prometheus/client_golang/prometheus"ApplyMetricOpts"token"create_authorization"create_authorization"find_authorization_by_id"find_authorization_by_id"find_authorization_by_token"find_authorization_by_token"update_authorization"update_authorization"delete_authorization"delete_authorization" RED metrics/Users/austinjaybecker/projects/abeck-go-testing/authorization/mock_tenant.go tenantService is a mock implementation of an authorization.tenantService FindUserByID returns a single User by ID. FindUsers returns a list of Users that match filter and the total count of matching Users.FindOrganizationByID calls FindOrganizationByIDF.FindOrganization calls FindOrganizationF./Users/austinjaybecker/projects/abeck-go-testing/authorization/service.gonowstkvrandgithub.com/influxdata/influxdb/v2/kv"github.com/influxdata/influxdb/v2/kv"github.com/influxdata/influxdb/v2/rand"github.com/influxdata/influxdb/v2/rand"NewTokenGenerator641 FindAuthorizationByToken returns a authorization by token for a particular authorization. FindAuthorizations retrieves all authorizations that match an arbitrary authorization filter. Filters using ID, or Token should be efficient. Other filters will do a linear scan across all authorizations searching for a match./Users/austinjaybecker/projects/abeck-go-testing/authorization/storage.gofnencodedIDspansnowflaketracinggithub.com/influxdata/influxdb/v2/kit/tracing"github.com/influxdata/influxdb/v2/kit/tracing"github.com/influxdata/influxdb/v2/snowflake"github.com/influxdata/influxdb/v2/snowflake"1001000byteauthorizationsv1"authorizationsv1"authorizationindexv1"authorizationindexv1"GeneratorMachineIDNextStringAppendNextNewDefaultIDGeneratorBackgroundSpanSpanContextForeachBaggageItemFinishOptionsLogRecordfieldTypenumericValstringValinterfaceVallfFieldsLogDataEventPayloadToLogRecordldFinishTimeLogRecordsBulkLogDataTracerStartSpanOptionStartSpanOptionsSpanReferenceSpanReferenceTypeReferencedContextReferencesStartTimeExtractInjectStartSpanBaggageItemFinishFinishWithOptionsLogEventLogEventWithPayloadLogFieldsLogKVSetBaggageItemSetOperationNameSetTagStartSpanFromContextIsNotFound View opens up a transaction that will not write to any data. Implementing interfaces should take care to ensure that all view transactions do not mutate any data. Update opens up a transaction that will mutate data. generateSafeID attempts to create ids for buckets and orgs that are without backslash, commas, and spaces, BUT ALSO do not already exist. TODO: this is probably unnecessary but for testing we need to keep it in. After KV is cleaned out we can update the tests and remove this.EncoderLazyLoggerEmitBoolEmitFloat32EmitFloat64EmitIntEmitInt32EmitInt64EmitLazyLoggerEmitObjectEmitStringEmitUint32EmitUint6411/Users/austinjaybecker/projects/abeck-go-testing/authorization/storage_authorization.goidxidKeyfilterFnpredcurindexBucketindexKeygotexistsprevFnjsonpjsonparsergithub.com/buger/jsonparser"github.com/buger/jsonparser"github.com/influxdata/influxdb/v2/pkg/jsonparser"github.com/influxdata/influxdb/v2/pkg/jsonparser"inactiveunknown authorization status"unknown authorization status"UnmarshaltrueWithCursorHintPredicateNotUniqueErrorUnexpectedIndexErrorValueTypevtGetOptionalID CreateAuthorization takes an Authorization object and saves it in storage using its token using its token property as an index if the provided ID is invalid, or already maps to an existing Auth, then generate a new one GetAuthorization gets an authorization by its ID from the auth bucket in kv use the token to look up the authorization's ID ListAuthorizations returns all the authorizations matching a set of FindOptions. This function is used for FindAuthorizationByID, FindAuthorizationByToken, and FindAuthorizations in the AuthorizationService implementation forEachAuthorization will iterate through all authorizations while fn returns true. preallocate Permissions to reduce multiple slice re-allocations UpdateAuthorization updates the status and description only of an authorization DeleteAuthorization removes an authorization from storage by returning a generic error we are trying to hide when a token is non-unique. otherwise, this is some sort of internal server error and we should provide some debugging information. if not found then this token is unique. no error means this is not unique any other error is some sort of internal server error uniqueID returns nil if the ID provided is unique, returns an error otherwise if not found then the ID is unique if any errors occur reading the JSON data, the predicate will always return true to ensure the value is included and handled higher up. it is assumed that token never has escaped string data Filter by org and user/Users/austinjaybecker/projects/abeck-go-testing/authorizer/Users/austinjaybecker/projects/abeck-go-testing/authorizer/agent.goAuthAgentAuthorizeFindBucketsAuthorizeFindChecksAuthorizeFindDBRPsAuthorizeFindDashboardsAuthorizeFindLabelsAuthorizeFindNotificationEndpointsAuthorizeFindNotificationRulesAuthorizeFindOrganizationsAuthorizeFindScrapersAuthorizeFindSourcesAuthorizeFindTasksAuthorizeFindTelegrafsAuthorizeFindUserResourceMappingsAuthorizeFindUsersAuthorizeFindVariablesAuthorizeOrgReadResourceAuthorizeOrgWriteResourceAuthorizeReadBucketAuthorizeReadGlobalAuthorizeReadOrgAuthorizeWriteGlobalAuthorizeWriteOrgErrFailedPermissionErrInactiveTaskIsAllowedAllIsAllowedAnyNewAuthorizationServiceNewBackupServiceNewBucketServiceNewCheckServiceNewDashboardServiceNewDocumentServiceNewLabelServiceWithOrgNewNotificationEndpointServiceNewNotificationRuleStoreNewOrgServiceNewPasswordServiceNewRestoreServiceNewScraperTargetStoreServiceNewSecretServiceNewSourceServiceNewTaskServiceNewTelegrafConfigServiceNewURMServiceNewUserServiceNewVariableServiceOrgIDResolverOrgServicePasswordServiceTelegrafConfigServiceURMServiceauthErrorauthorizeauthorizeReadSystemBucketdocumentStoreisAllowedisAllowedAllnewDocumentPermissiontaskServiceValidatortoPermsactionOrgPermissionsIsWritablerestorgErrresTyperesTypeErrreadinvalid action provided: "invalid action provided: "not authorized to create "not authorized to create " AuthAgent provides a means to authenticate users with resource and their associate actions. It makes for a clear dependency, to an auth middleware for instance. OrgPermissions identifies if a user has access to the org by the specified action.FindResourceOrganizationIDorgIDResolverprocessPermissionErrorAuthzErrorae/Users/austinjaybecker/projects/abeck-go-testing/authorizer/auth.go AuthorizationService wraps a influxdb.AuthorizationService and authorizes actions against it appropriately. NewAuthorizationService constructs an instance of an authorizing authorization service. FindAuthorizationByID checks to see if the authorizer on context has read access to the id provided. FindAuthorizationByToken retrieves the authorization and checks to see if the authorizer on context has read access to the authorization. FindAuthorizations retrieves all authorizations that match the provided filter and then filters the list down to only the resources that are authorized. CreateAuthorization checks to see if the authorizer on context has write access to the global authorizations resource. UpdateAuthorization checks to see if the authorizer on context has write access to the authorization provided. DeleteAuthorization checks to see if the authorizer on context has write access to the authorization provided./Users/austinjaybecker/projects/abeck-go-testing/authorizer/authorize.gopermissionspsetoidridrtbid%s is unauthorized"%s is unauthorized"none of %v is authorized"none of %v is authorized" IsAllowedAll checks to see if an action is authorized by ALL permissions. Also see IsAllowed. IsAllowed checks to see if an action is authorized by retrieving the authorizer off of context and authorizing the action appropriately. AuthorizeReadBucket exists because buckets are a special case and should use this method. I.e., instead of:  AuthorizeRead(ctx, influxdb.BucketsResourceType, b.ID, b.OrgID) use:  AuthorizeReadBucket(ctx, b.Type, b.ID, b.OrgID) AuthorizeRead authorizes the user in the context to read the specified resource (identified by its type, ID, and orgID). NOTE: authorization will pass even if the user only has permissions for the resource type and organization ID only. AuthorizeWrite authorizes the user in the context to write the specified resource (identified by its type, ID, and orgID). AuthorizeRead authorizes the user in the context to read the specified resource (identified by its type, ID). NOTE: authorization will pass only if the user has a specific permission for the given resource. AuthorizeWrite authorizes the user in the context to write the specified resource (identified by its type, ID). AuthorizeOrgReadResource authorizes the given org to read the resources of the given type. NOTE: this is pretty much the same as AuthorizeRead, in the case that the resource ID is ignored. Use it in the case that you do not know which resource in particular you want to give access to. AuthorizeOrgWriteResource authorizes the given org to write the resources of the given type. NOTE: this is pretty much the same as AuthorizeWrite, in the case that the resource ID is ignored. AuthorizeCreate authorizes a user to create a resource of the given type for the given org. AuthorizeReadOrg authorizes the user to read the given org. AuthorizeWriteOrg authorizes the user to write the given org. AuthorizeReadGlobal authorizes to read resources of the given type. AuthorizeWriteGlobal authorizes to write resources of the given type./Users/austinjaybecker/projects/abeck-go-testing/authorizer/authorize_find.gorrsrsosdashboardssourcestaskstelegrafsvariablesscraperslabelsnotificationRulesnotificationEndpointschecks AuthorizeFindDBRPs takes the given items and returns only the ones that the user is authorized to access. This filters without allocating https://github.com/golang/go/wiki/SliceTricks#filtering-without-allocating N.B. we have to check both read and write permissions here to support the legacy write-path, which calls AuthorizeFindDBRPs when locating the bucket underlying a DBRP target. AuthorizeFindAuthorizations takes the given items and returns only the ones that the user is authorized to read. AuthorizeFindBuckets takes the given items and returns only the ones that the user is authorized to read. AuthorizeFindDashboards takes the given items and returns only the ones that the user is authorized to read. AuthorizeFindOrganizations takes the given items and returns only the ones that the user is authorized to read. AuthorizeFindSources takes the given items and returns only the ones that the user is authorized to read. AuthorizeFindTasks takes the given items and returns only the ones that the user is authorized to read. AuthorizeFindTelegrafs takes the given items and returns only the ones that the user is authorized to read. AuthorizeFindUsers takes the given items and returns only the ones that the user is authorized to read. AuthorizeFindVariables takes the given items and returns only the ones that the user is authorized to read. AuthorizeFindScrapers takes the given items and returns only the ones that the user is authorize to read. AuthorizeFindLabels takes the given items and returns only the ones that the user is authorized to read. AuthorizeFindNotificationRules takes the given items and returns only the ones that the user is authorized to read. AuthorizeFindNotificationEndpoints takes the given items and returns only the ones that the user is authorized to read. AuthorizeFindChecks takes the given items and returns only the ones that the user is authorized to read. AuthorizeFindUserResourceMappings takes the given items and returns only the ones that the user is authorized to read./Users/austinjaybecker/projects/abeck-go-testing/authorizer/backup.goshardIDsinceio"io"uint64 BackupService wraps a influxdb.BackupService and authorizes actions NewBackupService constructs an instance of an authorizing backup service./Users/austinjaybecker/projects/abeck-go-testing/authorizer/bucket.gobs BucketService wraps a influxdb.BucketService and authorizes actions NewBucketService constructs an instance of an authorizing bucket service. FindBucketByID checks to see if the authorizer on context has read access to the id provided. FindBucketByName returns a bucket by name for a particular organization. FindBucket retrieves the bucket and checks to see if the authorizer on context has read access to the bucket. FindBuckets retrieves all buckets that match the provided filter and then filters the list down to only the resources that are authorized. CreateBucket checks to see if the authorizer on context has write access to the global buckets resource. UpdateBucket checks to see if the authorizer on context has write access to the bucket provided. DeleteBucket checks to see if the authorizer on context has write access to the bucket provided./Users/austinjaybecker/projects/abeck-go-testing/authorizer/check.gochkchksurm CheckService wraps a influxdb.CheckService and authorizes actions NewCheckService constructs an instance of an authorizing check service. FindCheckByID checks to see if the authorizer on context has read access to the id provided. FindChecks retrieves all checks that match the provided filter and then filters the list down to only the resources that are authorized. FindCheck will return the check. CreateCheck checks to see if the authorizer on context has write access to the global check resource. UpdateCheck checks to see if the authorizer on context has write access to the check provided. PatchCheck checks to see if the authorizer on context has write access to the check provided. DeleteCheck checks to see if the authorizer on context has write access to the check provided./Users/austinjaybecker/projects/abeck-go-testing/authorizer/dashboard.godscellIDdashboardID DashboardService wraps a influxdb.DashboardService and authorizes actions NewDashboardService constructs an instance of an authorizing dashboard service. FindDashboardByID checks to see if the authorizer on context has read access to the id provided. FindDashboards retrieves all dashboards that match the provided filter and then filters the list down to only the resources that are authorized. CreateDashboard checks to see if the authorizer on context has write access to the global dashboards resource. UpdateDashboard checks to see if the authorizer on context has write access to the dashboard provided. DeleteDashboard checks to see if the authorizer on context has write access to the dashboard provided./Users/austinjaybecker/projects/abeck-go-testing/authorizer/document.godiddocumentscannot authorize document creation without any orgID"cannot authorize document creation without any orgID" NewDocumentService constructs an instance of an authorizing document service./Users/austinjaybecker/projects/abeck-go-testing/authorizer/label.gols LabelService wraps a influxdb.LabelService and authorizes actions NewLabelServiceWithOrg constructs an instance of an authorizing label service. Replaces NewLabelService. FindLabelByID checks to see if the authorizer on context has read access to the label id provided. FindLabels retrieves all labels that match the provided filter and then filters the list down to only the resources that are authorized. FindResourceLabels retrieves all labels belonging to the filtering resource if the authorizer on context has read access to it. Then it filters the list down to only the labels that are authorized. CreateLabel checks to see if the authorizer on context has write access to the new label's org. CreateLabelMapping checks to see if the authorizer on context has write access to the label and the resource contained by the label mapping in creation. UpdateLabel checks to see if the authorizer on context has write access to the label provided. DeleteLabel checks to see if the authorizer on context has write access to the label provided. DeleteLabelMapping checks to see if the authorizer on context has write access to the label and the resource of the label mapping to delete./Users/austinjaybecker/projects/abeck-go-testing/authorizer/notification_endpoint.goedpedpscannot process a request without a org or user filter"cannot process a request without a org or user filter" NotificationEndpointService wraps a influxdb.NotificationEndpointService and authorizes actions NewNotificationEndpointService constructs an instance of an authorizing notification endpoint service. FindNotificationEndpointByID checks to see if the authorizer on context has read access to the id provided. FindNotificationEndpoints retrieves all notification endpoints that match the provided filter and then filters the list down to only the resources that are authorized. TODO: This is a temporary fix as to not fetch the entire collection when no filter is provided. CreateNotificationEndpoint checks to see if the authorizer on context has write access to the global notification endpoint resource. UpdateNotificationEndpoint checks to see if the authorizer on context has write access to the notification endpoint provided. PatchNotificationEndpoint checks to see if the authorizer on context has write access to the notification endpoint provided. DeleteNotificationEndpoint checks to see if the authorizer on context has write access to the notification endpoint provided./Users/austinjaybecker/projects/abeck-go-testing/authorizer/notification_rule.gonrnrs NotificationRuleStore wraps a influxdb.NotificationRuleStore and authorizes actions NewNotificationRuleStore constructs an instance of an authorizing notification rule service. FindNotificationRuleByID checks to see if the authorizer on context has read access to the id provided. FindNotificationRules retrieves all notification rules that match the provided filter and then filters the list down to only the resources that are authorized. CreateNotificationRule checks to see if the authorizer on context has write access to the global notification rule resource. UpdateNotificationRule checks to see if the authorizer on context has write access to the notification rule provided. PatchNotificationRule checks to see if the authorizer on context has write access to the notification rule provided. DeleteNotificationRule checks to see if the authorizer on context has write access to the notification rule provided./Users/austinjaybecker/projects/abeck-go-testing/authorizer/org.gouserid OrgService wraps a influxdb.OrganizationService and authorizes actions NewOrgService constructs an instance of an authorizing org service. FindOrganizationByID checks to see if the authorizer on context has read access to the id provided. FindOrganization retrieves the organization and checks to see if the authorizer on context has read access to the org. FindOrganizations retrieves all organizations that match the provided filter and then filters the list down to only the resources that are authorized. if the user doesnt have permission to look up all orgs we need to add this users id to the filter to save lookup time CreateOrganization checks to see if the authorizer on context has write access to the global orgs resource. UpdateOrganization checks to see if the authorizer on context has write access to the organization provided. DeleteOrganization checks to see if the authorizer on context has write access to the organization provided./Users/austinjaybecker/projects/abeck-go-testing/authorizer/password.gosvcnewoldpanicnot implemented"not implemented" PasswordService is a new authorization middleware for a password service. NewPasswordService wraps an existing password service with auth middleware. SetPassword overrides the password of a known user. ComparePassword checks if the password matches the password recorded. Passwords that do not match return errors. CompareAndSetPassword checks the password and if they match updates to the new password./Users/austinjaybecker/projects/abeck-go-testing/authorizer/restore.godbishardIDMap RestoreService wraps a influxdb.RestoreService and authorizes actions NewRestoreService constructs an instance of an authorizing restore service./Users/austinjaybecker/projects/abeck-go-testing/authorizer/scraper.goss ScraperTargetStoreService wraps a influxdb.ScraperTargetStoreService and authorizes actions NewScraperTargetStoreService constructs an instance of an authorizing scraper target store service. GetTargetByID checks to see if the authorizer on context has read access to the id provided. ListTargets retrieves all scraper targets that match the provided filter and then filters the list down to only the resources that are authorized. AddTarget checks to see if the authorizer on context has write access to the global scraper target resource. UpdateTarget checks to see if the authorizer on context has write access to the scraper target provided. RemoveTarget checks to see if the authorizer on context has write access to the scraper target provided./Users/austinjaybecker/projects/abeck-go-testing/authorizer/secret.gokeyssecretssecret SecretService wraps a influxdb.SecretService and authorizes actions NewSecretService constructs an instance of an authorizing secret service. LoadSecret checks to see if the authorizer on context has read access to the secret key provided. GetSecretKeys checks to see if the authorizer on context has read access to all the secrets belonging to orgID. PutSecret checks to see if the authorizer on context has write access to the secret key provided. PutSecrets checks to see if the authorizer on context has read and write access to the secret keys provided. PutSecrets operates on intersection between m and keys beloging to orgID. We need to have read access to those secrets since it deletes the secrets (within the intersection) that have not be overridden. PatchSecrets checks to see if the authorizer on context has write access to the secret keys provided. DeleteSecret checks to see if the authorizer on context has write access to the secret keys provided./Users/austinjaybecker/projects/abeck-go-testing/authorizer/source.go SourceService wraps a influxdb.SourceService and authorizes actions NewSourceService constructs an instance of an authorizing source service. DefaultSource checks to see if the authorizer on context has read access to the default source. FindSourceByID checks to see if the authorizer on context has read access to the id provided. FindSources retrieves all sources that match the provided options and then filters the list down to only the resources that are authorized. TODO: we'll likely want to push this operation into the database since fetching the whole list of data will likely be expensive. CreateSource checks to see if the authorizer on context has write access to the global source resource. UpdateSource checks to see if the authorizer on context has write access to the source provided. DeleteSource checks to see if the authorizer on context has write access to the source provided./Users/austinjaybecker/projects/abeck-go-testing/authorizer/task.gologgerFieldstaskunauthenticatedTasksrunIDtaskIDscheduledForpermission failed for auth (%s): %s"permission failed for auth (%s): %s"inactive task"inactive task""unauthorized"Authorization failed"Authorization failed"user_id"user_id"auth_kind"auth_kind"auth_id"auth_id"disallowed_permission"disallowed_permission""method""FindTaskByID"Stringertask_id"task_id""CreateTask""UpdateTask""DeleteTask"-1"FindRuns""FindRunByID"run_id"run_id""CancelRun""RetryRun"int64"ForceRun" TaskService wraps ts and checks appropriate permissions before calling requested methods on ts. Authorization failures are logged to the logger. Unauthenticated task lookup, to identify the task's organization. Get the tasks in the organization, without authentication. Look up the task first, through the validator, to ensure we have permission to view the task. If we can find the task, we can read its logs. TODO(lyon): If the user no longer has permission to the organization we might fail or filter here?/Users/austinjaybecker/projects/abeck-go-testing/authorizer/telegraf.go TelegrafConfigService wraps a influxdb.TelegrafConfigStore and authorizes actions NewTelegrafConfigService constructs an instance of an authorizing telegraf service. FindTelegrafConfigByID checks to see if the authorizer on context has read access to the id provided. FindTelegrafConfigs retrieves all telegraf configs that match the provided filter and then filters the list down to only the resources that are authorized. CreateTelegrafConfig checks to see if the authorizer on context has write access to the global telegraf config resource. UpdateTelegrafConfig checks to see if the authorizer on context has write access to the telegraf config provided. DeleteTelegrafConfig checks to see if the authorizer on context has write access to the telegraf config provided./Users/austinjaybecker/projects/abeck-go-testing/authorizer/urm.gourmsresourceID/Users/austinjaybecker/projects/abeck-go-testing/authorizer/user.gouidus UserService wraps a influxdb.UserService and authorizes actions NewUserService constructs an instance of an authorizing user service. FindUserByID checks to see if the authorizer on context has read access to the id provided. FindUser retrieves the user and checks to see if the authorizer on context has read access to the user. FindUsers retrieves all users that match the provided filter and then filters the list down to only the resources that are authorized. CreateUser checks to see if the authorizer on context has write access to the global users resource. UpdateUser checks to see if the authorizer on context has write access to the user provided. DeleteUser checks to see if the authorizer on context has write access to the user provided./Users/austinjaybecker/projects/abeck-go-testing/authorizer/variable.govs VariableService wraps a influxdb.VariableService and authorizes actions NewVariableService constructs an instance of an authorizing variable service. FindVariableByID checks to see if the authorizer on context has read access to the id provided. FindVariables retrieves all variables that match the provided filter and then filters the list down to only the resources that are authorized. CreateVariable checks to see if the authorizer on context has write access to the global variable resource. UpdateVariable checks to see if the authorizer on context has write access to the variable provided. ReplaceVariable checks to see if the authorizer on context has write access to the variable provided. DeleteVariable checks to see if the authorizer on context has write access to the variable provided./Users/austinjaybecker/projects/abeck-go-testing/authz.gopermOrgIDpOrgIDpermIDpIDbucketIDfilepath"os"path/filepath"path/filepath"your authorizer is not supported, please use *platform.Authorization as authorizer"your authorizer is not supported, please use *platform.Authorization as authorizer"unknown resource type for permission"unknown resource type for permission"unknown action for permission"unknown action for permission"false"read""write"json:"type"`json:"type"`json:"id,omitempty"`json:"id,omitempty"`json:"orgID,omitempty"`json:"orgID,omitempty"`Join"authorizations""buckets""dashboards""orgs""sources""tasks""telegrafs""users""variables""scrapers""secrets""labels"views"views""documents""notificationRules""notificationEndpoints""checks"dbrp"dbrp"LookupEnvMATCHER_BEHAVIOR"MATCHER_BEHAVIOR"Printfv1: old match used: p.Resource.OrgID=%s perm.Resource.OrgID=%s p.Resource.ID=%s"v1: old match used: p.Resource.OrgID=%s perm.Resource.OrgID=%s p.Resource.ID=%s"v2: old match used: p.Resource.OrgID=%s perm.Resource.OrgID=%s p.Resource.ID=%s"v2: old match used: p.Resource.OrgID=%s perm.Resource.OrgID=%s p.Resource.ID=%s"%s:%s"%s:%s"invalid resource type for permission"invalid resource type for permission"invalid action type for permission"invalid action type for permission"invalid org id for permission"invalid org id for permission"invalid id for permission"invalid id for permission" ErrAuthorizerNotSupported notes that the provided authorizer is not supported for the action you are trying to perform. ErrInvalidResourceType notes that the provided resource is invalid ErrInvalidAction notes that the provided action is invalid Authorizer will authorize a permission. PermissionSet returns the PermissionSet associated with the authorizer ID returns an identifier used for auditing. Kind metadata for auditing. PermissionAllowed determines if a permission is allowed. Action is an enum defining all possible resource operations ReadAction is the action for reading. 1 WriteAction is the action for writing. 2 Valid checks if the action is a member of the Action enum ResourceType is an enum defining all resource types that have a permission model in platform Resource is an authorizable resource. String stringifies a resource AuthorizationsResourceType gives permissions to one or more authorizations. 0 BucketsResourceType gives permissions to one or more buckets. DashboardsResourceType gives permissions to one or more dashboards. OrgsResourceType gives permissions to one or more orgs. 3 SourcesResourceType gives permissions to one or more sources. 4 TasksResourceType gives permissions to one or more tasks. 5 TelegrafsResourceType type gives permissions to a one or more telegrafs. 6 UsersResourceType gives permissions to one or more users. 7 VariablesResourceType gives permission to one or more variables. 8 ScraperResourceType gives permission to one or more scrapers. 9 SecretsResourceType gives permission to one or more secrets. 10 LabelsResourceType gives permission to one or more labels. 11 ViewsResourceType gives permission to one or more views. 12 DocumentsResourceType gives permission to one or more documents. 13 NotificationRuleResourceType gives permission to one or more notificationRules. 14 NotificationEndpointResourceType gives permission to one or more notificationEndpoints. 15 ChecksResourceType gives permission to one or more Checks. 16 DBRPType gives permission to one or more DBRPs. 17 AllResourceTypes is the list of all known resource types. NOTE: when modifying this list, please update the swagger for components.schemas.Permission resource enum. OrgResourceTypes is the list of all known resource types that belong to an organization. Valid checks if the resource type is a member of the ResourceType enum. Permission defines an action and a resource. Matches returns whether or not one permission matches the other. Valid checks if there the resource and action provided is known. NewPermission returns a permission with provided arguments. NewResourcePermission returns a permission with provided arguments. NewGlobalPermission constructs a global permission capable of accessing any resource of type rt. NewPermissionAtID creates a permission with the provided arguments. OperPermissions are the default permissions for those who setup the application. ReadAllPermissions represents permission to read all data and metadata. Like OperPermissions, but allows read-only users. OwnerPermissions are the default permissions for those who own a resource. MePermissions is the permission to read/write myself. MemberPermissions are the default permissions for those who can see a resource./Users/austinjaybecker/projects/abeck-go-testing/backup.gorpiData20060102T150405Z"20060102T150405Z"json:"kv"`json:"kv"`json:"files"`json:"files"`json:"organizationID,omitempty"`json:"organizationID,omitempty"`json:"bucketID,omitempty"`json:"bucketID,omitempty"`json:"organizationID"`json:"organizationID"`json:"organizationName"`json:"organizationName"`json:"bucketID"`json:"bucketID"`json:"bucketName"`json:"bucketName"`json:"shardID"`json:"shardID"`json:"fileName"`json:"fileName"`json:"size"`json:"size"`json:"lastModified"`json:"lastModified"` BackupService represents the data backup functions of InfluxDB. BackupKVStore creates a live backup copy of the metadata database. BackupShard downloads a backup file for a single shard. RestoreService represents the data restore functions of InfluxDB. RestoreKVStore restores & replaces metadata database. RestoreKVStore restores the metadata database. RestoreShard uploads a backup file for a single shard. Manifest lists the KV and shard file information contained in the backup. These fields are only set if filtering options are set on the CLI. ManifestEntry contains the data information for a backed up shard. ManifestKVEntry contains the KV store information for a backup. Size returns the size of the manifest./Users/austinjaybecker/projects/abeck-go-testing/bolt/Users/austinjaybecker/projects/abeck-go-testing/bolt/bbolt.goDefaultFilenameKVOptionKVStoreNewClientNewKVStoreWithNoSyncauthorizationBucketboltReadsDescboltWritesDescbucketBucketbucketsDescdashboardBucketdashboardsDescdecodeIDerrIDNotFoundidsBucketinstaTickerorganizationBucketorgsDescscraperBucketscrapersDesctelegrafBuckettelegrafPluginstelegrafPluginsBuckettelegrafPluginsDesctelegrafsDesctickertokensDescuserBucketusersDescDBFreelistTypeFileModeIsDirIsRegularfileFDfdMutexrsemawsemaincrefmuincrefAndClosedecrefrwlockrwunlockpollDescruntimeCtxpdcloseevictprepareprepareReadprepareWritewaitwaitReadwaitWritewaitCanceledpollableIoveciovfdmuSysfdiovecscsemaisBlockingIsStreamZeroReadIsEOFisFileFsyncfdreadLockreadUnlockwriteLockwriteUnlockOpenDireofErrorShutdownFchownFtruncateRawControldestroySetBlockingPreadReadFromReadFromInet4ReadFromInet6ReadMsgReadMsgInet4ReadMsgInet6PwriteWriteToInet4WriteToInet6WriteMsgWriteMsgInet4WriteMsgInet6ReadDirentFchmodFchdirFstatDupWaitWriteWriteOnceRawReadRawWriteSetsockoptIntSetsockoptInet4AddrSetsockoptLingerSetsockoptByteSetsockoptIPMreqSetsockoptIPv6MreqWritevdirInfodirpfddirinfononblockstdoutOrErrappendModeReaddirReaddirnamesReadDirreaddirWriteAtWriteStringwrapErrChmodSyscallConnpreadpwritechmodChownChdirsetDeadlinesetReadDeadlinesetWriteDeadlinecheckValidFdseekreadFromStat281474976710655metapgidrootsequencetxidmagicversionpageSizeflagsfreelistchecksumvalidatecopysum64pageleafPageElementleafPageElementsbranchPageElementbranchPageElementshexdumpinodesinodeunbalancedspilledminKeyssizeLessThanpageElementSizechildAtchildIndexnumChildrennextSiblingprevSiblingdelsplitsplitTwosplitIndexspillrebalanceremoveChilddereferencefreerootNodeFillPercentRootWritableopenBucketCreateBucketIfNotExistsSequenceSetSequenceNextSequenceForEachStatsforEachPageforEachPageNode_forEachPageNodeinlineablemaxInlineBucketSizepageNodeTxStatsPageCountPageAllocCursorCountNodeCountNodeDerefRebalanceRebalanceTimeSpillSpillTimeWriteTimewritablemanageddbpagesstatscommitHandlersWriteFlagOnCommitcommitFreelistRollbacknonPhysicalRollbackrollbackCopyFileallocatewriteMetaPageFreePageNPendingPageNFreeAllocFreelistInuseTxNOpenTxNtxPendingidsalloctxlastReleaseBeginpidSetpgidsmergefreelistTypeallocspendingcachefreemapsforwardMapbackwardMapfree_countmergeSpansgetFreePageIDsreadIDsarrayFreeCountpending_countcopyallarrayAllocatereleasereleaseRangefreedarrayReadIDsarrayGetFreePageIDsreloadnoSyncReloadreindexarrayMergeSpanshashmapFreeCounthashmapAllocatehashmapReadIDshashmapGetFreePageIDshashmapMergeSpansmergeWithExistingSpanaddSpandelSpanOncedonedoSlowbatchTimerruntimeTimerppperiodseqnextwhenstatusCtimercallstriggerrunwriteAtStrictModeNoSyncNoFreelistSyncNoGrowSyncMmapFlagsMaxBatchSizeMaxBatchDelayAllocSizepathopenFiledatarefdataszfileszmeta0meta1openedrwtxtxsfreelistLoadpagePoolbatchMumetalockmmaplockstatlockopsreadOnlyloadFreelisthasSyncedFreelistmmapmunmapmmapSizeBeginbeginTxbeginRWTxfreePagesremoveTxBatchpageInBufferIsReadOnlyfreepagesinitializeinitializeIDgetIDgenerateIDbktNamebktsboltplatformgo.etcd.io/bbolt"go.etcd.io/bbolt"influxd.bolt"influxd.bolt"IDGeneratorOpNewIDGeneratorMkdirAllDir4480700unable to create directory %s: %v"unable to create directory %s: %v"FileInfoModTimeModeSysIsNotExistTimeoutReadOnlyInitialMmapSizePageSizeOpenFile38406001000000000unable to open boltdb; is there a chronograf already running?  %v"unable to open boltdb; is there a chronograf already running?  %v"Resources opened"Resources opened""path" Client is a client for the boltDB data store. NewClient returns an instance of a Client. DB returns the clients DB. Open / create boltDB file. Ensure the required directory structure exists. Open database file. initialize creates Buckets that are missing Always create ID bucket. TODO: is this still needed? TODO: make card to normalize everything under kv? Close the connection to the bolt databaseStat_tTimespecSecNsecNanoDevNlinkInoUidGidRdevPad_cgo_0AtimespecMtimespecCtimespecBirthtimespecBlocksBlksizeFlagsGenLspareQspareDirEntryreaddirModeSockaddrInet4RawSockaddrInet48FamilyZerorawsaksizevsizeDataSockaddrInet616RawSockaddrInet6FlowinfoScope_idZoneIdelemRefstackfirstlastsearchsearchNodesearchPagensearchkeyValueRawConnControlLingerOnoffBucketStatsBranchPageNBranchOverflowNLeafPageNLeafOverflowNKeyNDepthBranchAllocBranchInuseLeafAllocLeafInuseBucketNInlineBucketNInlineBucketInuseIPMreqMultiaddrticktimeChnoSynctempPathopenDBFlushcleanBucketWithDBPageInfoCountOverflowCountcursorconfigclosedseenmissingPrefixatLimitIPv6Mreq/Users/austinjaybecker/projects/abeck-go-testing/bolt/id.gomath/rand"math/rand"idsv1"idsv1"source not found"source not found"Unable to load id"Unable to load id"provided value is too short to contain an ID. Please report this error"provided value is too short to contain an ID. Please report this error" ID retrieves the unique ID for this influx instance. if any error occurs return a random number This should not happen./Users/austinjaybecker/projects/abeck-go-testing/bolt/kv.gobktfssync"bytes""sync"github.com/influxdata/influxdb/v2/pkg/fs"github.com/influxdata/influxdb/v2/pkg/fs"SchemaStore.tmp".tmp"Removeunable to remove boltdb partial restore file: %w"unable to remove boltdb partial restore file: %w"unable to open boltdb file %v"unable to open boltdb file %v"IsErrBucketNotFoundRenameFileWithReplacementbucket %q: %w"bucket %q: %w"ErrKeyNotFoundErrTxNotWritableNewCursorConfigCursorDescendingHasPrefixseek bytes %q not prefixed with %q: %w"seek bytes %q not prefixed with %q: %w"ErrSeekMissingPrefix check that *KVStore implement kv.SchemaStore interface. KVStore is a kv.Store backed by boltdb. WithNoSync WARNING: this is useful for tests only this skips fsyncing on every commit to improve write performance in exchange for no guarantees that the db will persist. NewKVStore returns an instance of KVStore with the file at the provided path. tempPath returns the path to the temporary file used by Restore(). Open creates boltDB file it doesn't exists and opens it otherwise. Remove any temporary file created during a failed restore. DB returns a reference to the current Bolt database. Flush removes all bolt keys within each bucket. nested bucket recursion base case: clean out nexted buckets WithDB sets the boltdb on the store. View opens up a view transaction against the store. Update opens up an update transaction against the store. CreateBucket creates a bucket in the underlying boltdb store if it does not already exist DeleteBucket creates a bucket in the underlying boltdb store if it Backup copies all K:Vs to a writer, in BoltDB format. Restore replaces the underlying database with the data from r. Swap and reopen under lock. Atomically swap temporary file with current DB file. Reopen with new database file. clean up on error Tx is a light wrapper around a boltdb transaction. It implements kv.Tx. Context returns the context for the transaction. WithContext sets the context for the transaction. Bucket retrieves the bucket named b. Bucket implements kv.Bucket. Get retrieves the value at the provided key. GetBatch retrieves the values for the provided keys. Put sets the value at the provided key. Delete removes the provided key. ForwardCursor retrieves a cursor for iterating through the entries in the key value store in a given direction (ascending / descending). only remember first seeked item if not skipped Cursor retrieves a cursor for iterating through the entries in the key value store. Cursor is a struct for iterating through the entries previously seeked key/value Close sets the closed to closed Seek seeks for the first key that matches the prefix provided. First retrieves the first key value pair in the bucket. Last retrieves the last key value pair in the bucket. Next retrieves the next key in the bucket. get and unset previously seeked values if they exist Prev retrieves the previous key in the bucket. Err always returns nil as nothing can go wrongâ„¢ during iteration/Users/austinjaybecker/projects/abeck-go-testing/bolt/metrics.gopStatsrawPluginschreadstokenswritesbucketsv1"bucketsv1"dashboardsv2"dashboardsv2"organizationsv1"organizationsv1"scraperv2"scraperv2"telegrafv1"telegrafv1"telegrafPluginsv1"telegrafPluginsv1"usersv1"usersv1"NewDescinfluxdb_organizations_total"influxdb_organizations_total"Number of total organizations on the server"Number of total organizations on the server"influxdb_buckets_total"influxdb_buckets_total"Number of total buckets on the server"Number of total buckets on the server"influxdb_users_total"influxdb_users_total"Number of total users on the server"Number of total users on the server"influxdb_tokens_total"influxdb_tokens_total"Number of total tokens on the server"Number of total tokens on the server"influxdb_dashboards_total"influxdb_dashboards_total"Number of total dashboards on the server"Number of total dashboards on the server"influxdb_scrapers_total"influxdb_scrapers_total"Number of total scrapers on the server"Number of total scrapers on the server"influxdb_telegrafs_total"influxdb_telegrafs_total"Number of total telegraf configurations on the server"Number of total telegraf configurations on the server"influxdb_telegraf_plugins_count"influxdb_telegraf_plugins_count"Number of individual telegraf plugins configured"Number of individual telegraf plugins configured"plugin"plugin"boltdb_writes_total"boltdb_writes_total"Total number of boltdb writes"Total number of boltdb writes"boltdb_reads_total"boltdb_reads_total"Total number of boltdb reads"Total number of boltdb reads"float64TickerNewTicker60000000000593540000000000MustNewConstMetricCounterValueGaugeValue available buckets TODO: nuke this whole thing? Describe returns all descriptions of the collector. ticker is this influx' timer for when to renew the cache of configured plugin metrics. telegrafPlugins is a cache of this influx' metrics of configured plugins. Initialize a simple channel that will instantly "tick", backed by a time.Ticker's channel. Collect returns the current state of all metrics of the collector. Only process and store telegraf configs once per hour. Clear plugins from last check. Loop through all reported number of plugins in the least intrusive way (vs a global map and locking every time a config is updated). Adds a label for plugin type.name./Users/austinjaybecker/projects/abeck-go-testing/bucket.gootherpartsstrings"strings"3600000000000248640000000000076048000000000003259200000000000_tasks"_tasks"_monitoring"_monitoring"json:"name"`json:"name"`json:"rp,omitempty"`json:"rp,omitempty"`json:"retentionPeriod"`json:"retentionPeriod"`system"system""FindBucketByID""FindBucket""FindBuckets""CreateBucket"PutBucket"PutBucket""UpdateBucket""DeleteBucket"json:"retentionPeriod,omitempty"`json:"retentionPeriod,omitempty"`"bucket"Bucket ID: "Bucket ID: "Bucket Name: "Bucket Name: "Org ID: "Org ID: "Org Name: "Org Name: "["[", ", "]"]"unexpected error in buckets; Err: %v"unexpected error in buckets; Err: %v" BucketTypeUser is a user created bucket BucketTypeSystem is an internally created bucket that cannot be deleted/renamed. MonitoringSystemBucketRetention is the time we should retain monitoring system bucket information TasksSystemBucketRetention is the time we should retain task system bucket information Bucket names constants InfiniteRetention is default infinite retention period. Bucket is a bucket. ðŸŽ‰ This to support v1 sources Clone returns a shallow copy of b. BucketType differentiates system buckets from user buckets. String converts a BucketType into a human-readable string. ParseBucketType parses a bucket type from a string ops for buckets error and buckets op logs. BucketService represents a service for managing bucket data. FindBucketByID returns a single bucket by ID. FindBucket returns the first bucket that matches filter. FindBuckets returns a list of buckets that match filter and the total count of matching buckets. CreateBucket creates a new bucket and sets b.ID with the new identifier. UpdateBucket updates a single bucket with changeset. Returns the new bucket state after update. DeleteBucket removes a bucket by ID. BucketUpdate represents updates to a bucket. Only fields which are set are updated. BucketFilter represents a set of filter that restrict the returned results. QueryParams Converts BucketFilter fields to url query params. String returns a human-readable string of the BucketFilter, particularly useful for error messages. There should always be exactly 2 fields set, but if it's somehow more, that's fine./Users/austinjaybecker/projects/abeck-go-testing/build.gocommit BuildInfo represents the information about InfluxDB build. Version is the current git tag with v prefix stripped Commit is the current git commit SHA Date is the build date in RFC3339 SetBuildInfo sets the build information for the binary. GetBuildInfo returns the current build information for the binary./Users/austinjaybecker/projects/abeck-go-testing/check.golangdescription500"FindCheckByID""FindCheck""FindChecks""CreateCheck""UpdateCheck""DeleteCheck"Check Name can't be empty"Check Name can't be empty"Check Description can't be empty"Check Description can't be empty""name" consts for checks config. Check represents the information required to generate a periodic check task. ops for checks error CheckService represents a service for managing checks. FindCheckByID returns a single check by ID. FindCheck returns the first check that matches filter. FindChecks returns a list of checks that match filter and the total count of matching checks. CreateCheck creates a new check and sets b.ID with the new identifier. UpdateCheck updates the whole check. Returns the new check state after update. PatchCheck updates a single bucket with changeset. DeleteCheck will delete the check by id. CheckUpdate are properties than can be updated on a check CheckCreate represent data to create a new Check Valid returns err is the update is invalid. CheckFilter represents a set of filters that restrict the returned results. QueryParams Converts CheckFilter fields to url query params./Users/austinjaybecker/projects/abeck-go-testing/checks/Users/austinjaybecker/projects/abeck-go-testing/checks/service.gofilterChecksFnnewCheckStorederrIndexStoreStoreBaseEncodeEntFnEntityEncodeFnPKUniqueKeyDecodeBucketValFnConvertValToEntFnBktNameEncodeEntKeyFnEncodeEntBodyFnDecodeEntFnEntKeyDeleteEntFindEntputValidatebucketCursorbucketDeletebucketGetbucketPutdecodeEntencodeEntstartSpanEntStorefindByIndexvalidNewvalidUpdatetimeGeneratoridGeneratorcheckStorefindCheckByIDfindCheckByNamecreateCheckTaskPutCheckputCheckupdateCheckTaskpatchCheckTaskupdateCheckpatchCheckentdecodedValchkValdecEndpointEntFndecValToEntFnchValscriptfluxtuc0currentfluxlanggithub.com/influxdata/influxdb/v2/notification/check"github.com/influxdata/influxdb/v2/notification/check"github.com/influxdata/influxdb/v2/query/fluxlang"github.com/influxdata/influxdb/v2/query/fluxlang""check"IsErrUnexpectedDecodeValEncIDEncStringNewStoreBasechecksv1"checksv1"EncIDKeyEncBodyJSONNewOrgNameKeyStorecheckindexv1"checkindexv1"FindOptsFindCaptureFnFilterFnCaptureFnFilterEntFncheck not found"check not found"DefaultServiceCould not create task from check"Could not create task from check"PutOptionFnputOptionisNewisUpdatePutNewerror removing task %q for check %q in org %q"error removing task %q for check %q in org %q"check name is not unique"check name is not unique"PutUpdate Service is a check service It provides all the operations needed to manage checks NewService constructs and configures a new checks.Service FindCheckByID retrieves a check by id. FindCheck retrieves a check using an arbitrary check filter. Filters using ID, or OrganizationID and check Name should be efficient. Other filters will do a linear scan across checks until it finds a match. FindChecks retrieves all checks that match an arbitrary check filter. Other filters will do a linear scan across all checks searching for a match. CreateCheck creates a influxdb check and sets ID. create task initially in inactive state something went wrong persisting new check so remove associated task update task to be in matching state to check task initially in inactive state to ensure it isn't scheduled until check is persisted and active PutCheck will put a check without setting an ID. PatchCheck updates a check according the parameters set on upd. UpdateCheck updates the check. ID and OrganizationID can not be updated DeleteCheck deletes a check and prunes it from the index.DeleteOptsDeleteRelationsFnDeleteRelationFns/Users/austinjaybecker/projects/abeck-go-testing/chronograf/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/base.goAlertRuleAuthConfigBuildBucketBuildKeyBuildStoreColorConfigBucketConfigStoreDashboardCellDashboardsBucketDashboardsStoreDefaultOrganizationIDDefaultOrganizationNameDefaultOrganizationRoleIsMigrationCompleteLayoutLayoutsBucketLayoutsStoreMappingMappingsBucketMappingsStoreMarkMigrationAsCompleteMigrateAllMigrationOrganizationConfigBucketOrganizationConfigStoreOrganizationsBucketOrganizationsStoreSchemaVersionBucketServerServersBucketServersStoreSourcesBucketSourcesStoreTemplateTemplateQueryTemplateValueTimeShiftUsersBucketUsersStoreWithBackupchangeIntervalToDurationconfigIDdownfileDescriptorInternalitobmigrationsnewOrganizationConfigu64tobupupdateDashboardmigrationcompleteUpDownMigrateSchemaVersions"SchemaVersions"RFC33392006-01-02T15:04:05Z07:00PipeWriterpipeonceErrorwrMuwrChrdChoncererrwerrcloseReadcloseWritereadCloseErrorwriteCloseErrorCloseWithErrorWithFieldGenerateupdateAllalldeleteresetDefaultSourcesetRandomDefaultresetActiveServerAddIDseachNumuserExistsCreateDefaultnameIsUniqueDefaultOrganizationInitializeFindOrCreateLayoutIDsmigratebackupRunning migration "Running migration " SchemaVersionBucket stores ids of completed migrations IsMigrationComplete checks for the presence of a particular migration id MarkMigrationAsComplete adds the migration id to the schema bucket Migration defines a database state/schema transition  ID: 	After the migration is run, this id is stored in the database.      	We don't want to run a state transition twice  Up: 	The forward-transition function. After a version upgrade, a number 				of these will run on database startup in order to bring a user's 				schema in line with struct definitions in the new version.  Down: The backward-transition function. We don't expect these to be 				run on a user's database -- if the user needs to rollback 				to a previous version, it will be easier for them to replace 				their current database with one of their backups. The primary 				purpose of a Down() function is to help contributors move across 				development branches that have different schema definitions. Migrate runs one migration's Up() function, if it has not already been run MigrateAll iterates through all known migrations and runs them in orderSuperAdminNewUsersGetSuperAdminNewUsersGetLegacyBoundsGetBoundsGetPrefixGetSuffixGetBaseGetScaleUpperLowerGetUpperGetLowerQuantityGetUnitGetQuantityCommandRPGroupBysWheresShiftsGetCommandGetDBGetRPGetGroupBysGetWheresGetRangeGetSourceGetShiftsGetHexGetOrientationGetInternalNameGetDisplayNameGetVisibleFieldNamesGetVerticalTimeAxisGetSortByGetWrappingGetFieldNamesGetFixFirstColumnColorsGetXGetYGetWGetHGetQueriesGetAxesGetColorsGetLegendGetTableOptionsGetSelectedDbRpMeasurementTagKeyFieldKeyGetDbGetRpGetMeasurementGetTagKeyGetFieldKeyTempVarGetTempVarGetValuesGetQueryTemplatesGetCellsGetTemplatesGetOrganizationAllowancesUsersPasswdRolesProviderSuperAdminDefaultRoleIYrangesYlabelsGetIGetYrangesGetYlabelsDashboardIDQueryConfigArgsGroupByDurationRangeAreTagsAcceptedFillRawTextCellColorCellColorsNoteVisibilityTemplateVarVarTemplateIDGetVersionGetCommitProviderOrganizationEpochApplicationAutoflowSrcIDGetApplicationGetAutoflowGetProviderGetSchemeGetProviderOrganizationGetUsernameGetPasswordGetURLGetSrcIDGetActiveGetInsecureSkipVerifyGetDefaultGetTelegrafGetMetaURLGetSharedSecretGetRoleOrganizationConfigLogViewerConfigColumnEncodingEncodingsLogViewerGetDefaultRoleGetRolesGetSuperAdminOrganizationQueryJSONKapaIDGetJSONGetKapaIDUserQueryGetAuth/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/build.gobuilddefaultBuildchronografinternalgithub.com/influxdata/influxdb/v2/chronograf"github.com/influxdata/influxdb/v2/chronograf"github.com/influxdata/influxdb/v2/chronograf/bolt/internal"github.com/influxdata/influxdb/v2/chronograf/bolt/internal"Build"Build""build"pre-1.4.0.0"pre-1.4.0.0"UnmarshalBuildMarshalBuild Ensure BuildStore struct implements chronograf.BuildStore interface BuildBucket is the bolt bucket used to store Chronograf build information BuildKey is the constant key used in the bolt bucket BuildStore is a bolt implementation to store Chronograf build information Get retrieves Chronograf build information from the database Update overwrites the current Chronograf build information in the database Migrate simply stores the current version in the database get retrieves the current build, falling back to a default when missing/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/change_interval_to_duration.goboardquerycellproto"log"github.com/gogo/protobuf/proto"github.com/gogo/protobuf/proto"59b0cda4fc7909ff84ee5c4f9cb4b655b6a26620"59b0cda4fc7909ff84ee5c4f9cb4b655b6a26620"Replace:interval:":interval:"time(:interval:)"time(:interval:)"unmarshalling error: "unmarshalling error: "marshaling error: "marshaling error: "error updating dashboard: "error updating dashboard: "error updating dashboards: "error updating dashboards: "Dashoard"Dashoard"protobuf:"varint,1,opt,name=ID,proto3" json:"ID,omitempty"`protobuf:"varint,1,opt,name=ID,proto3" json:"ID,omitempty"`protobuf:"bytes,2,opt,name=Name,proto3" json:"Name,omitempty"`protobuf:"bytes,2,opt,name=Name,proto3" json:"Name,omitempty"`protobuf:"bytes,3,opt,name=Type,proto3" json:"Type,omitempty"`protobuf:"bytes,3,opt,name=Type,proto3" json:"Type,omitempty"`protobuf:"bytes,4,opt,name=Username,proto3" json:"Username,omitempty"`protobuf:"bytes,4,opt,name=Username,proto3" json:"Username,omitempty"`protobuf:"bytes,5,opt,name=Password,proto3" json:"Password,omitempty"`protobuf:"bytes,5,opt,name=Password,proto3" json:"Password,omitempty"`protobuf:"bytes,6,opt,name=URL,proto3" json:"URL,omitempty"`protobuf:"bytes,6,opt,name=URL,proto3" json:"URL,omitempty"`protobuf:"varint,7,opt,name=Default,proto3" json:"Default,omitempty"`protobuf:"varint,7,opt,name=Default,proto3" json:"Default,omitempty"`protobuf:"bytes,8,opt,name=Telegraf,proto3" json:"Telegraf,omitempty"`protobuf:"bytes,8,opt,name=Telegraf,proto3" json:"Telegraf,omitempty"`protobuf:"varint,9,opt,name=InsecureSkipVerify,proto3" json:"InsecureSkipVerify,omitempty"`protobuf:"varint,9,opt,name=InsecureSkipVerify,proto3" json:"InsecureSkipVerify,omitempty"`protobuf:"bytes,10,opt,name=MetaURL,proto3" json:"MetaURL,omitempty"`protobuf:"bytes,10,opt,name=MetaURL,proto3" json:"MetaURL,omitempty"`protobuf:"bytes,11,opt,name=SharedSecret,proto3" json:"SharedSecret,omitempty"`protobuf:"bytes,11,opt,name=SharedSecret,proto3" json:"SharedSecret,omitempty"`protobuf:"bytes,12,opt,name=Organization,proto3" json:"Organization,omitempty"`protobuf:"bytes,12,opt,name=Organization,proto3" json:"Organization,omitempty"`protobuf:"bytes,13,opt,name=Role,proto3" json:"Role,omitempty"`protobuf:"bytes,13,opt,name=Role,proto3" json:"Role,omitempty"`CompactTextStringprotobuf:"bytes,3,rep,name=cells" json:"cells,omitempty"`protobuf:"bytes,3,rep,name=cells" json:"cells,omitempty"`protobuf:"bytes,4,rep,name=templates" json:"templates,omitempty"`protobuf:"bytes,4,rep,name=templates" json:"templates,omitempty"`protobuf:"bytes,5,opt,name=Organization,proto3" json:"Organization,omitempty"`protobuf:"bytes,5,opt,name=Organization,proto3" json:"Organization,omitempty"`int32protobuf:"varint,1,opt,name=x,proto3" json:"x,omitempty"`protobuf:"varint,1,opt,name=x,proto3" json:"x,omitempty"`protobuf:"varint,2,opt,name=y,proto3" json:"y,omitempty"`protobuf:"varint,2,opt,name=y,proto3" json:"y,omitempty"`protobuf:"varint,3,opt,name=w,proto3" json:"w,omitempty"`protobuf:"varint,3,opt,name=w,proto3" json:"w,omitempty"`protobuf:"varint,4,opt,name=h,proto3" json:"h,omitempty"`protobuf:"varint,4,opt,name=h,proto3" json:"h,omitempty"`protobuf:"bytes,5,rep,name=queries" json:"queries,omitempty"`protobuf:"bytes,5,rep,name=queries" json:"queries,omitempty"`protobuf:"bytes,6,opt,name=name,proto3" json:"name,omitempty"`protobuf:"bytes,6,opt,name=name,proto3" json:"name,omitempty"`protobuf:"bytes,7,opt,name=type,proto3" json:"type,omitempty"`protobuf:"bytes,7,opt,name=type,proto3" json:"type,omitempty"`protobuf:"bytes,8,opt,name=ID,proto3" json:"ID,omitempty"`protobuf:"bytes,8,opt,name=ID,proto3" json:"ID,omitempty"`protobuf:"bytes,9,rep,name=axes" json:"axes,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value"`protobuf:"bytes,9,rep,name=axes" json:"axes,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value"`protobuf:"bytes,10,rep,name=colors" json:"colors,omitempty"`protobuf:"bytes,10,rep,name=colors" json:"colors,omitempty"`protobuf:"bytes,11,opt,name=legend" json:"legend,omitempty"`protobuf:"bytes,11,opt,name=legend" json:"legend,omitempty"`protobuf:"bytes,12,opt,name=tableOptions" json:"tableOptions,omitempty"`protobuf:"bytes,12,opt,name=tableOptions" json:"tableOptions,omitempty"`protobuf:"varint,2,opt,name=verticalTimeAxis,proto3" json:"verticalTimeAxis,omitempty"`protobuf:"varint,2,opt,name=verticalTimeAxis,proto3" json:"verticalTimeAxis,omitempty"`protobuf:"bytes,3,opt,name=sortBy" json:"sortBy,omitempty"`protobuf:"bytes,3,opt,name=sortBy" json:"sortBy,omitempty"`protobuf:"bytes,4,opt,name=wrapping,proto3" json:"wrapping,omitempty"`protobuf:"bytes,4,opt,name=wrapping,proto3" json:"wrapping,omitempty"`protobuf:"bytes,5,rep,name=fieldNames" json:"fieldNames,omitempty"`protobuf:"bytes,5,rep,name=fieldNames" json:"fieldNames,omitempty"`protobuf:"varint,6,opt,name=fixFirstColumn,proto3" json:"fixFirstColumn,omitempty"`protobuf:"varint,6,opt,name=fixFirstColumn,proto3" json:"fixFirstColumn,omitempty"`protobuf:"bytes,1,opt,name=internalName,proto3" json:"internalName,omitempty"`protobuf:"bytes,1,opt,name=internalName,proto3" json:"internalName,omitempty"`protobuf:"bytes,2,opt,name=displayName,proto3" json:"displayName,omitempty"`protobuf:"bytes,2,opt,name=displayName,proto3" json:"displayName,omitempty"`protobuf:"varint,3,opt,name=visible,proto3" json:"visible,omitempty"`protobuf:"varint,3,opt,name=visible,proto3" json:"visible,omitempty"`protobuf:"bytes,1,opt,name=ID,proto3" json:"ID,omitempty"`protobuf:"bytes,1,opt,name=ID,proto3" json:"ID,omitempty"`protobuf:"bytes,2,opt,name=Type,proto3" json:"Type,omitempty"`protobuf:"bytes,2,opt,name=Type,proto3" json:"Type,omitempty"`protobuf:"bytes,3,opt,name=Hex,proto3" json:"Hex,omitempty"`protobuf:"bytes,3,opt,name=Hex,proto3" json:"Hex,omitempty"`protobuf:"bytes,4,opt,name=Name,proto3" json:"Name,omitempty"`protobuf:"bytes,4,opt,name=Name,proto3" json:"Name,omitempty"`protobuf:"bytes,5,opt,name=Value,proto3" json:"Value,omitempty"`protobuf:"bytes,5,opt,name=Value,proto3" json:"Value,omitempty"`5protobuf:"bytes,1,opt,name=Type,proto3" json:"Type,omitempty"`protobuf:"bytes,1,opt,name=Type,proto3" json:"Type,omitempty"`protobuf:"bytes,2,opt,name=Orientation,proto3" json:"Orientation,omitempty"`protobuf:"bytes,2,opt,name=Orientation,proto3" json:"Orientation,omitempty"`6protobuf:"varint,1,rep,packed,name=legacyBounds" json:"legacyBounds,omitempty"`protobuf:"varint,1,rep,packed,name=legacyBounds" json:"legacyBounds,omitempty"`protobuf:"bytes,2,rep,name=bounds" json:"bounds,omitempty"`protobuf:"bytes,2,rep,name=bounds" json:"bounds,omitempty"`protobuf:"bytes,3,opt,name=label,proto3" json:"label,omitempty"`protobuf:"bytes,3,opt,name=label,proto3" json:"label,omitempty"`protobuf:"bytes,4,opt,name=prefix,proto3" json:"prefix,omitempty"`protobuf:"bytes,4,opt,name=prefix,proto3" json:"prefix,omitempty"`protobuf:"bytes,5,opt,name=suffix,proto3" json:"suffix,omitempty"`protobuf:"bytes,5,opt,name=suffix,proto3" json:"suffix,omitempty"`protobuf:"bytes,6,opt,name=base,proto3" json:"base,omitempty"`protobuf:"bytes,6,opt,name=base,proto3" json:"base,omitempty"`protobuf:"bytes,7,opt,name=scale,proto3" json:"scale,omitempty"`protobuf:"bytes,7,opt,name=scale,proto3" json:"scale,omitempty"`protobuf:"bytes,2,opt,name=temp_var,json=tempVar,proto3" json:"temp_var,omitempty"`protobuf:"bytes,2,opt,name=temp_var,json=tempVar,proto3" json:"temp_var,omitempty"`protobuf:"bytes,3,rep,name=values" json:"values,omitempty"`protobuf:"bytes,3,rep,name=values" json:"values,omitempty"`protobuf:"bytes,4,opt,name=type,proto3" json:"type,omitempty"`protobuf:"bytes,4,opt,name=type,proto3" json:"type,omitempty"`protobuf:"bytes,5,opt,name=label,proto3" json:"label,omitempty"`protobuf:"bytes,5,opt,name=label,proto3" json:"label,omitempty"`protobuf:"bytes,6,opt,name=query" json:"query,omitempty"`protobuf:"bytes,6,opt,name=query" json:"query,omitempty"`protobuf:"bytes,1,opt,name=type,proto3" json:"type,omitempty"`protobuf:"bytes,1,opt,name=type,proto3" json:"type,omitempty"`protobuf:"bytes,2,opt,name=value,proto3" json:"value,omitempty"`protobuf:"bytes,2,opt,name=value,proto3" json:"value,omitempty"`protobuf:"varint,3,opt,name=selected,proto3" json:"selected,omitempty"`protobuf:"varint,3,opt,name=selected,proto3" json:"selected,omitempty"`9protobuf:"bytes,1,opt,name=command,proto3" json:"command,omitempty"`protobuf:"bytes,1,opt,name=command,proto3" json:"command,omitempty"`protobuf:"bytes,2,opt,name=db,proto3" json:"db,omitempty"`protobuf:"bytes,2,opt,name=db,proto3" json:"db,omitempty"`protobuf:"bytes,3,opt,name=rp,proto3" json:"rp,omitempty"`protobuf:"bytes,3,opt,name=rp,proto3" json:"rp,omitempty"`protobuf:"bytes,4,opt,name=measurement,proto3" json:"measurement,omitempty"`protobuf:"bytes,4,opt,name=measurement,proto3" json:"measurement,omitempty"`protobuf:"bytes,5,opt,name=tag_key,json=tagKey,proto3" json:"tag_key,omitempty"`protobuf:"bytes,5,opt,name=tag_key,json=tagKey,proto3" json:"tag_key,omitempty"`protobuf:"bytes,6,opt,name=field_key,json=fieldKey,proto3" json:"field_key,omitempty"`protobuf:"bytes,6,opt,name=field_key,json=fieldKey,proto3" json:"field_key,omitempty"`10protobuf:"bytes,3,opt,name=Username,proto3" json:"Username,omitempty"`protobuf:"bytes,3,opt,name=Username,proto3" json:"Username,omitempty"`protobuf:"bytes,4,opt,name=Password,proto3" json:"Password,omitempty"`protobuf:"bytes,4,opt,name=Password,proto3" json:"Password,omitempty"`protobuf:"bytes,5,opt,name=URL,proto3" json:"URL,omitempty"`protobuf:"bytes,5,opt,name=URL,proto3" json:"URL,omitempty"`protobuf:"varint,6,opt,name=SrcID,proto3" json:"SrcID,omitempty"`protobuf:"varint,6,opt,name=SrcID,proto3" json:"SrcID,omitempty"`protobuf:"varint,7,opt,name=Active,proto3" json:"Active,omitempty"`protobuf:"varint,7,opt,name=Active,proto3" json:"Active,omitempty"`protobuf:"bytes,8,opt,name=Organization,proto3" json:"Organization,omitempty"`protobuf:"bytes,8,opt,name=Organization,proto3" json:"Organization,omitempty"`protobuf:"bytes,2,opt,name=Application,proto3" json:"Application,omitempty"`protobuf:"bytes,2,opt,name=Application,proto3" json:"Application,omitempty"`protobuf:"bytes,3,opt,name=Measurement,proto3" json:"Measurement,omitempty"`protobuf:"bytes,3,opt,name=Measurement,proto3" json:"Measurement,omitempty"`protobuf:"bytes,4,rep,name=Cells" json:"Cells,omitempty"`protobuf:"bytes,4,rep,name=Cells" json:"Cells,omitempty"`protobuf:"varint,5,opt,name=Autoflow,proto3" json:"Autoflow,omitempty"`protobuf:"varint,5,opt,name=Autoflow,proto3" json:"Autoflow,omitempty"`12protobuf:"bytes,6,opt,name=i,proto3" json:"i,omitempty"`protobuf:"bytes,6,opt,name=i,proto3" json:"i,omitempty"`protobuf:"bytes,7,opt,name=name,proto3" json:"name,omitempty"`protobuf:"bytes,7,opt,name=name,proto3" json:"name,omitempty"`protobuf:"varint,8,rep,packed,name=yranges" json:"yranges,omitempty"`protobuf:"varint,8,rep,packed,name=yranges" json:"yranges,omitempty"`protobuf:"bytes,9,rep,name=ylabels" json:"ylabels,omitempty"`protobuf:"bytes,9,rep,name=ylabels" json:"ylabels,omitempty"`protobuf:"bytes,10,opt,name=type,proto3" json:"type,omitempty"`protobuf:"bytes,10,opt,name=type,proto3" json:"type,omitempty"`protobuf:"bytes,11,rep,name=axes" json:"axes,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value"`protobuf:"bytes,11,rep,name=axes" json:"axes,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value"`13protobuf:"bytes,1,opt,name=Command,proto3" json:"Command,omitempty"`protobuf:"bytes,1,opt,name=Command,proto3" json:"Command,omitempty"`protobuf:"bytes,2,opt,name=DB,proto3" json:"DB,omitempty"`protobuf:"bytes,2,opt,name=DB,proto3" json:"DB,omitempty"`protobuf:"bytes,3,opt,name=RP,proto3" json:"RP,omitempty"`protobuf:"bytes,3,opt,name=RP,proto3" json:"RP,omitempty"`protobuf:"bytes,4,rep,name=GroupBys" json:"GroupBys,omitempty"`protobuf:"bytes,4,rep,name=GroupBys" json:"GroupBys,omitempty"`protobuf:"bytes,5,rep,name=Wheres" json:"Wheres,omitempty"`protobuf:"bytes,5,rep,name=Wheres" json:"Wheres,omitempty"`protobuf:"bytes,6,opt,name=Label,proto3" json:"Label,omitempty"`protobuf:"bytes,6,opt,name=Label,proto3" json:"Label,omitempty"`protobuf:"bytes,7,opt,name=Range" json:"Range,omitempty"`protobuf:"bytes,7,opt,name=Range" json:"Range,omitempty"`protobuf:"bytes,8,opt,name=Source,proto3" json:"Source,omitempty"`protobuf:"bytes,8,opt,name=Source,proto3" json:"Source,omitempty"`protobuf:"bytes,9,rep,name=Shifts" json:"Shifts,omitempty"`protobuf:"bytes,9,rep,name=Shifts" json:"Shifts,omitempty"`14protobuf:"bytes,1,opt,name=Label,proto3" json:"Label,omitempty"`protobuf:"bytes,1,opt,name=Label,proto3" json:"Label,omitempty"`protobuf:"bytes,2,opt,name=Unit,proto3" json:"Unit,omitempty"`protobuf:"bytes,2,opt,name=Unit,proto3" json:"Unit,omitempty"`protobuf:"bytes,3,opt,name=Quantity,proto3" json:"Quantity,omitempty"`protobuf:"bytes,3,opt,name=Quantity,proto3" json:"Quantity,omitempty"`15protobuf:"varint,1,opt,name=Upper,proto3" json:"Upper,omitempty"`protobuf:"varint,1,opt,name=Upper,proto3" json:"Upper,omitempty"`protobuf:"varint,2,opt,name=Lower,proto3" json:"Lower,omitempty"`protobuf:"varint,2,opt,name=Lower,proto3" json:"Lower,omitempty"`protobuf:"bytes,2,opt,name=JSON,proto3" json:"JSON,omitempty"`protobuf:"bytes,2,opt,name=JSON,proto3" json:"JSON,omitempty"`protobuf:"varint,3,opt,name=SrcID,proto3" json:"SrcID,omitempty"`protobuf:"varint,3,opt,name=SrcID,proto3" json:"SrcID,omitempty"`protobuf:"varint,4,opt,name=KapaID,proto3" json:"KapaID,omitempty"`protobuf:"varint,4,opt,name=KapaID,proto3" json:"KapaID,omitempty"`17protobuf:"bytes,3,opt,name=Provider,proto3" json:"Provider,omitempty"`protobuf:"bytes,3,opt,name=Provider,proto3" json:"Provider,omitempty"`protobuf:"bytes,4,opt,name=Scheme,proto3" json:"Scheme,omitempty"`protobuf:"bytes,4,opt,name=Scheme,proto3" json:"Scheme,omitempty"`protobuf:"bytes,5,rep,name=Roles" json:"Roles,omitempty"`protobuf:"bytes,5,rep,name=Roles" json:"Roles,omitempty"`protobuf:"varint,6,opt,name=SuperAdmin,proto3" json:"SuperAdmin,omitempty"`protobuf:"varint,6,opt,name=SuperAdmin,proto3" json:"SuperAdmin,omitempty"`18protobuf:"bytes,1,opt,name=Organization,proto3" json:"Organization,omitempty"`protobuf:"bytes,1,opt,name=Organization,proto3" json:"Organization,omitempty"`19protobuf:"bytes,1,opt,name=Provider,proto3" json:"Provider,omitempty"`protobuf:"bytes,1,opt,name=Provider,proto3" json:"Provider,omitempty"`protobuf:"bytes,2,opt,name=Scheme,proto3" json:"Scheme,omitempty"`protobuf:"bytes,2,opt,name=Scheme,proto3" json:"Scheme,omitempty"`protobuf:"bytes,3,opt,name=ProviderOrganization,proto3" json:"ProviderOrganization,omitempty"`protobuf:"bytes,3,opt,name=ProviderOrganization,proto3" json:"ProviderOrganization,omitempty"`protobuf:"bytes,4,opt,name=ID,proto3" json:"ID,omitempty"`protobuf:"bytes,4,opt,name=ID,proto3" json:"ID,omitempty"`20protobuf:"bytes,3,opt,name=DefaultRole,proto3" json:"DefaultRole,omitempty"`protobuf:"bytes,3,opt,name=DefaultRole,proto3" json:"DefaultRole,omitempty"`21protobuf:"bytes,1,opt,name=Auth" json:"Auth,omitempty"`protobuf:"bytes,1,opt,name=Auth" json:"Auth,omitempty"`22protobuf:"varint,1,opt,name=SuperAdminNewUsers,proto3" json:"SuperAdminNewUsers,omitempty"`protobuf:"varint,1,opt,name=SuperAdminNewUsers,proto3" json:"SuperAdminNewUsers,omitempty"`23protobuf:"bytes,1,opt,name=Version,proto3" json:"Version,omitempty"`protobuf:"bytes,1,opt,name=Version,proto3" json:"Version,omitempty"`protobuf:"bytes,2,opt,name=Commit,proto3" json:"Commit,omitempty"`protobuf:"bytes,2,opt,name=Commit,proto3" json:"Commit,omitempty"`310x1f1390x8b0x080x000x022550xff1880xbc870x57950x5f1430x8f2190xdb680x440x101510x971470x93560x381370x89390x272150xd72270xe30x64780x4e1730xad410x290x120x0a0x161270x7f1940xc21590x9f300x1e850x55420x2a1640xa41700xaa1300x82740x4a1850xb90x0b450x2d710x471750xaf1890xbd2350xeb2300xe62380xee1200x78660x422130xd5380x261530x99360x24860x56290x1d1720xac2370xed1870xbb1520x981920xc01350x870x40470x2f1280x802250xe11190x77620x3e0x012090xd12360xec1740xae2310xe71460x92690x451110x6f1020x66600x3c2430xf30x0c940x5e1440x901600xa01840xb81910xbf0x17480x300x091420x8e2210xdd630x3f430x2b800x501320x841690xa90x181610xa10x0d1490x951950xc31900xbe990x631800xb41410x8d0x0e2510xfb1820xb61810xb51240x7c1650xa51090x6d1160x74440x2c2070xcf0x3b2050xcd340x221380x8a700x461030x67520x34980x622100xd2540x361670xa72330xe90x052220xde2030xcb1400x8c830x532410xf11140x721080x6c2390xef2450xf5570x391170x75730x491660xa61630xa30x131580x9e2500xfa2110xd31040x68270x1b1570x9d330x21790x4f1560x9c1230x7b0x0f320x201980xc6810x511120x702400xf02200xdc2060xce0xcc1770xb10x1c1860xba2290xe5490x310x06910x5b1310x83250x192240xe0720x480x602260xe21330x85500x321990xc72020xca0x03970x611960xc41070x6b750x4b1180x76530x352340xea1290x812230xdf1060x6a2180xda1050x690x15650x412440xf4510x33370x251010x651930xc11210x792280xe40x041550x9b2540xfe2170xd9770x4d2420xf2550x37760x4c920x5c1540x9a0xc91790xb31830xb71480x942490xf91340x86260x1a2140xd6840x542520xfc0x142530xfd0xc8400x28670x430x110x071760xb01500x96890x591970xc51780xb21710xab580x3a2160xd81450x91350x231220x7a1150x731250x7d930x5d2460xf6610x3d1100x6e820x522120xd4460x2e2080xd01620xa21360x882320xe82470xf71260x7e880x581130x711680xa82480xf8 changeIntervalToDuration Before, we supported queries that included `GROUP BY :interval:` After, we only support queries with `GROUP BY time(:interval:)` thereby allowing non_negative_derivative(_____, :interval) For each dashboard Migrate the dashboard
	Import protobuf types and bucket names that are pertinent to this migration.
	This isolates the migration from the codebase, and prevents a future change
	to a type definition from invalidating the migration functions.
 N.B. leave the misspelling for backwards-compat! 1586 bytes of a gzipped FileDescriptorProto/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/client.gobackupDirfromFiletoFiletoNametoPathlastBuildgithub.com/influxdata/influxdb/v2/chronograf/id"github.com/influxdata/influxdb/v2/chronograf/id"UUIDunable to backup your database prior to migrations:  %v"unable to backup your database prior to migrations:  %v"unable to boot boltdb:  %v"unable to boot boltdb:  %v"unable to migrate boltdb:  %v"unable to migrate boltdb:  %v""backup"Mkdir%s.%s"%s.%s"O_RDWRO_CREATE512514Successfully created "Successfully created "Moving from version "Moving from version "Moving to version "Moving to version " NewClient initializes all stores WithDB sets the boltdb database for a client. It should not be called after a call to Open. Option to change behavior of Open() WithBackup returns a Backup Backup tells Open to perform a backup prior to initialization Backup returns true Always create SchemaVersions bucket. Always create Organizations bucket. Always create Sources bucket. Always create Servers bucket. Always create Layouts bucket. Always create Dashboards bucket. Always create Users bucket. Always create Config bucket. Always create Build bucket. Always create Mapping bucket. Always create OrganizationConfig bucket. migrate moves data from an old schema to a new schema in each Store Runtime migrations copy creates a copy of the database in toFile backup makes a copy of the database to the backup/ directory, if necessary: - If this is a fresh install, don't create a backup and store the current version - If we are on the same version, don't create a backup - If the version has changed, create a backup and store the current version The database was pre-existing, and the version has changed and so create a backup/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/config.gocfgConfigV1"ConfigV1"config/v1"config/v1"ErrConfigNotFoundcannot find configurationUnmarshalConfigconfig provided was nil"config provided was nil"MarshalConfig Ensure ConfigStore implements chronograf.ConfigStore. ConfigBucket is used to store chronograf application state configID is the boltDB key where the configuration object is stored ConfigStore uses bolt to store and retrieve global application configuration/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/dashboards.gocidstrIDsrcsdashboardsdefaultOrgstrconv"strconv"UnmarshalDashboardItoaMarshalDashboarddashboard not found Ensure DashboardsStore implements chronograf.DashboardsStore. DashboardsBucket is the bolt bucket dashboards are stored in DashboardsStore is the bolt implementation of storing dashboards AddIDs is a migration function that adds ID information to existing dashboards If there are is no id set, we generate one and update the dashboard Migrate updates the dashboards at runtime 1. Add UUIDs to cells without one All returns all known dashboards Add creates a new Dashboard in the DashboardsStore TODO: use FormatInt Get returns a Dashboard if the id exists. Delete the dashboard from DashboardsStore Update the dashboard in DashboardsStore Get an existing dashboard with the same ID./Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/internal/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/internal/internal.goMarshalAlertRuleMarshalConfigPBMarshalLayoutMarshalMappingMarshalMappingPBMarshalOrganizationMarshalOrganizationConfigMarshalOrganizationConfigPBMarshalOrganizationPBMarshalRoleMarshalRolePBMarshalServerMarshalSourceMarshalUserMarshalUserPBScopedAlertUnmarshalAlertRuleUnmarshalConfigPBUnmarshalLayoutUnmarshalMappingUnmarshalMappingPBUnmarshalOrganizationUnmarshalOrganizationConfigUnmarshalOrganizationConfigPBUnmarshalOrganizationPBUnmarshalRoleUnmarshalRolePBUnmarshalServerUnmarshalSourceUnmarshalUserUnmarshalUserPBfileDescriptor_41f4a519b878ee3bxxx_messageInfo_AlertRulexxx_messageInfo_AuthConfigxxx_messageInfo_Axisxxx_messageInfo_BuildInfoxxx_messageInfo_Cellxxx_messageInfo_Colorxxx_messageInfo_ColumnEncodingxxx_messageInfo_Configxxx_messageInfo_Dashboardxxx_messageInfo_DashboardCellxxx_messageInfo_DecimalPlacesxxx_messageInfo_Layoutxxx_messageInfo_Legendxxx_messageInfo_LogViewerColumnxxx_messageInfo_LogViewerConfigxxx_messageInfo_Mappingxxx_messageInfo_Organizationxxx_messageInfo_OrganizationConfigxxx_messageInfo_Queryxxx_messageInfo_Rangexxx_messageInfo_RenamableFieldxxx_messageInfo_Rolexxx_messageInfo_Serverxxx_messageInfo_Sourcexxx_messageInfo_TableOptionsxxx_messageInfo_Templatexxx_messageInfo_TemplateQueryxxx_messageInfo_TemplateValuexxx_messageInfo_TimeShiftxxx_messageInfo_UserpbmetadatajaxesqueriescellsshiftshiftscolorfieldcolorsdecimalPlacesfieldOptionssortBytableOptionstemplatetemplatescellTypelegendrolerolesencodingscolumncolumnsGetDefaultRPMetadataJSONGetMetadataJSONGetIsEnforcedGetDigitsGetFieldOptionsGetTimeFormatGetDecimalPlacesGetKey"10"linear"linear"line"line"TICKScriptAlertNodesTCPAddressEmailToExecFilePathVictorOpsRoutingKeyPagerDutyServiceKeyPushoverUserKeyDeviceTitleURLTitleSoundSensuSlackChannelIconEmojiWorkspaceTelegramChatIDParseModeDisableWebPagePreviewDisableNotificationHipChatRoomAlertaEnvironmentOriginOpsGenieTeamsRecipientsTalkKafkaTopicIsStateChangesOnlyUseFlappingPostsTCPsPagerDuty2OpsGenie2TriggerValuesChangeShiftRangeValueDBRPDetailsTriggerDBRPsExecutingCreatedModifiedLastEnabledauth config is nil"auth config is nil"GetPositionGetEncodingsGetColumnsGetOrganizationIDGetLogViewerlog Viewer config is nil"log Viewer config is nil"go:generate protoc --plugin ../../../scripts/protoc-gen-gogo --gogo_out=. internal.proto MarshalBuild encodes a build to binary protobuf format. UnmarshalBuild decodes a build from binary protobuf data. MarshalSource encodes a source to binary protobuf format. UnmarshalSource decodes a source from binary protobuf data. MarshalServer encodes a server to binary protobuf format. UnmarshalServer decodes a server from binary protobuf data. MarshalLayout encodes a layout to binary protobuf format. UnmarshalLayout decodes a layout from binary protobuf data. MarshalDashboard encodes a dashboard to binary protobuf format. UnmarshalDashboard decodes a layout from binary protobuf data. axis base defaults to 10 FIXME: this is merely for legacy cells and        should be removed as soon as possible ScopedAlert contains the source and the kapacitor id MarshalAlertRule encodes an alert rule to binary protobuf format. UnmarshalAlertRule decodes an alert rule from binary protobuf data. MarshalUser encodes a user to binary protobuf format. We are ignoring the password for now. MarshalUserPB encodes a user to binary protobuf format. UnmarshalUser decodes a user from binary protobuf data. UnmarshalUserPB decodes a user from binary protobuf data. MarshalRole encodes a role to binary protobuf format. MarshalRolePB encodes a role to binary protobuf format. UnmarshalRole decodes a role from binary protobuf data. UnmarshalRolePB decodes a role from binary protobuf data. MarshalOrganization encodes a organization to binary protobuf format. MarshalOrganizationPB encodes a organization to binary protobuf format. UnmarshalOrganization decodes a organization from binary protobuf data. UnmarshalOrganizationPB decodes a organization from binary protobuf data. MarshalConfig encodes a config to binary protobuf format. MarshalConfigPB encodes a config to binary protobuf format. UnmarshalConfig decodes a config from binary protobuf data. UnmarshalConfigPB decodes a config from binary protobuf data. MarshalOrganizationConfig encodes a config to binary protobuf format. MarshalOrganizationConfigPB encodes a config to binary protobuf format. UnmarshalOrganizationConfig decodes a config from binary protobuf data. UnmarshalOrganizationConfigPB decodes a config from binary protobuf data. MarshalMapping encodes a mapping to binary protobuf format. MarshalMappingPB encodes a mapping to binary protobuf format. UnmarshalMapping decodes a mapping from binary protobuf data. UnmarshalMappingPB decodes a mapping from binary protobuf data.InternalMessageInfomarshalInfomarshalFieldInfosizerisNiltoInt64toInt64PtrtoInt64SlicetoInt32getInt32PtrsetInt32PtrgetInt32SlicesetInt32SliceappendInt32SlicetoUint64toUint64PtrtoUint64SlicetoUint32toUint32PtrtoUint32SlicetoBooltoBoolPtrtoBoolSlicetoFloat64toFloat64PtrtoFloat64SlicetoFloat32toFloat32PtrtoFloat32SlicetoStringtoStringPtrtoStringSlicetoBytestoBytesSlicetoExtensionstoOldExtensionsgetPointerSlicesetPointerSlicegetPointersetPointerappendPointergetInterfacePointerasPointerTogetRefappendRefgetSlicemarshalermarshalElemInfowiretagtagsizeisptrisPointerrequiredoneofElemscomputeMarshalFieldInfoficomputeOneofFieldInfosetTagsetMarshalerfieldsunrecognizedextensionsv1extensionssizecachemessagesethasmarshalerextElemshassizerhasprotosizerbytesExtensionscachedsizemarshalcomputeMarshalInfogetExtElemInfosizeExtensionsappendExtensionssizeMessageSetappendMessageSetsizeV1ExtensionsappendV1ExtensionsunmarshalInfounmarshalFieldInfounmarshalerreqMaskExtensionRangelockreqFieldsoldExtensionsextensionRangesisMessageSetcomputeUnmarshalInfomergeInfomergeFieldInfobasicWidthmicomputeMergeInfodiscardInfodiscardFieldInfodiscarddicomputeDiscardInfoDiscardUnknownMergeXXX_InternalExtensionsExtensionDescExtendedTypeExtensionTyperepeatededencthisextensionMapextensionsWriteextensionsRead/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/internal/internal.pb.godeterministicmath"math"InfGoGoProtoPackageIsVersion3protobuf:"bytes,14,opt,name=DefaultRP,proto3" json:"DefaultRP,omitempty"`protobuf:"bytes,14,opt,name=DefaultRP,proto3" json:"DefaultRP,omitempty"`json:"-"`json:"-"`protobuf:"bytes,3,rep,name=cells,proto3" json:"cells,omitempty"`protobuf:"bytes,3,rep,name=cells,proto3" json:"cells,omitempty"`protobuf:"bytes,4,rep,name=templates,proto3" json:"templates,omitempty"`protobuf:"bytes,4,rep,name=templates,proto3" json:"templates,omitempty"`protobuf:"bytes,5,rep,name=queries,proto3" json:"queries,omitempty"`protobuf:"bytes,5,rep,name=queries,proto3" json:"queries,omitempty"`protobuf:"bytes,9,rep,name=axes,proto3" json:"axes,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`protobuf:"bytes,9,rep,name=axes,proto3" json:"axes,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`protobuf:"bytes,10,rep,name=colors,proto3" json:"colors,omitempty"`protobuf:"bytes,10,rep,name=colors,proto3" json:"colors,omitempty"`protobuf:"bytes,11,opt,name=legend,proto3" json:"legend,omitempty"`protobuf:"bytes,11,opt,name=legend,proto3" json:"legend,omitempty"`protobuf:"bytes,12,opt,name=tableOptions,proto3" json:"tableOptions,omitempty"`protobuf:"bytes,12,opt,name=tableOptions,proto3" json:"tableOptions,omitempty"`protobuf:"bytes,13,rep,name=fieldOptions,proto3" json:"fieldOptions,omitempty"`protobuf:"bytes,13,rep,name=fieldOptions,proto3" json:"fieldOptions,omitempty"`protobuf:"bytes,14,opt,name=timeFormat,proto3" json:"timeFormat,omitempty"`protobuf:"bytes,14,opt,name=timeFormat,proto3" json:"timeFormat,omitempty"`protobuf:"bytes,15,opt,name=decimalPlaces,proto3" json:"decimalPlaces,omitempty"`protobuf:"bytes,15,opt,name=decimalPlaces,proto3" json:"decimalPlaces,omitempty"`protobuf:"varint,1,opt,name=isEnforced,proto3" json:"isEnforced,omitempty"`protobuf:"varint,1,opt,name=isEnforced,proto3" json:"isEnforced,omitempty"`protobuf:"varint,2,opt,name=digits,proto3" json:"digits,omitempty"`protobuf:"varint,2,opt,name=digits,proto3" json:"digits,omitempty"`protobuf:"bytes,3,opt,name=sortBy,proto3" json:"sortBy,omitempty"`protobuf:"bytes,3,opt,name=sortBy,proto3" json:"sortBy,omitempty"`protobuf:"varint,1,rep,packed,name=legacyBounds,proto3" json:"legacyBounds,omitempty"`protobuf:"varint,1,rep,packed,name=legacyBounds,proto3" json:"legacyBounds,omitempty"`protobuf:"bytes,2,rep,name=bounds,proto3" json:"bounds,omitempty"`protobuf:"bytes,2,rep,name=bounds,proto3" json:"bounds,omitempty"`protobuf:"bytes,3,rep,name=values,proto3" json:"values,omitempty"`protobuf:"bytes,3,rep,name=values,proto3" json:"values,omitempty"`protobuf:"bytes,6,opt,name=query,proto3" json:"query,omitempty"`protobuf:"bytes,6,opt,name=query,proto3" json:"query,omitempty"`protobuf:"bytes,4,opt,name=key,proto3" json:"key,omitempty"`protobuf:"bytes,4,opt,name=key,proto3" json:"key,omitempty"`protobuf:"bytes,10,opt,name=Type,proto3" json:"Type,omitempty"`protobuf:"bytes,10,opt,name=Type,proto3" json:"Type,omitempty"`protobuf:"bytes,11,opt,name=MetadataJSON,proto3" json:"MetadataJSON,omitempty"`protobuf:"bytes,11,opt,name=MetadataJSON,proto3" json:"MetadataJSON,omitempty"`protobuf:"bytes,4,rep,name=Cells,proto3" json:"Cells,omitempty"`protobuf:"bytes,4,rep,name=Cells,proto3" json:"Cells,omitempty"`protobuf:"varint,8,rep,packed,name=yranges,proto3" json:"yranges,omitempty"`protobuf:"varint,8,rep,packed,name=yranges,proto3" json:"yranges,omitempty"`protobuf:"bytes,9,rep,name=ylabels,proto3" json:"ylabels,omitempty"`protobuf:"bytes,9,rep,name=ylabels,proto3" json:"ylabels,omitempty"`protobuf:"bytes,11,rep,name=axes,proto3" json:"axes,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`protobuf:"bytes,11,rep,name=axes,proto3" json:"axes,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`protobuf:"bytes,4,rep,name=GroupBys,proto3" json:"GroupBys,omitempty"`protobuf:"bytes,4,rep,name=GroupBys,proto3" json:"GroupBys,omitempty"`protobuf:"bytes,5,rep,name=Wheres,proto3" json:"Wheres,omitempty"`protobuf:"bytes,5,rep,name=Wheres,proto3" json:"Wheres,omitempty"`protobuf:"bytes,7,opt,name=Range,proto3" json:"Range,omitempty"`protobuf:"bytes,7,opt,name=Range,proto3" json:"Range,omitempty"`protobuf:"bytes,9,rep,name=Shifts,proto3" json:"Shifts,omitempty"`protobuf:"bytes,9,rep,name=Shifts,proto3" json:"Shifts,omitempty"`protobuf:"bytes,5,rep,name=Roles,proto3" json:"Roles,omitempty"`protobuf:"bytes,5,rep,name=Roles,proto3" json:"Roles,omitempty"`protobuf:"bytes,1,opt,name=Auth,proto3" json:"Auth,omitempty"`protobuf:"bytes,1,opt,name=Auth,proto3" json:"Auth,omitempty"`protobuf:"bytes,1,opt,name=OrganizationID,proto3" json:"OrganizationID,omitempty"`protobuf:"bytes,1,opt,name=OrganizationID,proto3" json:"OrganizationID,omitempty"`protobuf:"bytes,2,opt,name=LogViewer,proto3" json:"LogViewer,omitempty"`protobuf:"bytes,2,opt,name=LogViewer,proto3" json:"LogViewer,omitempty"`protobuf:"bytes,1,rep,name=Columns,proto3" json:"Columns,omitempty"`protobuf:"bytes,1,rep,name=Columns,proto3" json:"Columns,omitempty"`protobuf:"bytes,1,opt,name=Name,proto3" json:"Name,omitempty"`protobuf:"bytes,1,opt,name=Name,proto3" json:"Name,omitempty"`protobuf:"varint,2,opt,name=Position,proto3" json:"Position,omitempty"`protobuf:"varint,2,opt,name=Position,proto3" json:"Position,omitempty"`protobuf:"bytes,3,rep,name=Encodings,proto3" json:"Encodings,omitempty"`protobuf:"bytes,3,rep,name=Encodings,proto3" json:"Encodings,omitempty"`protobuf:"bytes,2,opt,name=Value,proto3" json:"Value,omitempty"`protobuf:"bytes,2,opt,name=Value,proto3" json:"Value,omitempty"`protobuf:"bytes,3,opt,name=Name,proto3" json:"Name,omitempty"`protobuf:"bytes,3,opt,name=Name,proto3" json:"Name,omitempty"`RegisterTypeinternal.Source"internal.Source"internal.Dashboard"internal.Dashboard"internal.DashboardCell"internal.DashboardCell"RegisterMapTypeinternal.DashboardCell.AxesEntry"internal.DashboardCell.AxesEntry"internal.DecimalPlaces"internal.DecimalPlaces"internal.TableOptions"internal.TableOptions"internal.RenamableField"internal.RenamableField"internal.Color"internal.Color"internal.Legend"internal.Legend"internal.Axis"internal.Axis"internal.Template"internal.Template"internal.TemplateValue"internal.TemplateValue"internal.TemplateQuery"internal.TemplateQuery"internal.Server"internal.Server"internal.Layout"internal.Layout"internal.Cell"internal.Cell"internal.Cell.AxesEntry"internal.Cell.AxesEntry"internal.Query"internal.Query"internal.TimeShift"internal.TimeShift"internal.Range"internal.Range"internal.AlertRule"internal.AlertRule"internal.User"internal.User"internal.Role"internal.Role"internal.Mapping"internal.Mapping"internal.Organization"internal.Organization"internal.Config"internal.Config"internal.AuthConfig"internal.AuthConfig"internal.OrganizationConfig"internal.OrganizationConfig"internal.LogViewerConfig"internal.LogViewerConfig"internal.LogViewerColumn"internal.LogViewerColumn"internal.ColumnEncoding"internal.ColumnEncoding"internal.BuildInfo"internal.BuildInfo"RegisterFileinternal.proto"internal.proto"900x5a Code generated by protoc-gen-gogo. DO NOT EDIT. source: internal.proto Reference imports to suppress errors if they are not otherwise used. This is a compile-time assertion to ensure that this generated file is compatible with the proto package it is being compiled against. A compilation error at this line likely means your copy of the proto package needs to be updated. please upgrade the proto package 1810 bytes of a gzipped FileDescriptorProto/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/layouts.go"Layout"ErrLayoutNotFoundlayout not found Ensure LayoutsStore implements chronograf.LayoutsStore. LayoutsBucket is the bolt bucket layouts are stored in LayoutsStore is the bolt implementation to store layouts All returns all known layouts Add creates a new Layout in the LayoutsStore. Delete removes the Layout from the LayoutsStore Get returns a Layout if the id exists. Update a Layout Get an existing layout with the same ID./Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/mapping.gomappingsMappingsV1"MappingsV1"%d"%d"ErrMappingNotFoundmapping not found Ensure MappingsStore implements chronograf.MappingsStore. MappingsBucket is the bucket where organizations are stored. MappingsStore uses bolt to store and retrieve Mappings Migrate sets the default organization at runtime Add creates a new Mapping in the MappingsStore All returns all known organizations Delete the organization from MappingsStore Get returns a Mapping if the id exists. Update the organization in MappingsStore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/org_config.goOrganizationConfigV1"OrganizationConfigV1"ErrOrganizationConfigNotFoundcould not find organization configvisibility"visibility"hidden"hidden"severity"severity"visible"visible""label"icon"icon"text"text""color"emerg"emerg"ruby"ruby"alert"alert"fire"fire"crit"crit"curacao"curacao""err"tiger"tiger"warning"warning"pineapple"pineapple"notice"notice"rainforest"rainforest"info"info"star"star"debug"debug"wolf"wolf"timestamp"timestamp"message"message"facility"facility"procid"procid"displayName"displayName"Proc ID"Proc ID"appname"appname""Application"host"host" Ensure OrganizationConfigStore implements chronograf.OrganizationConfigStore. OrganizationConfigBucket is used to store chronograf organization configurations OrganizationConfigStore uses bolt to store and retrieve organization configurations Get retrieves an OrganizationConfig from the store FindOrCreate gets an OrganizationConfig from the store or creates one if none exists for this organization Put replaces the OrganizationConfig in the store/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/organizations.gosourceserverdashboardmappingdashboardsStoreserversserversStoresourcesStoreusersStoreorganizationsgithub.com/influxdata/influxdb/v2/chronograf/organizations"github.com/influxdata/influxdb/v2/chronograf/organizations"OrganizationsV1"OrganizationsV1"default"default""Default"member"member"MappingWildcard*ErrOrganizationNotFoundorganization not foundErrOrganizationAlreadyExistsorganization already existsErrCannotDeleteDefaultOrganizationcannot delete default organizationWithValuecontextKeyContextKeyorganizationNewSourcesStoreNewServersStoreNewDashboardsStoreNewUsersStoremust specify either ID, or Name in OrganizationQuery"must specify either ID, or Name in OrganizationQuery" Ensure OrganizationsStore implements chronograf.OrganizationsStore. OrganizationsBucket is the bucket where organizations are stored. DefaultOrganizationID is the ID of the default organization. DefaultOrganizationName is the Name of the default organization DefaultOrganizationRole is the DefaultRole for the Default organization OrganizationsStore uses bolt to store and retrieve Organizations CreateDefault does a findOrCreate on the default organization DefaultOrganizationID returns the ID of the default organization Add creates a new Organization in the OrganizationsStore Delete the organization from OrganizationsStore Dependent Delete of all resources Each of the associated organization stores expects organization to be set on the context. Get returns a Organization if the id exists. If an ID is provided in the query, the lookup time for an organization will be O(1). If Name is provided, the lookup time will be O(n). Get expects that only one of ID or Name will be specified, but will prefer ID over Name if both are specified. Update the organization in OrganizationsStore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/servers.goServers"Servers"ErrServerNotFoundserver not found Ensure ServersStore implements chronograf.ServersStore. ServersBucket is the bolt bucket to store lists of servers ServersStore is the bolt implementation to store servers in a store. Used store servers that are associated in some way with a source All returns all known servers Add creates a new Server in the ServerStore. make the newly added source "active" Delete removes the Server from the ServersStore Get returns a Server if the id exists. Update a Server Get an existing server with the same ID. only one server can be active at a time resetActiveServer unsets the Active flag on all sources/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/sources.gotargetgithub.com/influxdata/influxdb/v2/chronograf/roles"github.com/influxdata/influxdb/v2/chronograf/roles"Sources"Sources"MaxInt322147483647autogen"autogen"influx"influx"http://localhost:8086"http://localhost:8086"ViewerRoleNameviewer Ensure SourcesStore implements chronograf.SourcesStore. SourcesBucket is the bolt bucket used to store source information DefaultSource is a temporary measure for single-binary. Use large number to avoid possible collisions in older chronograf. SourcesStore is a bolt implementation to store time-series source information. Migrate adds the default source to an existing boltdb. All returns all known sources Add creates a new Source in the SourceStore. force first source added to be default Delete removes the Source from the SourcesStore Get returns a Source if the id exists. Update a Source Put updates the source. Get an existing source with the same ID. resetDefaultSource unsets the Default flag on all sources setRandomDefault will locate a source other than the provided chronograf.Source and set it as the default source. If no other sources are available, the provided source will be set to the default source if is not already. It assumes that the provided chronograf.Source has been persisted. Check if requested source is the current default Locate another source to be the new default avoid selecting the source we're about to delete as the new default set the other to be the default/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/users.goUsersV2"UsersV2"ErrUserNotFounduser not foundmust specify either ID, or Name, Provider, and Scheme in UserQuery"must specify either ID, or Name, Provider, and Scheme in UserQuery"user provided is nil"user provided is nil"ErrUserAlreadyExistsuser already exists Ensure UsersStore implements chronograf.UsersStore. UsersBucket is used to store users local to chronograf UsersStore uses bolt to store and retrieve users get searches the UsersStore for user with id and returns the bolt representation Num returns the number of users in the UsersStore Get searches the UsersStore for user with name Add a new User to the UsersStore. Delete a user from the UsersStore Update a user All returns all users/Users/austinjaybecker/projects/abeck-go-testing/chronograf/bolt/util.gobinaryencoding/binary"encoding/binary"bigEndianUint16PutUint16PutUint32PutUint64BigEndian itob returns an 8-byte big endian representation of v. u64tob returns an 8-byte big endian representation of v./Users/austinjaybecker/projects/abeck-go-testing/chronograf/canned/Users/austinjaybecker/projects/abeck-go-testing/chronograf/canned/TODO.goAssetAssetNamesBinLayoutsStorecannedno assets included in binary"no assets included in binary" +build !assets The functions defined in this file are placeholders when the binary is compiled without assets. Asset returns an error stating no assets were included in the binary. AssetNames returns nil because there are no assets included in the binary./Users/austinjaybecker/projects/abeck-go-testing/chronograf/canned/bin.golayoutoctetslayoutsnamescomponent"component"apps"apps"Invalid Layout: "Invalid Layout: "ErrLayoutInvalidlayout is invalidUnable to read layout:"Unable to read layout:"add to BinLayoutsStore not supported"add to BinLayoutsStore not supported"delete to BinLayoutsStore not supported"delete to BinLayoutsStore not supported"Layout not found"Layout not found"update to BinLayoutsStore not supported"update to BinLayoutsStore not supported"go:generate env GO111MODULE=on go run github.com/kevinburke/go-bindata/go-bindata -o bin_gen.go -tags assets -ignore README|apps|.sh|go -pkg canned . BinLayoutsStore represents a layout store using data generated by go-bindata All returns the set of all layouts Add is not support by BinLayoutsStore Delete is not support by BinLayoutsStore Get retrieves Layout if `ID` exists. Update not supported/Users/austinjaybecker/projects/abeck-go-testing/chronograf/chronograf.goAllScopeAnnotationAnnotationStoreAssetsDBScopeDatabasesErrAlertNotFoundErrAnnotationNotFoundErrAuthenticationErrDashboardInvalidErrInvalidAxisErrInvalidCellOptionsColumnsErrInvalidCellOptionsSortErrInvalidCellOptionsTextErrInvalidColorErrInvalidColorTypeErrInvalidLegendErrInvalidLegendOrientErrInvalidLegendTypeErrServerInvalidErrSourceInvalidErrUninitializedErrUpstreamTimeoutInfluxDBInfluxEnterpriseInfluxRelayKapacitorNodeKapacitorPropertyNoopLoggerPointRolesStoreTSDBStatusTimeSerieslimitstoprequest to backend timed out"request to backend timed out""server not found""layout not found""dashboard not found""user not found""layout is invalid"dashboard is invalid"dashboard is invalid"source is invalid"source is invalid"server is invalid"server is invalid"alert not found"alert not found"user not authenticated"user not authenticated"client uninitialized. Call Open() method"client uninitialized. Call Open() method"Unexpected axis in cell. Valid axes are 'x', 'y', and 'y2'"Unexpected axis in cell. Valid axes are 'x', 'y', and 'y2'"Invalid color type. Valid color types are 'min', 'max', 'threshold', 'text', and 'background'"Invalid color type. Valid color types are 'min', 'max', 'threshold', 'text', and 'background'"Invalid color. Accepted color format is #RRGGBB"Invalid color. Accepted color format is #RRGGBB"Invalid legend. Both type and orientation must be set"Invalid legend. Both type and orientation must be set"Invalid legend type. Valid legend type is 'static'"Invalid legend type. Valid legend type is 'static'"Invalid orientation type. Valid orientation types are 'top', 'bottom', 'right', 'left'"Invalid orientation type. Valid orientation types are 'top', 'bottom', 'right', 'left'""user already exists""organization not found""mapping not found""organization already exists""cannot delete default organization""cannot find configuration"annotation not found"annotation not found"invalid text wrapping option. Valid wrappings are 'truncate', 'wrap', and 'single line'"invalid text wrapping option. Valid wrappings are 'truncate', 'wrap', and 'single line'"cell options sortby cannot be empty'"cell options sortby cannot be empty'"cell options columns cannot be empty'"cell options columns cannot be empty'""could not find organization config"DELETEGETPATCHPOSTPUTinflux-enterprise"influx-enterprise"influx-relay"influx-relay"Pingjson:"permissions,omitempty"`json:"permissions,omitempty"`json:"users,omitempty"`json:"users,omitempty"`json:"organization,omitempty"`json:"organization,omitempty"`json:"upper"`json:"upper"`json:"lower"`json:"lower"`json:"value"`json:"value"`json:"selected"`json:"selected"`json:"key,omitempty"`json:"key,omitempty"`json:"tempVar"`json:"tempVar"`json:"values"`json:"values"`json:"label"`json:"label"`json:"query,omitempty"`json:"query,omitempty"`json:"query"`json:"query"`json:"db,omitempty"`json:"db,omitempty"`json:"epoch,omitempty"`json:"epoch,omitempty"`json:"wheres,omitempty"`json:"wheres,omitempty"`json:"groupbys,omitempty"`json:"groupbys,omitempty"`json:"label,omitempty"`json:"label,omitempty"`json:"range,omitempty"`json:"range,omitempty"`json:"queryConfig,omitempty"`json:"queryConfig,omitempty"`json:"source"`json:"source"`json:"type,omitempty"`json:"type,omitempty"`json:"influxql"`json:"influxql"`json:"measurement"`json:"measurement"`json:"tagKey"`json:"tagKey"`json:"fieldKey"`json:"fieldKey"`json:"id,string"`json:"id,string"`json:"username,omitempty"`json:"username,omitempty"`json:"password,omitempty"`json:"password,omitempty"`json:"sharedSecret,omitempty"`json:"sharedSecret,omitempty"`json:"url"`json:"url"`json:"metaUrl,omitempty"`json:"metaUrl,omitempty"`json:"insecureSkipVerify,omitempty"`json:"insecureSkipVerify,omitempty"`json:"default"`json:"default"`json:"telegraf"`json:"telegraf"`json:"organization"`json:"organization"`json:"role,omitempty"`json:"role,omitempty"`json:"defaultRP"`json:"defaultRP"`json:"db"`json:"db"`json:"rp"`json:"rp"`json:"tickscript"`json:"tickscript"`json:"every"`json:"every"`json:"alertNodes"`json:"alertNodes"`json:"message"`json:"message"`json:"details"`json:"details"`json:"trigger"`json:"trigger"`json:"dbrps"`json:"dbrps"`json:"executing"`json:"executing"`json:"error"`json:"error"`json:"created"`json:"created"`json:"modified"`json:"modified"`json:"last-enabled,omitempty"`json:"last-enabled,omitempty"`json:"change,omitempty"`json:"change,omitempty"`json:"period,omitempty"`json:"period,omitempty"`json:"shift,omitempty"`json:"shift,omitempty"`json:"operator,omitempty"`json:"operator,omitempty"`json:"value,omitempty"`json:"value,omitempty"`json:"rangeValue"`json:"rangeValue"`json:"alias"`json:"alias"`json:"args,omitempty"`json:"args,omitempty"`json:"time"`json:"time"`json:"tags"`json:"tags"`json:"unit"`json:"unit"`json:"quantity"`json:"quantity"`json:"database"`json:"database"`json:"retentionPolicy"`json:"retentionPolicy"`json:"fields"`json:"fields"`json:"groupBy"`json:"groupBy"`json:"areTagsAccepted"`json:"areTagsAccepted"`json:"fill,omitempty"`json:"fill,omitempty"`json:"rawText"`json:"rawText"`json:"range"`json:"range"`json:"shifts"`json:"shifts"`json:"args"`json:"args"`json:"properties"`json:"properties"`json:"srcId,string"`json:"srcId,string"`json:"username"`json:"username"`json:"password"`json:"password"`json:"insecureSkipVerify"`json:"insecureSkipVerify"`json:"active"`json:"active"`json:"metadata"`json:"metadata"`"all"database"database"json:"scope"`json:"scope"`json:"allowed"`json:"allowed"`json:"id,string,omitempty"`json:"id,string,omitempty"`json:"roles"`json:"roles"`json:"provider,omitempty"`json:"provider,omitempty"`json:"scheme,omitempty"`json:"scheme,omitempty"`json:"superAdmin,omitempty"`json:"superAdmin,omitempty"`ReplicationShardDurationjson:"duration,omitempty"`json:"duration,omitempty"`json:"replication,omitempty"`json:"replication,omitempty"`json:"shardDuration,omitempty"`json:"shardDuration,omitempty"`json:"isDefault,omitempty"`json:"isDefault,omitempty"`AllDBAllRPCreateDBCreateRPDropDBDropRPGetMeasurementsUpdateRPEndTimejson:"cells"`json:"cells"`json:"templates"`json:"templates"`json:"bounds"`json:"bounds"`json:"prefix"`json:"prefix"`json:"suffix"`json:"suffix"`json:"base"`json:"base"`json:"scale"`json:"scale"`json:"hex"`json:"hex"`json:"orientation,omitempty"`json:"orientation,omitempty"`json:"i"`json:"i"`json:"x"`json:"x"`json:"y"`json:"y"`json:"w"`json:"w"`json:"h"`json:"h"`json:"queries"`json:"queries"`json:"axes"`json:"axes"`json:"colors"`json:"colors"`json:"legend"`json:"legend"`json:"tableOptions,omitempty"`json:"tableOptions,omitempty"`json:"fieldOptions"`json:"fieldOptions"`json:"timeFormat"`json:"timeFormat"`json:"decimalPlaces"`json:"decimalPlaces"`json:"note,omitempty"`json:"note,omitempty"`json:"noteVisibility,omitempty"`json:"noteVisibility,omitempty"`json:"internalName"`json:"internalName"`json:"displayName"`json:"displayName"`json:"visible"`json:"visible"`json:"verticalTimeAxis"`json:"verticalTimeAxis"`json:"sortBy"`json:"sortBy"`json:"wrapping"`json:"wrapping"`json:"fixFirstColumn"`json:"fixFirstColumn"`json:"isEnforced"`json:"isEnforced"`json:"digits"`json:"digits"`json:"app"`json:"app"`json:"autoflow"`json:"autoflow"`"*"json:"organizationId"`json:"organizationId"`json:"provider"`json:"provider"`json:"scheme"`json:"scheme"`json:"providerOrganization"`json:"providerOrganization"`json:"defaultRole,omitempty"`json:"defaultRole,omitempty"`json:"auth"`json:"auth"`json:"superAdminNewUsers"`json:"superAdminNewUsers"`json:"logViewer"`json:"logViewer"`json:"columns"`json:"columns"`json:"position"`json:"position"`json:"encodings"`json:"encodings"`TelegrafSystemIntervaljson:"telegrafSystemInterval"`json:"telegrafSystemInterval"` General errors. Error is a domain error encountered while processing chronograf requests Logger represents an abstracted structured logging implementation. It provides methods to trigger log messages at various alert levels and a WithField method to set keys for a structured log message. Logger can be transformed into an io.Writer. That writer is the end of an io.Pipe and it is your responsibility to close it. NoopLogger is a chronograf logger that does nothing. Router is an abstracted Router based on the API provided by the julienschmidt/httprouter package. Assets returns a handler to serve the website. Supported time-series databases InfluxDB is the open-source time-series database InfluxEnteprise is the clustered HA time-series database InfluxRelay is the basic HA layer over InfluxDB TSDBStatus represents the current status of a time series database Connect will connect to the time series using the information in `Source`. Ping returns version and TSDB type of time series database if reachable. Version returns the version of the TSDB database Type returns the type of the TSDB database Point is a field set in a series TimeSeries represents a queryable time series database. Query retrieves time series data from the database. Write records points into a series UsersStore represents the user accounts within the TimeSeries database Permissions returns all valid names permissions in this database Roles represents the roles associated with this TimesSeriesDatabase Role is a restricted set of permissions assigned to a set of users. RolesStore is the Storage and retrieval of authentication information All lists all roles from the RolesStore Create a new Role in the RolesStore Delete the Role from the RolesStore Get retrieves a role if name exists. Update the roles' users or permissions Range represents an upper and lower bound for data Upper is the upper bound Lower is the lower bound TemplateValue is a value use to replace a template in an InfluxQL query Value is the specific value used to replace a template in an InfluxQL query Type can be tagKey, tagValue, fieldKey, csv, map, measurement, database, constant, influxql Selected states that this variable has been picked to use for replacement Key is the key for the Value if the Template Type is 'map' TemplateVar is a named variable within an InfluxQL query to be replaced with Values Var is the string to replace within InfluxQL Values are the replacement values within InfluxQL TemplateID is the unique ID used to identify a template Template represents a series of choices to replace TemplateVars within InfluxQL ID is the unique ID associated with this template Type can be fieldKeys, tagKeys, tagValues, csv, constant, measurements, databases, map, influxql, text Label is a user-facing description of the Template Query is used to generate the choices for a template Query retrieves a Response from a TimeSeries. Command is the query itself DB is optional and if empty will not be used. RP is a retention policy and optional; if empty will not be used. Epoch is the time format for the return results Wheres restricts the query to certain attributes GroupBys collate the query by these tags Label is the Y-Axis label for the data Range is the default Y-Axis range for the data DashboardQuery includes state for the query builder.  This is a transition struct while we move to the full InfluxQL AST QueryConfig represents the query state that is understood by the data explorer Source is the optional URI to the data source for this queryConfig Shifts represents shifts to apply to an influxql query's time range.  Clients expect the shift to be in the generated QueryConfig This was added after this code was brought over to influxdb. TemplateQuery is used to retrieve choices for template replacement Measurement is the optionally selected measurement for the query TagKey is the optionally selected tag key for the query FieldKey is the optionally selected field key for the query Response is the result of a query against a TimeSeries Source is connection information to a time-series data store. ID is the unique ID of the source Name is the user-defined name for the source Type specifies which kinds of source (enterprise vs oss) Username is the username to connect to the source Password is in CLEARTEXT ShareSecret is the optional signing secret for Influx JWT authorization URL are the connections to the source MetaURL is the url for the meta node InsecureSkipVerify as true means any certificate presented by the source is accepted. Default specifies the default source for the application Telegraf is the db telegraf is written to.  By default it is "telegraf" Organization is the organization ID that resource belongs to Not Currently Used. Role is the name of the minimum role that a user must possess to access the resource. DefaultRP is the default retention policy used in database queries to this source SourcesStore stores connection information for a `TimeSeries` All returns all sources in the store Add creates a new source in the SourcesStore and returns Source with ID Delete the Source from the store Get retrieves Source if `ID` exists Update the Source in the store. DBRP represents a database and retention policy for a time series source AlertRule represents rules for building a tickscript alerting task ID is the unique ID of the alert TICKScript is the raw tickscript associated with this Alert Query is the filter of data for the alert. Every how often to check for the alerting criteria AlertNodes defines the destinations for the alert Message included with alert Details is generally used for the Email alert.  If empty will not be added. Trigger is a type that defines when to trigger the alert Defines the values that cause the alert to trigger Name is the user-defined name for the alert Represents the task type where stream is data streamed to kapacitor and batch is queried by kapacitor List of database retention policy pairs the task is allowed to access Represents if this rule is enabled or disabled in kapacitor Whether the task is currently executing Any error encountered when kapacitor executes the task Date the task was first created Date the task was last modified Date the task was last set to status enabled TICKScript task to be used by kapacitor Ticker generates tickscript tasks for kapacitor Generate will create the tickscript to be used as a kapacitor task TriggerValues specifies the alerting logic for a specific trigger type Change specifies if the change is a percent or absolute Period length of time before deadman is alerted Shift is the amount of time to look into the past for the alert to compare to the present Operator for alert comparison Value is the boundary value when alert goes critical RangeValue is an optional value for range comparisons Field represent influxql fields and functions from the UI GroupBy represents influxql group by tags from the UI DurationRange represents the lower and upper durations of the query config TimeShift represents a shift to apply to an influxql query's time range Label user facing description Unit influxql time unit representation i.e. ms, s, m, h, d Quantity number of units QueryConfig represents UI query from the data explorer KapacitorNode adds arguments and properties to an alert In the future we could add chaining methods here. KapacitorProperty modifies the node they are called on Server represents a proxy connection to an HTTP server ID is the unique ID of the server SrcID of the data source Name is the user-defined name for the server Username is the username to connect to the server URL are the connections to the server InsecureSkipVerify as true means any certificate presented by the server is accepted. Is this the active server for the source? Type is the kind of service (e.g. kapacitor or flux) Metadata is any other data that the frontend wants to store about this service ServersStore stores connection information for a `Server` All returns all servers in the store Add creates a new source in the ServersStore and returns Server with ID Delete the Server from the store Get retrieves Server if `ID` exists Update the Server in the store. ID creates uniq ID string Generate creates a unique ID string AllScope grants permission for all databases. DBScope grants permissions for a specific database Permission is a specific allowance for User or Role bound to a scope of the data source Permissions represent the entire set of permissions a User or Role may have Allowances defines what actions a user can have on a scoped permission Scope defines the location of access of a permission User represents an authenticated user. UserQuery represents the attributes that a user may be retrieved by. It is predominantly used in the UsersStore.Get method. It is expected that only one of ID or Name, Provider, and Scheme will be specified, but all are provided UserStores should prefer ID. UsersStore is the Storage and retrieval of authentication information While not necessary for the app to function correctly, it is expected that Implementors of the UsersStore will take care to guarantee that the combinartion of a  users Name, Provider, and Scheme are unique. All lists all users from the UsersStore Create a new User in the UsersStore Delete the User from the UsersStore Get retrieves a user if name exists. Update the user's permissions or roles Database represents a database in a time series source a unique string identifier for the database the duration (when creating a default retention policy) the replication factor (when creating a default retention policy) the shard duration (when creating a default retention policy) RetentionPolicy represents a retention policy in a time series source a unique string identifier for the retention policy the duration the replication factor the shard duration whether the RP should be the default Measurement represents a measurement in a time series source a unique string identifier for the measurement Databases represents a databases in a time series source AllDB lists all databases in the current data source Connect connects to a database in the current data source CreateDB creates a database in the current data source DropDB drops a database in the current data source AllRP lists all retention policies in the current data source CreateRP creates a retention policy in the current data source UpdateRP updates a retention policy in the current data source DropRP drops a retention policy in the current data source GetMeasurements lists measurements in the current data source Annotation represents a time-based metadata associated with a source ID is the unique annotation identifier StartTime starts the annotation EndTime ends the annotation Text is the associated user-facing text describing the annotation Type describes the kind of annotation AnnotationStore represents storage and retrieval of annotations All lists all Annotations between start and stop Add creates a new annotation in the store Delete removes the annotation from the store Get retrieves an annotation Update replaces annotation DashboardID is the dashboard ID Dashboard represents all visual and query data for a dashboard Axis represents the visible extents of a visualization bounds are an arbitrary list of client-defined strings that specify the viewport for a cell legacy bounds are for testing a migration from an earlier version of axis label is a description of this Axis Prefix represents a label prefix for formatting axis values Suffix represents a label suffix for formatting axis values Base represents the radix for formatting axis values Scale is the axis formatting scale. Supported: "log", "linear" CellColor represents the encoding of data into visualizations ID is the unique id of the cell color Type is how the color is used. Accepted (min,max,threshold) Hex is the hex number of the color Name is the user-facing name of the hex color Value is the data value mapped to this color Legend represents the encoding of data into a legend DashboardCell holds visual and query information for a cell These were added after this code was brought over to influxdb. RenamableField is a column/row field in a DashboardCell of type Table TableOptions is a type of options for a DashboardCell with type Table DecimalPlaces indicates whether decimal places should be enforced, and how many digits it should show. DashboardsStore is the storage and retrieval of dashboards All lists all dashboards from the DashboardStore Create a new Dashboard in the DashboardStore Delete the Dashboard from the DashboardStore if `ID` exists. Get retrieves a dashboard if `ID` exists. Update replaces the dashboard information Cell is a rectangle and multiple time series queries to visualize. Layout is a collection of Cells for visualization LayoutsStore stores dashboards and associated Cells All returns all dashboards in the store Add creates a new dashboard in the LayoutsStore Delete the dashboard from the store Get retrieves Layout if `ID` exists Update the dashboard in the store. MappingWildcard is the wildcard value for mappings A Mapping is the structure that is used to determine a users role within an organization. The high level idea is to grant certain roles to certain users without them having to be given explicit role within the organization. One can think of a mapping like so:     Provider:Scheme:Group -> Organization     github:oauth2:influxdata -> Happy     beyondcorp:ldap:influxdata -> TheBillHilliettas Any of Provider, Scheme, or Group may be provided as a wildcard *     github:oauth2:* -> MyOrg     *:*:* -> AllOrg MappingsStore is the storage and retrieval of Mappings Add creates a new Mapping. The Created mapping is returned back to the user with the ID field populated. All lists all Mapping in the MappingsStore Delete removes an Mapping from the MappingsStore Get retrieves an Mapping from the MappingsStore Update updates an Mapping in the MappingsStore Organization is a group of resources under a common name DefaultRole is the name of the role that is the default for any users added to the organization OrganizationQuery represents the attributes that a organization may be retrieved by. It is predominantly used in the OrganizationsStore.Get method. It is expected that only one of ID or Name will be specified, but will prefer ID over Name if both are specified. OrganizationsStore is the storage and retrieval of Organizations expected that Implementors of the OrganizationsStore will take care to guarantee that the Organization.Name is unique. Allowing for duplicate names creates a confusing UX experience for the User. Add creates a new Organization. The Created organization is returned back to the user with the All lists all Organizations in the OrganizationsStore Delete removes an Organization from the OrganizationsStore Get retrieves an Organization from the OrganizationsStore Update updates an Organization in the OrganizationsStore CreateDefault creates the default organization DefaultOrganization returns the DefaultOrganization Config is the global application Config for parameters that can be set via API, with different sections, such as Auth AuthConfig is the global application config section for auth parameters SuperAdminNewUsers configuration option that specifies which users will auto become super admin ConfigStore is the storage and retrieval of global application Config Initialize creates the initial configuration Get retrieves the whole Config from the ConfigStore Update updates the whole Config in the ConfigStore OrganizationConfig is the organization config for parameters that can be set via API, with different sections, such as LogViewer LogViewerConfig is the configuration settings for the Log Viewer UI LogViewerColumn is a specific column of the Log Viewer UI ColumnEncoding is the settings for a specific column of the Log Viewer UI OrganizationConfigStore is the storage and retrieval of organization Configs FindOrCreate gets an existing OrganizationConfig and creates one if none exists Put replaces the whole organization config in the OrganizationConfigStore BuildInfo is sent to the usage client to track versions and commits BuildStore is the storage and retrieval of Chronograf build information Environment is the set of front-end exposed environment variables that were set on the server/Users/austinjaybecker/projects/abeck-go-testing/chronograf/cmd/Users/austinjaybecker/projects/abeck-go-testing/chronograf/cmd/chronoctl/Users/austinjaybecker/projects/abeck-go-testing/chronograf/cmd/chronoctl/add.goAddCommandListCommandNewBoltClientNewTabWriterWriteHeadersWriteUseraddCommandlistCommandmainoptionsparserorgQueryargsBoltPathExecuteshort:"b" long:"bolt-path" description:"Full path to boltDB file (e.g. './chronograf-v1.db')" env:"BOLT_PATH" default:"chronograf-v1.db"`short:"b" long:"bolt-path" description:"Full path to boltDB file (e.g. './chronograf-v1.db')" env:"BOLT_PATH" default:"chronograf-v1.db"`short:"i" long:"id" description:"Users ID. Must be id for existing user"`short:"i" long:"id" description:"Users ID. Must be id for existing user"`short:"n" long:"name" description:"Users name. Must be Oauth-able email address or username"`short:"n" long:"name" description:"Users name. Must be Oauth-able email address or username"`short:"p" long:"provider" description:"Name of the Auth provider (e.g. google, github, auth0, or generic)"`short:"p" long:"provider" description:"Name of the Auth provider (e.g. google, github, auth0, or generic)"`short:"s" long:"scheme" description:"Authentication scheme that matches auth provider (e.g. oauth2)" default:"oauth2"`short:"s" long:"scheme" description:"Authentication scheme that matches auth provider (e.g. oauth2)" default:"oauth2"`short:"o" long:"orgs" description:"A comma separated list of organizations that the user should be added to" default:"default"`short:"o" long:"orgs" description:"A comma separated list of organizations that the user should be added to" default:"default"`OrgLoop,","widthhtaboutputminwidthtabwidthpaddingpadbytesendCharlineswidthsaddLinedumpwrite0writeNwritePaddingwriteLinesformatupdateWidthstartEscapeendEscapeterminateCellhandlePanicflushflushNoDefersmultiTagcachedGetManySetManyShortNameLongNameEnvDefaultKeyEnvDefaultDelimOptionalArgumentOptionalValueRequiredValueNameChoicesHiddengroupiniQuoteisSetisSetDefaultpreventDefaultdefaultLiteralLongNameWithNamespaceoptionIsSetIsSetDefaultcanClicanArgumentemptyValueemptyclearDefaultvalueIsDefaultisUnmarshalerisBoolisSignedNumberisFuncupdateDefaultLiteralshortAndLongNameShortDescriptionLongDescriptionNamespacegroupsisBuiltinHelpAddGroupGroupsfindOptionFindOptionByLongNameFindOptionByShortNameoptionByNameeachGroupscanStructcheckForDuplicateFlagsscanSubGroupHandlerscanTypegroupByNameRequiredMaximumisRemainingSubcommandsOptionalAliasesArgsRequiredcommandshasBuiltinHelpGroupCommandsscanSubcommandHandlereachOptioneachCommandeachActiveGroupaddHelpGroupsmakeLookupfillLookupsortedVisibleCommandsvisibleCommandshasCliOptionsfillParseStateaddHelpGroupParserSplitArgumentCompletionItemCommanderNamespaceDelimiterUnknownOptionHandlerCompletionHandlerCommandHandlerinternalErrorgetAlignmentInfowriteHelpOptionWriteHelpWriteManPageParseArgsparseOptionparseLongsplitShortConcatArgparseShortparseNonOptionshowBuiltinHelpprintErrorclearIsSetadd-superadmin"add-superadmin"Creates a new superadmin user"Creates a new superadmin user"The add-user command will create a new user with superadmin status"The add-user command will create a new user with superadmin status" TODO(desa): Apply mapping to user and update their roles Check to see is user is already a part of the organizationalignmentInfomaxLongLenhasShorthasValueNameterminalColumnsindentdescriptionStartupdateLenUnmarshalerUnmarshalFlagscanHandlershortNameslongNamesretargspositionalcommandpopcheckRequiredestimateCommandaddArgswrAvailableAvailableBufferWriteByteWriteRuneErrorType/Users/austinjaybecker/projects/abeck-go-testing/chronograf/cmd/chronoctl/list.golist-users"list-users"Lists users"Lists users"The list-users command will list all users in the chronograf boltdb instance"The list-users command will list all users in the chronograf boltdb instance"/Users/austinjaybecker/projects/abeck-go-testing/chronograf/cmd/chronoctl/main.goflagsErrgithub.com/jessevdk/go-flags"github.com/jessevdk/go-flags"NewParserErrHelpExitFprintlnStdout/Users/austinjaybecker/projects/abeck-go-testing/chronograf/cmd/chronoctl/util.gobimockstabwritertext/tabwriter"text/tabwriter"github.com/influxdata/influxdb/v2/chronograf/bolt"github.com/influxdata/influxdb/v2/chronograf/bolt"github.com/influxdata/influxdb/v2/chronograf/mocks"github.com/influxdata/influxdb/v2/chronograf/mocks"NewLoggerNewWriter	'\t'ID	Name	Provider	Scheme	SuperAdmin	Organization(s)"ID\tName\tProvider\tScheme\tSuperAdmin\tOrganization(s)"Fprintf%d	%s	%s	%s	%t	%s
"%d\t%s\t%s\t%s\t%t\t%s\n"/Users/austinjaybecker/projects/abeck-go-testing/chronograf/cmd/chronograf/Users/austinjaybecker/projects/abeck-go-testing/chronograf/cmd/chronograf/main.gofecodesrvgithub.com/influxdata/influxdb/v2/chronograf/server"github.com/influxdata/influxdb/v2/chronograf/server"CompleteListenerPprofEnabledCertInfluxDBURLInfluxDBUsernameInfluxDBPasswordKapacitorURLKapacitorUsernameKapacitorPasswordNewSourcesDevelopCannedPathResourcesPathTokenSecretJwksURLUseIDTokenAuthDurationGithubClientIDGithubClientSecretGithubOrgsGoogleClientIDGoogleClientSecretGoogleDomainsPublicURLHerokuClientIDHerokuSecretHerokuOrganizationsGenericNameGenericClientIDGenericClientSecretGenericScopesGenericDomainsGenericAuthURLGenericTokenURLGenericAPIURLGenericAPIKeyAuth0DomainAuth0ClientIDAuth0ClientSecretAuth0OrganizationsAuth0SuperAdminOrgStatusFeedURLCustomLinksReportingDisabledLogLevelBasepathShowVersionUseGithubUseGoogleUseHerokuUseAuth0UseGenericOAuth2githubOAuthgoogleOAuthherokuOAuthgenericOAuthauth0OAuthgenericRedirectURLuseAuthuseTLSNewListenernewBuildersServeChronograf`Chronograf`Options for Chronograf`Options for Chronograf`Chronograf %s (git: %s)
"Chronograf %s (git: %s)\n"Fatalln Build flagsbuildersLayoutBuilderLayoutsStoresSourcesBuildermultiKapacitorBuilderKapacitorStoreDashboardBuilderOrganizationBuilderKapacitorsDashboardsAuthenticatorPrincipalIssuedAtExpireExtendEndpointAuthStyleAuthURLTokenURLClientIDClientSecretRedirectURLScopesAuthCodeURLPasswordCredentialsTokenExchangeTokenSourceRoundTripperRoundTripCookieJarSetCookiesTransportCheckRedirectJardeadlinetransportcheckRedirectmakeHeadersCopierCloseIdleConnectionsPrincipalIDSecretCallbackLoginLogoutAuthCodeOptionsetValueAccessTokenTokenTypeRefreshTokenExpirySetAuthHeaderWithExtraExtraexpired/Users/austinjaybecker/projects/abeck-go-testing/chronograf/dist/Users/austinjaybecker/projects/abeck-go-testing/chronograf/dist/TODO.goAssetDirAssetInfoBindataAssetsDebugAssetsNewDirerrNoAssetsdist The functions defined in this file are placeholders when the binary is compiled without assets. AssetInfo returns an error stating no assets were included in the binary. AssetDir returns nil because there are no assets included in the binary.DefaultContentTypeaddCacheHeaders/Users/austinjaybecker/projects/abeck-go-testing/chronograf/dist/dir.godef Dir functions like http.Dir except returns the content of a default file if not found. NewDir constructs a Dir with a default file Open will return the file in the dir if it exists, or, the Default file otherwise./Users/austinjaybecker/projects/abeck-go-testing/chronograf/dist/dist.goetagfilenamehourminutesecondassetfsgithub.com/elazarl/go-bindata-assetfs"github.com/elazarl/go-bindata-assetfs"FileSystemFileServerCache-Control"Cache-Control"public, max-age=3600"public, max-age=3600""%d%d%d%d%d"`"%d%d%d%d%d"`ETag"ETag"Content-Type"Content-Type"AssetFSgo:generate env GO111MODULE=on go run github.com/kevinburke/go-bindata/go-bindata -o dist_gen.go -ignore 'map|go' -tags assets -pkg dist ../../ui/build/... DebugAssets serves assets via a specified directory Dir is a directory location of asset files Default is the file to serve if file is not found. Handler is an http.FileServer for the Dir BindataAssets serves assets from go-bindata, but, also serves Default if assent doesn't exist This is to support single-page react-apps with its own router. Prefix is prepended to the http file request Default is the file to serve if the file is not found DefaultContentType is the content type of the default file Handler serves go-bindata using a go-bindata-assetfs faÃ§ade addCacheHeaders requests an hour of Cache-Control and sets an ETag based on file size and modtime ServeHTTP wraps http.FileServer by returning a default asset if the asset doesn't exist.  This supports single-page react-apps with its own built-in router.  Additionally, we override the content-type if the Default file is used. def wraps the assets to return the default file if the file doesn't exist If the named asset exists, then return it directly. If this is at / then we just error out so we can return a Directory This directory will then be redirected by go to the /index.html If this is anything other than slash, we just return the default asset.  This default asset will handle the routing. Additionally, because we know we are returning the default asset, we need to set the default asset's content-type./Users/austinjaybecker/projects/abeck-go-testing/chronograf/enterprise/Users/austinjaybecker/projects/abeck-go-testing/chronograf/enterprise/enterprise.goCtrlDataNodeDifferenceLDAPConfigMetaClientNewClientWithTimeSeriesNewClientWithURLNewMetaClientRoleActionToChronografToEnterpriseUserActionUserStoredefaultClientdefaultTransportjsonLDAPConfigparseMetaURLpermissionsDifferenceskipVerifyTransportpasswdcldataSrcdnNewNameTCPAddrHTTPAddrHTTPSchemeDataNodesMetaNodesAddRoleUsersChangePasswordCreateRoleDeleteRoleRemoveRoleUsersSetRolePermsSetRoleUsersSetUserPermsShowClusterUserRolesRingMoveLinkUnlinkdataNodesnextDataNodeclusterpointsctrlinsecurelgmetaURLseriestlsringurlenterprisecontainer/ring"container/ring"net/url"net/url"github.com/influxdata/influxdb/v2/chronograf/influx"github.com/influxdata/influxdb/v2/chronograf/influx"requestLDAPChannelGetLDAPConfigCreateUpdateUserRemoveUserPermsRemoveRolePermsgetRPshowDatabasesshowRetentionPoliciesshowMeasurementspingTimeoutpingwritePointshowUsersgrantPermissionrevokePermissionuserPermissionsupdatePasswordNoPermissions"NoPermissions"ViewAdmin"ViewAdmin"ViewChronograf"ViewChronograf"CreateDatabase"CreateDatabase"CreateUserAndRole"CreateUserAndRole"AddRemoveNode"AddRemoveNode"DropDatabase"DropDatabase"DropData"DropData"ReadData"ReadData"WriteData"WriteData""Rebalance"ManageShard"ManageShard"ManageContinuousQuery"ManageContinuousQuery"ManageQuery"ManageQuery"ManageSubscription"ManageSubscription"Monitor"Monitor"CopyShard"CopyShard"KapacitorAPI"KapacitorAPI"KapacitorConfigAPI"KapacitorConfigAPI""http"https"https" Ctrl represents administrative controls over an Influx Enterprise cluster Client is a device for retrieving time series data from an Influx Enterprise cluster. It is configured using the addresses of one or more meta node URLs. Data node URLs are retrieved automatically from the meta nodes and queries are appropriately load balanced across the cluster. NewClientWithTimeSeries initializes a Client with a known set of TimeSeries. NewClientWithURL initializes an Enterprise client with a URL to a Meta Node. Acceptable URLs include host:port combinations as well as scheme://host:port varieties. TLS is used when the URL contains "https" or when the TLS parameter is set.  authorizer will add the correct `Authorization` headers on the out-bound request. Connect prepares a Client to process queries. It must be called prior to calling Query return early if we already have dataNodes Query retrieves timeseries information pertaining to a specified query. It can be cancelled by using a provided context. Write records points into a time series Users is the interface to the users within Influx Enterprise Roles provide a grouping of permissions given to a grouping of users Permissions returns all Influx Enterprise permission strings nextDataNode retrieves the next available data node parseMetaURL constructs a url from either a host:port combination or a scheme://host:port combo. The optional TLS parameter takes precedence over any TLS preference found in the provided URLconnectMethodKeyproxyschemeonlyH1persistConnrequestAndChanincomparablecancelKeyresponseAndErroraddedGzipcontinueChcallerGonewriteRequesttransportRequestextratraceextraHeaderssetErrorlogfaltcacheKeyconntlsStatebrbwnwritereqchwritechclosechisProxysawEOFreadLimitwriteErrChwriteLoopDoneidleAtidleTimernumExpectedResponsescanceledErrbrokenreusedmutateHeaderFuncshouldRetryRequestaddTLSpconnmaxHeaderResponseSizeisBrokencanceledisReusedgotIdleConnTracecancelRequestcloseConnIfStillIdlemapRoundTripErrorreadLoopreadLoopPeekFailLockedreadResponsewaitForContinuewriteLoopwroteRequestroundTripmarkReusedcloseLockedwantConnQueuewantConnconnectMethodproxyURLtargetSchemetargetAddrproxyAuthcmtlsHostreadybeforeDialafterDialwaitingtryDelivercancelheadPospushBackpopFrontpeekFrontcleanFrontconnLRUListElementlistFrontBacklazyInitinsertinsertValueremovemovePushFrontPushBackInsertBeforeInsertAfterMoveToFrontMoveToBackMoveBeforeMoveAfterPushBackListPushFrontListllremoveOldestPrivateKeySignatureSchemeSupportedSignatureAlgorithmsOCSPStapleLeafleafClientHelloInfoCurveIDCipherSuitesSupportedCurvesSupportedPointsSignatureSchemesSupportedProtosSupportedVersionsSupportsCertificateCertificateRequestInfoAcceptableCAscriClientAuthTypeClientSessionCacheClientSessionStatesessionTicketverscipherSuitemasterSecretserverCertificatesverifiedChainsreceivedAtocspResponsesctsnonceuseByageAddRenegotiationSupportticketKeykeyNameaesKeyhmacKeycreatedCertificatesNameToCertificateGetCertificateGetClientCertificateGetConfigForClientVerifyPeerCertificateVerifyConnectionRootCAsNextProtosClientAuthClientCAsPreferServerCipherSuitesSessionTicketsDisabledSessionTicketKeyMinVersionMaxVersionCurvePreferencesDynamicRecordSizingDisabledRenegotiationKeyLogWritermutexsessionTicketKeysautoSessionTicketKeysticketKeyFromBytesinitLegacySessionTicketKeyRLockedticketKeysSetSessionTicketKeyscipherSuitessupportedVersionsmaxSupportedVersioncurvePreferencessupportsCurvemutualVersiongetCertificateBuildNameToCertificatewriteKeyLogactiveCerthalfConnHashBlockSizeSumciphermacscratchBufnextCiphernextMactrafficSecretsetErrorLockedhcprepareCipherSpecchangeCipherSpecsetTrafficSecretincSeqexplicitNonceLendecryptencryptBufferreadOplastReadtryGrowByReslicereadSliceprevRuneisClienthandshakeFnisHandshakeCompletehandshakeMutexhandshakeErrhaveVershandshakesdidResumepeerCertificatesactiveCertHandlesserverNamesecureRenegotiationresumptionSecretclientFinishedIsFirstcloseNotifyErrcloseNotifySentclientFinishedserverFinishedclientProtocolinoutrawInputhandbufferingsendBufbytesSentpacketsSentretryCountactiveCalltmpNetConnnewRecordHeaderErrorreadRecordreadChangeCipherSpecreadRecordOrCCSretryReadRecordreadFromUntilsendAlertLockedsendAlertmaxPayloadSizeForWritewriteRecordLockedwriteHandshakeRecordwriteChangeCipherRecordreadHandshakehandleRenegotiationhandlePostHandshakeMessagehandleKeyUpdateCloseWritecloseNotifyHandshakeHandshakeContexthandshakeContextconnectionStateLockedmakeClientHelloclientHandshakeloadSessionpickTLSVersionverifyServerCertificategetClientCertificatehandleNewSessionTicketserverHandshakereadClientHelloprocessCertsFromClientencryptTicketdecryptTicketh2TransportidleMucloseIdleidleConnidleConnWaitidleLRUreqMureqCanceleraltMualtProtoconnsPerHostMuconnsPerHostconnsPerHostWaitProxyOnProxyConnectResponseDialContextDialDialTLSContextDialTLSTLSClientConfigTLSHandshakeTimeoutDisableKeepAlivesDisableCompressionMaxIdleConnsMaxIdleConnsPerHostMaxConnsPerHostIdleConnTimeoutResponseHeaderTimeoutExpectContinueTimeoutTLSNextProtoProxyConnectHeaderGetProxyConnectHeaderMaxResponseHeaderBytesWriteBufferSizeReadBufferSizenextProtoOnceh2transporttlsNextProtoWasNilForceAttemptHTTP2writeBufferSizereadBufferSizehasCustomTLSDialeronceSetNextProtoDefaultsuseRegisteredProtocolalternateRoundTripperRegisterProtocolCancelRequestconnectMethodForRequestputOrCloseIdleConnmaxIdleConnsPerHosttryPutIdleConnqueueForIdleConnremoveIdleConnremoveIdleConnLockedsetReqCancelerreplaceReqCancelerdialcustomDialTLSgetConnqueueForDialdialConnFordecConnsPerHostdialConnLeaderAuthedCheckRedirectStructuredcipherSuiteTLS13aeadAEADNonceSizeOverheadSealHashFunckeyLenexpandLabelderiveSecretextractnextTrafficSecrettrafficKeyfinishedHashexportKeyingMaterialrecordTypenewSessionTicketMsgTLS13lifetimemaxEarlyDataserverHelloMsgkeySharesessionIdcompressionMethodocspStaplingticketSupportedsecureRenegotiationSupportedalpnProtocolsupportedVersionserverShareselectedIdentityPresentselectedIdentitysupportedPointscookieselectedGroupclientHelloMsgpskIdentityobfuscatedTicketAgecompressionMethodssupportedCurvessupportedSignatureAlgorithmssupportedSignatureAlgorithmsCertalpnProtocolskeySharesearlyDatapskModespskIdentitiespskBindersmarshalWithoutBindersupdateBinderskeyUpdateMsgupdateRequestedhandshakeMessagetranscriptHashRecordHeaderErrorRecordHeaderCurvePublicKeyECDHcurvepublicKeyboringGenerateKeyNewPrivateKeyNewPublicKeyecdhprivateKeyToPublicKeyPrivateKeyECDHprivateKeypublicKeyOnceECDHPublic/Users/austinjaybecker/projects/abeck-go-testing/chronograf/enterprise/meta.gobodychannelresultctxterrorChresponseChannelrevokeuruserRolesfoundwanthaveswantshavepreserveviarespsioutilcrypto/tls"crypto/tls"io/ioutil"io/ioutil"json:"enabled"`json:"enabled"`json:"structured"`json:"structured"`/ldap/v1/config"/ldap/v1/config""GET"CancelFuncWithTimeout2000000000ReadAll/show-cluster"/show-cluster"/user"/user"no user found"no user found"create"create"change-password"change-password""delete"remove-permissions"remove-permissions"add-permissions"add-permissions"/role"/role"no role found"no role found"add-users"add-users"remove-users"remove-users"NewReader"POST"NewRequestapplication/json"application/json"too many redirects"too many redirects""Authorization" Shared transports for all clients to prevent leaking connections MetaClient represents a Meta node in an Influx Enterprise cluster NewMetaClient represents a meta node in an Influx Enterprise cluster LDAPConfig represents the configuration for ldap from influxdb GetLDAPConfig get the current ldap config response from influxdb enterprise ShowCluster returns the cluster configuration (not health) Users gets all the users.  If name is not nil it filters for a single user User returns a single Influx Enterprise user CreateUser adds a user to Influx Enterprise ChangePassword updates a user's password in Influx Enterprise CreateUpdateUser is a helper function to POST to the /user Influx Enterprise endpoint DeleteUser removes a user from Influx Enterprise RemoveUserPerms revokes permissions for a user in Influx Enterprise SetUserPerms removes permissions not in set and then adds the requested perms first, revoke all the permissions the user currently has, but, shouldn't... ... next, add any permissions the user should have UserRoles returns a map of users to all of their current roles Roles gets all the roles.  If name is not nil it filters for a single role Role returns a single named role CreateRole adds a role to Influx Enterprise DeleteRole removes a role from Influx Enterprise RemoveRolePerms revokes permissions from a role SetRolePerms removes permissions not in set and then adds the requested perms to role first, revoke all the permissions the role currently has, but, ... next, add any permissions the role should have SetRoleUsers removes users not in role and then adds the requested users to role Difference compares two sets and returns a set to be removed and a set to be added AddRoleUsers updates a role to have additional users. No permissions to add, so, role is in the right state RemoveRoleUsers updates a role to remove some users. Post is a helper function to POST to Influx Enterprise Do is a helper function to interface with Influx Enterprise's Meta API Meta servers will redirect (307) to leader. We need special handling to preserve authentication headers. AuthedCheckRedirect tries to follow the Influx Enterprise pattern of redirecting to the leader but preserving authentication headers. Do is a cancelable function to interface with Influx Enterprise's Meta API/Users/austinjaybecker/projects/abeck-go-testing/chronograf/enterprise/roles.go RolesStore uses a control client operate on Influx Enterprise roles.  Roles are groups of permissions applied to groups of users Add creates a new Role in Influx Enterprise This must be done in three smaller steps: creating, setting permissions, setting users. Delete the Role from Influx Enterprise Get retrieves a Role if name exists. Hydrate all the users to gather their permissions and their roles. Update the Role's permissions and roles All is all Roles in influx ToChronograf converts enterprise roles to chronograf/Users/austinjaybecker/projects/abeck-go-testing/chronograf/enterprise/types.gojson:"data"`json:"data"`json:"meta"`json:"meta"`json:"tcpAddr"`json:"tcpAddr"`json:"httpAddr"`json:"httpAddr"`json:"httpScheme"`json:"httpScheme"`json:"addr"`json:"addr"`json:"newName,omitempty"`json:"newName,omitempty"`json:"roles,omitempty"`json:"roles,omitempty"`json:"role"`json:"role"` Cluster is a collection of data nodes and non-data nodes within a Plutonium cluster. DataNode represents a data node in an Influx Enterprise Cluster Meta store ID. RPC addr, e.g., host:8088. Client addr, e.g., host:8086. "http" or "https" for HTTP addr. The cluster status of the node. Node represent any meta or data node in an Influx Enterprise cluster Permissions maps resources to a set of permissions. Specifically, it maps a database to a set of permissions User represents an enterprise user. Users represents a set of enterprise users. UserAction represents and action to be taken with a user. Roles is a set of roles RoleAction represents an action to be taken with a role. Error is JSON error message return by Influx Enterprise's meta API./Users/austinjaybecker/projects/abeck-go-testing/chronograf/enterprise/users.gocrquery must specify name"query must specify name" UserStore uses a control client operate on Influx Enterprise users Add creates a new User in Influx Enterprise Delete the User from Influx Enterprise Num of users in Influx For now we are removing all users from a role being returned. Only allow one type of change at a time. If it is a password change then do it and return without any changes to permissions Make a list of the roles we want this user to have: Find the list of all roles this user is currently in Make a list of the roles the user currently has Calculate the roles the user will be removed from and the roles the user will be added to. First, add the user to the new roles ... and now remove the user from an extra roles All is all users in influx ToEnterprise converts chronograf permission shape to enterprise Enterprise uses empty string as the key for all databases ToChronograf converts enterprise permissions shape to chronograf shape/Users/austinjaybecker/projects/abeck-go-testing/chronograf/filestore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/filestore/apps.goAppExtAppsDashExtKapExtNewAppsNewDashboardsNewKapacitorsNewOrganizationsOrgExtSrcExtcreateLayoutdashboardFileenvenvironfileNamekapacitorFileloadloadFilesourceFiletemplatedtemplatedFromEnvdirnameidToFilefilesfilestore.json".json"%s%s"%s%s"ReadFileCreateFileMarshalIndent    "    "ExtUnable to generate ID"Unable to generate ID"Unable to write layout:"Unable to write layout:"Unable to remove layout:"Unable to remove layout:"Unable to read file"Unable to read file"File is not a layout"File is not a layout" AppExt is the the file extension searched for in the directory for layout files Apps are canned JSON layouts.  Implements LayoutsStore. Dir is the directory contained the pre-canned applications. Load loads string name and return a Layout Filename takes dir and layout and returns loadable file Create will write layout to file. ReadDir reads the directory named by dirname and returns a list of directory entries sorted by filename. Remove file IDs generate unique ids for new application layouts NewApps constructs a layout store wrapping a file system directory All returns all layouts from the directory We want to load all files we can. Add creates a new layout within the directory Delete removes a layout file from the directory Get returns an app file from the layout directory Update replaces a layout from the file system directory idToFile takes an id and finds the associated filename Because the entire layout information is not known at this point, we need to try to find the name of the file through matching the ID in the layout content with the ID passed.findOrg/Users/austinjaybecker/projects/abeck-go-testing/chronograf/filestore/dashboards.gogenID.dashboard".dashboard"resource %s not found"resource %s not found""dashboard"AtoiUnable to convert ID"Unable to convert ID"Invalid Dashboard: "Invalid Dashboard: "Unable to write dashboard:"Unable to write dashboard:"Unable to remove dashboard:"Unable to remove dashboard:"File is not a dashboard"File is not a dashboard" DashExt is the the file extension searched for in the directory for dashboard files Dashboards are JSON dashboards stored in the filesystem Dir is the directory containing the dashboards. Load loads string name and dashboard passed in as interface Create will write dashboard to file. IDs generate unique ids for new dashboards NewDashboards constructs a dashboard store wrapping a file system directory All returns all dashboards from the directory Add creates a new dashboard within the directory Delete removes a dashboard file from the directory Get returns a dashboard file from the dashboard directory Update replaces a dashboard from the file system directory Because the entire dashboard information is not known at this point, we need to try to find the name of the file through matching the ID in the dashboard/Users/austinjaybecker/projects/abeck-go-testing/chronograf/filestore/environ.goenvVarenvVarsEnvironSplitN="=" environ returns a map of all environment variables in the running process/Users/austinjaybecker/projects/abeck-go-testing/chronograf/filestore/kapacitors.gokapacitorfmtErrkapacitors.kap".kap"error loading kapacitor configuration from %v:
%v"error loading kapacitor configuration from %v:\n%v""kapacitor"Invalid Server: "Invalid Server: "Unable to write kapacitor:"Unable to write kapacitor:"Unable to remove kapacitor:"Unable to remove kapacitor:"File is not a kapacitor"File is not a kapacitor" KapExt is the the file extension searched for in the directory for kapacitor files Kapacitors are JSON kapacitors stored in the filesystem Dir is the directory containing the kapacitors. Create will write kapacitor to file. IDs generate unique ids for new kapacitors NewKapacitors constructs a kapacitor store wrapping a file system directory All returns all kapacitors from the directory Add creates a new kapacitor within the directory Delete removes a kapacitor file from the directory Get returns a kapacitor file from the kapacitor directory Update replaces a kapacitor from the file system directory Because the entire kapacitor information is not known at this point, we need to try to find the name of the file through matching the ID in the kapacitor/Users/austinjaybecker/projects/abeck-go-testing/chronograf/filestore/organizations.go.org".org"unable to add organizations to the filesystem"unable to add organizations to the filesystem"unable to delete an organization from the filesystem"unable to delete an organization from the filesystem"unable to update organizations on the filesystem"unable to update organizations on the filesystem"unable to create default organizations on the filesystem"unable to create default organizations on the filesystem"unable to get default organizations from the filestore"unable to get default organizations from the filestore" OrgExt is the the file extension searched for in the directory for org files Organizations are JSON orgs stored in the filesystem Dir is the directory containing the orgs. Load loads string name and org passed in as interface NewOrganizations constructs a org store wrapping a file system directory All returns all orgs from the directory Get returns a org file from the org directory Add is not allowed for the filesystem organization store Delete is not allowed for the filesystem organization store Update is not allowed for the filesystem organization store CreateDefault is not allowed for the filesystem organization store DefaultOrganization is not allowed for the filesystem organization store findOrg takes an OrganizationQuery and finds the associated filename Because the entire org information is not known at this point, we need to try to find the name of the file through matching the ID or name in the org/Users/austinjaybecker/projects/abeck-go-testing/chronograf/filestore/sources.go.src".src"error loading source configuration from %v:
%v"error loading source configuration from %v:\n%v""source"Invalid Source: "Invalid Source: "Unable to write source:"Unable to write source:"Unable to remove source:"Unable to remove source:"File is not a source"File is not a source" SrcExt is the the file extension searched for in the directory for source files Sources are JSON sources stored in the filesystem Dir is the directory containing the sources. Create will write source to file. IDs generate unique ids for new sources NewSources constructs a source store wrapping a file system directory All returns all sources from the directory Add creates a new source within the directory Delete removes a source file from the directory Get returns a source file from the source directory Update replaces a source from the file system directory Because the entire source information is not known at this point, we need to try to find the name of the file through matching the ID in the source/Users/austinjaybecker/projects/abeck-go-testing/chronograf/filestore/templates.gofilenameshtml/template"html/template"TreeListNodeBuildercopyCheckwriteToNodesCopyListlexeritemitemTypelexOptionsemitCommentbreakOKcontinueOKleftDelimrightDelimatEOFparenDepthstartLineinsideActionthisItememitemitItemignoreacceptacceptRunerrorfnextItematRightDelimatTerminatorscanNumberParseNamefuncslexpeekCountvarstreeSetactionLinerangeDepthnewListnewTextnewCommentnewPipelinenewActionnewCommandnewVariablenewDotnewNilnewFieldnewChainnewBoolnewNumbernewEndnewElsenewIfnewBreaknewContinuenewRangenewWithnewTemplatebackup2backup3nextNonSpacepeekNonSpaceErrorContextexpectexpectOneOfunexpectedrecoverstartParsestopParseparseparseDefinitionitemListtextOrActionclearActionLinebreakControlcontinueControlpipelinecheckPipelineparseControlifControlrangeControlwithControlendControlelseControlblockControltemplateControlparseTemplateNameoperandtermhasFunctionpopVarsuseVarmissingKeyActionmissingKeyFuncMaptmplmuTmplmuFuncsparseFuncsexecFuncsExecuteTemplateexecuteDefinedTemplatesParseFilesParseGlobParseFSsetOptionAddParseTreeDelimsFuncsassociatenameSpaceescaperdelimurlPartjsCtxattrelementeqmangleActionNodePipeNodeVariableNodeIdentCommandNodeIsAssignDeclCmdsCopyPipeTemplateNodeTextNoderangeContextouterbreakscontinuesderivedcalledactionNodeEditstemplateNodeEditstextNodeEditsescapeescapeActionescapeBranchescapeListescapeListConditionallyescapeTemplateescapeTreecomputeOutCtxescapeTemplateBodyescapeTexteditActionNodeeditTemplateNodeeditTextNodearbitraryTemplateescapedescescapeErrcheckCanParselookupAndEscapeTemplatemissingkey=error"missingkey=error" templated returns all files templated using data If a key in the file exists but is not in the data we immediately fail with a missing key error templatedFromEnv returns all files templated against environment variablesDotNodeBoolNodeTrueBreakNodeRangeNodeBranchNodeElseListendNodestateFnCommentNodeChainNodeIfNodeWithNodeFSelseNodeNilNodeFieldNodeNumberNodeIsIntIsUintIsFloatIsComplexComplex128simplifyComplexContinueNodeStringNodeQuoted/Users/austinjaybecker/projects/abeck-go-testing/chronograf/id/Users/austinjaybecker/projects/abeck-go-testing/chronograf/id/time.goNewTimetm tm generates an id based on current time NewTime builds a chronograf.ID generator based on current time Generate creates a string based on the current time as an integer/Users/austinjaybecker/projects/abeck-go-testing/chronograf/id/uuid.gouuidgithub.com/satori/go.uuid"github.com/satori/go.uuid"decodeCanonicaldecodeHashLikedecodeBraceddecodeURNdecodePlainVariantSetVersionSetVariantNewV4 UUID generates a V4 uuid Generate creates a UUID v4 string/Users/austinjaybecker/projects/abeck-go-testing/chronograf/influx/Users/austinjaybecker/projects/abeck-go-testing/chronograf/influx/annotations.goAllAnnotationsAllPrivilegesAllowAllAdminAllowAllDBAllowReadAllowWriteAnnotationsDBBearerJWTDefaultAuthorizationDefaultMeasurementGetAnnotationIDJWTNewAnnotationStoreNoAuthorizationNoPrivilegesParseTimeTimeRangeAsEpochNanoToGrantToInfluxQLToPrivToRevokeWhereTokenadminPermsannotationResultescapeFieldStringsescapeKeysescapeMeasurementescapeTagValueshasTimeRangeinfluxResultsisDurationisNowisPreviousTimeisStringisTagFilterisTagLogicisTimeisTimeRangeisVarRefpingResultshortDurshowResultssupportedFuncstagFiltertimeRangeVisitortoDeletedPointtoLineProtocoltoPointqueryAnnotationsannoannosresultsSeriesAnnotationssort"sort"SELECT "start_time", "modified_time_ns", "text", "type", "id" FROM "annotations" WHERE "deleted"=false AND time >= %dns and "start_time" <= %d ORDER BY time DESC`SELECT "start_time", "modified_time_ns", "text", "type", "id" FROM "annotations" WHERE "deleted"=false AND time >= %dns and "start_time" <= %d ORDER BY time DESC`SELECT "start_time", "modified_time_ns", "text", "type", "id" FROM "annotations" WHERE "id"='%s' AND "deleted"=false ORDER BY time DESC`SELECT "start_time", "modified_time_ns", "text", "type", "id" FROM "annotations" WHERE "id"='%s' AND "deleted"=false ORDER BY time DESC`"chronograf"annotations"annotations""ns"deleted"deleted"start_time"start_time"modified_time_ns"modified_time_ns"type"type"index %d does not exist in values"index %d does not exist in values"Numbervalue at index %d is not int64, but, %T"value at index %d is not int64, but, %T"value at index %d is not string, but, %T"value at index %d is not string, but, %T"json:"series"`json:"series"`modTime AllAnnotations returns all annotations from the chronograf database GetAnnotationID returns all annotations from the chronograf database where id is %s AnnotationsDB is chronograf.  Perhaps later we allow this to be changed DefaultRP is autogen. Perhaps later we allow this to be changed DefaultMeasurement is annotations. AnnotationStore stores annotations within InfluxDB NewAnnotationStore constructs an annoation store with a client All lists all Annotations Update replaces annotation; if the annotation's time is different, it also removes the previous annotation If the updated annotation has a different time, then, we must delete the previous annotation queryAnnotations queries the chronograf db and produces all annotations annotationResult is an intermediate struct to track the latest modified time of an annotation modTime is bookkeeping to handle the case when an update fails; the latest modTime will be the record returned Annotations converts AllAnnotations query to annotations If there are two annotations with the same id, take the annotation with the latest modification time This is to prevent issues when an update or delete fails. Updates and deletes are multiple step queries.RetentionPoliciesMeasurementsReplacerreplaceroldnewbuildOnceExprOkVisitResultsVisitor/Users/austinjaybecker/projects/abeck-go-testing/chronograf/influx/authorization.gosharedSecretjwtgithub.com/dgrijalva/jwt-go"github.com/dgrijalva/jwt-go"Bearer "Bearer "SigningMethodAlgClaimsSignedStringSigningString"typ""JWT"alg"alg"SigningMethodHMACSigningMethodHS512MapClaimsVerifyAudienceVerifyExpiresAtVerifyIssuedAtVerifyIssuerVerifyNotBefore"username""exp" Authorizer adds optional authorization header to request Set may manipulate the request by adding the Authorization header NoAuthorization does not add any authorization headers Set does not add authorization DefaultAuthorization creates either a shared JWT builder, basic auth or Noop Optionally, add the shared secret JWT token creation BasicAuth adds Authorization: Basic to the request header Set adds the basic auth headers to the request BearerJWT is the default Bearer for InfluxDB Set adds an Authorization Bearer to the request if has a shared secret Token returns the expected InfluxDB JWT signed with the sharedSecret JWT returns a token string accepted by InfluxDB using the sharedSecret as an Authorization: Bearer header/Users/austinjaybecker/projects/abeck-go-testing/chronograf/influx/databases.gorpbufferqueryResretentionPoliciesmeasurementsshowCREATE DATABASE "%s"`CREATE DATABASE "%s"`DROP DATABASE "%s"`DROP DATABASE "%s"`unknown retention policy"unknown retention policy"CREATE RETENTION POLICY "%s" ON "%s" DURATION %s REPLICATION %d`CREATE RETENTION POLICY "%s" ON "%s" DURATION %s REPLICATION %d`%s SHARD DURATION %s`%s SHARD DURATION %s`%s DEFAULT`%s DEFAULT`ALTER RETENTION POLICY "%s" ON "%s"`ALTER RETENTION POLICY "%s" ON "%s"` DURATION " DURATION " REPLICATION " REPLICATION " SHARD DURATION " SHARD DURATION " DEFAULT" DEFAULT"DROP RETENTION POLICY "%s" ON "%s"`DROP RETENTION POLICY "%s" ON "%s"`SHOW DATABASES`SHOW DATABASES`SHOW RETENTION POLICIES ON "%s"`SHOW RETENTION POLICIES ON "%s"`SHOW MEASUREMENTS ON "%s"`SHOW MEASUREMENTS ON "%s"` LIMIT %d" LIMIT %d" OFFSET %d" OFFSET %d" AllDB returns all databases from within Influx CreateDB creates a database within Influx DropDB drops a database within Influx AllRP returns all the retention policies for a specific database CreateRP creates a retention policy for a specific database UpdateRP updates a specific retention policy for a specific database The ALTER RETENTION POLICIES statements puts the error within the results itself So, we have to crack open the results to see what happens At last, we can check if there are any error strings DropRP removes a specific retention policy for a specific database GetMeasurements returns measurements in a specified database, paginated by optional limit and offset. If no limit or offset is provided, it defaults to a limit of 100 measurements with no offset./Users/austinjaybecker/projects/abeck-go-testing/chronograf/influx/influx.gopointtsdbTypedecErrlogsresponselperrChanjson:"error,omitempty"`json:"error,omitempty"`"query""proxy""command""db""rp""q"epoch"epoch"ms"ms"InjectToHTTPRequestError setting authorization header "Error setting authorization header "received status code %d from server: err: %s"received status code %d from server: err: %s"EOF"EOF"influx_status"influx_status"Error parsing results from influxdb: err:"Error parsing results from influxdb: err:"Received non-200 response from influxdb"Received non-200 response from influxdb"received status code %d from server"received status code %d from server"roles not support in open-source InfluxDB.  Roles are support in Influx Enterprise"roles not support in open-source InfluxDB.  Roles are support in Influx Enterprise""ping"X-Influxdb-Build"X-Influxdb-Build"ENT"ENT"X-Influxdb-Version"X-Influxdb-Version"-c"-c"relay"relay"hinted handoff queue not empty"hinted handoff queue not empty"database not found"database not found"text/plain; charset=utf-8"text/plain; charset=utf-8" Client is a device for retrieving time series data from an InfluxDB instance Response is a partial JSON decoded InfluxQL response used to check for some errors MarshalJSON returns the raw results bytes from the response ignore this error if we got an invalid status code If we got a valid decode error, send that back If we don't have an error in our json response, and didn't get statusOK then send back an error Query issues a request to a configured InfluxDB instance for time series information specified by query. Queries must be "fully-qualified," and include both the database and retention policy. In-flight requests can be cancelled using the provided context. Connect caches the URL and optional Bearer Authorization for the data source Only allow acceptance of all certs if the scheme is https AND the user opted into to the setting. Users transforms InfluxDB into a user store Roles aren't support in OSS Ping hits the influxdb ping endpoint and returns the type of influx Version hits the influxdb ping endpoint and returns the version of influx Type hits the influxdb ping endpoint and returns the type of influx running Write POSTs line protocol to a database and retention policy Some influxdb errors should not be treated as errors This is an informational message If the database was not found, try to recreate it: retry the write/Users/austinjaybecker/projects/abeck-go-testing/chronograf/influx/lineprotocol.gomeasurementtagsNewReplacer`,`\,`\,` ` `\ `\ `"`"`\"`\"``=`\=`\=`\`\`\\`\\`measurement required to write point"measurement required to write point"at least one field required to write point"at least one field required to write point"%s=%s"%s=%s"Stringsint16int8%s=%di"%s=%di"uint32uint16uint8uint%s=%du"%s=%du"float32%s=%f"%s=%f"%s="%s"`%s="%s"`%s=%t"%s=%t",%s",%s" %s" %s" %d" %d" to  it is faster to insert data into influx db if the tags are sorted/Users/austinjaybecker/projects/abeck-go-testing/chronograf/influx/now.go Now returns the current time/Users/austinjaybecker/projects/abeck-go-testing/chronograf/influx/permissions.goadminreplicationsdurationdurationprivprepositionaahasReadhasWriteWRITE"WRITE"READ"READ"ALL"ALL"NO PRIVILEGES"NO PRIVILEGES"ALL PRIVILEGES"ALL PRIVILEGES"%s ALL PRIVILEGES %s "%s"`%s ALL PRIVILEGES %s "%s"`%s ALL PRIVILEGES ON "%s" %s "%s"`%s ALL PRIVILEGES ON "%s" %s "%s"`%s %s ON "%s" %s "%s"`%s %s ON "%s" %s "%s"`REVOKE"REVOKE"FROM"FROM"GRANT"GRANT"TO"TO" AllowAllDB means a user gets both read and write permissions for a db AllowAllAdmin means a user gets both read and write permissions for an admin AllowRead means a user is only able to read the database. AllowWrite means a user is able to only write to the database NoPrivileges occasionally shows up as a response for a users grants. AllPrivileges means that a user has both read and write perms All means a user has both read and write perms. Alternative to AllPrivileges Read means a user can read a database Write means a user can write to a database Permissions return just READ and WRITE for OSS Influx showResults is used to deserialize InfluxQL SHOW commands Users converts SHOW USERS to chronograf Users Databases converts SHOW DATABASES to chronograf Databases Measurements converts SHOW MEASUREMENTS to chronograf Measurement Permissions converts SHOW GRANTS to chronograf.Permissions sometimes influx reports back NO PRIVILEGES ToInfluxQL converts the permission into InfluxQL All privileges are to be removed for this user on this database ToRevoke converts the permission into InfluxQL revokes ToGrant converts the permission into InfluxQL grants ToPriv converts chronograf allowances to InfluxQL Difference compares two permission sets and returns a set to be revoked and a set to be added/Users/austinjaybecker/projects/abeck-go-testing/chronograf/influx/queries/Users/austinjaybecker/projects/abeck-go-testing/chronograf/influx/queries/select.goBinaryExprBooleanLiteralConditionDimensionDimensionsDistinctIntegerLiteralLimitsListLiteralLiteralJSONNumberLiteralParenExprParseSelectRegexLiteralSelectStatementSortFieldSortFieldsTimeLiteralVarRefWildcardliteralJSONtokPrecedenceisOperatorLHSRHSlhsrhsValNewCallFillOptionlitlitTypeValsIsTimeLiteralToTimeLiteralDataTypeLessThanRequiredPrivilegesAliasNamesorderAscendingTargetRegexIsTargetSystemIteratorSLimitSOffsetgroupByIntervalIsRawQueryFillValueTimeAliasOmitTimeStripNameEmitNameDedupeTimeAscendingTimeFieldNameRewriteFieldsRewriteRegexConditionsRewriteDistinctRewriteTimeFieldsColumnNamesFieldExprByNameReduceHasWildcardHasFieldWildcardHasDimensionWildcardGroupByIntervalGroupByOffsetSetTimeRangerewriteWithoutTimeDimensionsdimgroupBysinfluxqlreflectregexp"reflect""regexp"github.com/influxdata/influxql"github.com/influxdata/influxql"json:"expr"`json:"expr"`json:"val"`json:"val"`ExecutionPrivilegesExecutionPrivilegePrivilegeAdminbufScannerScannerRuneScannerCharunreadcurrscanWhitespaceskipUntilNewlineskipUntilEndCommentscanIdentscanStringScanRegexscanDigitsscanFuncUnscanSetParamsParseQueryParseStatementparseSetPasswordUserStatementparseKillQueryStatementparseCreateSubscriptionStatementparseCreateRetentionPolicyStatementparseAlterRetentionPolicyStatementParseIntParseUInt64ParseDurationParseIdentParseIdentListparseSegmentedIdentsparseStringparseStringListparseRevokeStatementparseRevokeOnStatementparseRevokeAdminStatementparseGrantStatementparseGrantOnStatementparseGrantAdminStatementparsePrivilegeparseSelectStatementparseTargetparseDeleteStatementparseShowSeriesStatementparseShowSeriesCardinalityStatementparseShowMeasurementCardinalityStatementparseShowMeasurementsStatementparseShowQueriesStatementparseShowRetentionPoliciesStatementparseShowTagKeyCardinalityStatementparseShowTagKeysStatementparseShowTagValuesStatementparseShowTagValuesCardinalityStatementparseTagKeyExprparseShowUsersStatementparseShowSubscriptionsStatementparseShowFieldKeyCardinalityStatementparseShowFieldKeysStatementparseDropMeasurementStatementparseDropSeriesStatementparseDropShardStatementparseShowContinuousQueriesStatementparseGrantsForUserStatementparseShowDatabasesStatementparseCreateContinuousQueryStatementparseCreateDatabaseStatementparseDropDatabaseStatementparseDropSubscriptionStatementparseDropRetentionPolicyStatementparseCreateUserStatementparseDropUserStatementparseExplainStatementparseShowShardGroupsStatementparseShowShardsStatementparseShowStatsStatementparseShowDiagnosticsStatementparseDropContinuousQueryStatementparseFieldsparseFieldparseAliasparseSourcespeekRuneparseSourceparseConditionparseDimensionsparseDimensionparseFillparseLocationParseOptionalTokenAndIntparseOrderByparseSortFieldsparseSortFieldParseVarRefParseExprparseUnaryExprparseRegexparseCallparseResampleScanIgnoreWhitespaceconsumeWhitespaceparseTokenserror parsing query: not a SELECT statement"error parsing query: not a SELECT statement"json:"op"`json:"op"`json:"lhs"`json:"lhs"`json:"rhs"`json:"rhs"`"binary""call"{"expr": "distinct", "val": "%s"}`{"expr": "distinct", "val": "%s"}`NullFillnull"null"NoFillnone"none"PreviousFillprevious"previous"LinearFillNumberFill%v"%v"{"expr": "paren", "val": %s}`{"expr": "paren", "val": %s}`"literal"boolean"boolean""duration"integer"integer"number"number"regex"regex""%s"`"%s"`"string"RFC3339Nano2006-01-02T15:04:05.999999999Z07:00Unknown{"expr": "reference", "val": "%s", "type": "%s"}`{"expr": "reference", "val": "%s", "type": "%s"}`{"expr": "reference",  "val": "%s"}`{"expr": "reference",  "val": "%s"}`{"expr": "wildcard", "val": "%s"}`{"expr": "wildcard", "val": "%s"}`error marshaling query: unknown type %s"error marshaling query: unknown type %s"json:"regex,omitempty"`json:"regex,omitempty"`"measurement"error marshaling source.  Subqueries not supported yet"error marshaling source.  Subqueries not supported yet"json:"alias,omitempty"`json:"alias,omitempty"`json:"column"`json:"column"`ascending"ascending"descending"descending"Orderjson:"order,omitempty"`json:"order,omitempty"`json:"limit,omitempty"`json:"limit,omitempty"`json:"offset,omitempty"`json:"offset,omitempty"`json:"slimit,omitempty"`json:"slimit,omitempty"`json:"soffset,omitempty"`json:"soffset,omitempty"`"fields"groupBy"groupBy"condition"condition"limits"limits"orderbys"orderbys"time dimension offset function must be now()"time dimension offset function must be now()"time dimension expected 1 or 2 arguments"time dimension expected 1 or 2 arguments"time dimension must have duration argument"time dimension must have duration argument""now"time dimension offset now() function requires no arguments"time dimension offset now() function requires no arguments"now()"now()"time dimension offset must be duration or now()"time dimension offset must be duration or now()"Intervaljson:"interval"`json:"interval"`json:"time,omitempty"`json:"time,omitempty"`json:"tags,omitempty"`json:"tags,omitempty"` TODO: I don't think list is right TODO: Handle subqueries Make sure there is exactly one argument. Ensure the argument is a duration.SetPasswordUserStatementCreateRetentionPolicyStatementShardGroupDurationDefaultDatabaseStatementsShowShardGroupsStatementDropUserStatementShowRetentionPoliciesStatementGrantStatementOnRevokeStatementDropShardStatementGrantAdminStatementShowFieldKeysStatementDropRetentionPolicyStatementShowShardsStatementShowQueriesStatementDropContinuousQueryStatementLiteralShowContinuousQueriesStatementRevokeAdminStatementCreateUserStatementShowDiagnosticsStatementModuleAlterRetentionPolicyStatementtargetRequirementDropSubscriptionStatementShowStatsStatementValuerFieldMapperTypeMapperMapTypeFieldDimensionsShowUsersStatementDropMeasurementStatementDropDatabaseStatementShowDatabasesStatementExplainStatementAnalyzeDropSeriesStatementShowTagKeysStatementCreateContinuousQueryStatementResampleEveryResampleForCreateDatabaseStatementRetentionPolicyCreateRetentionPolicyDurationRetentionPolicyReplicationRetentionPolicyShardGroupDurationShowGrantsForUserStatementCreateSubscriptionStatementDestinationsKillQueryStatementQueryIDShowSubscriptionsStatementShowMeasurementsStatement/Users/austinjaybecker/projects/abeck-go-testing/chronograf/influx/query.gomaxmintmaxtmintrangeinfluxQLnowValwherereffldArgsfldintervalTimeitsDashboardTimelogicqcreducedhasDurbintflhsOKrhsOKtlLHStlRHShasValueTimeRangeMinMaxIntersectMinTimeMaxTimeMinTimeNanoMaxTimeNanoConditionExpr-9223372036854775806WHERE"WHERE"ToUppernot a relative duration"not a relative duration"NowValuer8675309ns"8675309ns":dashboardTime:":dashboardTime:"now() - 15m"now() - 15m"auto"auto""field"FormatIntFormatFloat'f'wildcard"wildcard"func"func"=="=="now() - "now() - "ToLowerSUBLTLTEGTGTEWalkORANDEQNEQ!="!="mean"mean"median"median""count""min""max"sum"sum""first""last"spread"spread"stddev"stddev"percentile"percentile"top"top"bottom"bottom"HasSuffixm0s"m0s"h0m"h0m" TimeRangeAsEpochNano extracts the min and max epoch times from the expression TODO(desa): is this OK? WhereToken is used to parse the time expression from an influxql query ParseTime extracts the duration of the time range of the query Convert changes an InfluxQL query to a QueryConfig Query config doesn't support limits Query config doesn't support sorting Query config doesn't allow SELECT INTO Query config only allows selecting from one source at a time. Add fill to queryConfig only if there's a `GROUP BY time` only support certain query config functions If the condition has a time range we report back its duration tagFilter represents a single tag that is filtered by some condition either side can be now just comparing to now Either side could be time timeRangeVisitor implements influxql.Visitor to search for time ranges there must only be one kind of ops Either tag op value or value op tag shortDur converts duration into the queryConfig duration format/Users/austinjaybecker/projects/abeck-go-testing/chronograf/influx/users.goCREATE USER "%s" WITH PASSWORD '%s'`CREATE USER "%s" WITH PASSWORD '%s'`DROP USER "%s"`DROP USER "%s"`SHOW USERS`SHOW USERS`SHOW GRANTS FOR "%s"`SHOW GRANTS FOR "%s"`SET PASSWORD for "%s" = '%s'`SET PASSWORD for "%s" = '%s'` Add a new User in InfluxDB Delete the User from InfluxDB The DROP USER statement puts the error within the results itself All users in influx For all users we need to look up permissions to add to the user. Num is the number of users in DB showUsers runs SHOW USERS InfluxQL command and returns chronograf users. The SET PASSWORD statements puts the error within the results itself/Users/austinjaybecker/projects/abeck-go-testing/chronograf/integrations/Users/austinjaybecker/projects/abeck-go-testing/chronograf/integrations/utils.gohostAndPortjsonEqualnewBoltFileportportStrxso1o2s1s2httptestintegrationsnet/http/httptest"net/http/httptest"github.com/google/go-cmp/cmp"github.com/google/go-cmp/cmp"ConnStateisDiscardSetOutputformatHeaderOutputPrintPrintlnPaniclnSetFlagsSetPrefixconnReaderCondnotifyListnotifycopyCheckerLcheckerWaitSignalBroadcasthasBytebyteBufinReadabortedremainunlockstartBackgroundReadbackgroundReadabortPendingReadsetReadLimitsetInfiniteReadLimithitReadLimithandleReadErroralign64cancelCtxrwcremoteAddrbufrbufwlastMethodcurReqcurStatehijackedvhijackedhijackLockedreadRequestfinalFlushcloseWriteAndWaitsetStategetStateserveWaitGroupwgDisableGeneralOptionsHandlerTLSConfigReadTimeoutReadHeaderTimeoutWriteTimeoutIdleTimeoutMaxHeaderBytesErrorLogBaseContextConnContextinShutdowndisableKeepAlivesnextProtoErrlistenersactiveConnonShutdownlistenerGroupnewConnmaxHeaderBytesinitialReadLimitSizetlsHandshakeTimeoutRegisterOnShutdowncloseIdleConnscloseListenersLockedListenAndServeshouldConfigureHTTP2ForServeServeTLStrackListenertrackConnidleTimeoutreadHeaderTimeoutdoKeepAlivesshuttingDownSetKeepAlivesEnabledListenAndServeTLSsetupHTTP2_ServeTLSsetupHTTP2_ServeonceSetNextProtoDefaults_ServeEnableHTTP2certificateconnsStartTLSlogCloseHangDebugInfoCloseClientConnectionsgoServewrapcloseConncloseConnChanNewServer:":"TempFilechronograf-bolt-"chronograf-bolt-"ResultNumSameNumDiffSimilarPathSteppapointerPathUintptrmyPushreporterreporterIfaceresultFlagsByIgnoreByMethodByFuncByCyclePopStepPushStepReportrecCheckerrcdynCheckerdcexportercurPathcurPtrsreportersexportersprocessOptionstatelessComparecompareAnytryOptionstryMethodcallTRFunccallTTBFunccompareStructcompareSlicecompareMapcomparePtrcompareInterfacereportapplicableOptionTransformtransformpathStepvxvytransformerisCorefncisFilteredtransReadWriterchunkWriterheaderwroteHeaderchunkingcwwriteHeaderreqBodywroteContinuewants10KeepAlivecanWriteContinuewriteContinueMuhandlerHeadercalledHeaderwrittencontentLengthcloseAfterReplyrequestBodyLimitHittrailershandlerDonedateBufclenBufstatusBufcloseNotifyChdidCloseNotifyfinalTrailersdeclareTrailerrequestTooLargebodyAllowedfinishRequestshouldReuseConnectionclosedRequestBodyEarlyFlushErrorsendExpectationFailedHijackCloseNotify/Users/austinjaybecker/projects/abeck-go-testing/chronograf/kapacitor.gojson:"stateChangesOnly"`json:"stateChangesOnly"`json:"useFlapping"`json:"useFlapping"`json:"post"`json:"post"`json:"tcp"`json:"tcp"`json:"email"`json:"email"`json:"exec"`json:"exec"`json:"log"`json:"log"`json:"victorOps"`json:"victorOps"`json:"pagerDuty"`json:"pagerDuty"`json:"pagerDuty2"`json:"pagerDuty2"`json:"pushover"`json:"pushover"`json:"sensu"`json:"sensu"`json:"slack"`json:"slack"`json:"telegram"`json:"telegram"`json:"hipChat"`json:"hipChat"`json:"alerta"`json:"alerta"`json:"opsGenie"`json:"opsGenie"`json:"opsGenie2"`json:"opsGenie2"`json:"talk"`json:"talk"`json:"kafka"`json:"kafka"`json:"headers"`json:"headers"`json:"filePath"`json:"filePath"`json:"event"`json:"event"`json:"environment"`json:"environment"`json:"group"`json:"group"`json:"origin"`json:"origin"`json:"service"`json:"service"`json:"command"`json:"command"`json:"address"`json:"address"`json:"to"`json:"to"`json:"routingKey"`json:"routingKey"`json:"serviceKey"`json:"serviceKey"`json:"room"`json:"room"`json:"handlers"`json:"handlers"`json:"userKey"`json:"userKey"`json:"device"`json:"device"`json:"title"`json:"title"`json:"urlTitle"`json:"urlTitle"`json:"sound"`json:"sound"`json:"channel"`json:"channel"`json:"iconEmoji"`json:"iconEmoji"`json:"workspace"`json:"workspace"`json:"chatId"`json:"chatId"`json:"parseMode"`json:"parseMode"`json:"disableWebPagePreview"`json:"disableWebPagePreview"`json:"disableNotification"`json:"disableNotification"`json:"teams"`json:"teams"`json:"recipients"`json:"recipients"`json:"cluster"`json:"cluster"`json:"kafka-topic"`json:"kafka-topic"`json:"template"`json:"template"`json:"typeOf"`json:"typeOf"` AlertNodes defines all possible kapacitor interactions with an alert. IsStateChangesOnly will only send alerts on state changes. UseFlapping enables flapping detection. Flapping occurs when a service or host changes state too frequently, resulting in a storm of problem and recovery notification HTTPPost  will post the JSON alert data to the specified URLs. TCP  will send the JSON alert data to the specified endpoint via TCP. Email  will send alert data to the specified emails. Exec  will run shell commands when an alert triggers Log  will log JSON alert data to files in JSON lines format. VictorOps  will send alert to all VictorOps PagerDuty  will send alert to all PagerDuty PagerDuty2  will send alert to  PagerDuty v2 Pushover  will send alert to all Pushover Sensu  will send alert to all Sensu Slack  will send alert to Slack Telegram  will send alert to all Telegram HipChat  will send alert to all HipChat Alerta  will send alert to all Alerta OpsGenie  will send alert to all OpsGenie OpsGenie2  will send alert to all OpsGenie v2 Talk will send alert to all Talk Kafka will send alert to all Kafka Post will POST alerts to a destination URL URL is the destination of the POST. Headers are added to the output POST Log sends the output of the alert to a file Absolute path the the log file; it will be created if it does not exist. Alerta sends the output of the alert to an alerta service Token is the authentication token that overrides the global configuration. Resource under alarm, deliberately not host-centric Event is the event name eg. NodeDown, QUEUE:LENGTH:EXCEEDED Environment is the effected environment; used to namespace the resource Group is an event group used to group events of similar type Value is the event value eg. 100%, Down, PingFail, 55ms, ORA-1664 Origin is the name of monitoring component that generated the alert Service is the list of affected services Exec executes a shell command on an alert Command is the space separated command and args to execute. TCP sends the alert to the address Endpoint is the Address and port to send the alert Email sends the alert to a list of email addresses ToList is the list of email recipients. VictorOps sends alerts to the victorops.com service RoutingKey is what is used to map the alert to a team PagerDuty sends alerts to the pagerduty.com service ServiceKey is the GUID of one of the "Generic API" integrations HipChat sends alerts to stride.com Room is the HipChat room to post messages. Token is the HipChat authentication token. Sensu sends alerts to sensu or sensuapp.org Source is the check source, used to create a proxy client for an external resource Handlers are Sensu event handlers are for taking action on events Pushover sends alerts to pushover.net UserKey is the User/Group key of your user (or you), viewable when logged into the Pushover dashboard. Often referred to as USER_KEY in the Pushover documentation. Device is the users device name to send message directly to that device, rather than all of a user's devices (multiple device names may be separated by a comma) Title is your message's title, otherwise your apps name is used URL is a supplementary URL to show with your message URLTitle is a title for your supplementary URL, otherwise just URL is shown Sound is the name of one of the sounds supported by the device clients to override the user's default sound choice Slack sends alerts to a slack.com channel Slack channel in which to post messages. Username of the Slack bot. IconEmoji is an emoji name surrounded in ':' characters; The emoji image will replace the normal user icon for the slack bot. Workspace is the slack workspace for the alert handler Telegram sends alerts to telegram.org ChatID is the Telegram user/group ID to post messages to. ParseMode tells telegram how to render the message (Markdown or HTML) IsDisableWebPagePreview will disables link previews in alert messages. IsDisableNotification will disables notifications on iOS devices and disables sounds on Android devices. Android users continue to receive notifications. OpsGenie sends alerts to opsgenie.com Teams that the alert will be routed to send notifications Recipients can be a single user, group, escalation, or schedule (https://docs.opsgenie.com/docs/alert-recipients-and-teams) Talk sends alerts to Jane Talk (https://jianliao.com/site) Kafka sends alerts to any Kafka brokers specified in the handler config MarshalJSON converts AlertNodes to JSON/Users/austinjaybecker/projects/abeck-go-testing/chronograf/memdb/Users/austinjaybecker/projects/abeck-go-testing/chronograf/memdb/kapacitors.gokapKapacitormemdbin-memory KapacitorStore does not support adding a Kapacitor"in-memory KapacitorStore does not support adding a Kapacitor"unable to find Kapacitor with id %d"unable to find Kapacitor with id %d" Ensure KapacitorStore implements chronograf.ServersStore. KapacitorStore implements the chronograf.ServersStore interface, and keeps an in-memory Kapacitor according to startup configuration All will return a slice containing a configured source Add does not have any effect Delete removes the in-memory configured Kapacitor if its ID matches what's provided Get returns the in-memory Kapacitor if its ID matches what's provided Update overwrites the in-memory configured Kapacitor if its ID matches what's provided/Users/austinjaybecker/projects/abeck-go-testing/chronograf/memdb/sources.goin-memory SourcesStore does not support adding a Source"in-memory SourcesStore does not support adding a Source"unable to find Source with id %d"unable to find Source with id %d" SourcesStore implements the chronograf.SourcesStore interface Delete removes the SourcesStore.Source if it matches the provided Source Get returns the configured source if the id matches Update does nothing/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/auth.goLogMessageNewResponseTestLoggerValidateErrExtendErrSerializedValidAuthorizationSerializeserializedAuthorizationoauth2github.com/influxdata/influxdb/v2/chronograf/oauth2"github.com/influxdata/influxdb/v2/chronograf/oauth2"SetCookie Authenticator implements a OAuth2 authenticator Validate returns Principal associated with authenticated and authorized entity if successful. Extend will extend the lifetime of a already validated Principal Authorize will grant privileges to a Principal Expire revokes privileges from a Principal ValidAuthorization returns the Principal Serialize the serialized values stored on the AuthenticatorConnectFQueryFWriteFUsersFPermissionsFRolesFAllDBFCreateDBFDropDBFAllRPFCreateRPFUpdateRPFDropRPFGetMeasurementsFAllFAddFDeleteFGetFUpdateFFindOrCreateFPutFNumFMappingsCreateDefaultFDefaultOrganizationFMessagestlHasMessagestringifystringifyArgDumpchattyPrinterlastNameMulastNameUpdatefranfailedskippedhelperPCshelperNamescleanupscleanupNamecleanupPcfinishedinFuzzFnchattybenchhasSubcleanupStartedraceErrorsrunnerisParallellevelcreatorbarriersignaltempDirMutempDirErrtempDirSeqcheckFuzzFnframeSkipdecorateflushToParentsetRanFailFailedFailNowlogDepthLogfSkipSkipfSkipNowSkippedHelperCleanupTempDirSetenvrunCleanuptestContextmatcherfilterMatchmatchesverifymatchFuncsubNamesfullNameclearSubNamesisFuzzingstartParallelrunningnumWaitingmaxParallelwaitParallelisEnvSetParallelFrameopaquefuncInfoFileLine_funcfuncIDfuncFlagentryOffdeferreturnpcsppcfilepclnnpcdatacuOffsetnfuncdataisInlinedmoduledatapcHeaderpad1pad2minLCptrSizenfuncnfilestextStartfuncnameOffsetfiletabOffsetpctabOffsetpclnOffsetfunctabentryofffuncofftextsectvaddrbaseaddritabinterfacetype_typepkgpathisEmbeddedreadvarintisBlankimethoditypmhdrinterfunptabEntrymodulehashmodulenamelinktimehashruntimehashbitvectorbytedataptrbitbvfuncnametabcutabfiletabpctabpclntableftabfindfunctabminpcmaxpcetextnoptrdataenoptrdataedatabssebssnoptrbssenoptrbsscovctrsecovctrsgcbsstypesetypesrodatagofunctextsectmaptypelinksitablinksptabpluginpathpkghashesmodulehasheshasmaingcdatamaskgcbssmasktypemapbadtextAddrmddatapvalid_FuncpanicHandlinguncommontype/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/config.go ConfigStore stores global application configuration Initialize is noop in mocks store Get returns the whole global application configuration Update updates the whole global application configuration/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/dashboards.gonewDashboard/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/databases.gorpXrpY Databases mock allows all databases methods to be set for testing/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/kapacitor_client.go TODO(desa): resolve kapacitor dependencyvar _ kapacitor.KapaClient = &KapaClient{}// Client is a mock Kapacitor clienttype KapaClient struct {	CreateTaskF func(opts client.CreateTaskOptions) (client.Task, error)	DeleteTaskF func(link client.Link) error	ListTasksF  func(opts *client.ListTasksOptions) ([]client.Task, error)	TaskF       func(link client.Link, opts *client.TaskOptions) (client.Task, error)	UpdateTaskF func(link client.Link, opts client.UpdateTaskOptions) (client.Task, error)}func (p *KapaClient) CreateTask(opts client.CreateTaskOptions) (client.Task, error) {	return p.CreateTaskF(opts)func (p *KapaClient) DeleteTask(link client.Link) error {	return p.DeleteTaskF(link)func (p *KapaClient) ListTasks(opts *client.ListTasksOptions) ([]client.Task, error) {	return p.ListTasksF(opts)func (p *KapaClient) Task(link client.Link, opts *client.TaskOptions) (client.Task, error) {	return p.TaskF(link, opts)func (p *KapaClient) UpdateTask(link client.Link, opts client.UpdateTaskOptions) (client.Task, error) {	return p.UpdateTaskF(link, opts)/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/layouts.go/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/logger.gotesting"testing""error"PipeReader" "UNKNOWN"UNKNOWN"== Dumping Test Logs =="== Dumping Test Logs =="lvl: %s, msg: %s"lvl: %s, msg: %s" NewLogger returns a mock logger that implements chronograf.Logger TestLogger is a chronograf.Logger which allows assertions to be made on the contents of its messages. HasMessage will return true if the TestLogger has been called with an exact match of a particular log message at a particular log level Dump dumps out logs into a given testing.T's logs/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/mapping.go/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/org_config.go/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/organizations.go/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/response.go NewResponse returns a mocked chronograf.Response Response is a mocked chronograf.Response MarshalJSON returns the res and err as the fake response./Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/roles.go RolesStore mock allows all functions to be set for testing All lists all Roles from the RolesStore Add a new Role in the RolesStore Update the Role's permissions or users/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/servers.go ServersStore mock allows all functions to be set for testing/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/sources.go SourcesStore mock allows all functions to be set for testing/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/store.go Store is a server.DataStore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/timeseries.go TimeSeries is a mockable chronograf time series by overriding the functions. Write records points into the TimeSeries RolesF represents the roles. Roles group permissions and Users New implements TimeSeriesClient Write records a point into the time series Users represents the user accounts within the TimeSeries database Roles represents the roles. Roles group permissions and Users/Users/austinjaybecker/projects/abeck-go-testing/chronograf/mocks/users.go UsersStore mock allows all functions to be set for testing Add a new User in the UsersStore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/multistore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/multistore/dashboards.gookayboardSetmultistore DashboardsStore implements the chronograf.DashboardsStore interface, and delegates to all contained DashboardsStores All concatenates the Dashboards of all contained Stores If this Store is unable to return an array of dashboards, skip to the next Store. We've received a response from at least one Store Enforce that the dashboard has a unique ID If the ID has been seen before, ignore the dashboard We have a new dashboard We just care that the ID is unique Add the dashboard to the first responsive Store Delete delegates to all Stores, returns success if one Store is successful Get finds the Dashboard by id among all contained Stores Update the first responsive Store/Users/austinjaybecker/projects/abeck-go-testing/chronograf/multistore/kapacitors.gokapskapSet KapacitorStore implements the chronograf.ServersStore interface, and delegates to all contained KapacitorStores All concatenates the Kapacitors of all contained Stores If this Store is unable to return an array of kapacitors, skip to the Enforce that the kapacitor has a unique ID If the ID has been seen before, ignore the kapacitor We have a new kapacitor Add the kap to the first responsive Store Get finds the Source by id among all contained Stores/Users/austinjaybecker/projects/abeck-go-testing/chronograf/multistore/layouts.golayoutSet Layouts is a LayoutsStore that contains multiple LayoutsStores The All method will return the set of all Layouts. Each method will be tried against the Stores slice serially. Try to load as many layouts as possible Enforce that the layout has a unique ID If the layout has been seen before then skip Add creates a new dashboard in the LayoutsStore.  Tries each store sequentially until success. Delete the dashboard from the store.  Searches through all stores to find Layout and then deletes from that store. Get retrieves Layout if `ID` exists.  Searches through each store sequentially until success. Update the dashboard in the store.  Searches through each store sequentially until success./Users/austinjaybecker/projects/abeck-go-testing/chronograf/multistore/organizations.goorgSetunknown error while adding organization: %s"unknown error while adding organization: %s"unknown error while deleting organization: %s"unknown error while deleting organization: %s"unknown error while updating organization: %s"unknown error while updating organization: %s"unknown error while creating default organization: %s"unknown error while creating default organization: %s"unknown error while getting default organization: %s"unknown error while getting default organization: %s" OrganizationsStore implements the chronograf.OrganizationsStore interface, and delegates to all contained OrganizationsStores All concatenates the Organizations of all contained Stores If this Store is unable to return an array of orgs, skip to the Enforce that the org has a unique ID If the ID has been seen before, ignore the org We have a new org Add the org to the first responsive Store Get finds the Organization by id among all contained Stores CreateDefault makes a default organization in the first responsive Store DefaultOrganization returns the first successful DefaultOrganization/Users/austinjaybecker/projects/abeck-go-testing/chronograf/multistore/sources.gosourceSet SourcesStore delegates to the SourcesStores that compose it All concatenates the Sources of all contained Stores If this Store is unable to return an array of sources, skip to the Enforce that the source has a unique ID If the source has been seen before, don't override what we already have We have a new Source! Add the src to the first Store to respond successfully Delete delegates to all stores, returns success if one Store is successful Update the first store to return a successful response/Users/austinjaybecker/projects/abeck-go-testing/chronograf/noop/Users/austinjaybecker/projects/abeck-go-testing/chronograf/noop/config.gonoopcannot initialize"cannot initialize"cannot update conifg"cannot update conifg" ensure ConfigStore implements chronograf.ConfigStore TODO(desa): this really should be removed/Users/austinjaybecker/projects/abeck-go-testing/chronograf/noop/dashboards.gono dashboards found"no dashboards found"failed to add dashboard"failed to add dashboard"failed to delete dashboard"failed to delete dashboard"failed to update dashboard"failed to update dashboard" ensure DashboardsStore implements chronograf.DashboardsStore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/noop/layouts.gono layouts found"no layouts found"failed to add layout"failed to add layout"failed to delete layout"failed to delete layout"failed to update layout"failed to update layout" ensure LayoutsStore implements chronograf.LayoutsStore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/noop/mappings.gono mappings found"no mappings found"failed to add mapping"failed to add mapping"failed to delete mapping"failed to delete mapping"failed to update mapping"failed to update mapping" ensure MappingsStore implements chronograf.MappingsStore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/noop/org_config.gocannot replace config"cannot replace config" ensure OrganizationConfigStore implements chronograf.OrganizationConfigStore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/noop/organizations.gofailed to add organization"failed to add organization"failed to retrieve default organization"failed to retrieve default organization"no organizations found"no organizations found"failed to delete organization"failed to delete organization"failed to update organization"failed to update organization" ensure OrganizationsStore implements chronograf.OrganizationsStore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/noop/servers.gono servers found"no servers found"failed to add server"failed to add server"failed to delete server"failed to delete server"failed to update server"failed to update server" ensure ServersStore implements chronograf.ServersStore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/noop/sources.gono sources found"no sources found"failed to add source"failed to add source"failed to delete source"failed to delete source"failed to update source"failed to update source" ensure SourcesStore implements chronograf.SourcesStore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/noop/users.gono users found"no users found"failed to add user"failed to add user"failed to delete user"failed to delete user"failed to update user"failed to update user"failed to get number of users"failed to get number of users" ensure UsersStore implements chronograf.UsersStore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/oauth2/Users/austinjaybecker/projects/abeck-go-testing/chronograf/oauth2/auth0.goAuth0AuthMuxDefaultCookieNameDefaultInactivityDurationDefaultNowTimeErrOrgMembershipExtendedProviderGenericGithubGoogleGoogleEndpointHerokuHerokuAccountRouteJWKJWKSNewAuth0NewAuthMuxNewCookieJWTNewJWTPrincipalKeyTenMinutesTokenizerUserEmailgetOrganizationsgetPrimarygetPrimaryEmailgetVerifiedisMemberlogResponseErrorofDomainprimaryEmailprincipalKeyrandomStringAccountPageNameRequiredScopesDomainsAPIURLAPIKeyPrincipalIDFromClaimsGroupFromClaimsactprovidera0apiURLauth0DomainauthURLclientIDclientSecretdomainredirectURLtokenURL/authorize"/authorize"/oauth/token"/oauth/token"/userinfo"/userinfo"auth0"auth0"openid"openid"email"email" the set of allowed organizations users may belong to check for organization membership if requiredExtendedPrincipalGetClaimsValidPrincipalTokensSuccessURLFailureURLLifespanInactivitysetCookiePlanSpaceCollaboratorsPrivateReposGetCollaboratorsGetPrivateReposGetSpaceNodeIDAvatarURLHTMLURLCompanyBlogPublicReposPublicGistsFollowersFollowingTotalPrivateReposOwnedPrivateReposPrivateGistsDiskUsageBillingEmailEventsURLHooksURLIssuesURLMembersURLPublicMembersURLReposURLGetAvatarURLGetBillingEmailGetBlogGetCompanyGetCreatedAtGetDiskUsageGetEmailGetEventsURLGetFollowersGetFollowingGetHooksURLGetHTMLURLGetIssuesURLGetLocationGetLoginGetMembersURLGetNodeIDGetOwnedPrivateReposGetPlanGetPrivateGistsGetPublicGistsGetPublicMembersURLGetPublicReposGetReposURLGetTotalPrivateReposGetUpdatedAtPrimaryVerifiedJwksurlKeyFuncKeyFuncRS256ValidClaimsGetPrimaryGetVerifiedKtyKidX5tNEX5cRemainingActivityServiceListFeedsListEventsListRepositoryEventsListIssueEventsForRepositoryListEventsForRepoNetworkListEventsForOrganizationListEventsPerformedByUserListEventsReceivedByUserListUserEventsForOrganizationListNotificationsListRepositoryNotificationsMarkNotificationsReadMarkRepositoryNotificationsReadGetThreadMarkThreadReadGetThreadSubscriptionSetThreadSubscriptionDeleteThreadSubscriptionListStargazersListStarredIsStarredStarUnstarListWatchersListWatchedGetRepositorySubscriptionSetRepositorySubscriptionDeleteRepositorySubscriptionAdminServiceUpdateUserLDAPMappingUpdateTeamLDAPMappingGetAdminStatsAppsServiceListInstallationsGetInstallationListUserInstallationsCreateInstallationTokenFindOrganizationInstallationFindRepositoryInstallationFindUserInstallationgetInstallationListReposListUserReposAddRepositoryRemoveRepositoryAuthorizationsServiceGetOrCreateForAppEditRevokeListGrantsGetGrantDeleteGrantCreateImpersonationDeleteImpersonationChecksServiceGetCheckRunGetCheckSuiteCreateCheckRunUpdateCheckRunListCheckRunAnnotationsListCheckRunsForRefListCheckRunsCheckSuiteListCheckSuitesForRefSetCheckSuitePreferencesCreateCheckSuiteRequestCheckSuiteGistsServiceListAllGetRevisionListCommitsForkListForksListCommentsGetCommentCreateCommentEditCommentDeleteCommentGitServiceGetBlobGetBlobRawCreateBlobCreateCommitGetRefGetRefsListRefsCreateRefUpdateRefDeleteRefGetTagCreateTagGetTreeCreateTreeGitignoresServiceIssuesServiceListByOrglistIssuesListByRepoListAssigneesIsAssigneeAddAssigneesRemoveAssigneesListIssueEventsGetEventListLabelsEditLabelListLabelsByIssueAddLabelsToIssueRemoveLabelForIssueReplaceLabelsForIssueRemoveLabelsForIssueListLabelsForMilestoneListMilestonesGetMilestoneCreateMilestoneEditMilestoneDeleteMilestoneListIssueTimelineLicensesServiceMarketplaceServiceStubbedListPlansListPlanAccountsForPlanListPlanAccountsForAccountListMarketplacePurchasesForUsermarketplaceURIMigrationServiceStartMigrationListMigrationsMigrationStatusMigrationArchiveURLDeleteMigrationUnlockRepoStartImportImportProgressUpdateImportCommitAuthorsMapCommitAuthorSetLFSPreferenceLargeFilesCancelImportStartUserMigrationListUserMigrationsUserMigrationStatusUserMigrationArchiveURLDeleteUserMigrationUnlockUserRepoOrganizationsServiceGetByIDListHooksGetHookCreateHookEditHookPingHookDeleteHookListMembersIsMemberIsPublicMemberRemoveMemberPublicizeMembershipConcealMembershipListOrgMembershipsGetOrgMembershipEditOrgMembershipRemoveOrgMembershipListPendingOrgInvitationsCreateOrgInvitationListOrgInvitationTeamsListOutsideCollaboratorsRemoveOutsideCollaboratorConvertMemberToOutsideCollaboratorListProjectsCreateProjectListBlockedUsersIsBlockedBlockUserUnblockUserProjectsServiceGetProjectUpdateProjectDeleteProjectListProjectColumnsGetProjectColumnCreateProjectColumnUpdateProjectColumnDeleteProjectColumnMoveProjectColumnListProjectCardsGetProjectCardCreateProjectCardUpdateProjectCardDeleteProjectCardMoveProjectCardPullRequestsServiceGetRawListFilesIsMergedCreateCommentInReplyToRequestReviewersListReviewersRemoveReviewersListReviewsGetReviewDeletePendingReviewListReviewCommentsCreateReviewSubmitReviewDismissReviewReactionsServiceListCommentReactionsCreateCommentReactionListIssueReactionsCreateIssueReactionListIssueCommentReactionsCreateIssueCommentReactionListPullRequestCommentReactionsCreatePullRequestCommentReactionDeleteReactionRepositoriesServiceGetCodeOfConductListContributorsListLanguagesListTeamsListTagsListBranchesGetBranchGetBranchProtectionGetRequiredStatusChecksListRequiredStatusChecksContextsUpdateBranchProtectionRemoveBranchProtectionUpdateRequiredStatusChecksLicenseGetPullRequestReviewEnforcementUpdatePullRequestReviewEnforcementDisableDismissalRestrictionsRemovePullRequestReviewEnforcementGetAdminEnforcementAddAdminEnforcementRemoveAdminEnforcementListAllTopicsReplaceAllTopicsTransferListCollaboratorsIsCollaboratorGetPermissionLevelAddCollaboratorRemoveCollaboratorListCommitCommentsUpdateCommentGetCommitRawGetCommitSHA1CompareCommitsGetCommunityHealthMetricsGetReadmeDownloadContentsGetContentsUpdateFileDeleteFileGetArchiveLinkListDeploymentsGetDeploymentCreateDeploymentListDeploymentStatusesGetDeploymentStatusCreateDeploymentStatusCreateForkTestHookListInvitationsDeleteInvitationUpdateInvitationListKeysCreateKeyEditKeyDeleteKeyGetPagesInfoListPagesBuildsGetLatestPagesBuildGetPageBuildRequestPageBuildListPreReceiveHooksGetPreReceiveHookUpdatePreReceiveHookDeletePreReceiveHookListReleasesGetReleaseGetLatestReleaseGetReleaseByTaggetSingleReleaseCreateReleaseEditReleaseDeleteReleaseListReleaseAssetsGetReleaseAssetDownloadReleaseAssetEditReleaseAssetDeleteReleaseAssetUploadReleaseAssetListContributorsStatsListCommitActivityListCodeFrequencyListParticipationListPunchCardListStatusesCreateStatusGetCombinedStatusListTrafficReferrersListTrafficPathsListTrafficViewsListTrafficClonesSearchServiceRepositoriesCommitsIssuesTeamsServiceGetTeamCreateTeamEditTeamDeleteTeamListChildTeamsListTeamReposIsTeamRepoAddTeamRepoRemoveTeamRepoListUserTeamsListDiscussionsGetDiscussionCreateDiscussionEditDiscussionDeleteDiscussionListTeamMembersIsTeamMemberGetTeamMembershipAddTeamMembershipRemoveTeamMembershipListPendingTeamInvitationsUsersServiceGetHovercardAcceptInvitationDeclineInvitationPromoteSiteAdminDemoteSiteAdminSuspendUnsuspendListEmailsAddEmailsDeleteEmailsListFollowersListFollowingIsFollowingFollowUnfollowListGPGKeysGetGPGKeyCreateGPGKeyDeleteGPGKeyclientMuBaseURLUploadURLrateMurateLimitsActivityAuthorizationsChecksGistsGitGitignoresLicensesMarketplaceMigrationsProjectsPullRequestsReactionsSearchNewUploadRequestcheckRateLimitBeforeDoRateLimitsMarkdownListEmojisListCodesOfConductAPIMetaOctocatZenListServiceHooksNextPagePrevPageFirstPageLastPagepopulatePageValuesStandardClaimsAudienceOrgsGistListOptionsListOptionsPerPageGistTextMatchIndicesGetTextObjectURLObjectTypePropertyGetFragmentGetObjectTypeGetObjectURLGetPropertyGravatarIDHireableBioSuspendedAtSiteAdminFollowingURLFollowersURLGistsURLOrganizationsURLReceivedEventsURLStarredURLSubscriptionsURLTextMatchesGetBioGetFollowersURLGetFollowingURLGetGistsURLGetGravatarIDGetHireableGetOrganizationsURLGetPermissionsGetReceivedEventsURLGetSiteAdminGetStarredURLGetSubscriptionsURLGetSuspendedAtGistFilenameGistFileRawURLGetContentGetFilenameGetLanguageGetRawURLGetSizeCommentsGitPullURLGitPushURLGetCommentsGetGitPullURLGetGitPushURLGetOwnerGetPublicSubscriptionSubscribedIgnoredReasonRepositoryURLThreadURLGetIgnoredGetReasonGetRepositoryURLGetSubscribedGetThreadURLProjectColumnMoveOptionsIssueGetColorMilestoneLabelsURLCreatorOpenIssuesClosedIssuesClosedAtDueOnGetClosedAtGetClosedIssuesGetCreatorGetDueOnGetLabelsURLGetNumberGetOpenIssuesGetStateGetTitlePullRequestLinksDiffURLPatchURLGetDiffURLGetPatchURLRepositoryCodeOfConductSPDXIDFeaturedImplementationConditionsLimitationsGetConditionsGetFeaturedGetImplementationGetLimitationsGetSPDXIDFullNameHomepageDefaultBranchMasterBranchPushedAtCloneURLGitURLMirrorURLSSHURLSVNURLForksCountNetworkCountOpenIssuesCountStargazersCountSubscribersCountWatchersCountAutoInitParentAllowRebaseMergeAllowSquashMergeAllowMergeCommitTopicsPrivateHasIssuesHasWikiHasPagesHasProjectsHasDownloadsLicenseTemplateGitignoreTemplateArchivedTeamIDArchiveURLAssigneesURLBlobsURLBranchesURLCollaboratorsURLCommentsURLCommitsURLCompareURLContentsURLContributorsURLDeploymentsURLDownloadsURLForksURLGitCommitsURLGitRefsURLGitTagsURLIssueCommentURLIssueEventsURLKeysURLLanguagesURLMergesURLMilestonesURLNotificationsURLPullsURLReleasesURLStargazersURLStatusesURLSubscribersURLSubscriptionURLTagsURLTreesURLTeamsURLGetAllowMergeCommitGetAllowRebaseMergeGetAllowSquashMergeGetArchivedGetArchiveURLGetAssigneesURLGetAutoInitGetBlobsURLGetBranchesURLGetCloneURLGetCollaboratorsURLGetCommentsURLGetCommitsURLGetCompareURLGetContentsURLGetContributorsURLGetDefaultBranchGetDeploymentsURLGetDownloadsURLGetForkGetForksCountGetForksURLGetFullNameGetGitCommitsURLGetGitignoreTemplateGetGitRefsURLGetGitTagsURLGetGitURLGetHasDownloadsGetHasIssuesGetHasPagesGetHasProjectsGetHasWikiGetHomepageGetIssueCommentURLGetIssueEventsURLGetKeysURLGetLanguagesURLGetLicenseGetLicenseTemplateGetMasterBranchGetMergesURLGetMilestonesURLGetMirrorURLGetNetworkCountGetNotificationsURLGetOpenIssuesCountGetParentGetPrivateGetPullsURLGetPushedAtGetReleasesURLGetSSHURLGetStargazersCountGetStargazersURLGetStatusesURLGetSubscribersCountGetSubscribersURLGetSubscriptionURLGetSVNURLGetTagsURLGetTeamIDGetTeamsURLGetTreesURLGetWatchersCountTotalCountPlusOneMinusOneLaughConfusedHeartHoorayGetConfusedGetHeartGetHoorayGetLaughGetMinusOneGetPlusOneGetTotalCountLockedAssigneeClosedByAssigneesActiveLockReasonGetActiveLockReasonGetAssigneeGetClosedByGetLockedGetPullRequestLinksGetReactionsGetRepositoryGetUserIsPullRequestWeeklyCommitActivityDaysTotalWeekGetTotalGetWeekGetReadOnlyReleaseAssetDownloadCountBrowserDownloadURLUploaderGetBrowserDownloadURLGetContentTypeGetDownloadCountGetUploaderGistCommentTeamSlugPrivacyMembersCountReposCountRepositoriesURLLDAPDNGetLDAPDNGetMembersCountGetPermissionGetPrivacyGetReposCountGetRepositoriesURLGetSlugInstallationTokenGetExpiresAtGetTokenDiscussionCommentListOptionsDiscussionCommentAuthorBodyHTMLBodyVersionLastEditedAtDiscussionURLGetAuthorGetBodyHTMLGetBodyVersionGetDiscussionURLGetLastEditedAtListCollaboratorsOptionsAffiliationDeploymentStatusTargetURLDeploymentURLGetDeploymentURLGetTargetURLCombinedStatusRepoStatusGetContextSHAStatusesCommitURLGetCommitURLGetSHAHookEventsPreReceiveHookEnforcementConfigURLGetConfigURLGetEnforcementPullRequestCommentInReplyToDiffHunkPullRequestReviewIDOriginalPositionCommitIDOriginalCommitIDAuthorAssociationPullRequestURLGetAuthorAssociationGetCommitIDGetDiffHunkGetInReplyToGetOriginalCommitIDGetOriginalPositionGetPathGetPullRequestReviewIDGetPullRequestURLRepositoryLicenseDownloadURLEncodingGetDownloadURLGetEncodingWeeklyStatsAdditionsDeletionsGetAdditionsGetCommitsGetDeletionsRepositoryReleaseTagNameTargetCommitishDraftPrereleasePublishedAtAssetsURLZipballURLTarballURLGetAssetsURLGetDraftGetPrereleaseGetPublishedAtGetTagNameGetTarballURLGetTargetCommitishGetUploadURLGetZipballURLListCheckRunsOptionsCheckNameFilterGetCheckNameGetFilterListCheckRunsResultsCheckRunCheckRunOutputCheckRunAnnotationBlobHRefStartLineEndLineWarningLevelRawDetailsGetBlobHRefGetEndLineGetFileNameGetMessageGetRawDetailsGetStartLineGetWarningLevelCheckRunImageAltImageURLCaptionGetAltGetCaptionGetImageURLAnnotationsCountAnnotationsURLImagesGetAnnotationsCountGetAnnotationsURLCheckSuiteAppExternalURLGetExternalURLPullRequestPullRequestBranchRefRepoGetRepoMergedAtMergedMergeableMergeableStateMergedByMergeCommitSHAChangedFilesIssueURLReviewCommentsURLReviewCommentURLMaintainerCanModifyRequestedReviewersGetChangedFilesGetHeadGetIssueURLGetMaintainerCanModifyGetMergeableGetMergeableStateGetMergeCommitSHAGetMergedGetMergedAtGetMergedByGetReviewCommentsURLGetReviewCommentURLHeadBranchHeadSHABeforeSHAAfterSHAConclusionGetAfterSHAGetAppGetBeforeSHAGetConclusionGetHeadBranchGetHeadSHAExternalIDCompletedAtGetCompletedAtGetExternalIDGetOutputGetStartedAtCheckRunsGUIDLockRepositoriesExcludeAttachmentsGetExcludeAttachmentsGetGUIDGetLockRepositoriesRepositoryContentGetOptionsTeamAddTeamRepoOptionsSourceImportAuthorRemoteIDRemoteNameImportURLGetImportURLGetRemoteIDGetRemoteNameImportVCSURLVCSVCSUsernameVCSPasswordTFVCProjectUseLFSHasLargeFilesLargeFilesSizeLargeFilesCountCommitCountStatusTextAuthorsCountPercentPushPercentAuthorsURLFailedStepHumanNameProjectChoicesGetAuthorsCountGetAuthorsURLGetCommitCountGetFailedStepGetHasLargeFilesGetHumanNameGetLargeFilesCountGetLargeFilesSizeGetPercentGetPushPercentGetStatusTextGetTFVCProjectGetUseLFSGetVCSGetVCSPasswordGetVCSURLGetVCSUsernameInstallationInstallationPermissionsContentsSingleFileGetIssuesGetMetadataGetSingleFileAppIDTargetIDAccessTokensURLTargetTypeSingleFileNameRepositorySelectionGetAccessTokensURLGetAccountGetAppIDGetRepositorySelectionGetSingleFileNameGetTargetIDGetTargetTypeAuthorizationUpdateRequestAddScopesRemoveScopesNoteURLFingerprintGetFingerprintGetNoteGetNoteURLAuthorizationAppGetClientIDTokenLastEightHashedTokenGetHashedTokenGetTokenLastEightAdminEnforcementUserMigrationMarketplacePurchaseMarketplacePlanAccountsURLMonthlyPriceInCentsYearlyPriceInCentsPriceModelUnitNameBulletsGetAccountsURLGetBulletsGetMonthlyPriceInCentsGetPriceModelGetUnitNameGetYearlyPriceInCentsMarketplacePlanAccountOrganizationBillingEmailGetMarketplacePurchaseGetOrganizationBillingEmailBillingCycleNextBillingDateUnitCountGetBillingCycleGetNextBillingDateGetUnitCountAuthorizationRequestGetClientSecretPagesCNAMECustom404GetCNAMEGetCustom404RepositoryInvitationInviteeInviterGetInviteeGetInviterGitignoreRepositoryListByOrgOptionsReactionRepositoryContentFileOptionsCommitAuthorGetDateBranchCommitterGetCommitterRepositoryContentResponseRepositoryContentTreeEntryGetModeEntriesTruncatedGetTruncatedCommitStatsSignatureVerificationGetPayloadGetSignatureParentsVerificationCommentCountGetCommentCountGetStatsGetVerificationrateLimitCategoryRateLimitErrorLargeFileRefNameOIDGetOIDGetRefNameIssueCommentMilestoneListOptionsCreateOrgInvitationOptionsInviteeIDGetInviteeIDInvitationTeamCountInvitationTeamURLGetInvitationTeamURLGetTeamCountTeamAddTeamMembershipOptionsMembershipOrganizationURLGetOrganizationURLRawPayloadActorParsePayloadGetActorGetOrgGetRawPayloadNotificationNotificationSubjectLatestCommentURLGetLatestCommentURLUnreadLastReadAtGetLastReadAtGetSubjectGetUnreadGrantPullRequestReviewDismissalRequestPullRequestReviewSubmittedAtGetSubmittedAtAdminStatsIssueStatsTotalIssuesGetTotalIssuesHookStatsTotalHooksActiveHooksInactiveHooksGetActiveHooksGetInactiveHooksGetTotalHooksMilestoneStatsTotalMilestonesOpenMilestonesClosedMilestonesGetClosedMilestonesGetOpenMilestonesGetTotalMilestonesOrgStatsTotalOrgsDisabledOrgsTotalTeamsTotalTeamMembersGetDisabledOrgsGetTotalOrgsGetTotalTeamMembersGetTotalTeamsCommentStatsTotalCommitCommentsTotalGistCommentsTotalIssueCommentsTotalPullRequestCommentsGetTotalCommitCommentsGetTotalGistCommentsGetTotalIssueCommentsGetTotalPullRequestCommentsPageStatsTotalPagesGetTotalPagesUserStatsTotalUsersAdminUsersSuspendedUsersGetAdminUsersGetSuspendedUsersGetTotalUsersGistStatsTotalGistsGetTotalGistsPullStatsTotalPullsMergedPullsMergablePullsUnmergablePullsGetMergablePullsGetMergedPullsGetTotalPullsGetUnmergablePullsRepoStatsTotalReposRootReposForkReposOrgReposTotalPushesTotalWikisGetForkReposGetOrgReposGetRootReposGetTotalPushesGetTotalReposGetTotalWikisHooksMilestonesPullsReposGetGistsGetHooksGetMilestonesGetOrgsGetPagesGetPullsGetReposGetUsersProjectCardListOptionsArchivedStateGetArchivedStateProjectCardColumnURLContentURLColumnIDGetColumnIDGetColumnURLGetContentURLRawOptionsRawTypesearchParametersRepositoryIDSearchOptionsUploadOptionsProjectOptionsProjectOwnerURLGetOwnerURLGitObjectTaggerGetObjectGetTaggerReviewersRequestReviewersTeamReviewersPullRequestOptionsCommitTitleMergeMethodPullRequestMergeResultRepositoryCommitCommitFileChangesBlobURLGetBlobURLGetChangesGetPatchReferenceListOptionsReferenceRequiredStatusChecksRequestStrictContextsGetStrictRequiredStatusChecksTrafficReferrerReferrerUniquesGetCountGetReferrerGetUniquesIssueEventRenameFromGetFromGetToAssignerLockReasonGetAssignerGetIssueGetLockReasonGetRenameHovercardOptionsSubjectTypeSubjectIDHovercardUserContextOcticonGetOcticonVerifiablePasswordAuthenticationImporterGetVerifiablePasswordAuthenticationServiceHookSupportedEventsSchemaCheckSuitePreferenceOptionsPreferenceListAutoTriggerCheckSettingGetSettingAutoTriggerChecksGetPreferenceListCheckSuitePreferenceResultsPreferencesGetPreferencesIssueListCommentsOptionsIssueListOptionsFeedsFeedLinkHRefGetHRefTimelineCurrentUserPublicCurrentUserCurrentUserActorCurrentUserOrganizationCurrentUserOrganizationsTimelineURLUserURLCurrentUserPublicURLCurrentUserURLCurrentUserActorURLCurrentUserOrganizationURLCurrentUserOrganizationURLsGetCurrentUserActorURLGetCurrentUserOrganizationURLGetCurrentUserPublicURLGetCurrentUserURLGetTimelineURLGetUserURLNotificationListOptionsParticipatingGPGKeyGPGEmailPrimaryKeyIDKeyIDEmailsSubkeysCanSignCanEncryptCommsCanEncryptStorageCanCertifyGetCanCertifyGetCanEncryptCommsGetCanEncryptStorageGetCanSignGetKeyIDGetPrimaryKeyIDGetPublicKeyStargazerStarredAtGetStarredAtProjectListOptionsPagesBuildPagesErrorPusherGetDurationGetErrorGetPusherTeamDiscussionCommentsCountPinnedTeamURLGetCommentsCountGetPinnedGetTeamURLRepositoryListOptionsVisibilityPullRequestReviewsEnforcementDismissalRestrictionsDismissStaleReviewsRequireCodeOwnerReviewsRequiredApprovingReviewCountPullRequestReviewRequestDraftReviewCommentPullRequestListOptionsBlobGistForkgfKeyfuncCreateCheckRunOptionsDetailsURLGetDetailsURLLabelsSearchResultLabelResultScoreGetScoreIncompleteResultsGetIncompleteResultsProjectColumnOptionsProjectColumnProjectURLGetProjectURLRepositoriesSearchResultListOutsideCollaboratorsOptionsDeploymentsListOptionsDeploymentGetEnvironmentGetTaskPullRequestReviewsEnforcementUpdateDismissalRestrictionsRequestGetTeamsGetDismissalRestrictionsRequestGetDismissStaleReviewsRepositoryListForksOptionsCommitsComparisonBaseCommitMergeBaseCommitAheadByBehindByTotalCommitsPermalinkURLGetAheadByGetBaseCommitGetBehindByGetMergeBaseCommitGetPermalinkURLGetTotalCommitsTrafficBreakdownOptionsPerTrafficViewsTrafficDataViewsLockIssueOptionsNewTeamMaintainersRepoNamesParentTeamIDGetParentTeamIDDiscussionListOptionsRepositoryMergeRequestCommitMessageGetCommitMessageRepositoryPermissionLevelProjectCardOptionsContentIDRepositoryCommentGetCoreGetSearchRepositoryCreateForkOptionsIssuesSearchResultMigrationOptionsOrganizationsListOptionsUsersSearchResultTeamLDAPMappingNewPullRequestListCheckSuiteOptionsListCheckSuiteResultsCheckSuitesRequestCheckSuiteOptionsUpdateCheckRunOptionsTransferRequestNewOwnerMarkdownOptionsRepositoryListAllOptionsUserListOptionsTrafficPathListOrgMembershipsOptionsRepositoryParticipationRepositoryAddCollaboratorOptionsTrafficClonesClonesProtectedGetProtectedPullRequestListCommentsOptionsPunchCardGetDayGetHourTeamListTeamMembersOptionsGistCommitChangeStatusCommittedAtgcGetChangeStatusGetCommittedAtarchiveFormatListMembersOptionsPublicOnlyCreateCheckSuiteOptionsCommitsSearchResultCommitResultListContributorsOptionsAnonContributorContributionsGetContributionsDeploymentStatusRequestLogURLEnvironmentURLAutoInactiveGetAutoInactiveGetEnvironmentURLGetLogURLCodeSearchResultCodeResultCodeResultsCommitsListOptionsUntilProjectCardMoveOptionsCommunityHealthMetricsCommunityHealthFilesContributingIssueTemplatePullRequestTemplateReadmeGetContributingGetIssueTemplateGetPullRequestTemplateHealthPercentageGetFilesGetHealthPercentageUserLDAPMappingProtectionRequestPullRequestReviewsEnforcementRequestBranchRestrictionsRequestRequiredPullRequestReviewsEnforceAdminsRestrictionsGetRequiredPullRequestReviewsGetRestrictionsProtectionBranchRestrictionsGetEnforceAdminsIssueRequestGetAssigneesGetLabelsUserMigrationOptionsContributorStatsWeeksIssueListByRepoOptionsMentionedRepositoryTagActivityListStarredOptionsStarredRepositoryDeploymentRequestAutoMergeRequiredContextsTransientEnvironmentProductionEnvironmentGetAutoMergeGetProductionEnvironmentGetRequiredContextsGetTransientEnvironment/Users/austinjaybecker/projects/abeck-go-testing/chronograf/oauth2/cookies.goinactivitylifespansession"session"300000000000-3600000000000 DefaultCookieName is the name of the stored cookie DefaultInactivityDuration is the duration a token is valid without any new activity cookie represents the location and expiration time of new cookies. Name is the name of the cookie stored on the browser Lifespan is the expiration date of the cookie. 0 means session cookie Inactivity is the length of time a token is valid if there is no activity NewCookieJWT creates an Authenticator that uses cookies for auth Server interprets a token duration longer than the cookie lifespan as a token that was issued by a server with a longer auth-duration and is thus invalid, as a security precaution. So, inactivity must be set to be less than lifespan. half of the lifespan ensures tokens can be refreshed once. Validate returns Principal of the Cookie if the Token is valid. Extend will extend the lifetime of the Token by the Inactivity time.  Assumes Principal is already valid. Refresh the token by extending its life another Inactivity duration Creating a new token with the extended principal Cookie lifespan can be indirectly figured out by taking the token's issued at time and adding the lifespan setting  The token's issued at time happens to correspond to the cookie's original issued at time. Once the token has been extended, write it out as a new cookie. Authorize will create cookies containing token information.  It'll create a token with cookie.Duration of life to be stored as the cookie's value. Principal will be issued at Now() and will expire c.Inactivity into the future The time when the cookie expires setCookie creates a cookie with value expiring at exp and writes it as a cookie into the response Cookie has a Token baked into it Only set a cookie to be persistent (endure beyond the browser session) if auth duration is greater than zero Expire returns a cookie that will expire an existing cookie to expire cookie set the time in the past/Users/austinjaybecker/projects/abeck-go-testing/chronograf/oauth2/doc.go Package oauth2 provides http.Handlers necessary for implementing Oauth2 authentication with multiple Providers. This is how the pieces of this package fit together:  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚github.com/influxdata/influxdb/chronograf/oauth2 â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚  â”‚â”‚   <<interface>>    â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚  â”‚â”‚   Authenticator    â”‚        â”‚         AuthMux         â”‚                    â”‚  â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                    â”‚  â”‚â”‚Authorize()         â”‚   Auth â”‚+SuccessURL : string     â”‚                    â”‚  â”‚â”‚Validate()          â—€â”€â”€â”€â”€â”€â”€â”€â”€â”‚+FailureURL : string     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  ||Expire()            |        |+Now : func() time.Time  |          |         |  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–³â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          |         |  â”‚           â”‚                               â”‚                       â”‚         |  â”‚           â”‚                               â”‚                       â”‚         â”‚  â”‚           â”‚                       Providerâ”‚                       â”‚         â”‚  â”‚           â”‚                           â”Œâ”€â”€â”€â”˜                       â”‚         â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚                           â–½         â”‚  â”‚â”‚         Tokenizer     â”‚              â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â–¼                   â”‚ <<interface>> â”‚ â”‚  â”‚â”‚Create()               â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   OAuth2Mux   â”‚ â”‚  â”‚â”‚ValidPrincipal()       â”‚      â”‚ <<interface>> â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   Provider    â”‚           â”‚Login()        â”‚ â”‚  â”‚                               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤           â”‚Logout()       â”‚ â”‚  â”‚                               â”‚ID()           â”‚           â”‚Callback()     â”‚ â”‚  â”‚                               â”‚Scopes()       â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚                               â”‚Secret()       â”‚                             â”‚  â”‚                               â”‚Authenticator()â”‚                             â”‚  â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚  â”‚                                       â–³                                     â”‚  â”‚                                       â”‚                                     â”‚  â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  â”‚             â”‚                         â”‚                         â”‚           â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚ â”‚        Github         â”‚ â”‚        Google        â”‚  â”‚        Heroku        â”‚â”‚  â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚  â”‚ â”‚+ClientID : string     â”‚ â”‚+ClientID : string    â”‚  â”‚+ClientID : string    â”‚â”‚  â”‚ â”‚+ClientSecret : string â”‚ â”‚+ClientSecret : stringâ”‚  â”‚+ClientSecret : stringâ”‚â”‚  â”‚ â”‚+Orgs : []string       â”‚ â”‚+Domains : []string   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚+RedirectURL : string â”‚                          â”‚  â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ The design focuses on an Authenticator, a Provider, and an OAuth2Mux. Their responsibilities, respectively, are to decode and encode secrets received from a Provider, to perform Provider specific operations in order to extract information about a user, and to produce the handlers which persist secrets. To add a new provider, You need only implement the Provider interface, and add its endpoints to the server Mux. The Oauth2 flow between a browser, backend, and a Provider that this package implements is pictured below for reference.      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚ Browser â”‚                â”‚Chronograf â”‚                     â”‚Providerâ”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚                           â”‚                                â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€ GET /auth â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶                                â”‚           â—€ â”€ â”€ â”€302 to Provider  â”€ â”€ â”¤                                â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GET /auth w/ callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶           â—€â”€ â”€ â”€ â”€ â”€ â”€ â”€   302 to Chronograf Callback  â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”¤           â”‚   Code and State from     â”‚                                â”‚           â”‚        Provider           â”‚                                â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶    Request token w/ code &     â”‚           â”‚                           â”‚             state              â”‚           â”‚                           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶           â”‚                           â”‚          Response with         â”‚           â”‚                           â”‚              Token             â”‚           â”‚   Set cookie, Redirect    â”‚â—€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”¤           â”‚           to /            â”‚                                â”‚           â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                â”‚ The browser ultimately receives a cookie from Chronograf, authorizing it. Its contents are encoded as a JWT whose "sub" claim is the user's email address for whatever provider they have authenticated with. Each request to Chronograf will validate the contents of this JWT against the `TOKEN_SECRET` and checked for expiration. The JWT's "sub" becomes the https://en.wikipedia.org/wiki/Principal_(computer_security) used for authorization to resources. The Mux is responsible for providing three http.Handlers for servicing the above interaction. These are mounted at specific endpoints by convention shared with the front end. Any future Provider routes should follow the same convention to ensure compatibility with the front end logic. These routes and their responsibilities are:   /oauth/{provider}/login The `/oauth` endpoint redirects to the Provider for OAuth.  Chronograf sets the OAuth `state` request parameter to a JWT with a random "sub".  Using $TOKEN_SECRET `/oauth/github/callback` can validate the `state` parameter without needing `state` to be saved.   /oauth/{provider}/callback The `/oauth/github/callback` receives the OAuth `authorization code`  and `state`. First, it will validate the `state` JWT from the `/oauth` endpoint. `JWT` validation only requires access to the signature token.  Therefore, there is no need for `state` to be saved.  Additionally, multiple Chronograf servers will not need to share third party storage to synchronize `state`. If this validation fails, the request will be redirected to `/login`. Secondly, the endpoint will use the `authorization code` to retrieve a valid OAuth token with the `user:email` scope.  If unable to get a token from Github, the request will be redirected to `/login`. Finally, the endpoint will attempt to get the primary email address of the user.  Again, if not successful, the request will redirect to `/login`. The email address is used as the subject claim for a new JWT.  This JWT becomes the value of the cookie sent back to the browser. The cookie is valid for thirty days. Next, the request is redirected to `/`. For all API calls to `/chronograf/v1`, the server checks for the existence and validity of the JWT within the cookie value. If the request did not have a valid JWT, the API returns `HTTP/1.1 401 Unauthorized`.   /oauth/{provider}/logout Simply expires the session cookie and redirects to `/`./Users/austinjaybecker/projects/abeck-go-testing/chronograf/oauth2/generic.goclaimsemailsemailsEndpointemailDomainrequiredDomainsgojwtgolang.org/x/oauth2"golang.org/x/oauth2"generic"generic"Not a member of required domain."Not a member of required domain."not a member of required domain"not a member of required domain"@"@"malformed email address, expected %q to contain @ symbol"malformed email address, expected %q to contain @ symbol"json:"email,omitempty"`json:"email,omitempty"`json:"primary,omitempty"`json:"primary,omitempty"`json:"verified,omitempty"`json:"verified,omitempty"`/emails"/emails"Unable to retrieve primary email "Unable to retrieve primary email "no primary email address"no primary email address"@%s"@%s"no claim for %s"no claim for %s"DEFAULT"DEFAULT" ExtendedProvider extendts the base Provider interface with optional methods get PrincipalID from id_token Generic provides OAuth Login and Callback server and is modeled after the Github OAuth2 provider. Callback will set an authentication cookie.  This cookie's value is a JWT containing the user's primary email address. Name displayed on the login page Optional email domain checking APIURL returns OpenID Userinfo APIKey is the JSON key to lookup email address in APIURL response Name is the name of the provider ID returns the generic application client id Secret returns the generic application client secret Scopes for generic provider required of the client. Config is the Generic OAuth2 exchange information and endpoints PrincipalID returns the email address of the user. If we did not receive an email address, try to lookup the email in a similar way as github If we need to restrict to a set of domains, we first get the org and filter. If not in the domain deny permission Group returns the domain that a user belongs to in the the generic OAuth. UserEmail represents user's email address getPrimaryEmail gets the private email account for the authenticated user. ofDomain makes sure that the email is in one of the required domains PrincipalIDFromClaims verifies an optional id_token and extracts email address of the user GroupFromClaims verifies an optional id_token, extracts the email address of the user and splits off the domain part/Users/austinjaybecker/projects/abeck-go-testing/chronograf/oauth2/github.goscopeslengthuserOrgrequiredOrgrequiredOrgsuserOrgsallOrgsbase64githuboghcrypto/rand"crypto/rand"encoding/base64"encoding/base64"github.com/google/go-github/github"github.com/google/go-github/github"golang.org/x/oauth2/github"golang.org/x/oauth2/github""github"user:email"user:email"read:org"read:org"Not a member of required github organization"Not a member of required github organization"ReadFull256encodedecodeMappadCharstrictWithPaddingEncodeToStringEncodedLendecodeQuantumDecodeStringDecodedLenStdEncodingStatusUnauthorized401StatusForbidden403OAuth access to email address forbidden "OAuth access to email address forbidden "Unable to retrieve Github email "Unable to retrieve Github email "TODOUnable to retrieve primary Github email "Unable to retrieve primary Github email " Github provides OAuth Login and Callback server. Callback will set an authentication cookie.  This cookie's value is a JWT containing the user's primary Github email address. Optional github organization checking Name is the name of the provider. ID returns the github application client id. Secret returns the github application client secret. Scopes for github is only the email address and possible organizations if we are filtering by organizations. In order to access a users orgs, we need the "read:org" scope even if g.Orgs == 0 Config is the Github OAuth2 exchange information and endpoints. PrincipalID returns the github email address of the user. If we need to restrict to a set of organizations, we first get the org Not a member, so, deny permission Group returns a comma delimited string of Github organizations that a user belongs to in Github isMember makes sure that the user is in one of the required organizations. getOrganizations gets all organization for the currently authenticated user. Get all pages of results Get the organizations for the current authenticated user. getPrimaryEmail gets the primary email account for the authenticated user./Users/austinjaybecker/projects/abeck-go-testing/chronograf/oauth2/google.gorequiredDomaingoauth2google.golang.org/api/oauth2/v2"google.golang.org/api/oauth2/v2"google.golang.org/api/option"google.golang.org/api/option"https://accounts.google.com/o/oauth2/auth"https://accounts.google.com/o/oauth2/auth"https://accounts.google.com/o/oauth2/token"https://accounts.google.com/o/oauth2/token"google"google"UserinfoEmailScopehttps://www.googleapis.com/auth/userinfo.emailUserinfoProfileScopehttps://www.googleapis.com/auth/userinfo.profileUserinfoServiceUserinfoV2ServiceUserinfoV2MeServiceMeV2BasePathuserAgentGetCertForOpenIdConnectTokeninfoClientOptionDialSettingsCredentialsProjectIDDialOptiondialOptionsUnaryClientInterceptorClientConnAuthorityconnectivityStateManagernotifyChanchannelzIDupdateStatecsmgetNotifyChanBuildOptionsTransportCredentialsAuthInfoAuthTypeProtocolInfoProtocolVersionSecurityProtocolSecurityVersionClientHandshakeOverrideServerNameServerHandshakeBundlePerRPCCredentialsGetRequestMetadataRequireTransportSecurityNewWithModeDialCredsCredsBundleDialerChannelzParentIDpickerWrapperV2PickerPickInfoFullMethodNameCtxPickResultSubConnAttributesWithValuesAddressTypeUpdateAddressesDoneInfoMDBytesSentBytesReceivedServerLoadPickconnErrupdateConnectionErrorconnectionErrorblockingChpickerupdatePickerpwupdatePickerV2pickccResolverWrapperResolverResolveNowOptionsResolveNowfiredFireHasFiredParseResultisServiceConfigAddressesServiceConfigccresolverMuresolverpollingMupollingresolveNowccrpollUpdateStateReportErrorNewAddressNewServiceConfigParseServiceConfigaddChannelzTraceEventlbConfigLoadBalancingConfigisLoadBalancingConfigMethodConfigretryPolicymaxAttemptsinitialBackoffmaxBackoffbackoffMultiplierretryableStatusCodesWaitForReadyMaxReqSizeMaxRespSizeretryThrottlingPolicyMaxTokensTokenRatiohealthCheckConfigServiceNameLBMethodsretryThrottlingrawJSONStringaddrConnNewSubConnOptionsHealthCheckEnabledClientTransportStreamServerTransportAnyTypeUrlGetTypeUrlGetCodeGetDetailsWithDetailsDrainHandleStreamsIncrMsgRecvIncrMsgSentWriteStatushttp2ClientloopyWritersidecontrolBufferitemNodeitenqueueildequeuedequeueAllisEmptyconsumerWaitingtransportResponseFramestrfChanthrottleexecuteAndPutfinishoutStreamoutStreamStatewriteQuotaquotareplenishrealReplenishitlbytesOutStandingwqdeleteSelfoutStreamListframerbufWriterbatchSizeonFlushFramerFrameHeaderFrameTypeLengthStreamIDwriteDebuginvalidatedynamicTableheaderFieldTableHeaderFieldSensitiveIsPseudohfpairNameValueentsevictCountbyNameValueaddEntryevictOldestidToIndextablemaxSizeallowedMaxSizesetMaxSizedtdynTabemitEnabledmaxStrLensaveBuffirstFieldSetMaxStringLengthSetEmitFuncSetEmitEnabledEmitEnabledSetMaxDynamicTableSizeSetAllowedMaxDynamicTableSizemaxTableIndexatDecodeFullparseHeaderFieldReprparseFieldIndexedparseFieldLiteralcallEmitparseDynamicTableSizeUpdatereadStringframeCacheDataFrameStreamEndeddataFramegetDataFramefclastFrameerrDetaillastHeaderStreammaxReadSizeheaderBufgetReadBufreadBufmaxWriteSizewbufAllowIllegalWritesAllowIllegalReadsReadMetaHeadersMaxHeaderListSizelogReadslogWritesdebugFramerdebugFramerBufdebugReadLoggerfdebugWriteLoggerfmaxHeaderListSizefrstartWriteendWritelogWritewriteBytewriteByteswriteUint16writeUint32SetReuseFramesSetMaxReadFrameSizeErrorDetailReadFrameconnErrorcheckFrameOrderWriteDataPaddedWriteSettingsWriteSettingsAckWritePingWriteGoAwayWriteWindowUpdateWritePriorityWriteRSTStreamWriteContinuationWritePushPromiseWriteRawFramemaxHeaderStringLenreadMetaFramewriterminSizemaxSizeLimittableSizeUpdateWriteFieldsearchTableSetMaxDynamicTableSizeLimitshouldIndexbdpEstimatorsentAtbdpsamplebwMaxisSentupdateFlowControlsampleCountrtttimesnapcalculategoAwayErrCodedebugDataheadsUpisTransportResponseFramecbufsendQuotaoiwsestdStreamsactiveStreamshBufhEncbdpEstdrainingssGoAwayHandleroutgoingWindowUpdateHandlerincomingWindowUpdateHandleroutgoingSettingsHandlerincomingSettingsHandlerregisterStreamHandlerheaderHandleroriginateStreampreprocessDatapingHandleroutFlowControlSizeRequestHandlercleanupStreamHandlerincomingGoAwayHandlergoAwayHandlerapplySettingsprocessDatatrInFlowunackedeffectiveWindowSizenewLimitonDataupdateEffectiveWindowSizegetSizeClientParametersPermitWithoutStreamConnStatsIsClientisConnStatsRPCStatsisRPCStatsConnTagInfoRPCTagInfoFailFastHandleConnHandleRPCTagConnTagRPCtransportStateGoAwayReasonchannelzDatakpCountstreamsStartedstreamsSucceededstreamsFailedlastStreamCreatedTimemsgSentmsgRecvlastMsgSentTimelastMsgRecvTimebufferPoolctxDoneloopylocalAddrauthInforeaderDonewriterDonecontrolBufisSecureperRPCCredskpkeepaliveEnabledstatsHandlerinitialWindowSizemaxSendHeaderListSizeonPrefaceReceiptmaxConcurrentStreamsstreamQuotastreamsQuotaAvailablewaitingStreamsnextIDprevGoAwayIDgoAwayReasonkpDormancyCondkpDormantczDataonGoAwayonCloseconnectionIDnewStreamgetPeercreateHeaderFieldscreateAudiencegetTrAuthDatagetCallAuthDataNewStreamCloseStreamcloseStreamGracefulClosegetStreamadjustWindowupdateWindowhandleDatahandleRSTStreamhandleSettingshandlePinghandleGoAwaysetGoAwayReasonGetGoAwayReasonhandleWindowUpdateoperateHeaderskeepaliveGoAwayChannelzMetricgetOutFlowWindowrecvBufferrecvMsgbackloginFlowpendingDatapendingUpdatedeltamaybeAdjustonReadstreamStatectrecvCompresssendCompresstrReaderrequestReadheaderChanheaderChanClosedheaderValidhdrMutrailernoHeadersheaderSentbytesReceivedunprocessedcontentSubtypeisHeaderSentupdateHeaderSentswapStatecompareAndSwapStatewaitOnHeaderRecvCompressSetSendCompressTrailersOnlyContentSubtypeSetHeaderSendHeaderSetTrailerUnprocessedCallHdrSendCompressCredsPreviousAttemptscallsStartedcallsFailedcallsSucceededlastCallStartedTimedoptsacbwscoptscurAddraddrsbackoffIdxresetBackoffconnectactryUpdateAddrsupdateConnectivityStateadjustParamsresetTransporttryAllAddrscreateTransportstartHealthCheckresetConnectBackoffgetReadyTransporttearDownincrCallsStartedincrCallsSucceededincrCallsFailedccBalancerWrapperBalancerHandleResolvedAddrsHandleSubConnStateChangeUnboundedacBalancerWrappergetAddrConnbalancerMubalancerscBuffersubConnswatcherccbhandleSubConnStateChangeupdateClientConnStateresolverErrorNewSubConnRemoveSubConnUpdateBalancerStateparsedTargetauthoritycsMgrbalancerBuildOptsblockingpickerresolverWrapperscmkpcurBalancerNamebalancerWrapperretryThrottlerfirstResolveEventInvokeWaitForStateChangescWatcherwaitForResolvedAddrsmaybeApplyDefaultServiceConfigupdateResolverStateswitchBalancernewAddrConnremoveAddrConnchannelzMetricGetMethodConfiggetTransportapplyServiceConfigAndBalancerResetConnectBackoffgetResolverUnaryInvokerCallOptioncallInfoClientStreamCloseSendRecvMsgSendMsgbaseCodeccompressorTypefailFaststreammaxReceiveMessageSizemaxSendMessageSizecredscodecmaxRetryRPCBufferSizeafterbeforeStreamClientInterceptorStreamDescStreamHandlerServerStreamStreamNameServerStreamsClientStreamsStreamerCompressorDecompressorStrategyBackoffConnectOptionsFailOnNonTempDialErrorKeepaliveParamsStatsHandlerInitialWindowSizeInitialConnWindowSizePickerConnectivityStateHealthCheckerDisableServiceConfigunaryIntstreamIntchainUnaryIntschainStreamIntscpblocktimeoutscChancoptscallOptionsbalancerBuilderchannelzParentIDdisableServiceConfigdisableRetrydisableHealthCheckhealthCheckFuncminConnectTimeoutdefaultServiceConfigdefaultServiceConfigRawJSONresolveNowBackoffresolversConnPoolClientConnInterfaceCredentialsFileCredentialsJSONAudiencesHTTPClientGRPCDialOptsGRPCConnGRPCConnPoolGRPCConnPoolSizeNoAuthTelemetryDisabledQuotaProjectRequestReasonWithHTTPClientUnable to communicate with Google "Unable to communicate with Google "UserinfoplusServerResponseHTTPStatusCodeFamilyNameGenderGivenNameHdLocalePictureVerifiedEmailForceSendFieldsNullFieldsUserinfoGetCallSetMultiurlParams_ifNoneMatch_ctx_header_IfNoneMatchdoRequestUnable to retrieve Google email "Unable to retrieve Google email "Domain '"Domain '"' is not a member of required Google domain(s): "' is not a member of required Google domain(s): "not in required domain"not in required domain" GoogleEndpoint is Google's OAuth 2.0 endpoint. Copied here to remove tons of package dependencies Google is an oauth2 provider supporting google. Optional google email domain checking ID returns the google application client id Secret returns the google application client secret Scopes for google is only the email address Documentation is here: https://developers.google.com/+/web/api/rest/oauth#email Config is the Google OAuth2 exchange information and endpoints PrincipalID returns the google email address of the user. No domain filtering required, so, the user is authenticated. Check if the account domain is acceptable Group returns the string of domain a user belongs to in GoogleincomingWindowUpdatestreamIDincrementregisterStreamHeadersFrameParamPriorityParamStreamDepExclusiveWeightBlockFragmentEndStreamEndHeadersPadLengthPriorityPingFrameIsAckGoAwayFrameLastStreamIDDebugDataGetCertForOpenIdConnectCallcleanupStreamrstrstCodeonWriteUserinfoV2MeGetCallcbItemheaderFrameendStreaminitStreamcleanuponOrphanedoutgoingWindowUpdateincomingSettingsSettingIDRSTStreamFrameonEachWriteTokeninfoCallIdTokenTokenHandleChannelInternalMetricCallsStartedCallsSucceededCallsFailedLastCallStartedTimestampSettingsFrameNumSettingsHasDuplicatesForeachSettingClientConnStateResolverStateBalancerConfigWindowUpdateFrameIncrementSocketInternalMetricSocketOptionDataGetsockoptChannelzSecurityValueisChannelzSecurityValueStreamsStartedStreamsSucceededStreamsFailedMessagesSentMessagesReceivedKeepAlivesSentLastLocalStreamCreatedTimestampLastRemoteStreamCreatedTimestampLastMessageSentTimestampLastMessageReceivedTimestampLocalFlowControlWindowRemoteFlowControlWindowSocketOptionsSecurityoutgoingSettingsHeadersFrameheaderFragBufHeaderBlockFragmentHeadersEndedHasPriorityMetaHeadersFramePseudoValuemhRegularFieldsPseudoFieldscheckPseudosPeerackoutFlowControlSizeRequestincomingGoAwayPushPromiseParamPromiseIDindexTypeindexedsensitiveJwkJwkKeysAccessTypeExpiresInIssuedToUserId/Users/austinjaybecker/projects/abeck-go-testing/chronograf/oauth2/heroku.goDefaultOrgaccounthrkgolang.org/x/oauth2/heroku"golang.org/x/oauth2/heroku"https://api.heroku.com/account"https://api.heroku.com/account"heroku"heroku"json:"default_organization"`json:"default_organization"`"Accept"application/vnd.heroku+json; version=3"application/vnd.heroku+json; version=3"unable to GET user data from %s. Status: %s"unable to GET user data from %s. Status: %s"Unable to communicate with Heroku. err:"Unable to communicate with Heroku. err:"Unable to decode response from Heroku. err:"Unable to decode response from Heroku. err:"identity"identity" Ensure that Heroku is an oauth2.Provider HerokuAccountRoute is required for interacting with Heroku API Heroku is an OAuth2 Provider allowing users to authenticate with Heroku to gain access to Chronograf OAuth2 Secrets set of organizations permitted to access the protected resource. Empty means "all" Config returns the OAuth2 exchange information and endpoints ID returns the Heroku application client ID Name returns the name of this provider (heroku) PrincipalID returns the Heroku email address of the user. Requests fail to Heroku unless this Accept header is set. check if member of org Group returns the Heroku organization that user belongs to. Scopes for heroku is "identity" which grants access to user account information. This will grant us access to the user's email address which is used as the Principal's identifier. Secret returns the Heroku application client secret/Users/austinjaybecker/projects/abeck-go-testing/chronograf/oauth2/jwt.goextensionprincipaltokenStringjwtTokenjwksurljwkcertPkixjwksrriatx509crypto/x509"crypto/x509"json:"grp,omitempty"`json:"grp,omitempty"`claim has no subject"claim has no subject"TimeFuncSigningMethodRSAunexpected signing method: %v"unexpected signing method: %v"json:"kty"`json:"kty"`json:"use"`json:"use"`json:"alg"`json:"alg"`json:"kid"`json:"kid"`json:"x5t"`json:"x5t"`json:"n"`json:"n"`json:"e"`json:"e"`json:"x5c"`json:"x5c"`json:"keys"`json:"keys"`unsupported signing method: %v"unsupported signing method: %v"token JWKSURL not specified, cannot validate RS256 signature"token JWKSURL not specified, cannot validate RS256 signature"kid"kid"base64 decode error for JWK kid %v"base64 decode error for JWK kid %v"no signing key found for kid %v"no signing key found for kid %v"ParseCertificateParseWithClaimsunable to convert claims to standard claims"unable to convert claims to standard claims"claims duration is different from auth lifespan"claims duration is different from auth lifespan"token is not valid"token is not valid"token has no claims"token has no claims"NewWithClaimsSigningMethodHS256 Ensure JWT conforms to the Tokenizer interface JWT represents a javascript web token that can be validated or marshaled into string. NewJWT creates a new JWT using time.Now secret is used for signing and validating signatures (HS256/HMAC) jwksurl is used for validating RS256 signatures. Ensure Claims implements the jwt.Claims interface Claims extends jwt.StandardClaims' Valid to make sure claims has a subject. We were unable to find a standard claim at https://www.iana.org/assignments/jwt/jwt.xhtml that felt appropriate for Organization. As a result, we added a custom `org` field. that felt appropriate for a users Group(s). As a result we added a custom `grp` field. Multiple groups may be specified by comma delimiting the various group. The singlular `grp` was chosen over the `grps` to keep consistent with the JWT naming convention (it is common for singlularly named values to actually be arrays, see `given_name`, `family_name`, and `middle_name` in the iana link provided above). I should add the discalimer I'm currently sick, so this thought process might be off. Valid adds an empty subject test to the StandardClaims checks. ValidPrincipal checks if the jwtToken is signed correctly and validates with Claims.  lifespan is the maximum valid lifetime of a token.  If the lifespan is 0 then the auth lifespan duration is not checked. Check for expected signing method. KeyFunc verifies HMAC or RSA/RS256 signatures For the id_token, the recommended signature algorithm is RS256, which means we need to verify the token against a public key. This public key is available from the key discovery service in JSON Web Key (JWK). JWK is specified in RFC 7517. The location of the key discovery service (JWKSURL) is published in the OpenID Provider Configuration Information at /.well-known/openid-configuration implements rfc7517 section 4.7 "x5c" (X.509 Certificate Chain) Parameter JWK defines a JSON Web KEy nested struct JWKS defines a JKW[] KeyFuncRS256 verifies RS256 signed JWT tokens, it looks up the signing key in the key discovery service Don't forget to validate the alg is what you expect: read JWKS document from key discovery service parse json to struct extract cert when kid and alg match FIXME: optionally walk the key chain, see rfc7517 section 4.7 parse certificate (from PKIX format) and return signing key ValidClaims validates a token with StandardClaims 1. Checks for expired tokens 2. Checks if time is after the issued at 3. Check if time is after not before (nbf) 4. Check if subject is not empty 5. Check if duration less than auth lifespan at time of this writing and researching the docs, token.Valid seems to be always true at time of this writing and researching the docs, there will always be claims If the duration of the claim is longer than the auth lifespan then this is an invalid claim because server assumes that lifespan is the maximum possible duration.  However, a lifespan of zero means that the duration comparison against the auth duration is not needed. GetClaims extracts claims from id_token Create creates a signed JWT token from user that expires at Principal's ExpireAt time. Create a new token object, specifying signing method and the claims you would like it to contain. Sign and get the complete encoded token as a string using the secret this will only fail if the JSON can't be encoded correctly ExtendedPrincipal sets the expires at to be the current time plus the extention into the future Extend the time of expiration.  Do not change IssuedAt as the lifetime of the token is extended, but, NOT the original time of issue. This is used to enforce a maximum lifetime of a token/Users/austinjaybecker/projects/abeck-go-testing/chronograf/oauth2/mux.gooauthClientconfcsrfbasepath600000000000/login"/login"remote_addr"remote_addr""url"Internal authentication error: "Internal authentication error: "StatusInternalServerErrorAccessTypeOnlineRedirectStatusTemporaryRedirect307"state"Invalid OAuth state received: "Invalid OAuth state received: ""code"Unable to exchange code for token "Unable to exchange code for token "id_token"id_token"Found an extra id_token, but option --useidtoken is not set"Found an extra id_token, but option --useidtoken is not set"Found an extra id_token"Found an extra id_token"Provider implements PrincipalIDFromClaims()"Provider implements PrincipalIDFromClaims()"Cannot cast id_token as string"Cannot cast id_token as string"Parsing extra id_token failed:"Parsing extra id_token failed:"Found claims: "Found claims: "Requested claim not found in id_token:"Requested claim not found in id_token:"Provider does not implement PrincipalIDFromClaims()"Provider does not implement PrincipalIDFromClaims()"Unable to get principal identifier "Unable to get principal identifier "Unable to get OAuth Group"Unable to get OAuth Group"Unable to get add session to response "Unable to get add session to response "User "User " is authenticated" is authenticated" Check to ensure AuthMux is an oauth2.Mux TenMinutes is the default length of time to get a response back from the OAuth provider NewAuthMux constructs a Mux handler that checks a cookie against the authenticator AuthMux services an Oauth2 interaction with a provider and browser and stores the resultant token in the user's browser as a cookie. The benefit of this is that the cookie's authenticity can be verified independently by any Chronograf instance as long as the Authenticator has no external dependencies (e.g. on a Database). Provider is the OAuth2 service Auth is used to Authorize after successful OAuth2 callback and Expire on Logout Tokens is used to create and validate OAuth2 "state" Logger is used to give some more information about the OAuth2 process SuccessURL is redirect location after successful authorization FailureURL is redirect location after authorization failure Now returns the current time (for testing) UseIDToken enables OpenID id_token support Login uses a Cookie with a random string as the state validation method.  JWTs are a good choice here for encoding because they can be validated without storing state. Login returns a handler that redirects to the providers OAuth login. We are creating a token with an encoded random string to prevent CSRF attacks This token will be validated during the OAuth callback. We'll give our users 10 minutes from this point to type in their oauth2 provider's password. If the callback is not received within 10 minutes, then authorization will fail. 32 is not important... just long This token will be valid for 10 minutes.  Any chronograf server will be able to validate this token. This is likely an internal server error Callback is used by OAuth2 provider after authorization is granted.  If granted, Callback will set a cookie with a month-long expiration.  It is recommended that the value of the cookie be encoded as a JWT because the JWT can be validated without the need for saving state. The JWT contains the principal's identifier (e.g.  email address). Check if the OAuth state token is valid to prevent CSRF The state variable we set is actually a token.  We'll check if the token is valid.  We don't need to know anything about the contents of the principal only that it hasn't expired. Exchange the code back with the provider to the the token if we received an extra id_token, inspect it otherwise perform an additional lookup Using the token get the principal identifier from the provider Logout handler will expire our authentication cookie and redirect to the successURL/Users/austinjaybecker/projects/abeck-go-testing/chronograf/oauth2/oauth2.go"principal"not a member of the required organization"not a member of the required organization" PrincipalKey is used to pass principal via context.Context to request-scoped functions. ErrAuthentication means that oauth2 exchange failed ErrOrgMembership means that the user is not in the OAuth2 filtered group Types  Principal is any entity that can be authenticated Interfaces  Provider are the common parameters for all providers (RFC 6749) ID is issued to the registered client by the authorization (RFC 6749 Section 2.2) Secret associated is with the ID (Section 2.2) Scopes is used by the authorization server to "scope" responses (Section 3.3) Config is the OAuth2 configuration settings for this provider PrincipalID with fetch the identifier to be associated with the principal. Name is the name of the Provider Group is a comma delimited list of groups and organizations for a provider TODO: This will break if there are any group names that contain commas.       I think this is okay, but I'm not 100% certain. Mux is a collection of handlers responsible for servicing an Oauth2 interaction between a browser and a provider Authenticator represents a service for authenticating users. Token represents a time-dependent reference (i.e. identifier) that maps back to the sensitive data through a tokenization system Tokenizer substitutes a sensitive data element (Principal) with a non-sensitive equivalent, referred to as a token, that has no extrinsic or exploitable meaning or value. Create issues a token at Principal's IssuedAt that lasts until Principal's ExpireAt ValidPrincipal checks if the token has a valid Principal and requires a lifespan duration to ensure it complies with possible server runtime arguments. ExtendedPrincipal adds the extention to the principal's lifespan. GetClaims returns a map with verified claims/Users/austinjaybecker/projects/abeck-go-testing/chronograf/oauth2/time.go DefaultNowTime returns UTC time at the present moment/Users/austinjaybecker/projects/abeck-go-testing/chronograf/organizations/Users/austinjaybecker/projects/abeck-go-testing/chronograf/organizations/dashboards.goNewOrganizationConfigStoreNewOrganizationsStorevalidOrganizationvalidOrganizationRoles ensure that DashboardsStore implements chronograf.DashboardStore DashboardsStore facade on a DashboardStore that filters dashboards by organization. NewDashboardsStore creates a new DashboardsStore from an existing chronograf.DashboardStore and an organization string All retrieves all dashboards from the underlying DashboardStore and filters them This filters dashboards without allocating Add creates a new Dashboard in the DashboardsStore with dashboard.Organization set to be the organization from the dashboard store. Get returns a Dashboard if the id exists and belongs to the organization that is set. Update the dashboard in DashboardsStore./Users/austinjaybecker/projects/abeck-go-testing/chronograf/organizations/org_config.gooc ensure that OrganizationConfig implements chronograf.OrganizationConfigStore OrganizationConfigStore facade on a OrganizationConfig that filters OrganizationConfigs by organization. NewOrganizationConfigStore creates a new OrganizationConfigStore from an existing chronograf.OrganizationConfigStore and an organization string FindOrCreate gets an organization's config or creates one if none exists Put the OrganizationConfig in OrganizationConfigStore./Users/austinjaybecker/projects/abeck-go-testing/chronograf/organizations/organizations.godefaultOrgID"organization"expect non nil context"expect non nil context"expected organization key to be a string"expected organization key to be a string"expected organization key to be set"expected organization key to be set"cannot create organization"cannot create organization" ContextKey is the key used to specify the organization via context prevents panic in case of nil context should never happen ensure that OrganizationsStore implements chronograf.OrganizationStore OrganizationsStore facade on a OrganizationStore that filters organizations NewOrganizationsStore creates a new OrganizationsStore from an existing chronograf.OrganizationStore and an organization string All retrieves all organizations from the underlying OrganizationStore and filters them This filters organizations without allocating Add creates a new Organization in the OrganizationsStore with organization.Organization set to be the organization from the organization store. Get returns a Organization if the id exists and belongs to the organization that is set. Update the organization in OrganizationsStore./Users/austinjaybecker/projects/abeck-go-testing/chronograf/organizations/servers.go ensure that ServersStore implements chronograf.ServerStore ServersStore facade on a ServerStore that filters servers NewServersStore creates a new ServersStore from an existing chronograf.ServerStore and an organization string All retrieves all servers from the underlying ServerStore and filters them This filters servers without allocating Add creates a new Server in the ServersStore with server.Organization set to be the organization from the server store. Delete the server from ServersStore Get returns a Server if the id exists and belongs to the organization that is set. Update the server in ServersStore./Users/austinjaybecker/projects/abeck-go-testing/chronograf/organizations/sources.go ensure that SourcesStore implements chronograf.SourceStore SourcesStore facade on a SourceStore that filters sources NewSourcesStore creates a new SourcesStore from an existing chronograf.SourceStore and an organization string All retrieves all sources from the underlying SourceStore and filters them This filters sources without allocating Add creates a new Source in the SourcesStore with source.Organization set to be the organization from the source store. Delete the source from SourcesStore Get returns a Source if the id exists and belongs to the organization that is set. Update the source in SourcesStore./Users/austinjaybecker/projects/abeck-go-testing/chronograf/organizations/users.gonumRolesInOrganizationusrusrsuser role must have an Organization"user role must have an Organization"organizationID %s does not match %s"organizationID %s does not match %s"user role must have a Name"user role must have a Name" UsersStore facade on a UserStore that filters a users roles The high level idea here is to use the same underlying store for all users. In particular, this is done by having all the users Roles field be a set of all of the users roles in all organizations. Each CRUD method here takes care to ensure that the only roles that are modified are the roles for the organization that was provided on the UsersStore. NewUsersStore creates a new UsersStore from an existing chronograf.UserStore and an organization string validOrganizationRoles ensures that each User Role has both an associated Organization and a Name Get searches the UsersStore for using the query. The roles returned on the user are filtered to only contain roles that are for the organization specified on the organization store. This filters a users roles so that the resulting struct only contains roles from the organization on the UsersStore. This means that the user does not belong to the organization and therefore, is not found. Add creates a new User in the UsersStore. It validates that the user provided only has roles for the organization set on the UsersStore. If a user is not found in the underlying, it calls the underlying UsersStore Add method. If a user is found, it removes any existing roles a user has for an organization and appends the roles specified on the provided user and calls the uderlying UsersStore Update method. Validates that the users roles are only for the current organization. retrieve the user from the underlying store If there is no error continue to the rest of the code If user is not found in the backed store, attempt to add the user return the error Filter the retrieved users roles so that the resulting struct only contains roles that are not from the organization on the UsersStore. If the user already has a role in the organization then the user cannot be "created". This can be thought of as: (total # of roles a user has) - (# of roles not in the organization) = (# of roles in organization) if this value is greater than 1 the user cannot be "added". Set the users roles to be the union of the roles set on the provided user and the user that was found in the underlying store u.SuperAdmin == true is logically equivalent to u.SuperAdmin, however it is more clear on a conceptual level to check equality TODO(desa): this should go away with https://github.com/influxdata/influxdb/chronograf/issues/2207 I do not like checking super admin here. The organization users store should only be concerned about organizations. If the user being added already existed in a previous organization, and was already a SuperAdmin, then this ensures that they retain their SuperAdmin status. And if they weren't a SuperAdmin, and the user being added has been granted SuperAdmin status, they will be promoted Update the user in the underlying store Return the provided user with ID set Delete a user from the UsersStore. This is done by stripping a user of any roles it has in the organization speicified on the UsersStore. Filter the retrieved users roles so that the resulting slice contains roles that are not scoped to the organization provided Update a user in the UsersStore. Make a copy of the usr so that we dont modify the underlying add roles on to the user that was passed in All returns all users where roles have been filters to be exclusively for the organization provided on the UsersStore. retrieve all users from the underlying UsersStore Filter users to only contain users that have at least one role in the provided organization. Only add users if they have a role in the associated organization This is unperformant, but should rarely be used./Users/austinjaybecker/projects/abeck-go-testing/chronograf/roles/Users/austinjaybecker/projects/abeck-go-testing/chronograf/roles/roles.goAdminRoleAdminRoleNameEditorRoleEditorRoleNameMemberRoleMemberRoleNameSuperAdminStatusViewerRoleWildcardRoleNamehasAuthorizedRolevalidRole"role"expected role key to be a string"expected role key to be a string"editorexpected role key to be set"expected role key to be set""viewer""editor""admin"superadmin"superadmin" role via context Chronograf User Roles Indicatior that the server should retrieve the default role for the organization. MemberRole is the role for a user who can only perform No operations. ViewerRole is the role for a user who can only perform READ operations on Dashboards, Rules, Sources, and Servers, EditorRole is the role for a user who can perform READ and WRITE operations on Dashboards, Rules, Sources, and Servers. AdminRole is the role for a user who can perform READ and WRITE operations on Dashboards, Rules, Sources, Servers, and Users/Users/austinjaybecker/projects/abeck-go-testing/chronograf/roles/sources.goprovidedRolesourceRole NOTE: This code is currently unused. however, it has been left in place because we anticipate that it may be used in the future. It was originally developed as a misunderstanding of https://github.com/influxdata/influxdb/chronograf/issues/1915 by minimum role required to access the source. The role is passed around on the context and set when the SourcesStore is instantiated. chronograf.SourceStore and an role string by role. Add creates a new Source in the SourcesStore with source.Role set to be the role from the source store. Get returns a Source if the id exists and belongs to the role that is set. hasAuthorizedRole checks that the role provided has at least the minimum role required./Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/TODO.goAddQueryConfigAddQueryConfigsAllRoutesAssetsOptsAuthAPIAuthRouteAuthRoutesAuthorizedTokenAuthorizedUserChunkSizeCorrectWidthHeightCustomLinkDashboardDefaultsDataStoreDebugDefaultDebugDirDefaultHeightDefaultWidthDirectStoreErrNotFlusherFlushingHandlerHSTSHasAuthorizedTokenHasCorrectAxesHasCorrectColorsHasCorrectLegendInfluxClientJSONTypeMountableRouterMoveTimeShiftMultiDashboardBuilderMultiKapacitorBuilderMultiLayoutBuilderMultiOrganizationBuilderMultiSourceBuilderMuxOptsNewCustomLinksNewDefaultURLPrefixerNewMuxNewServiceV2PathEscapeQueriesRequestQueriesResponseQueryRequestQueryResponseRawStoreAccessRedocRouteMatchesPrincipalServerContextKeySpecTimeSeriesClientToQueryConfigURLPrefixerUserContextKeyValidDashboardCellRequestValidDashboardRequestValidDatabaseRequestValidInfluxRequestValidRetentionPolicyRequestValidTemplateRequestValidateQueryConfigannotationLinksannotationResponseannotationsResponseapplyMappingauthConfigResponseclientUsageconfigLinksconfigResponsedashboardCellLinksdashboardCellResponsedashboardLinksdashboardResponsedbLinksdbResponsedbsResponseencodeJSONenvResponseflushingResponseWritergetConfigLinksResponsegetDashboardsResponsegetExternalLinksResponsegetFluxLinksResponsegetLayoutsResponsegetOrganizationConfigLinksResponsegetPrincipalgetRoutesResponsegetSchemegetValidPrincipalhasOrganizationContexthasRoleContexthasServerContexthasSuperAdminContexthasUserContextinvalidDatainvalidJSONlayoutResponselimitQuerylinklocationlogViewerConfigResponsemappingResponsemappingsRequestmappingsResponsematchGroupmeLinksmeRequestmeResponsemeasurementLinksmeasurementsResponsenewAnnotationRequestnewAnnotationResponsenewAnnotationsResponsenewAuthConfigResponsenewCellResponsenewCellResponsesnewConfigResponsenewDBResponsenewDashboardResponsenewEnvResponsenewLayoutResponsenewLogViewerConfigResponsenewMappingResponsenewMappingsResponsenewMeResponsenewMeasurementLinksnewNoAuthMeResponsenewOrganizationConfigResponsenewOrganizationResponsenewOrganizationsResponsenewServicenewTemplateResponsenewTemplateResponsesnewUserResponsenewUsersResponsenoAuthMeResponsenotFoundoffsetQueryoneOfopenServiceorganizationConfigLinksorganizationConfigResponseorganizationRequestorganizationResponseorganizationsResponseparamIDparamStrpatchServiceRequestpingFluxpostInfluxResponsepostServiceRequestprovidereportUsageStatsrpLinksrpResponseselfLinksserverContextserverContextKeyserviceLinksservicessetSuperAdminsingleJoiningSlashstartTimestatusWritersuperAdminProviderGroupstemplateLinkstemplateResponsetemplatesResponsestimeMilliFormatunknownErrorWithMessageuntilupdateAnnotationRequestuserContextKeyuserRequestuserResponseusersResponsevalidAnnotationQueryvalidBasepathvalidFieldTypesvalidLogViewerConfigvalidMeasurementQuerywrapResponseWriterUseAuthProviderFuncsTemplateVarsValidCreateValidUpdateValidRolesSuperAdminProviderGroupsEnvNewAnnotationRemoveAnnotationUpdateAnnotationDashboardCellsNewDashboardCellDashboardCellIDReplaceDashboardCellReplaceAuthConfigNewDashboardRemoveDashboardReplaceDashboardGetDatabasesNewDatabaseallRPsNewRetentionPolicyUpdateRetentionPolicyDropRetentionPolicyInfluxLayoutsIDmapPrincipalToSuperAdminmapPrincipalToRolesNewMappingUpdateMappingRemoveMappingorganizationExistsUpdateMefirstUsernewUsersAreSuperAdminusersOrganizationsOrganizationLogViewerConfigReplaceOrganizationLogViewerConfigNewOrganizationRemoveOrganizationProxyPostProxyPatchProxyGetProxyDeleteServicesServiceIDRemoveServiceUpdateServiceNewTemplateRemoveTemplateReplaceTemplateNewUserRemoveUservalidRolesWithLinksRPsbuilderSubstituteheaderWrittendupHeaderwrwQueryASTQueryTemplatedAttrsmaxlenASTSuggestionsUsageDataProductHrefRelStatusFeedarFlusherValidDefaultRoleCurrentOrganizationDelegateGetPrincipalLogoutLinkAllUsersDashboardsV2ExternalLinks/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/annotations.gostopTimeannoIDaux"since""until"2006-01-02T15:04:05.999Z07:00"2006-01-02T15:04:05.999Z07:00"json:"self"`json:"self"`json:"startTime"`json:"startTime"`json:"endTime"`json:"endTime"`json:"text"`json:"text"`/chronograf/v1/sources"/chronograf/v1/sources"%s/%d/annotations/%s"%s/%d/annotations/%s"json:"annotations"`json:"annotations"`since parameter is required"since parameter is required"StatusUnprocessableEntity422unable to connect to source %d: %v"unable to connect to source %d: %v"StatusBadRequest400error loading annotations: %v"error loading annotations: %v"aid"aid"error loading annotation: %v"error loading annotation: %v"json:"text,omitempty"`json:"text,omitempty"`Timeout waiting for response"Timeout waiting for response"StatusRequestTimeout408Timeout waiting for  response"Timeout waiting for  response"json:"startTime,omitempty"`json:"startTime,omitempty"`json:"endTime,omitempty"`json:"endTime,omitempty"`update request must have at least one field"update request must have at least one field" Self link mapping to this resource StartTime in RFC3339 of the start of the annotation EndTime in RFC3339 of the end of the annotation if until isn't stated, the default time is now Annotations returns all annotations within the annotations store Annotation returns a specified annotation id within the annotations store StartTime is the time in rfc3339 milliseconds EndTime is the time in rfc3339 milliseconds NewAnnotation adds the annotation from a POST body to the annotations store RemoveAnnotation removes the annotation from the time series source TODO: make sure that endtime is after starttime Update must have at least one field set UpdateAnnotation overwrite an existing annotation/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/assets.goassetsgithub.com/influxdata/influxdb/v2/chronograf/dist"github.com/influxdata/influxdb/v2/chronograf/dist"../ui/build"../ui/build"../ui/build/index.html"../ui/build/index.html"ui/build"ui/build"ui/build/index.html"ui/build/index.html"text/html; charset=utf-8"text/html; charset=utf-8""server"Serving assets"Serving assets" Dir is prefix of the assets in the bindata Default is the default item to load if 404 DebugDir is the prefix of the assets in development mode DebugDefault is the default item to load if 404 DefaultContentType is the content-type to return for the Default file AssetsOpts configures the asset middleware Develop when true serves assets from ui/build directory directly; false will use internal bindata. Logger will log the asset served Assets creates a middleware that will serve a single page app./Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/auth.goisServerisSuperAdminserverCtxtoken_auth"token_auth"Invalid principal"Invalid principal"Unable to extend principal"Unable to extend principal"raw_store"raw_store"User making request is not a SuperAdmin"User making request is not a SuperAdmin"User is not authorized"User is not authorized"role_auth"role_auth"Failed to retrieve the default organization: %v"Failed to retrieve the default organization: %v"Failed to retrieve principal from context"Failed to retrieve principal from context"Failed to retrieve scheme from context"Failed to retrieve scheme from context"Failed to retrieve organization %s from organizations store"Failed to retrieve organization %s from organizations store"Failed to retrieve user"Failed to retrieve user"User %d has too many role in organization. User: %#v.Please report this log at https://github.com/influxdata/influxdb/chronograf/issues/new"`User %d has too many role in organization. User: %#v.Please report this log at https://github.com/influxdata/influxdb/chronograf/issues/new"`please have administrator check logs and report error"please have administrator check logs and report error" HasAuthorizedToken extracts the token from a request and validates it using the authenticator. It is used by routes that need access to the token to populate links request. AuthorizedToken extracts the token and validates; if valid the next handler will be run.  The principal will be sent to the next handler via the request's Context.  It is up to the next handler to determine if the principal has access. On failure, will return http.StatusForbidden. We do not check the authorization of the principal.  Those served further down the chain should do so. If the principal is valid we will extend its lifespan into the future Send the principal to the next handler RawStoreAccess gives a super admin access to the data store without a facade. AuthorizedUser extracts the user name and provider from context. If the user and provider can be found on the context, we look up the user by their name and provider. If the user is found, we verify that the user has at at least the role supplied. If there is no auth, then set the organization id to be the default org id on context so that calls like hasOrganizationContext as used in Organization Config service method OrganizationConfig can successfully get the organization id And if there is no auth, then give the user raw access to the DataStore This is as if the user was logged into the default organization validate that the organization exists TODO: seems silly to look up a user twice In particular this is used by sever/users.go so that we know when and when not to allow users to make someone a super admin To access resources (servers, sources, databases, layouts) within a DataStore, an organization and a role are required even if you are a super admin or are not using auth. Every user's current organization is set on context to filter the resources accessed within a DataStore, including for super admin or when not using auth. In this way, a DataStore can treat all requests the same, including those from a super admin and when not using auth. As for roles, in the case of super admin or when not using auth, the user's role on context (though not on their JWT or user) is set to be admin. In order to access all resources belonging to their current organization. use the first role, since there should only ever be one for any particular organization and hasAuthorizedRole should ensure that at least one role for the org exists SuperAdmins should have been authorized before this. This is only meant to restrict access for non-superadmins./Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/builders.gobinAppsinfluxStorestoresmemStoregithub.com/influxdata/influxdb/v2/chronograf/canned"github.com/influxdata/influxdb/v2/chronograf/canned"github.com/influxdata/influxdb/v2/chronograf/filestore"github.com/influxdata/influxdb/v2/chronograf/filestore"github.com/influxdata/influxdb/v2/chronograf/memdb"github.com/influxdata/influxdb/v2/chronograf/memdb"github.com/influxdata/influxdb/v2/chronograf/multistore"github.com/influxdata/influxdb/v2/chronograf/multistore" LayoutBuilder is responsible for building Layouts MultiLayoutBuilder implements LayoutBuilder and will return a Layouts Build will construct a Layouts of canned and db-backed personalized layouts These apps are those handled from a directory These apps are statically compiled into chronograf Acts as a front-end to both the bolt layouts, filesystem layouts and binary statically compiled layouts. The idea here is that these stores form a hierarchy in which each is tried sequentially until the operation has success.  So, the database is preferred over filesystem over binary data. DashboardBuilder is responsible for building dashboards MultiDashboardBuilder builds a DashboardsStore backed by bolt and the filesystem Build will construct a Dashboard store of filesystem and db-backed dashboards These dashboards are those handled from a directory Acts as a front-end to both the bolt dashboard and filesystem dashboards. the operation has success.  So, the database is preferred over filesystem SourcesBuilder builds a MultiSourceStore MultiSourceBuilder implements SourcesBuilder Build will return a MultiSourceStore KapacitorBuilder builds a KapacitorStore MultiKapacitorBuilder implements KapacitorBuilder Build will return a multistore facade KapacitorStore over memdb and bolt OrganizationBuilder is responsible for building dashboards MultiOrganizationBuilder builds a OrganizationsStore backed by bolt and the filesystem Build will construct a Organization store of filesystem and db-backed dashboards These organization are those handled from a directory Acts as a front-end to both the bolt org and filesystem orgs./Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/cells.golbldIDnewAxesdcellsaxispropvalidOptscellidhttprouteridgengithub.com/influxdata/httprouter"github.com/influxdata/httprouter"/chronograf/v1/dashboards"/chronograf/v1/dashboards""x"y"y"y2"y2"%s/%d/cells/%s"%s/%d/cells/%s"chronograf dashboard cell was nil"chronograf dashboard cell was nil""2"threshold"threshold"background"background"scale"scale"right"right"left"left"static"static"Error creating cell ID of dashboard %d: %v"Error creating cell ID of dashboard %d: %v"Error adding cell %s to dashboard %d: %v"Error adding cell %s to dashboard %d: %v"ParamByNameParamsFromContext"cid"Error removing cell %s from dashboard %d: %v"Error removing cell %s from dashboard %d: %v"Error updating cell %s in dashboard %d: %v"Error updating cell %s in dashboard %d: %v" DefaultWidth is used if not specified DefaultHeight is used if not specified Copy to handle race condition ensure x, y, and y2 axes always returned ValidDashboardCellRequest verifies that the dashboard cells have a query and have the correct axes specified HasCorrectAxes verifies that only permitted axes exist within a DashboardCell HasCorrectColors verifies that the format of each color is correct HasCorrectLegend verifies that the format of the legend is correct No legend set Remember! if we add other types, update ErrInvalidLegendType oneOf reports whether a provided string is a member of a variadic list of valid options CorrectWidthHeight changes the cell to have at least the minimum width and height MoveTimeShift moves TimeShift from the QueryConfig to the DashboardQuery AddQueryConfig updates a cell by converting InfluxQL into queryconfigs If influxql cannot be represented by a full query config, then, the query config's raw text is set to the command. DashboardCells returns all cells from a dashboard within the store NewDashboardCell adds a cell to an existing dashboard DashboardCellID gets a specific cell from an existing dashboard RemoveDashboardCell removes a specific cell from an existing dashboard ReplaceDashboardCell replaces a cell entirely within an existing dashboard/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/config.goauthConfig/chronograf/v1/config"/chronograf/v1/config"/chronograf/v1/config/auth"/chronograf/v1/config/auth"Configuration object was nil"Configuration object was nil" Auth link to the auth config endpoint Config retrieves the global application configuration AuthConfig retrieves the auth section of the global application configuration ReplaceAuthConfig replaces the auth section of the global application configuration/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/context.go ServerContextKey is the key used to specify that the server is making the requet via context hasServerContext specifies if the context contains the ServerContextKey and that the value stored there is true/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/dashboards.goddidParamorignewDashjson:"dashboards"`json:"dashboards"`%s/%d"%s/%d"%s/%d/cells"%s/%d/cells"%s/%d/templates"%s/%d/templates"Error loading dashboards"Error loading dashboards"error storing dashboard %v: %v"error storing dashboard %v: %v"Could not parse dashboard ID: %s"Could not parse dashboard ID: %s"StatusNotFound404ID %d not found"ID %d not found"Error updating dashboard ID %d: %v"Error updating dashboard ID %d: %v"update must include either name or cells"update must include either name or cells" Cells link to the cells endpoint Templates link to the templates endpoint Dashboards returns all dashboards within the store DashboardID returns a single specified dashboard NewDashboard creates and returns a new dashboard object RemoveDashboard deletes a dashboard ReplaceDashboard completely replaces a dashboard UpdateDashboard completely updates either the dashboard name or the cells ValidDashboardRequest verifies that the dashboard cells have a query DashboardDefaults updates the dashboard with the default values if none are specified AddQueryConfigs updates all the celsl in the dashboard to have query config objects corresponding to their influxql queries./Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/databases.gorpssrcIDdatabasesdbsdbsvcpostedDBdropErrallRPpostedRPlimitParamoffsetParam"limit""offset"json:"retentionPolicies"`json:"retentionPolicies"`json:"measurements"`json:"measurements"`%s/%d/dbs/%s"%s/%d/dbs/%s"%s/%d/dbs/%s/rps"%s/%d/dbs/%s/rps"%s/%d/dbs/%s/measurements?limit=100&offset=0"%s/%d/dbs/%s/measurements?limit=100&offset=0"json:"databases"`json:"databases"`json:"duration"`json:"duration"`json:"replication"`json:"replication"`json:"shardDuration"`json:"shardDuration"`json:"isDefault"`json:"isDefault"`%s/%d/dbs/%s/rps/%s"%s/%d/dbs/%s/rps/%s"json:"first"`json:"first"`json:"next,omitempty"`json:"next,omitempty"`json:"prev,omitempty"`json:"prev,omitempty"`%s/%d/dbs/%s/measurements?limit=%d&offset=%d"%s/%d/dbs/%s/measurements?limit=%d&offset=%d"%s/%d/dbs/%s/measurements?limit=%d&offset=0"%s/%d/dbs/%s/measurements?limit=%d&offset=0"unable to connect get RPs %d: %v"unable to connect get RPs %d: %v"Unable to get measurements %d: %v"Unable to get measurements %d: %v"name is required"name is required"duration is required"duration is required"replication factor is invalid"replication factor is invalid" URL for retention policies for this database URL for measurements for this database RPs are the retention policies for a database Links are URI locations related to the database newDBResponse creates the response for the /databases endpoint WithLinks adds links to an rpResponse in place names of all measurements Links are the URI locations for measurements pages GetDatabases queries the list of all databases for a source NewDatabase creates a new database within the datastore DropDatabase removes a database from a data source RetentionPolicies lists retention policies within a database NewRetentionPolicy creates a new retention policy for a database UpdateRetentionPolicy modifies an existing retention policy for a database DropRetentionPolicy removes a retention policy from a database Measurements lists measurements within a database ValidDatabaseRequest checks if the database posted is valid ValidRetentionPolicyRequest checks if a retention policy is valid on POST/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/env.go/chronograf/v1/env"/chronograf/v1/env" Environment retrieves the global application configuration/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/helpers.go"Location"/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/hsts.goStrict-Transport-Security"Strict-Transport-Security"max-age=63072000; includeSubDomains"max-age=63072000; includeSubDomains" HSTS add HTTP Strict Transport Security header with a max-age of two years Inspired from https://blog.bracebin.com/achieving-perfect-ssl-labs-score-with-go/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/influx.godirectorhttputilnet"net"net/http/httputil"net/http/httputil"query field required"query field required"json:"results"`json:"results"`Timeout waiting for Influx response"Timeout waiting for Influx response"Error parsing source url: %v"Error parsing source url: %v"/write"/write"ReverseProxyProxyRequestSetURLSetXForwardedBufferPoolRewriteDirectorFlushIntervalModifyResponseErrorHandlerdefaultErrorHandlergetErrorHandlermodifyResponseflushIntervalcopyResponsecopyBufferhandleUpgradeResponseProxyFromEnvironmentShareddupschansDoChandoCallForgetUnsharedPreferGoStrictErrorslookupGroupresolveAddrListexchangetryOneNamegoLookupHostOrdergoLookupIPgoLookupIPCNAMEOrdergoLookupCNAMEgoLookupPTRinternetAddrListpreferGostrictErrorsgetLookupGroupLookupHostLookupIPAddrLookupIPLookupNetIPlookupIPAddrLookupPortLookupCNAMELookupSRVLookupMXLookupNSLookupTXTLookupAddrgoLookupSRVgoLookupMXgoLookupNSgoLookupTXTlookupHostlookupIPlookupPortlookupCNAMElookupSRVlookupMXlookupNSlookupTXTlookupAddrDualStackFallbackDelayControlContextdualStackfallbackDelay300000000009000000000010000000000 ValidInfluxRequest checks if queries specify a command. results from influx Influx proxies requests to influxdb. TODO: Here I want to return the error code from influx. Set the Host header of the original source URL Because we are acting as a proxy, influxdb needs to have the basic auth or bearer token information set as a header directly The connection to influxdb is using a self-signed certificate. This modifies uses the same values as http.DefaultTransport but specifies InsecureSkipVerifyNSaddrListforResolvepartitionuint128hiloisZeronotsubOneaddOnehalvesbitsSetFrombitsClearedFromcmpValresurrectedv4v6v6u16Is4Is4In6Is6UnmapWithZonewithoutZonehasZoneAs16As4AsSliceAppendTostring4appendTo4string6appendTo6StringExpandedmarshalBinaryWithTrailingBytesQuestionunpackCompressedbitsquestionsanswersauthoritiesadditionalssectionResourceHeaderfixLenSetEDNS0DNSSECAllowedExtendedRCoderesHeaderValidresHeadercheckAdvanceresourceHeaderskipResourceAllQuestionsSkipQuestionSkipAllQuestionsAnswerHeaderAnswerAllAnswersSkipAnswerSkipAllAnswersAuthorityHeaderAllAuthoritiesSkipAuthoritySkipAllAuthoritiesAdditionalHeaderAdditionalAllAdditionalsSkipAdditionalSkipAllAdditionalsCNAMEResourceMXResourceNSResourcePTRResourceSOAResourceTXTResourceSRVResourceAResourceAAAAResourceOPTResourceUnknownResourceOpCodeRCodeAuthoritativeRecursionDesiredRecursionAvailableAuthenticDataCheckingDisableddnsConfigndotsattemptsrotateunknownOptmtimesoffsetsingleRequestuseTCPtrustADnoReloadnameListserverOffsethostLookupOrderSRVMXPrefResourceBodyrealTypeTXTAAAAPTRIsSingleIPMaskedOverlapsMBoxSerialRefreshMinTTLA/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/kapacitors.gotype postKapacitorRequest struct {	Name               *string `json:"name"`               // User facing name of kapacitor instance.; Required: true	URL                *string `json:"url"`                // URL for the kapacitor backend (e.g. http://localhost:9092);/ Required: true	Username           string  `json:"username,omitempty"` // Username for authentication to kapacitor	Password           string  `json:"password,omitempty"`	InsecureSkipVerify bool    `json:"insecureSkipVerify"` // InsecureSkipVerify as true means any certificate presented by the kapacitor is accepted.	Active             bool    `json:"active"`	Organization       string  `json:"organization"` // Organization is the organization ID that resource belongs tofunc (p *postKapacitorRequest) Valid(defaultOrgID string) error {	if p.Name == nil || p.URL == nil {		return fmt.Errorf("name and url required")	}	if p.Organization == "" {		p.Organization = defaultOrgID	url, err := url.ParseRequestURI(*p.URL)	if err != nil {		return fmt.Errorf("invalid source URI: %v", err)	if len(url.Scheme) == 0 {		return fmt.Errorf("invalid URL; no URL scheme defined")	return niltype kapaLinks struct {	Proxy string `json:"proxy"` // URL location of proxy endpoint for this source	Self  string `json:"self"`  // Self link mapping to this resource	Rules string `json:"rules"` // Rules link for defining roles alerts for kapacitor	Tasks string `json:"tasks"` // Tasks link to define a task against the proxy	Ping  string `json:"ping"`  // Ping path to kapacitortype kapacitor struct {	ID                 int       `json:"id,string"`          // Unique identifier representing a kapacitor instance.	Name               string    `json:"name"`               // User facing name of kapacitor instance.	URL                string    `json:"url"`                // URL for the kapacitor backend (e.g. http://localhost:9092)	Username           string    `json:"username,omitempty"` // Username for authentication to kapacitor	Password           string    `json:"password,omitempty"`	InsecureSkipVerify bool      `json:"insecureSkipVerify"` // InsecureSkipVerify as true means any certificate presented by the kapacitor is accepted.	Active             bool      `json:"active"`	Links              kapaLinks `json:"links"` // Links are URI locations related to kapacitor// NewKapacitor adds valid kapacitor store store.func (s *Service) NewKapacitor(w http.ResponseWriter, r *http.Request) {	srcID, err := paramID("id", r)		Error(w, http.StatusUnprocessableEntity, err.Error(), s.Logger)		return	ctx := r.Context()	_, err = s.Store.Sources(ctx).Get(ctx, srcID)		notFound(w, srcID, s.Logger)	var req postKapacitorRequest	if err = json.NewDecoder(r.Body).Decode(&req); err != nil {		invalidJSON(w, s.Logger)	defaultOrg, err := s.Store.Organizations(ctx).DefaultOrganization(ctx)		unknownErrorWithMessage(w, err, s.Logger)	if err := req.Valid(defaultOrg.ID); err != nil {		invalidData(w, err, s.Logger)	srv := chronograf.Server{		SrcID:              srcID,		Name:               *req.Name,		Username:           req.Username,		Password:           req.Password,		InsecureSkipVerify: req.InsecureSkipVerify,		URL:                *req.URL,		Active:             req.Active,		Organization:       req.Organization,	if srv, err = s.Store.Servers(ctx).Add(ctx, srv); err != nil {		msg := fmt.Errorf("error storing kapacitor %v: %v", req, err)		unknownErrorWithMessage(w, msg, s.Logger)	res := newKapacitor(srv)	location(w, res.Links.Self)	encodeJSON(w, http.StatusCreated, res, s.Logger)func newKapacitor(srv chronograf.Server) kapacitor {	httpAPISrcs := "/chronograf/v1/sources"	return kapacitor{		ID:                 srv.ID,		Name:               srv.Name,		Username:           srv.Username,		URL:                srv.URL,		Active:             srv.Active,		InsecureSkipVerify: srv.InsecureSkipVerify,		Links: kapaLinks{			Self:  fmt.Sprintf("%s/%d/kapacitors/%d", httpAPISrcs, srv.SrcID, srv.ID),			Proxy: fmt.Sprintf("%s/%d/kapacitors/%d/proxy", httpAPISrcs, srv.SrcID, srv.ID),			Rules: fmt.Sprintf("%s/%d/kapacitors/%d/rules", httpAPISrcs, srv.SrcID, srv.ID),			Tasks: fmt.Sprintf("%s/%d/kapacitors/%d/proxy?path=/kapacitor/v1/tasks", httpAPISrcs, srv.SrcID, srv.ID),			Ping:  fmt.Sprintf("%s/%d/kapacitors/%d/proxy?path=/kapacitor/v1/ping", httpAPISrcs, srv.SrcID, srv.ID),		},type kapacitors struct {	Kapacitors []kapacitor `json:"kapacitors"`// Kapacitors retrieves all kapacitors from store.func (s *Service) Kapacitors(w http.ResponseWriter, r *http.Request) {	mrSrvs, err := s.Store.Servers(ctx).All(ctx)		Error(w, http.StatusInternalServerError, "Error loading kapacitors", s.Logger)	srvs := []kapacitor{}	for _, srv := range mrSrvs {		if srv.SrcID == srcID && srv.Type == "" {			srvs = append(srvs, newKapacitor(srv))		}	res := kapacitors{		Kapacitors: srvs,	encodeJSON(w, http.StatusOK, res, s.Logger)// KapacitorsID retrieves a kapacitor with ID from store.func (s *Service) KapacitorsID(w http.ResponseWriter, r *http.Request) {	id, err := paramID("kid", r)	srv, err := s.Store.Servers(ctx).Get(ctx, id)	if err != nil || srv.SrcID != srcID || srv.Type != "" {		notFound(w, id, s.Logger)// RemoveKapacitor deletes kapacitor from store.func (s *Service) RemoveKapacitor(w http.ResponseWriter, r *http.Request) {	if err = s.Store.Servers(ctx).Delete(ctx, srv); err != nil {	w.WriteHeader(http.StatusNoContent)type patchKapacitorRequest struct {	Name               *string `json:"name,omitempty"`     // User facing name of kapacitor instance.	URL                *string `json:"url,omitempty"`      // URL for the kapacitor	Username           *string `json:"username,omitempty"` // Username for kapacitor auth	Password           *string `json:"password,omitempty"`	InsecureSkipVerify *bool   `json:"insecureSkipVerify"` // InsecureSkipVerify as true means any certificate presented by the kapacitor is accepted.	Active             *bool   `json:"active"`func (p *patchKapacitorRequest) Valid() error {	if p.URL != nil {		url, err := url.ParseRequestURI(*p.URL)		if err != nil {			return fmt.Errorf("invalid source URI: %v", err)		if len(url.Scheme) == 0 {			return fmt.Errorf("invalid URL; no URL scheme defined")// UpdateKapacitor incrementally updates a kapacitor definition in the storefunc (s *Service) UpdateKapacitor(w http.ResponseWriter, r *http.Request) {	var req patchKapacitorRequest	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {	if err := req.Valid(); err != nil {	if req.Name != nil {		srv.Name = *req.Name	if req.URL != nil {		srv.URL = *req.URL	if req.Password != nil {		srv.Password = *req.Password	if req.Username != nil {		srv.Username = *req.Username	if req.InsecureSkipVerify != nil {		srv.InsecureSkipVerify = *req.InsecureSkipVerify	if req.Active != nil {		srv.Active = *req.Active	if err := s.Store.Servers(ctx).Update(ctx, srv); err != nil {		msg := fmt.Sprintf("Error updating kapacitor ID %d", id)		Error(w, http.StatusInternalServerError, msg, s.Logger)// KapacitorRulesPost proxies POST to kapacitorfunc (s *Service) KapacitorRulesPost(w http.ResponseWriter, r *http.Request) {	if err != nil || srv.SrcID != srcID {	c := kapa.NewClient(srv.URL, srv.Username, srv.Password, srv.InsecureSkipVerify)	var req chronograf.AlertRule	// TODO: validate this data	/*		if err := req.Valid(); err != nil {			invalidData(w, err)			return	*/	if req.Name == "" {		req.Name = req.ID	req.ID = ""	task, err := c.Create(ctx, req)	res := newAlertResponse(task, srv.SrcID, srv.ID)type alertLinks struct {	Self      string `json:"self"`	Kapacitor string `json:"kapacitor"`	Output    string `json:"output"`type alertResponse struct {	chronograf.AlertRule	Links alertLinks `json:"links"`// newAlertResponse formats task into an alertResponsefunc newAlertResponse(task *kapa.Task, srcID, kapaID int) *alertResponse {	res := &alertResponse{		AlertRule: task.Rule,		Links: alertLinks{			Self:      fmt.Sprintf("/chronograf/v1/sources/%d/kapacitors/%d/rules/%s", srcID, kapaID, task.ID),			Kapacitor: fmt.Sprintf("/chronograf/v1/sources/%d/kapacitors/%d/proxy?path=%s", srcID, kapaID, url.QueryEscape(task.Href)),			Output:    fmt.Sprintf("/chronograf/v1/sources/%d/kapacitors/%d/proxy?path=%s", srcID, kapaID, url.QueryEscape(task.HrefOutput)),	if res.AlertNodes.Alerta == nil {		res.AlertNodes.Alerta = []*chronograf.Alerta{}	for i, a := range res.AlertNodes.Alerta {		if a.Service == nil {			a.Service = []string{}			res.AlertNodes.Alerta[i] = a	if res.AlertNodes.Email == nil {		res.AlertNodes.Email = []*chronograf.Email{}	for i, a := range res.AlertNodes.Email {		if a.To == nil {			a.To = []string{}			res.AlertNodes.Email[i] = a	if res.AlertNodes.Exec == nil {		res.AlertNodes.Exec = []*chronograf.Exec{}	for i, a := range res.AlertNodes.Exec {		if a.Command == nil {			a.Command = []string{}			res.AlertNodes.Exec[i] = a	if res.AlertNodes.HipChat == nil {		res.AlertNodes.HipChat = []*chronograf.HipChat{}	if res.AlertNodes.Kafka == nil {		res.AlertNodes.Kafka = []*chronograf.Kafka{}	if res.AlertNodes.Log == nil {		res.AlertNodes.Log = []*chronograf.Log{}	if res.AlertNodes.OpsGenie == nil {		res.AlertNodes.OpsGenie = []*chronograf.OpsGenie{}	for i, a := range res.AlertNodes.OpsGenie {		if a.Teams == nil {			a.Teams = []string{}			res.AlertNodes.OpsGenie[i] = a		if a.Recipients == nil {			a.Recipients = []string{}	if res.AlertNodes.OpsGenie2 == nil {		res.AlertNodes.OpsGenie2 = []*chronograf.OpsGenie{}	for i, a := range res.AlertNodes.OpsGenie2 {			res.AlertNodes.OpsGenie2[i] = a	if res.AlertNodes.PagerDuty == nil {		res.AlertNodes.PagerDuty = []*chronograf.PagerDuty{}	if res.AlertNodes.PagerDuty2 == nil {		res.AlertNodes.PagerDuty2 = []*chronograf.PagerDuty{}	if res.AlertNodes.Posts == nil {		res.AlertNodes.Posts = []*chronograf.Post{}	for i, a := range res.AlertNodes.Posts {		if a.Headers == nil {			a.Headers = map[string]string{}			res.AlertNodes.Posts[i] = a	if res.AlertNodes.Pushover == nil {		res.AlertNodes.Pushover = []*chronograf.Pushover{}	if res.AlertNodes.Sensu == nil {		res.AlertNodes.Sensu = []*chronograf.Sensu{}	for i, a := range res.AlertNodes.Sensu {		if a.Handlers == nil {			a.Handlers = []string{}			res.AlertNodes.Sensu[i] = a	if res.AlertNodes.Slack == nil {		res.AlertNodes.Slack = []*chronograf.Slack{}	if res.AlertNodes.Talk == nil {		res.AlertNodes.Talk = []*chronograf.Talk{}	if res.AlertNodes.TCPs == nil {		res.AlertNodes.TCPs = []*chronograf.TCP{}	if res.AlertNodes.Telegram == nil {		res.AlertNodes.Telegram = []*chronograf.Telegram{}	if res.AlertNodes.VictorOps == nil {		res.AlertNodes.VictorOps = []*chronograf.VictorOps{}	if res.Query != nil {		if res.Query.ID == "" {			res.Query.ID = res.ID		if res.Query.Fields == nil {			res.Query.Fields = make([]chronograf.Field, 0)		if res.Query.GroupBy.Tags == nil {			res.Query.GroupBy.Tags = make([]string, 0)		if res.Query.Tags == nil {			res.Query.Tags = make(map[string][]string)	return res// ValidRuleRequest checks if the requested rule change is validfunc ValidRuleRequest(rule chronograf.AlertRule) error {	if rule.Query == nil {		return fmt.Errorf("invalid alert rule: no query defined")	var hasFuncs bool	for _, f := range rule.Query.Fields {		if f.Type == "func" && len(f.Args) > 0 {			hasFuncs = true	// All kapacitor rules with functions must have a window that is applied	// every amount of time	if rule.Every == "" && hasFuncs {		return fmt.Errorf(`invalid alert rule: functions require an "every" window`)// KapacitorRulesPut proxies PATCH to kapacitorfunc (s *Service) KapacitorRulesPut(w http.ResponseWriter, r *http.Request) {	tid := httprouter.GetParamFromContext(ctx, "tid")	// Check if the rule exists and is scoped correctly	if _, err = c.Get(ctx, tid); err != nil {		if err == chronograf.ErrAlertNotFound {			notFound(w, id, s.Logger)		Error(w, http.StatusInternalServerError, err.Error(), s.Logger)	// Replace alert completely with this new alert.	req.ID = tid	task, err := c.Update(ctx, c.Href(tid), req)// KapacitorStatus is the current state of a running tasktype KapacitorStatus struct {	Status string `json:"status"`// Valid check if the kapacitor status is enabled or disabledfunc (k *KapacitorStatus) Valid() error {	if k.Status == "enabled" || k.Status == "disabled" {		return nil	return fmt.Errorf("invalid Kapacitor status: %s", k.Status)// KapacitorRulesStatus proxies PATCH to kapacitor to enable/disable tasksfunc (s *Service) KapacitorRulesStatus(w http.ResponseWriter, r *http.Request) {	var req KapacitorStatus	_, err = c.Get(ctx, tid)	var task *kapa.Task	if req.Status == "enabled" {		task, err = c.Enable(ctx, c.Href(tid))	} else {		task, err = c.Disable(ctx, c.Href(tid))// KapacitorRulesGet retrieves all rulesfunc (s *Service) KapacitorRulesGet(w http.ResponseWriter, r *http.Request) {	tasks, err := c.All(ctx)	res := allAlertsResponse{		Rules: []*alertResponse{},	for _, task := range tasks {		ar := newAlertResponse(task, srv.SrcID, srv.ID)		res.Rules = append(res.Rules, ar)type allAlertsResponse struct {	Rules []*alertResponse `json:"rules"`// KapacitorRulesID retrieves specific taskfunc (s *Service) KapacitorRulesID(w http.ResponseWriter, r *http.Request) {	// Check if the rule exists within scope	task, err := c.Get(ctx, tid)// KapacitorRulesDelete proxies DELETE to kapacitorfunc (s *Service) KapacitorRulesDelete(w http.ResponseWriter, r *http.Request) {	// Check if the rule is linked to this server and kapacitor	if _, err := c.Get(ctx, tid); err != nil {	if err := c.Delete(ctx, c.Href(tid)); err != nil {/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/layout.gohrefhttpAPILayoutsrelfilteredgithub.com/bouk/httprouter"github.com/bouk/httprouter"json:"href"`json:"href"`json:"rel"`json:"rel"`json:"link"`json:"link"`/chronograf/v1/layouts"/chronograf/v1/layouts"%s/%s"%s/%s"json:"layouts"`json:"layouts"`app"app"Error loading layouts"Error loading layouts"GetParamFromContextID %s not found"ID %s not found" Layouts retrieves all layouts from store Construct a filter sieve for both applications and measurements If the length of the filter is zero then all values are acceptable. If filter contains either measurement or application remove duplicates filter for data that belongs to provided application or measurement LayoutsID retrieves layout with ID from store/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/links.gocustomLinkcustomLinkslinksjson:"ast"`json:"ast"`json:"suggestions"`json:"suggestions"`json:"statusFeed,omitempty"`json:"statusFeed,omitempty"`json:"custom,omitempty"`json:"custom,omitempty"`customLink missing key for Name"customLink missing key for Name"customLink missing value for URL"customLink missing value for URL" Location of the whole global application configuration Location of the auth section of the global application configuration Location of the organization configuration Location of the organization-specific log viewer configuration Location of the a JSON Feed for client's Status page News Feed Any custom external links for client's User menu CustomLink is a handler that returns a custom link to be used in server's routes response, within ExternalLinks NewCustomLinks transforms `--custom-link` CLI flag data or `CUSTOM_LINKS` ENV var data into a data structure that the Chronograf client will expect/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/logger.goelapsedlatersw"Request"response_time"response_time""status"Response: "Response: " statusWriterFlusher captures the status header of an http.ResponseWriter and is a flusher Flush is here because the underlying HTTP chunked transfer response writer to implement http.Flusher.  Without it data is silently buffered.  This was discovered when proxying kapacitor chunked logs. Logger is middleware that logs the request/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/logout.goroutenextURL Logout chooses the correct provider logout route and redirects to it/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/mapping.gosuperAdmincuMappingsLoop"oauth2"mapping must specify provider"mapping must specify provider"mapping must specify scheme"mapping must specify scheme"mapping must specify group"mapping must specify group"/chronograf/v1/mappings/%s"/chronograf/v1/mappings/%s"json:"mappings"`json:"mappings"`/chronograf/v1/mappings"/chronograf/v1/mappings"failed to retrieve mappings from database"failed to retrieve mappings from database"organization does not exist"organization does not exist"failed to add mapping to database"failed to add mapping to database"failed to update mapping in database"failed to update mapping in database"failed to retrieve mapping from database"failed to retrieve mapping from database"failed to remove mapping from database"failed to remove mapping from database" Valid determines if a mapping request is valid Mappings retrieves all mappings NewMapping adds a new mapping UpdateMapping updates a mapping RemoveMapping removes a mapping/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/me.gocurrentOrghasDefaultOrgRolenewUsernumUsersorgIDsgolang.org/x/net/context"golang.org/x/net/context"json:"organizations"`json:"organizations"`json:"currentOrganization,omitempty"`json:"currentOrganization,omitempty"`/chronograf/v1/me"/chronograf/v1/me"/chronograf/v1"/chronograf/v1"me"me"/chronograf/v1/organizations/%s/users"/chronograf/v1/organizations/%s/users"token not found"token not found"Invalid principal: %v"Invalid principal: %v"invalid principal"invalid principal"user's current organization was not found"user's current organization was not found"This Chronograf is private. To gain access, you must be explicitly added by an administrator."This Chronograf is private. To gain access, you must be explicitly added by an administrator."error storing user %s: %v"error storing user %s: %v"user was nil"user was nil" If new user response is nil, return an empty meResponse because it indicates authentication is not needed TODO: This Scheme value is hard-coded temporarily since we only currently support OAuth2. This hard-coding should be removed whenever we add support for other authentication schemes. Organization is the OrganizationID UpdateMe changes the user's current organization on the JWT and responds with the same semantics as Me validate that user belongs to organization If the user was not found, check to see if they are a super admin. If they are, add them to the organization. Since a user is not a part of this organization and not a super admin, we should tell them that they are Forbidden (403) from accessing this resource If the user is a super admin give them an admin role in the requested organization. TODO: change to principal.CurrentOrganization Me does a findOrCreate based on the username in the context If there's no authentication, return an empty user user exists The intent is to force a the user to go through another auth flow Because we didnt find a user, making a new one TODO(desa): this needs a better name If the user is a superadmin, give them a role in the default organization It's not necessary to enforce that the first user is superAdmin here, since superAdminNewUsers defaults to true, but there's nothing else in the application that dictates that it must be true. So for that reason, we kept this here for now. We've discussed the future possibility of allowing users to override default values via CLI and this case could possibly happen then. TODO(desa): better error There can be race conditions between deleting a organization and the me query Any other error should cause an error to be returned/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/middle.goorg_match"org_match""oid"Failed to look up default organization"Failed to look up default organization"Route organization does not match the organization on principal"Route organization does not match the organization on principal" RouteMatchesPrincipal checks that the organization on context matches the organization in the route./Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/mountable_router.golibpath MountableRouter is an implementation of a chronograf.Router which supports prefixing each route of a Delegated chronograf.Router with a prefix. DELETE defines a route responding to a DELETE request that will be prefixed with the configured route prefix GET defines a route responding to a GET request that will be prefixed POST defines a route responding to a POST request that will be prefixed PUT defines a route responding to a PUT request that will be prefixed PATCH defines a route responding to a PATCH request that will be prefixed Handler defines a prefixed route responding to a request type specified in the method parameter ServeHTTP is an implementation of http.Handler which delegates to the configured Delegate's implementation of http.Handler/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/mux.goEnsureAdminEnsureEditorEnsureMemberEnsureSuperAdminEnsureViewerallRoutescompressedensureOrgMatcheshrprefixedAssetsrawStoreAccessroutercallbackPathloginPathlogoutPathurlNamepfcleanPathrootPathtokenMiddlewareparamgziphandlerjhttprouternet/http/pprof"net/http/pprof"github.com/NYTimes/gziphandler"github.com/NYTimes/gziphandler"nodeTypewildChildnTypemaxParamsindicespriorityincrementChildPrioaddRouteinsertChildgetValuefindCaseInsensitivePathfindCaseInsensitivePathRectreesRedirectTrailingSlashRedirectFixedPathHandleMethodNotAllowedHandleOPTIONSPanicHandlerHEADOPTIONSServeFilesallowedGzipHandlerStripPrefix/debug/pprof/:thing"/debug/pprof/:thing"ServeMuxmuxEntryeshostsmuxredirectToPathSlashshouldRedirectRLockedDefaultServeMux/swagger.json"/swagger.json"/docs"/docs"/chronograf/v1/organizations"/chronograf/v1/organizations"/chronograf/v1/organizations/:oid"/chronograf/v1/organizations/:oid"/chronograf/v1/mappings/:id"/chronograf/v1/mappings/:id"/chronograf/v1/sources/:id/proxy"/chronograf/v1/sources/:id/proxy"/chronograf/v1/sources/:id/write"/chronograf/v1/sources/:id/write"/chronograf/v1/sources/:id/queries"/chronograf/v1/sources/:id/queries"/chronograf/v1/sources/:id/annotations"/chronograf/v1/sources/:id/annotations"/chronograf/v1/sources/:id/annotations/:aid"/chronograf/v1/sources/:id/annotations/:aid"/chronograf/v1/sources/:id/permissions"/chronograf/v1/sources/:id/permissions"/chronograf/v1/sources/:id/services"/chronograf/v1/sources/:id/services"/chronograf/v1/sources/:id/services/:kid"/chronograf/v1/sources/:id/services/:kid"/chronograf/v1/sources/:id/services/:kid/proxy"/chronograf/v1/sources/:id/services/:kid/proxy"/chronograf/v1/layouts/:id"/chronograf/v1/layouts/:id"/chronograf/v1/organizations/:oid/users"/chronograf/v1/organizations/:oid/users"/chronograf/v1/organizations/:oid/users/:id"/chronograf/v1/organizations/:oid/users/:id"/chronograf/v1/users"/chronograf/v1/users"/chronograf/v1/users/:id"/chronograf/v1/users/:id"/chronograf/v1/dashboards/:id"/chronograf/v1/dashboards/:id"/chronograf/v1/dashboards/:id/cells"/chronograf/v1/dashboards/:id/cells"/chronograf/v1/dashboards/:id/cells/:cid"/chronograf/v1/dashboards/:id/cells/:cid"/chronograf/v1/dashboards/:id/templates"/chronograf/v1/dashboards/:id/templates"/chronograf/v1/dashboards/:id/templates/:tid"/chronograf/v1/dashboards/:id/templates/:tid"/chronograf/v1/sources/:id/dbs"/chronograf/v1/sources/:id/dbs"/chronograf/v1/sources/:id/dbs/:db"/chronograf/v1/sources/:id/dbs/:db"/chronograf/v1/sources/:id/dbs/:db/rps"/chronograf/v1/sources/:id/dbs/:db/rps"/chronograf/v1/sources/:id/dbs/:db/rps/:rp"/chronograf/v1/sources/:id/dbs/:db/rps/:rp"/chronograf/v1/sources/:id/dbs/:db/measurements"/chronograf/v1/sources/:id/dbs/:db/measurements"/chronograf/v1/org_config"/chronograf/v1/org_config"/chronograf/v1/org_config/logviewer"/chronograf/v1/org_config/logviewer"/chronograf/v1/"/chronograf/v1/"/oauth/logout"/oauth/logout"/oauth"/oauth"login"login"logout"logout"callback"callback"CleanescapeHTMLindentBufindentPrefixindentValueSetIndentSetEscapeHTMLNewEncoder{"code": 500, "message":"server_error"}`{"code": 500, "message":"server_error"}`http_status "http_status "Error message "Error message "unparsable JSON"unparsable JSON"unknown error: %v"unknown error: %v"ID %v not found"ID %v not found"error converting ID %s"error converting ID %s" JSONType the mimetype for a json request MuxOpts are the options for the router.  Mostly related to auth. Develop loads assets from filesystem instead of bindata URL path prefix under which all chronograf routes will be mounted UseAuth turns on Github OAuth and JWT Auth is used to authenticate and authorize JSON Feed URL for the client Status page News Feed Mount pprof routes for profiling NewMux attaches all the route handlers; handler returned servers chronograf. React Application  Prefix any URLs found in the React assets with any configured basepath Compress the assets with gzip if an accepted encoding The react application handles all the routing if the server does not know about the route.  This means that we never have unknown routes on the server. Set route prefix for all routes if basepath is presentThe assets handler is always unaware of basepaths, so the basepath needs to always be removed before sending requests to it add profiling routes Documentation  API  Organizations Mappings Source Proxy to Influx; Has gzip compression around the handler Write proxies line protocol write requests to InfluxDB Queries is used to analyze a specific queries and does not create any resources. It's a POST because Queries are POSTed to InfluxDB, but this only modifies InfluxDB resources with certain metaqueries, e.g. DROP DATABASE. Admins should ensure that the InfluxDB source as the proper permissions intended for Chronograf Users with the Viewer Role type. Annotations are user-defined events associated with this source All possible permissions for users in this source Services are resources that chronograf proxies to Service Proxy Layouts Users associated with Chronograf Set current chronograf organization the user is logged into TODO(desa): what to do about admin's being able to set superadmin Dashboards Dashboard Cells Dashboard Templates Databases Retention Policies Measurements Global application config for Chronograf Organization config settings for Chronograf Authentication  Encapsulate the router with OAuth2 Create middleware that redirects to the appropriate provider logout AuthAPI adds the OAuth routes if auth is enabled. AuthRoutes are content served to the page. When Basepath is set, it says that all content served to the page will be prefixed with the basepath. Since these routes are consumed by JS, it will need the basepath set to traverse a proxy correctly Wrap the API with token validation middleware. compare ignoring path garbage, trailing slashes, etc. Error writes an JSON message/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/org_config.gologViewerConfigiconCounttextCountclmnameMatcherpositionMatcherOrganization not found on context"Organization not found on context"invalid log viewer config: must have at least 1 column"invalid log viewer config: must have at least 1 column"invalid log viewer config: Duplicate column name %s"invalid log viewer config: Duplicate column name %s"invalid log viewer config: Multiple columns with same position value"invalid log viewer config: Multiple columns with same position value"invalid log viewer config: invalid visibility in column %s"invalid log viewer config: invalid visibility in column %s"invalid log viewer config: missing visibility encoding in column %s"invalid log viewer config: missing visibility encoding in column %s"invalid log viewer config: invalid number of severity format encodings in column %s"invalid log viewer config: invalid number of severity format encodings in column %s" LogViewer link to the organization log viewer config endpoint OrganizationConfig retrieves the organization-wide config settings OrganizationLogViewerConfig retrieves the log viewer UI section of the organization config This uses a FindOrCreate function to ensure that any new organizations have default organization config values, without having to associate organization creation with organization config creation. ReplaceOrganizationLogViewerConfig replaces the log viewer UI section of the organization config validLogViewerConfig ensures that the request body log viewer UI config is valid to be valid, it must: not be empty, have at least one column, not have multiple columns with the same name or position value, each column must have a visibility of either "visible" or "hidden" and if a column is of type severity, it must have at least one severity format of type icon, text, or both check that each column has a unique value for the name and position properties/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/organizations.goorgsRespcoorgCtxjson:"defaultRole"`json:"defaultRole"`name required on Chronograf Organization request body"name required on Chronograf Organization request body"no fields to update"no fields to update"default role must be member, viewer, editor, or admin"default role must be member, viewer, editor, or admin"/chronograf/v1/organizations/%s"/chronograf/v1/organizations/%s"failed to retrieve user from context"failed to retrieve user from context"failed to add user to organization"failed to add user to organization" Organizations retrieves all organizations from store NewOrganization adds a new organization to store Now that the organization was created, add the user making the request to the organization Best attempt at cleanup the organization if there were any errors Best attempt at cleanup the organization if there were any errors adding user to org OrganizationID retrieves a organization with ID from store UpdateOrganization updates an organization in the organizations store RemoveOrganization removes an organization in the organizations store/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/path.go PathEscape escapes the string so it can be safely placed inside a URL path segment. Change to url.PathEscape for go 1.8/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/permissions.gohttpAPISrcs%s/%d/permissions"%s/%d/permissions" Permissions returns all possible permissions for this source. Links are URI locations related to user/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/prefixing_redirector.goflusheriw FlushingHandler may not actually do anything, but it was ostensibly implemented to flush response writers that can be flushed for the purposes in the comment above./Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/proxy.gouriaslashbslashpath query parameter required"path query parameter required"Error parsing kapacitor url: %v"Error parsing kapacitor url: %v" Proxy proxies requests to services using the path query parameter. To preserve any HTTP query arguments to the kapacitor path, we concat and parse them into u. Set the Host header of the original Kapacitor URL Because we are acting as a proxy, kapacitor needs to have the basic auth information set as a header directly Without a FlushInterval the HTTP Chunked response for kapacitor logs is buffered and flushed every 30 seconds. The connection to kapacitor is using a self-signed certificate. ProxyPost proxies POST to service ProxyPatch proxies PATCH to Service ProxyGet proxies GET to service ProxyDelete proxies DELETE to service/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/queries.goqrgithub.com/influxdata/influxdb/v2/chronograf/influx/queries"github.com/influxdata/influxdb/v2/chronograf/influx/queries"json:"tempVars,omitempty"`json:"tempVars,omitempty"`json:"durationMs"`json:"durationMs"`json:"queryConfig"`json:"queryConfig"`json:"queryAST,omitempty"`json:"queryAST,omitempty"`json:"queryTemplated,omitempty"`json:"queryTemplated,omitempty"`Millisecond1000000unable to connect to source: %v"unable to connect to source: %v"unable to load RPs from DB %s: %v"unable to load RPs from DB %s: %v" QueryRequest is query that will be converted to a queryConfig QueriesRequest converts all queries to queryConfigs with the help of the template variables QueryResponse is the return result of a QueryRequest including the raw query, the templated query, the queryConfig and the queryAST QueriesResponse is the response for a QueriesRequest Queries analyzes InfluxQL to produce front-end friendly QueryConfig DefaultRP will add the default retention policy to the QC if one has not been specified Only need to find the default RP IFF the qc's rp is empty For queries without databases, measurements, or fields we will not be able to find an RP/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/queryconfig.goinvalid field type "%s" ; expect func, field, integer, number, regex, wildcard`invalid field type "%s" ; expect func, field, integer, number, regex, wildcard` ToQueryConfig converts InfluxQL into queryconfigs query config's raw text is set to the query. ValidateQueryConfig checks any query config input/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/redoc.goswagger<!DOCTYPE html>
<html>
  <head>
    <title>Chronograf API</title>
    <!-- needed for adaptive design -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--
    ReDoc doesn't change outer page styles
    -->
    <style>
      body {
        margin: 0;
        padding: 0;
      }
    </style>
  </head>
  <body>
    <redoc spec-url='%s'></redoc>
    <script src="https://rebilly.github.io/ReDoc/releases/latest/redoc.min.js"> </script>
  </body>
</html>
`<!DOCTYPE html>
<html>
  <head>
    <title>Chronograf API</title>
    <!-- needed for adaptive design -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--
    ReDoc doesn't change outer page styles
    -->
    <style>
      body {
        margin: 0;
        padding: 0;
      }
    </style>
  </head>
  <body>
    <redoc spec-url='%s'></redoc>
    <script src="https://rebilly.github.io/ReDoc/releases/latest/redoc.min.js"> </script>
  </body>
</html>
` Redoc servers the swagger JSON using the redoc package./Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/routes.gojson:"login"`json:"login"`json:"logout"`json:"logout"`json:"callback"`json:"callback"`json:"users"`json:"users"`json:"allUsers"`json:"allUsers"`json:"sources"`json:"sources"`json:"me"`json:"me"`json:"config"`json:"config"`json:"dashboardsv2"`json:"dashboardsv2"`json:"logout,omitempty"`json:"logout,omitempty"`json:"external"`json:"external"`json:"orgConfig"`json:"orgConfig"`json:"flux"`json:"flux"`/chronograf/v2/dashboards"/chronograf/v2/dashboards"/chronograf/v2/cells"/chronograf/v2/cells"/chronograf/v1/flux"/chronograf/v1/flux"/chronograf/v1/flux/ast"/chronograf/v1/flux/ast"/chronograf/v1/flux/suggestions"/chronograf/v1/flux/suggestions" AuthRoute are the routes for each type of OAuth2 provider Name uniquely identifies the provider Label is a user-facing string to present in the UI Login is the route to the login redirect path Logout is the route to the logout redirect path Callback is the route the provider calls to exchange the code/state AuthRoutes contains all OAuth2 provider routes. Lookup searches all the routes for a specific provider Location of the layouts endpoint Location of the users endpoint Location of the raw users endpoint Location of the organizations endpoint Location of the application mappings endpoint Location of the sources endpoint Location of the me endpoint Location of the environment endpoint Location of the dashboards endpoint Location of the config endpoint and its various sections Location of the v2 cells Location of the v2 dashboards Location of all auth routes. Location of the logout route for all auth routes All external links for the client to use Location of the organization config endpoint AllRoutes is a handler that returns all links to resources in Chronograf server, as well as external links for the client to know about, such as for JSON feeds or custom side nav buttons. Optionally, routes for authentication can be returned. GetPrincipal is used to retrieve the principal on http request. Location of all auth routes. If no auth, this can be empty. Location of the logout route for all auth routes. If no auth, this can be empty. External link to the JSON Feed for the News Feed on the client's Status Page Custom external links for client's User menu, as passed in via CLI/ENV serveHTTP returns all top level routes and external links within chronograf If there is a principal, use the organization to populate the users routes otherwise use the default organization We want to return at least an empty array, rather than null The JSON response will have no field present for the LogoutLink if there is no logout link./Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/server.goconfigureghghMuxgoMuxhMuxgengenMuxredirectPathgenericNamepublicURLlistenerhttpServerproviderFuncsstdLogboltPathserverIDbboltgracefulruntime"runtime"github.com/influxdata/usage-client/v1"github.com/influxdata/usage-client/v1"github.com/tylerb/graceful"github.com/tylerb/graceful"long:"host" description:"The IP to listen on" default:"0.0.0.0" env:"HOST"`long:"host" description:"The IP to listen on" default:"0.0.0.0" env:"HOST"`long:"port" description:"The port to listen on for insecure connections, defaults to a random value" default:"8888" env:"PORT"`long:"port" description:"The port to listen on for insecure connections, defaults to a random value" default:"8888" env:"PORT"`long:"pprof-enabled" description:"Enable the /debug/pprof/* HTTP routes" env:"PPROF_ENABLED"`long:"pprof-enabled" description:"Enable the /debug/pprof/* HTTP routes" env:"PPROF_ENABLED"`long:"cert" description:"Path to PEM encoded public key certificate. " env:"TLS_CERTIFICATE"`long:"cert" description:"Path to PEM encoded public key certificate. " env:"TLS_CERTIFICATE"`long:"key" description:"Path to private key associated with given certificate. " env:"TLS_PRIVATE_KEY"`long:"key" description:"Path to private key associated with given certificate. " env:"TLS_PRIVATE_KEY"`long:"influxdb-url" description:"Location of your InfluxDB instance" env:"INFLUXDB_URL"`long:"influxdb-url" description:"Location of your InfluxDB instance" env:"INFLUXDB_URL"`long:"influxdb-username" description:"Username for your InfluxDB instance" env:"INFLUXDB_USERNAME"`long:"influxdb-username" description:"Username for your InfluxDB instance" env:"INFLUXDB_USERNAME"`long:"influxdb-password" description:"Password for your InfluxDB instance" env:"INFLUXDB_PASSWORD"`long:"influxdb-password" description:"Password for your InfluxDB instance" env:"INFLUXDB_PASSWORD"`long:"kapacitor-url" description:"Location of your Kapacitor instance" env:"KAPACITOR_URL"`long:"kapacitor-url" description:"Location of your Kapacitor instance" env:"KAPACITOR_URL"`long:"kapacitor-username" description:"Username of your Kapacitor instance" env:"KAPACITOR_USERNAME"`long:"kapacitor-username" description:"Username of your Kapacitor instance" env:"KAPACITOR_USERNAME"`long:"kapacitor-password" description:"Password of your Kapacitor instance" env:"KAPACITOR_PASSWORD"`long:"kapacitor-password" description:"Password of your Kapacitor instance" env:"KAPACITOR_PASSWORD"`long:"new-sources" description:"Config for adding a new InfluxDB source and Kapacitor server, in JSON as an array of objects, and surrounded by single quotes. E.g. --new-sources='[{\"influxdb\":{\"name\":\"Influx 1\",\"username\":\"user1\",\"password\":\"pass1\",\"url\":\"http://localhost:8086\",\"metaUrl\":\"http://metaurl.com\",\"type\":\"influx-enterprise\",\"insecureSkipVerify\":false,\"default\":true,\"telegraf\":\"telegraf\",\"sharedSecret\":\"cubeapples\"},\"kapacitor\":{\"name\":\"Kapa 1\",\"url\":\"http://localhost:9092\",\"active\":true}}]'" env:"NEW_SOURCES" hidden:"true"`long:"new-sources" description:"Config for adding a new InfluxDB source and Kapacitor server, in JSON as an array of objects, and surrounded by single quotes. E.g. --new-sources='[{\"influxdb\":{\"name\":\"Influx 1\",\"username\":\"user1\",\"password\":\"pass1\",\"url\":\"http://localhost:8086\",\"metaUrl\":\"http://metaurl.com\",\"type\":\"influx-enterprise\",\"insecureSkipVerify\":false,\"default\":true,\"telegraf\":\"telegraf\",\"sharedSecret\":\"cubeapples\"},\"kapacitor\":{\"name\":\"Kapa 1\",\"url\":\"http://localhost:9092\",\"active\":true}}]'" env:"NEW_SOURCES" hidden:"true"`short:"d" long:"develop" description:"Run server in develop mode."`short:"d" long:"develop" description:"Run server in develop mode."`short:"c" long:"canned-path" description:"Path to directory of pre-canned application layouts (/usr/share/chronograf/canned)" env:"CANNED_PATH" default:"canned"`short:"c" long:"canned-path" description:"Path to directory of pre-canned application layouts (/usr/share/chronograf/canned)" env:"CANNED_PATH" default:"canned"`long:"resources-path" description:"Path to directory of pre-canned dashboards, sources, kapacitors, and organizations (/usr/share/chronograf/resources)" env:"RESOURCES_PATH" default:"canned"`long:"resources-path" description:"Path to directory of pre-canned dashboards, sources, kapacitors, and organizations (/usr/share/chronograf/resources)" env:"RESOURCES_PATH" default:"canned"`short:"t" long:"token-secret" description:"Secret to sign tokens" env:"TOKEN_SECRET"`short:"t" long:"token-secret" description:"Secret to sign tokens" env:"TOKEN_SECRET"`long:"jwks-url" description:"URL that returns OpenID Key Discovery JWKS document." env:"JWKS_URL"`long:"jwks-url" description:"URL that returns OpenID Key Discovery JWKS document." env:"JWKS_URL"`long:"use-id-token" description:"Enable id_token processing." env:"USE_ID_TOKEN"`long:"use-id-token" description:"Enable id_token processing." env:"USE_ID_TOKEN"`long:"auth-duration" default:"720h" description:"Total duration of cookie life for authentication (in hours). 0 means authentication expires on browser close." env:"AUTH_DURATION"`long:"auth-duration" default:"720h" description:"Total duration of cookie life for authentication (in hours). 0 means authentication expires on browser close." env:"AUTH_DURATION"`short:"i" long:"github-client-id" description:"Github Client ID for OAuth 2 support" env:"GH_CLIENT_ID"`short:"i" long:"github-client-id" description:"Github Client ID for OAuth 2 support" env:"GH_CLIENT_ID"`short:"s" long:"github-client-secret" description:"Github Client Secret for OAuth 2 support" env:"GH_CLIENT_SECRET"`short:"s" long:"github-client-secret" description:"Github Client Secret for OAuth 2 support" env:"GH_CLIENT_SECRET"`short:"o" long:"github-organization" description:"Github organization user is required to have active membership" env:"GH_ORGS" env-delim:","`short:"o" long:"github-organization" description:"Github organization user is required to have active membership" env:"GH_ORGS" env-delim:","`long:"google-client-id" description:"Google Client ID for OAuth 2 support" env:"GOOGLE_CLIENT_ID"`long:"google-client-id" description:"Google Client ID for OAuth 2 support" env:"GOOGLE_CLIENT_ID"`long:"google-client-secret" description:"Google Client Secret for OAuth 2 support" env:"GOOGLE_CLIENT_SECRET"`long:"google-client-secret" description:"Google Client Secret for OAuth 2 support" env:"GOOGLE_CLIENT_SECRET"`long:"google-domains" description:"Google email domain user is required to have active membership" env:"GOOGLE_DOMAINS" env-delim:","`long:"google-domains" description:"Google email domain user is required to have active membership" env:"GOOGLE_DOMAINS" env-delim:","`long:"public-url" description:"Full public URL used to access Chronograf from a web browser. Used for OAuth2 authentication. (http://localhost:8888)" env:"PUBLIC_URL"`long:"public-url" description:"Full public URL used to access Chronograf from a web browser. Used for OAuth2 authentication. (http://localhost:8888)" env:"PUBLIC_URL"`long:"heroku-client-id" description:"Heroku Client ID for OAuth 2 support" env:"HEROKU_CLIENT_ID"`long:"heroku-client-id" description:"Heroku Client ID for OAuth 2 support" env:"HEROKU_CLIENT_ID"`long:"heroku-secret" description:"Heroku Secret for OAuth 2 support" env:"HEROKU_SECRET"`long:"heroku-secret" description:"Heroku Secret for OAuth 2 support" env:"HEROKU_SECRET"`long:"heroku-organization" description:"Heroku Organization Memberships a user is required to have for access to Chronograf (comma separated)" env:"HEROKU_ORGS" env-delim:","`long:"heroku-organization" description:"Heroku Organization Memberships a user is required to have for access to Chronograf (comma separated)" env:"HEROKU_ORGS" env-delim:","`long:"generic-name" description:"Generic OAuth2 name presented on the login page"  env:"GENERIC_NAME"`long:"generic-name" description:"Generic OAuth2 name presented on the login page"  env:"GENERIC_NAME"`long:"generic-client-id" description:"Generic OAuth2 Client ID. Can be used own OAuth2 service."  env:"GENERIC_CLIENT_ID"`long:"generic-client-id" description:"Generic OAuth2 Client ID. Can be used own OAuth2 service."  env:"GENERIC_CLIENT_ID"`long:"generic-client-secret" description:"Generic OAuth2 Client Secret" env:"GENERIC_CLIENT_SECRET"`long:"generic-client-secret" description:"Generic OAuth2 Client Secret" env:"GENERIC_CLIENT_SECRET"`long:"generic-scopes" description:"Scopes requested by provider of web client." default:"user:email" env:"GENERIC_SCOPES" env-delim:","`long:"generic-scopes" description:"Scopes requested by provider of web client." default:"user:email" env:"GENERIC_SCOPES" env-delim:","`long:"generic-domains" description:"Email domain users' email address to have (example.com)" env:"GENERIC_DOMAINS" env-delim:","`long:"generic-domains" description:"Email domain users' email address to have (example.com)" env:"GENERIC_DOMAINS" env-delim:","`long:"generic-auth-url" description:"OAuth 2.0 provider's authorization endpoint URL" env:"GENERIC_AUTH_URL"`long:"generic-auth-url" description:"OAuth 2.0 provider's authorization endpoint URL" env:"GENERIC_AUTH_URL"`long:"generic-token-url" description:"OAuth 2.0 provider's token endpoint URL" env:"GENERIC_TOKEN_URL"`long:"generic-token-url" description:"OAuth 2.0 provider's token endpoint URL" env:"GENERIC_TOKEN_URL"`long:"generic-api-url" description:"URL that returns OpenID UserInfo compatible information." env:"GENERIC_API_URL"`long:"generic-api-url" description:"URL that returns OpenID UserInfo compatible information." env:"GENERIC_API_URL"`long:"generic-api-key" description:"JSON lookup key into OpenID UserInfo. (Azure should be userPrincipalName)" default:"email" env:"GENERIC_API_KEY"`long:"generic-api-key" description:"JSON lookup key into OpenID UserInfo. (Azure should be userPrincipalName)" default:"email" env:"GENERIC_API_KEY"`long:"auth0-domain" description:"Subdomain of auth0.com used for Auth0 OAuth2 authentication" env:"AUTH0_DOMAIN"`long:"auth0-domain" description:"Subdomain of auth0.com used for Auth0 OAuth2 authentication" env:"AUTH0_DOMAIN"`long:"auth0-client-id" description:"Auth0 Client ID for OAuth2 support" env:"AUTH0_CLIENT_ID"`long:"auth0-client-id" description:"Auth0 Client ID for OAuth2 support" env:"AUTH0_CLIENT_ID"`long:"auth0-client-secret" description:"Auth0 Client Secret for OAuth2 support" env:"AUTH0_CLIENT_SECRET"`long:"auth0-client-secret" description:"Auth0 Client Secret for OAuth2 support" env:"AUTH0_CLIENT_SECRET"`long:"auth0-organizations" description:"Auth0 organizations permitted to access Chronograf (comma separated)" env:"AUTH0_ORGS" env-delim:","`long:"auth0-organizations" description:"Auth0 organizations permitted to access Chronograf (comma separated)" env:"AUTH0_ORGS" env-delim:","`long:"auth0-superadmin-org" description:"Auth0 organization from which users are automatically granted SuperAdmin status" env:"AUTH0_SUPERADMIN_ORG"`long:"auth0-superadmin-org" description:"Auth0 organization from which users are automatically granted SuperAdmin status" env:"AUTH0_SUPERADMIN_ORG"`long:"status-feed-url" description:"URL of a JSON Feed to display as a News Feed on the client Status page." default:"https://www.influxdata.com/feed/json" env:"STATUS_FEED_URL"`long:"status-feed-url" description:"URL of a JSON Feed to display as a News Feed on the client Status page." default:"https://www.influxdata.com/feed/json" env:"STATUS_FEED_URL"`long:"custom-link" description:"Custom link to be added to the client User menu. Multiple links can be added by using multiple of the same flag with different 'name:url' values, or as an environment variable with comma-separated 'name:url' values. E.g. via flags: '--custom-link=InfluxData:https://www.influxdata.com --custom-link=Chronograf:https://github.com/influxdata/influxdb/chronograf'. E.g. via environment variable: 'export CUSTOM_LINKS=InfluxData:https://www.influxdata.com,Chronograf:https://github.com/influxdata/influxdb/chronograf'" env:"CUSTOM_LINKS" env-delim:","`long:"custom-link" description:"Custom link to be added to the client User menu. Multiple links can be added by using multiple of the same flag with different 'name:url' values, or as an environment variable with comma-separated 'name:url' values. E.g. via flags: '--custom-link=InfluxData:https://www.influxdata.com --custom-link=Chronograf:https://github.com/influxdata/influxdb/chronograf'. E.g. via environment variable: 'export CUSTOM_LINKS=InfluxData:https://www.influxdata.com,Chronograf:https://github.com/influxdata/influxdb/chronograf'" env:"CUSTOM_LINKS" env-delim:","`long:"telegraf-system-interval" default:"1m" description:"Duration used in the GROUP BY time interval for the hosts list" env:"TELEGRAF_SYSTEM_INTERVAL"`long:"telegraf-system-interval" default:"1m" description:"Duration used in the GROUP BY time interval for the hosts list" env:"TELEGRAF_SYSTEM_INTERVAL"`short:"r" long:"reporting-disabled" description:"Disable reporting of usage stats (os,arch,version,cluster_id,uptime) once every 24hr" env:"REPORTING_DISABLED"`short:"r" long:"reporting-disabled" description:"Disable reporting of usage stats (os,arch,version,cluster_id,uptime) once every 24hr" env:"REPORTING_DISABLED"`short:"l" long:"log-level" value-name:"choice" choice:"debug" choice:"info" choice:"error" default:"info" description:"Set the logging level" env:"LOG_LEVEL"`short:"l" long:"log-level" value-name:"choice" choice:"debug" choice:"info" choice:"error" default:"info" description:"Set the logging level" env:"LOG_LEVEL"`short:"p" long:"basepath" description:"A URL path prefix under which all chronograf routes will be mounted. (Note: PREFIX_ROUTES has been deprecated. Now, if basepath is set, all routes will be prefixed with it.)" env:"BASE_PATH"`short:"p" long:"basepath" description:"A URL path prefix under which all chronograf routes will be mounted. (Note: PREFIX_ROUTES has been deprecated. Now, if basepath is set, all routes will be prefixed with it.)" env:"BASE_PATH"`short:"v" long:"version" description:"Show Chronograf version info"`short:"v" long:"version" description:"Show Chronograf version info"`/oauth/google/callback"/oauth/google/callback"oauth"oauth"Error parsing public URL: err:"Error parsing public URL: err:"Error parsing Auth0 domain: err:"Error parsing Auth0 domain: err:"JoinHostPortListentcp"tcp"LoadX509KeyPair"CustomLink""invalid"invalid basepath, must follow format "/mybasepath""invalid basepath, must follow format \"/mybasepath\"""basepath"ListenLimitTCPKeepAliveBeforeShutdownShutdownInitiatedNoSignalHandlingLogFuncInterruptedinterruptstopLockstopChanchanLockconnectionsidleConnectionsListenTLSListenAndServeTLSConfigStopChanmanageConnectionsinterruptChanhandleInterruptshutdownnewTCPListener5000000000Serving chronograf at "Serving chronograf at "://"://"Stopped serving chronograf at "Stopped serving chronograf at "boltstore"boltstore""LayoutsStore"Unable to construct a MultiLayoutsStore"Unable to construct a MultiLayoutsStore""DashboardsStore"Unable to construct a MultiDashboardsStore"Unable to construct a MultiDashboardsStore""SourcesStore"Unable to construct a MultiSourcesStore"Unable to construct a MultiSourcesStore""KapacitorStore"Unable to construct a MultiKapacitorStore"Unable to construct a MultiKapacitorStore""OrganizationsStore"Unable to construct a MultiOrganizationStore"Unable to construct a MultiOrganizationStore"FormatUintSaveRegistrationURLGOOSdarwinarch"arch"GOARCHarm64"version"cluster_id"cluster_id"uptime"uptime"usage"usage"reporting_addr"reporting_addr"freq"freq"24h"24h""stats"os,arch,version,cluster_id,uptime"os,arch,version,cluster_id,uptime"Reporting usage stats"Reporting usage stats"Saveablechronograf-ng"chronograf-ng"MustCompile(\/{1}[\w-]+)+`(\/{1}[\w-]+)+` Server for the chronograf API UseGithub validates the CLI parameters to enable github oauth support UseGoogle validates the CLI parameters to enable google oauth support UseHeroku validates the CLI parameters to enable heroku oauth support UseAuth0 validates the CLI parameters to enable Auth0 oauth support UseGenericOAuth2 validates the CLI parameters to enable generic oauth support NewListener will an http or https listener depending useTLS() If no key specified, therefore, we assume it is in the cert Serve starts and runs the chronograf server Add chronograf's version header to all requests Add HSTS to instruct all browsers to change from http to https Using a log writer for http server logging TODO: Remove graceful when changing to go 1.8 TODO(desa): what to do about logger reportUsageStats starts periodic server reporting.RegistrationClusterID/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/service.gogithub.com/influxdata/influxdb/v2/chronograf/enterprise"github.com/influxdata/influxdb/v2/chronograf/enterprise"json:"code"`json:"code"` Service handles REST calls to the persistence TimeSeriesClient returns the correct client for a time series database. ErrorMessage is the error response format for all service errors TimeSeries returns a new client connected to a time series database InfluxClient returns a new client to connect to OSS or Enterprise New creates a client to connect to OSS or enterprise/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/services.gomrSrvssrvsaddressgithub.com/influxdata/influxdb/v2/flux"github.com/influxdata/influxdb/v2/flux"name and url required"name and url required"type required"type required"ParseRequestURIinvalid source URI: %v"invalid source URI: %v"invalid URL; no URL scheme defined"invalid URL; no URL scheme defined"json:"proxy"`json:"proxy"`json:"sourceID,string"`json:"sourceID,string"`%s/%d/services/%d"%s/%d/services/%d"%s/%d/services/%d/proxy"%s/%d/services/%d/proxy"json:"services"`json:"services"`"flux"Unable to reach flux %s: %v"Unable to reach flux %s: %v"StatusGatewayTimeout504error storing service %v: %v"error storing service %v: %v"Error loading services"Error loading services"json:"url,omitempty"`json:"url,omitempty"`invalid service URI: %v"invalid service URI: %v"invalid type; type must not be an empty string"invalid type; type must not be an empty string"Error updating service ID %d"Error updating service ID %d" User facing name of service instance.; Required: true URL for the service backend (e.g. http://localhost:9092);/ Required: true Type is the kind of service (e.g. flux); Required Username for authentication to service InsecureSkipVerify as true means any certificate presented by the service is accepted. URL location of proxy endpoint for this source URL location of the parent source Unique identifier representing a service instance. User facing name of service instance. URL for the service backend (e.g. http://localhost:9092) Type is the kind of service (e.g. flux) Links are URI locations related to service NewService adds valid service store store. Services retrieves all services from store. ServiceID retrieves a service with ID from store. RemoveService deletes service from store. URL for the service Username for service auth UpdateService incrementally updates a service definition in the store/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/stores.gogithub.com/influxdata/influxdb/v2/chronograf/noop"github.com/influxdata/influxdb/v2/chronograf/noop" hasOrganizationContext retrieves organization specified on context under the organizations.ContextKey hasRoleContext retrieves organization specified on context UserContextKey is the context key for retrieving the user off of context hasUserContext specifies if the context contains the UserContextKey and that the value stored there is chronograf.User hasSuperAdminContext specifies if the context contains the UserContextKey user is a super admin DataStore is collection of resources that are used by the Service Abstracting this into an interface was useful for isolated testing ensure that Store implements a DataStore Store implements the DataStore interface Sources returns a noop.SourcesStore if the context has no organization specified and an organization.SourcesStore otherwise. Servers returns a noop.ServersStore if the context has no organization specified and an organization.ServersStore otherwise. Layouts returns all layouts in the underlying layouts store. Users returns a chronograf.UsersStore. If the context is a server context, then the underlying chronograf.UsersStore is returned. If there is an organization specified on context, then an organizations.UsersStore If neither are specified, a noop.UsersStore is returned. Dashboards returns a noop.DashboardsStore if the context has no organization specified and an organization.DashboardsStore otherwise. OrganizationConfig returns a noop.OrganizationConfigStore if the context has no organization specified and an organization.OrganizationConfigStore otherwise. Organizations returns the underlying OrganizationsStore. Config returns the underlying ConfigStore. Mappings returns the underlying MappingsStore. ensure that DirectStore implements a DataStore/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/swagger.goswagger.json"swagger.json"go:generate env GO111MODULE=on go run github.com/kevinburke/go-bindata/go-bindata -o swagger_gen.go -tags assets -ignore go -nocompress -pkg server . Spec servers the swagger.json file from bindata/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/templates.gotmpstidunknown template type %s"unknown template type %s"constant"constant"csv"csv"fieldKeys"fieldKeys"tagKeys"tagKeys"tagValues"tagValues""measurements""databases"map"map""influxql"unknown template variable type %s"unknown template variable type %s"fieldKey"fieldKey"tagKey"tagKey"tagValue"tagValue"templates of type 'map' require a 'key'"templates of type 'map' require a 'key'"no query set for template of type 'influxql'"no query set for template of type 'influxql'"%s/%d/templates/%s"%s/%d/templates/%s"Error creating template ID for dashboard %d: %v"Error creating template ID for dashboard %d: %v"Error adding template %s to dashboard %d: %v"Error adding template %s to dashboard %d: %v""tid"Error removing template %s from dashboard %d: %v"Error removing template %s from dashboard %d: %v"Error updating template %s in dashboard %d: %v"Error updating template %s in dashboard %d: %v" ValidTemplateRequest checks if the request sent to the server is the correct format. Templates returns all templates from a dashboard within the store NewTemplate adds a template to an existing dashboard TemplateID retrieves a specific template from a dashboard RemoveTemplate removes a specific template from an existing dashboard ReplaceTemplate replaces a template entirely within an existing dashboard/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/test_helpers.go/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/url_prefixer.goorigHeadermatchlenwindowisSVGnextReadnextWritewrittenCountsubjecttargetstlentgtbufio"bufio"Expected http.ResponseWriter to be an http.Flusher, but wasn't"Expected http.ResponseWriter to be an http.Flusher, but wasn't"Content-Length"Content-Length".svg$".svg$"Connection"Connection"Keep-Alive"Keep-Alive"Transfer-Encoding"Transfer-Encoding"chunked"chunked"NewBufferSplitFuncmaxTokenSizeemptiesscanCalledadvancesetErrNewScannerScanBytesprefixer"prefixer"Error encountered while scanning: err:"Error encountered while scanning: err:"src="`src="`href="`href="`url(`url(`data-basepath="`data-basepath="` URLPrefixer is a wrapper for an http.Handler that will prefix all occurrences of a relative URL with the configured Prefix the prefix to be appended after any detected Attrs the http.Handler which will generate the content to be modified by this handler a list of attrs that should have their URLs prefixed. For example `src="` or `href="` would be valid The logger where prefixing errors will be dispatched to Filter out content length header to prevent stopping writing Header() copies the Header map from the underlying ResponseWriter to prevent modifications to it by callers ChunkSize is the number of bytes per chunked transfer-encoding ServeHTTP implements an http.Handler that prefixes relative URLs from the Next handler with the configured prefix. It does this by examining the stream through the ResponseWriter, and appending the Prefix after any of the Attrs detected in the stream. extract the flusher for flushing chunks chunked transfer because we're modifying the response on the fly, so we won't know the final content-length number of bytes written to rw setup a buffer which is the max length of our target attrs prime the buffer with the start of the input Read next handler's response byte by byte advance a byte if window is not a src attr shift the next byte into buf advance to the relative URL add the src attr to the output write the prefix match compares the subject against a list of targets. If there is a match between any of them a non-zero value is returned. The returned value is the length of the match. It is assumed that subject's length > length of all targets. The matching []byte is also returned as the second return parameter maxlen returns the length of the largest []byte provided to it as an argument NewDefaultURLPrefixer returns a URLPrefixer that will prefix any src and href attributes found in HTML as well as any url() directives found in CSS with the provided prefix. Additionally, it will prefix any `data-basepath` attributes as well for informing front end logic about any prefixes. `next` is the next http.Handler that will have its output prefixed for forwarding basepath to frontend/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/users.goselfLinkusersRespidStrctxUserjson:"superAdmin"`json:"superAdmin"`name required on Chronograf User request body"name required on Chronograf User request body"provider required on Chronograf User request body"provider required on Chronograf User request body"scheme required on Chronograf User request body"scheme required on Chronograf User request body"no Roles to update"no Roles to update"no organization was provided"no organization was provided"duplicate organization %q in roles"duplicate organization %q in roles"unknown role %s. Valid roles are 'member', 'viewer', 'editor', 'admin', and '*'"unknown role %s. Valid roles are 'member', 'viewer', 'editor', 'admin', and '*'"/chronograf/v1/organizations/%s/users/%d"/chronograf/v1/organizations/%s/users/%d"/chronograf/v1/users/%d"/chronograf/v1/users/%d"ParseUintinvalid user id: %s"invalid user id: %s"cannot update Name"cannot update Name"cannot update Provider"cannot update Provider"cannot update Scheme"cannot update Scheme"user cannot modify their own SuperAdmin status"user cannot modify their own SuperAdmin status"user does not have authorization required to set SuperAdmin status. See https://github.com/influxdata/influxdb/chronograf/issues/2601 for more information"user does not have authorization required to set SuperAdmin status. See https://github.com/influxdata/influxdb/chronograf/issues/2601 for more information" This ensures that any user response with no roles returns an empty array instead of null when marshaled into JSON. That way, JavaScript doesn't need any guard on the key existing and it can simply be iterated over. UserID retrieves a Chronograf user with ID from store NewUser adds a new Chronograf user to store RemoveUser deletes a Chronograf user from store UpdateUser updates a Chronograf user in store ValidUpdate should ensure that req.Roles is not nil If the request contains a name, it must be the same as the one on the user. This is particularly useful to the front-end because they would like to provide the whole user object, including the name, provider, and scheme in update requests. But currently, it is not possible to change name, provider, or scheme via the API. Don't allow SuperAdmins to modify their own SuperAdmin status. Allowing them to do so could result in an application where there are no super admins. If the user being updated is the user making the request and they are changing their SuperAdmin status, return an unauthorized error Users retrieves all Chronograf users from store At a high level, this function checks the following   1. Is the user making the request a SuperAdmin.      If they are, allow them to make whatever changes they please.   2. Is the user making the request trying to change the SuperAdmin      status. If so, return an error.   3. If none of the above are the case, let the user make whichever      changes were requested. Only allow users to set SuperAdmin if they have the superadmin context TODO(desa): Refactor this https://github.com/influxdata/influxdb/chronograf/issues/2207 If req.SuperAdmin has been set, and the request was not made with the SuperAdmin context, return error verify that the organization exists/Users/austinjaybecker/projects/abeck-go-testing/chronograf/server/version.goX-Chronograf-Version"X-Chronograf-Version" Version handler adds X-Chronograf-Version header to responses/Users/austinjaybecker/projects/abeck-go-testing/cmd/Users/austinjaybecker/projects/abeck-go-testing/cmd/chronograf-migrator/Users/austinjaybecker/projects/abeck-go-testing/cmd/chronograf-migrator/dashboard.goConvert1To2DashboardchronografDBPathconvert1To2Cellconvert1To2VariableconvertAxesconvertColorsconvertLegendconvertQueriesdbrpMapperexecinfluxQLVarPatternoutputFiletranspileQueryd1d2hasTextColorhasThresholdColorpkgtqqueryTextqsastgithub.com/influxdata/flux/ast"github.com/influxdata/flux/ast"github.com/influxdata/influxdb/v2/query/influxql"github.com/influxdata/influxdb/v2/query/influxql"xy"xy"overlaid"overlaid"line-stacked"line-stacked"stacked"stacked"line-stepplot"line-stepplot""step"bar"bar"line-plus-single-stat"line-plus-single-stat"single-stat"single-stat"gauge"gauge""table"note"note"alerts"alerts"news"news"guide"guide"expected template variable to have non-nil query"expected template variable to have non-nil query"// %s"// %s"// SHOW DATABASES %s"// SHOW DATABASES %s"// SHOW FIELD KEYS FOR %s"// SHOW FIELD KEYS FOR %s"// SHOW TAG KEYS FOR %s"// SHOW TAG KEYS FOR %s"// SHOW TAG VALUES FOR %s"// SHOW TAG VALUES FOR %s"// SHOW MEASUREMENTS ON %s"// SHOW MEASUREMENTS ON %s"unknown variable type %s"unknown variable type %s""base"#00C9FF"#00C9FF"laser"laser""t"#4591ED"#4591ED"ocean"ocean"'?:(\w+):'?`'?:(\w+):'?`TranspilerDefaultRetentionPolicyFallbackToDBRPdbrpMappingSvcTranspileNewTranspilerWithConfig:upperDashboardTime:":upperDashboardTime:"'$1'"'$1'"// Failed to transpile query: %v
%s"// Failed to transpile query: %v\n%s"// Original Query:
%s

%s"// Original Query:\n%s\n\n%s"advanced"advanced"// cell had no queries"// cell had no queries"dbrpMapper does not support creating new mappings"dbrpMapper does not support creating new mappings"dbrpMapper does not support updating mappings"dbrpMapper does not support updating mappings"dbrpMapper does not support deleting mappings"dbrpMapper does not support deleting mappings" TODO(desa): maybe this needs to be stacked? TODO(desa): what to do about ShowNoteWhenEmpty? TODO(desa): these do not have 2.x equivalents trims `:` from variables prefix and suffix TODO(desa): replace all variables not using this hack if the query is influxql, add it as a comment and attempt to compile it to flux TODO(desa): foo FindBy returns the dbrp mapping for the specified ID. FindMany returns a list of dbrp mappings that match filter and the total count of matching dbrp mappings. Create creates a new dbrp mapping, if a different mapping exists an error is returned. Update a new dbrp mapping Delete removes a dbrp mapping./Users/austinjaybecker/projects/abeck-go-testing/cmd/chronograf-migrator/main.godashboardStoredbPathhasVarpkger"flag"github.com/influxdata/influxdb/v2/pkger"github.com/influxdata/influxdb/v2/pkger"OKischartKindboolShortdurationShortfloat64ShortintShortreferencesstringShortslcResourceslcStrmapStrStrAPIVersionAddAssociationsSetMetadataNameEnvRefdefaultValvalTypehasEnvRefStringValMetaNamesummarizeReferencesassociationMappingassocMapKeyassocMapValPkgNamesetMappingsummarizemappingSummaryretentionRulesretentionRulesortedLabelsRetentionRulescheckKindthresholdTypethreshTypeallValseveryreportZerostaleTimestatusMessagetimeSincethresholdscharttitleinfluxLegendinfluxViewColorshasTypesinfluxDashQueriesinfluxAxeshasAxesfieldOptionFieldNameSortByFieldNoteOnEmptyEnforceDecimalsShadeXColYColXPosYPosHeightpropertiesvalidPropertiesvalidBasePropsChartsrefsnotificationEndpointnotificationEndpointKindroutingKeyhttpTypeinfluxStatusnotificationRulecurLvlprevLvlmsgTemplatestatusRulestagRulesassociatedEndpointendpointNameendpointMetaNametoInfluxRulecrontelegrafvariableConstValuesMapValuesselectedinfluxVarArgsObjectsmLabelsmBucketsmChecksmDashboardsmNotificationEndpointsmNotificationRulesmTasksmTelegrafsmVariablesmEnvmEnvValsmSecretsisParsedapplyEnvRefsapplySecretsmissingEnvRefsmissingSecretslabelMappingsvalidResourcesgraphResourcesgraphBucketsgraphLabelsgraphChecksgraphDashboardsgraphNotificationEndpointsgraphNotificationRulesgraphTasksgraphTelegrafsgraphVariableseachResourceparseNestedLabelsparseNestedLabeltrackNamesgetRefWithKnownEnvssetRefsparseChartparseChartQueriesparseQueryDashboardToObjectFound duplicate variables with name %q skipping
"Found duplicate variables with name %q skipping\n"VariableToObjectEncodingYAMLStringVarpath to the chronograf database"path to the chronograf database""output"dashboards.yml"dashboards.yml"path to the output yaml file"path to the output yaml file"must supply db flag"must supply db flag"Stderr TODO(desa): not sure what we actually want to do herevalidationErrNestedSummaryReferenceEnvRefKeyValTypeDefaultValueSummaryTelegrafSummaryIdentifierEnvReferencesSummaryLabelSafeIDLabelAssociationsparseErrresourceErrIdxRootErrsAssociationErrsValidationErrsValidationErrIndexesResourcesrawErrsObjectAssociationValidateOptFnvalidateOptminResourcesskipValidateSummaryVariableSummaryNotificationRuleSummaryStatusRuleCurrentLevelPreviousLevelSummaryTagRuleEndpointIDEndpointMetaNameEndpointTypeMessageTemplateStatusRulesTagRulesSummaryCheckSummaryBucketvalidIDSummaryTaskSummaryNotificationEndpointSummaryLabelMappingStateStatusResourceMetaNameResourceNameLabelMetaNameLabelNameSummaryDashboardSummaryChartXPositionYPositionNotificationEndpointsNotificationRulesLabelMappingsMissingEnvsMissingSecretsTasksTelegrafConfigsVariablesExpression/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/authorization.goToBytesPerSecondaddMemberauthActiveCmdauthCRUDFlagsauthCreateCmdauthCreateFlagsauthDeleteCmdauthFindCmdauthInactiveCmdauthorizationActiveFauthorizationCreateFauthorizationDeleteFauthorizationFindFauthorizationFindFlagsauthorizationInactiveFbucketPrintOptbucketSVCsFnbytesUnitMultipliercheckSetupcheckSetupRunEMiddlewarecmdApplycmdAuthcmdBackupcmdBackupBuildercmdBucketcmdBucketBuildercmdConfigcmdConfigBuildercmdDashboardcmdDashboardBuildercmdDeletecmdDeleteBuildercmdExportcmdInfluxBuildercmdOrgBuildercmdOrganizationcmdPingcmdQuerycmdRestorecmdRestoreBuildercmdSecretcmdSecretBuildercmdSetupcmdSetupUsercmdStackcmdTaskcmdTelegrafcmdTelegrafBuildercmdTemplatecmdTemplateBuildercmdTranspilecmdUsercmdUserBuildercmdUserDepscmdV1AuthcmdV1DBRPcmdV1SubCommandscmdVersioncmdWritecobraRunEFncobraRunEMiddlewarecolIdxcolorRowcompletionCmdconfigPrintOptscreateTemplateBufdashboardSVCsFndefaultConfigPathdefaultConfigsPathdiffPrintereolerrMultipleMatchingAuthorizationsfetchSubCommandfindfixedWidthTimeFmtflagOptsfluxQueryFfluxWriteDryrunFfluxWriteFformatDurationformattergenericCLIOptFngenericCLIOptsgetConfigFromDefaultPathglobalFlagshttpClientinStdIninfluxCmdinputFormatCsvinputFormatLineProtocolinteractiveisCharacterDeviceisInteractivemakeV1AuthorizationCreateEmakeV1AuthorizationSetPasswordFmapKeysmaxTCPConnectionsmigrateOldCredentialminWidthsByTypemissingValKeysmustDefaultConfigPathnewAuthorizationServicenewBucketSVCsnewCmdBackupBuildernewCmdBucketBuildernewCmdDashboardBuildernewCmdOrgBuildernewCmdPkgerBuildernewCmdRestoreBuildernewCmdSecretBuildernewCmdTelegrafBuildernewCmdUserBuildernewConfigServicenewDashboardSVCsnewDiffPrinternewFormatternewHTTPClientnewInfluxCmdBuildernewOrderedColsnewOrgServicesnewOrganizationServicenewPkgerSVCnewResourcesToClonenewSecretSVCsnewTelegrafSVCsnewUserSVCnewUserServicenewV1AuthorizationServicenewV1DBRPServicenonInteractiveonboardingRequestorderedColsorgPrintOptorgSVCFnparseTemplateActionsprintTasksprintVarArgsqueryFlagsrateLimitRegexpreadFilesFromPathreadFluxQueryregisterPrintOptionsremoveMemberrunRetryFrunRetryFlagssecretPrintOptsecretSVCsFnseeHelpsetViperOptionssetupFsetupFlagssetupUserFtablePrintertaskCreateCmdtaskCreateFtaskCreateFlagstaskDeleteCmdtaskDeleteFtaskDeleteFlagstaskFindCmdtaskFindFtaskFindFlagstaskLogCmdtaskLogFindCmdtaskLogFindFtaskLogFindFlagstaskPrintFlagstaskPrintOptstaskRunCmdtaskRunFindCmdtaskRunFindFtaskRunFindFlagstaskRunRetryCmdtaskUpdateCmdtaskUpdateFtaskUpdateFlagstelegrafSVCsFntemplateKindFoldtemplateSVCsFntimeUnitMultipliertoInfluxIDstoMapInterfacetokenPrintOpttranspileFtranspileFlagsuserPrintOptsuserSVCsFnv1AuthActivateFlagsv1AuthCreateCmdv1AuthCreateFlagsv1AuthDeactivateFlagsv1AuthDeleteCmdv1AuthDeleteFlagsv1AuthFindCmdv1AuthLookupFlagsv1AuthSetActiveCmdv1AuthSetInactiveCmdv1AuthSetPasswordCmdv1AuthSetPasswordFlagsv1AuthorizationDeleteFv1AuthorizationFindFv1AuthorizationFindFlagsv1AuthorizationSetActiveFv1AuthorizationSetInactiveFv1DBRPCRUDFlagsv1DBRPCreateCmdv1DBRPCreateFv1DBRPCreateFlagsv1DBRPDeleteCmdv1DBRPDeleteFv1DBRPDeleteFlagsv1DBRPFindCmdv1DBRPFindFv1DBRPFindFlagsv1DBRPPrintOptv1DBRPUpdateCmdv1DBRPUpdateFv1DBRPUpdateFlagsv1FindOneAuthorizationv1Tokenv1TokenPrintOptv1WriteDBRPsv1WriteTokenswriteConfigToPathwriteDashboardRowswriteFlagswriteFlagsTypewriteJSONwriteStackRowswriteTelegrafRowswriteToHelperwriteTokenscmdbpprovideduserNamebucketPermsorgSvcprovidedPermuserSvcfIDuIDoIDprintOptstabWcobragithub.com/influxdata/influxdb/v2/authorization"github.com/influxdata/influxdb/v2/authorization"github.com/influxdata/influxdb/v2/cmd/influx/internal"github.com/influxdata/influxdb/v2/cmd/influx/internal"github.com/spf13/cobra"github.com/spf13/cobra"UserNamejson:"userName"`json:"userName"`ConfigsPreviousActiveSwitchcfgsskipVerifytraceDebugIDactiveConfigconfigsregisterFlagsViperFsWriterAtChtimesdefaultRemoteProvidersecretKeyringSecretKeyringStringReplacerFlagValueHasChangedValueStringPostfixDisableExpansionMustFlagMustGetClearCommentsSetCommentSetCommentsMustGetBoolgetBoolMustGetDurationGetParsedDurationMustGetParsedDurationMustGetFloat64getFloat64GetIntMustGetIntMustGetInt64getInt64GetUintMustGetUintMustGetUint64getUint64GetStringMustGetStringFilterRegexpFilterPrefixFilterStripPrefixSetValueMustSetWriteCommentMapFilterFunckeyDelimconfigPathsremoteProvidersconfigNameconfigFileconfigTypeconfigPermissionsenvPrefixautomaticEnvAppliedenvKeyReplacerallowEmptyEnvoverridedefaultskvstorepflagsaliasestypeByDefValueonConfigChangeOnConfigChangeWatchConfigSetConfigFileSetEnvPrefixmergeWithEnvPrefixAllowEmptyEnvgetEnvConfigFileUsedAddConfigPathAddRemoteProviderAddSecureRemoteProviderproviderPathExistssearchMapsearchMapWithPathPrefixesisPathShadowedInDeepMapisPathShadowedInFlatMapisPathShadowedInAutoEnvSetTypeByDefaultValueGetTimeGetIntSliceGetStringSliceGetStringMapGetStringMapStringGetStringMapStringSliceGetSizeInBytesUnmarshalKeyUnmarshalExactBindPFlagsBindPFlagBindFlagValuesBindFlagValueBindEnvAutomaticEnvSetEnvKeyReplacerRegisterAliasregisterAliasrealKeyInConfigSetDefaultReadInConfigMergeInConfigReadConfigMergeConfigMergeConfigMapWriteConfigSafeWriteConfigWriteConfigAsSafeWriteConfigAswriteConfigunmarshalReadermarshalWriterReadRemoteConfigWatchRemoteConfigWatchRemoteConfigOnChannelgetKeyValueConfiggetRemoteConfigwatchKeyValueConfigOnChannelwatchKeyValueConfigwatchRemoteConfigAllKeysflattenAndMergeMapmergeFlatMapAllSettingsSetFsSetConfigNameSetConfigTypeSetConfigPermissionsgetConfigTypegetConfigFilesearchInPathfindConfigFileShellCompDirectivePositionalArgsFParseErrWhitelistUnknownFlagsFlagSetParseErrorsWhitelistNormalizedNameShorthandDefValueChangedNoOptDefValDeprecatedShorthandDeprecateddefaultIsZeroValueErrorHandlingparsedactualformalerrorHandlingVisitAllPrintDefaultsdefaultUsageNFlagNArgBoolVarIntVarInt64VarUintVarUint64VarFloat64VarDurationVarTextVarsprintffailfparseOneParsedSortFlagsorderedActualsortedActualorderedFormalsortedFormalshorthandsargsLenAtDashinterspersednormalizeNameFuncaddedGoFlagSetsBoolVarPBoolPGetBoolSliceBoolSliceVarBoolSliceVarPBoolSliceBoolSlicePGetBytesHexBytesHexVarBytesHexVarPBytesHexBytesHexPGetBytesBase64BytesBase64VarBytesBase64VarPBytesBase64BytesBase64PCountVarCountVarPCountPDurationVarPDurationPGetDurationSliceDurationSliceVarDurationSliceVarPDurationSliceDurationSlicePSetNormalizeFuncGetNormalizeFuncnormalizeFlagNameHasFlagsHasAvailableFlagsShorthandLookupgetFlagTypeArgsLenAtDashMarkDeprecatedMarkShorthandDeprecatedMarkHiddenSetAnnotationFlagUsagesWrappedFlagUsagesVarPFVarPAddFlagAddFlagSetparseLongArgparseSingleShortArgparseShortArgparseArgsParseAllSetInterspersedFloat32VarFloat32VarPFloat32PGetFloat32SliceFloat32SliceVarFloat32SliceVarPFloat32SliceFloat32SlicePFloat64VarPFloat64PGetFloat64SliceFloat64SliceVarFloat64SliceVarPFloat64SliceFloat64SlicePAddGoFlagAddGoFlagSetIntVarPIntPInt16VarInt16VarPInt16Int16PInt32VarInt32VarPInt32PGetInt32SliceInt32SliceVarInt32SliceVarPInt32SliceInt32SlicePInt64VarPInt64PGetInt64SliceInt64SliceVarInt64SliceVarPInt64SliceInt64SlicePInt8VarInt8VarPInt8Int8PIntSliceVarIntSliceVarPIntSliceIntSlicePGetIPIPVarIPVarPIPPGetIPSliceIPSliceVarIPSliceVarPIPSliceIPSlicePGetIPv4MaskIPMaskVarIPMaskVarPIPMaskPGetIPNetIPNetVarIPNetVarPIPNetPStringVarPStringPGetStringArrayStringArrayVarStringArrayVarPStringArrayStringArrayPStringSliceVarStringSliceVarPStringSliceStringSlicePGetStringToIntStringToIntVarStringToIntVarPStringToIntStringToIntPGetStringToInt64StringToInt64VarStringToInt64VarPStringToInt64StringToInt64PGetStringToStringStringToStringVarStringToStringVarPStringToStringStringToStringPUintVarPUintPUint16VarUint16VarPUint16PUint32VarUint32VarPUint32PUint64VarPUint64PUint8VarUint8VarPUint8Uint8PGetUintSliceUintSliceVarUintSliceVarPUintSliceUintSlicePSuggestForShortLongExampleValidArgsValidArgsFunctionArgAliasesBashCompletionFunctionPersistentPreRunPersistentPreRunEPreRunPreRunERunEPostRunPostRunEPersistentPostRunPersistentPostRunESilenceErrorsSilenceUsageDisableFlagParsingDisableAutoGenTagDisableFlagsInUseLineDisableSuggestionsSuggestionsMinimumDistanceTraverseChildrencommandsMaxUseLencommandsMaxCommandPathLencommandsMaxNameLencommandsAreSortedcommandCalledAsflagErrorBuflflagsiflagsparentsPflagsglobNormFuncusageFuncusageTemplateflagErrorFunchelpTemplatehelpFunchelpCommandversionTemplateinReaderoutWritererrWriterGenBashCompletionGenBashCompletionFileSetArgsSetOutSetErrSetInSetUsageFuncSetUsageTemplateSetFlagErrorFuncSetHelpFuncSetHelpCommandSetHelpTemplateSetVersionTemplateSetGlobalNormalizationFuncOutOrStdoutOutOrStderrErrOrStderrInOrStdingetOutgetErrgetInUsageFuncHelpFuncUsageStringFlagErrorFuncUsagePaddingCommandPathPaddingNamePaddingUsageTemplateHelpTemplateVersionTemplatefindSuggestionsfindNextTraverseSuggestionsForVisitParentspreRunExecuteContextExecuteCValidateArgsvalidateRequiredFlagsInitDefaultHelpFlagInitDefaultVersionFlagInitDefaultHelpCmdResetCommandsRemoveCommandPrintErrPrintErrlnPrintErrfCommandPathUseLineDebugFlagsHasAliasCalledAshasNameOrAliasPrefixNameAndAliasesHasExampleRunnableHasSubCommandsIsAvailableCommandIsAdditionalHelpTopicCommandHasHelpSubCommandsHasAvailableSubCommandsHasParentGlobalNormalizationFuncLocalNonPersistentFlagsLocalFlagsInheritedFlagsNonInheritedFlagsPersistentFlagsResetFlagsHasPersistentFlagsHasLocalFlagsHasInheritedFlagsHasAvailablePersistentFlagsHasAvailableLocalFlagsHasAvailableInheritedFlagspersistentFlagParseFlagsmergePersistentFlagsupdateParentsPflagsRegisterFlagCompletionFuncinitCompleteCmdgetCompletionsGenFishCompletionGenFishCompletionFileGenPowerShellCompletionGenPowerShellCompletionFileMarkFlagRequiredMarkPersistentFlagRequiredMarkFlagFilenameMarkFlagCustomMarkPersistentFlagFilenameMarkFlagDirnameMarkPersistentFlagDirnameGenZshCompletionFileGenZshCompletionMarkZshCompPositionalArgumentFileMarkZshCompPositionalArgumentWordszshcompArgsAnnotationnIsDuplicatePositionzshCompGetArgsAnnotationszshCompSetArgsAnnotationserrWviperhideHeadersrunEWrapFnnewCmdnewTabWriterAuthorization management commands"Authorization management commands"registervalidOrgFlagswriteUserPermissionreadUserPermissionwriteBucketsPermissionreadBucketsPermissionwriteBucketPermissionsreadBucketPermissionswriteTasksPermissionreadTasksPermissionwriteTelegrafsPermissionreadTelegrafsPermissionwriteOrganizationsPermissionreadOrganizationsPermissionwriteDashboardsPermissionreadDashboardsPermissionwriteCheckPermissionreadCheckPermissionwriteNotificationRulePermissionreadNotificationRulePermissionwriteNotificationEndpointPermissionreadNotificationEndpointPermissionwriteDBRPPermissionreadDBRPPermissionCreate authorization"Create authorization""description""d"Token description"Token description""u"The user name"The user name"write-user"write-user"Grants the permission to perform mutative actions against organization users"Grants the permission to perform mutative actions against organization users"read-user"read-user"Grants the permission to perform read actions against organization users"Grants the permission to perform read actions against organization users"write-buckets"write-buckets"Grants the permission to perform mutative actions against organization buckets"Grants the permission to perform mutative actions against organization buckets"read-buckets"read-buckets"Grants the permission to perform read actions against organization buckets"Grants the permission to perform read actions against organization buckets"write-bucket"write-bucket"The bucket id"The bucket id"read-bucket"read-bucket"write-tasks"write-tasks"Grants the permission to create tasks"Grants the permission to create tasks"read-tasks"read-tasks"Grants the permission to read tasks"Grants the permission to read tasks"write-telegrafs"write-telegrafs"Grants the permission to create telegraf configs"Grants the permission to create telegraf configs"read-telegrafs"read-telegrafs"Grants the permission to read telegraf configs"Grants the permission to read telegraf configs"write-orgs"write-orgs"Grants the permission to create organizations"Grants the permission to create organizations"read-orgs"read-orgs"Grants the permission to read organizations"Grants the permission to read organizations"write-dashboards"write-dashboards"Grants the permission to create dashboards"Grants the permission to create dashboards"read-dashboards"read-dashboards"Grants the permission to read dashboards"Grants the permission to read dashboards"write-notificationRules"write-notificationRules"Grants the permission to create notificationRules"Grants the permission to create notificationRules"read-notificationRules"read-notificationRules"Grants the permission to read notificationRules"Grants the permission to read notificationRules"write-notificationEndpoints"write-notificationEndpoints"Grants the permission to create notificationEndpoints"Grants the permission to create notificationEndpoints"read-notificationEndpoints"read-notificationEndpoints"Grants the permission to read notificationEndpoints"Grants the permission to read notificationEndpoints"write-checks"write-checks"Grants the permission to create checks"Grants the permission to create checks"read-checks"read-checks"Grants the permission to read checks"Grants the permission to read checks"write-dbrps"write-dbrps"Grants the permission to create database retention policy mappings"Grants the permission to create database retention policy mappings"read-dbrps"read-dbrps"Grants the permission to read database retention policy mappings"Grants the permission to read database retention policy mappings"readPermwritePermjsonOut"list"List authorizations"List authorizations""find""ls"The user"The user"user-id"user-id"The user ID"The user ID""i"The authorization ID"The authorization ID"Delete authorization"Delete authorization"The authorization ID (required)"The authorization ID (required)""active"Active authorization"Active authorization""inactive"Inactive authorization"Inactive authorization"TabWriterHideHeaders"ID""Description""Token"User Name"User Name"User ID"User ID""Permissions"Deleted"Deleted"OrgClientServiceOpPrefixBucketClientServiceIndexMappingIndexBucketIndexSourceOnSourceBucketcanReadsourceBucketInsertOrgIDGenBucketIDGenurmByUserIndexuniqueBucketNameGetBucketByNameListBucketslistBucketsByOrguniqueOrgNameGetOrgByNameListOrgsCreateOrgUpdateOrgDeleteOrgCreateURMListURMsGetURMDeleteURMuniqueUserResourceMappinguniqueUserNameGetUserByNameListUsersDeletePasswordNewOrgHTTPHandlerNewBucketHTTPHandlerNewUserHTTPHandlerDatabaseInfoRetentionPolicyInfoShardGroupInfoShardInfoShardOwnersoOwnersOwnedBysiDeletedAtShardsTruncatedAtsgiShardForSubscriptionInfoReplicaNShardGroupsSubscriptionsToSpecrpiShardGroupByTimestampExpiredShardGroupsDeletedShardGroupsContinuousQueryInfocqiContinuousQueriesShardInfosUserInfoPrivilegesAuthorizeDatabaseuiAuthorizeSeriesReadAuthorizeSeriesWriteAuthorizeUnrestrictedAuthorizeQueryTermadminUserExistsMaxShardGroupIDMaxShardIDCloneDatabasesCreateRetentionPolicyDropShardShardGroupsByTimeRangeCreateShardGroupDeleteShardGroupCreateContinuousQueryDropContinuousQueryCreateSubscriptionDropSubscriptionDropUserCloneUsersSetPrivilegeSetAdminPrivilegeAdminUserExistsUserPrivilegesUserPrivilegeTruncateShardGroupshasAdminUserImportDataimportOneDBauthUserbhashsaltclosingchangedcacheDataauthCacheretentionAutoCreateAcquireLeaseCreateDatabaseWithRetentionPolicyhashWithSaltsaltedHashAuthenticateUserCountShardIDsShardsByTimeRangePruneShardGroupsCreateShardGroupWithShardsPrecreateShardGroupsSetDataWaitForDataChangedWithLoggerfullbucketNamenewBucketNamenewOrgNamekvEntryshardEntriesorgServicebucketServicerestoreServicemetaClientrestoreRunErestoreFullrestoreKVStorerestorePartialrestoreOrganizationsrestoreOrganizationrestoreBucketrestoreShardloadIncrementalStackEventStackEventTypeStackResourceStackResourceAssociationAssociationsEventTypeTemplateURLsLatestEventmakeFiltersvcFncmdDashboardslistRunEwriteDashboardsretentioncmdCreatecmdCreateRunEFncmdDeleteRunEFncmdListcmdListRunEFncmdUpdatecmdUpdateRunEFnregisterPrintFlagsprintBucketsExportOptFnExportOptExportByOrgIDOptResourceKindsResourceToCloneStackIDOrgIDsUImaskmaskValbReaderAsksetDefaultrawReadlinerawReadSelectcmdFindcmdFindRunEFnprintSecretscmdTelegrafscreateRunEFncmdRemoveremoveRunEFnupdateRunEFnwriteTelegrafConfigregisterTelegrafCfgFlagsreadConfigDeleteRequestfluxDeleteFMockclockTimersclockTimerTicktimersrunNextTimerAfterFuncSleepremoveClockTimermockstoppedChangeTypeclockauditvariableStorecreateDocumentStorecreateDocumentputDocumentputAtIDputDocumentContentputDocumentMetafindByIDfindDocumentMetaByIDfindDocumentContentByIDfindDocumentsgetKeyValueLogBoundsputKeyValueLogBoundsupdateKeyValueLogBoundsForEachLogEntryTxAddLogEntryTxputLogEntrygetLogEntryfirstLogEntrylastLogEntryscrapersBucketlistTargetsaddTargetremoveTargetupdateTargetfindTargetByIDPutTargetputTargetWithResourceLoggerWithStorefindSourceByIDfindSourcesPutSourceputSourceforEachSourceupdateSourcedeleteSourcefindTaskByIDfindTasksfindTasksByUserfindTasksByOrgfindAllTaskscreateTaskupdateTaskdeleteTaskfindLogsfindRunsfindRunByIDcancelRunretryRunforceRunCreateRuncreateRunCurrentlyRunningcurrentlyRunningManualRunsmanualRunsStartManualRunstartManualRunFinishRunfinishRunUpdateRunStateupdateRunStateAddRunLogaddRunLogfindOrganizationVariablesfindVariablesfindVariableByIDputVariableputVariableOrgsIndexremoveVariableOrgsIndexmanifestbaseNamebackupServicekvServicemanifestPathkvPathshardPathbackupRunEbackupKVStorebackupOrganizationsbackupBucketsbackupBucketbackupShardwriteManifestColMetaColTypeColReaderBooleanAllocatorAllocateFreeReallocaterefCountmutablememRetainBufMutableReserveResizeResizeNoShrinkresizedtypenullsbufferschildDataNullNBuffersnullBitmapBytesNullBitmapBytessetDataFloat64ValuesInt64ValuesGroupKeyColsHasColLabelValueValueBoolValueDurationValueFloatValueIntValueTimeValueUIntBinaryvalueOffsetsvalueBytesValueOffsetValueLenValueOffsetsValueBytesUint64ValuesBoolsFloatsIntsTimesUIntsEmptyindexMapcolsmaxWidthnewWidthsfmtBufmakePaddingBufferswriteHeaderSeparatorvalueBufzshCompArgsAnnotationzshCompArgHintTipeSVCApplyOptFnApplyOptActionSkipResourceEnvRefsResourcesToSkipKindsToSkipImpactSummaryDiffDiffBucketDiffIdentifierIsNewDiffBucketValuesOldhasConflictDiffCheckDiffCheckValuesDiffDashboardDiffDashboardValuesDiffChartDiffLabelDiffLabelValuesDiffLabelMappingResTypeResIDResNameResMetaNameDiffNotificationEndpointDiffNotificationEndpointValuesDiffNotificationRuleDiffNotificationRuleValuesEndpointNameDiffTaskDiffTaskValuesDiffTelegrafDiffVariableDiffVariableValuesTelegrafsHasConflictscommunityNameStackCreateListFilterStackIDsStackUpdateStackAdditionalResourceAdditionalResourcesDeleteStackDryRunExportInitStackListStacksReadStackUninstallStackUpdateStackenvRefsforceresourceTyperulesbucketNamescheckNamesdashboardNamesendpointNameslabelNamesruleNamestaskNamestelegrafNamesvariableNamesaddResourcesencodingfiltersdisableColordisableTableBordersquietrecursestackIDstackIDsurlsapplyOptsexportOptsupdateStackOptsapplyRunEFnexportRunEFncmdExportAllexportAllRunEFncmdExportStackexportStackRunEFncmdTemplateValidatecmdStackscmdStackInitstackInitRunEFnstackListRunEFncmdStackRemovestackRemoveRunEFncmdStackUpdatestackUpdateRunEFnwriteStackregisterTemplatePrintOptsregisterTemplateFileFlagsexportTemplatewriteTemplatereadRawTemplatesFromFilesreadRawTemplatesFromURLsreadTemplatereadLinesgetInputconvertURLEncodingconvertFileEncodingconvertEncodingprintTemplateDiffprintTemplateSummarytablePrinterGenparseFuncDecoderConfigOptionDecoderConfigDecodeHookFuncUnusedDecodeHookErrorUnusedZeroFieldsWeaklyTypedInputOptDestPEnvVarPersistentmustRegisterURLsSkipRowOnErrorSkipHeaderMaxLineLengthIgnoreDataTypeInColumnNameErrorsFileRateLimitcreateLineReaderRemoteProviderTextUnmarshalerTextMarshaleruserSVCpassSVCurmSVCgetPassFnmemberIDdeleteRunEFnfindRunEFnprintOrgcmdMembercmdMemberListmemberListRunEFncmdMemberAddmemberAddRunEFncmdMemberRemovemembersRemoveRunEFnmemberListBorderLeftRightTopBottomrowsfooterscaptioncaptionTextautoFmtautoWrapreflowTextmWpCenterpRowpColumntColumntRowhAlignfAlignnewLinerowLineautoMergeCellsnoWhiteSpacetablePaddinghdrLineborderscolSizeheaderParamscolumnsParamsfooterParamscolumnsAlignRenderSetFooterSetCaptionSetAutoFormatHeadersSetAutoWrapTextSetReflowDuringAutoWrapSetColWidthSetColMinWidthSetColumnSeparatorSetRowSeparatorSetCenterSeparatorSetHeaderAlignmentSetFooterAlignmentSetAlignmentSetNoWhiteSpaceSetTablePaddingSetColumnAlignmentSetNewLineSetHeaderLineSetRowLineSetAutoMergeCellsSetBorderSetBordersRichAppendBulkNumLinesClearRowsClearFootercenterprintLineprintLineOptionalCellSeparatorsprintHeadingprintFooterprintCaptiongetTableWidthprintRowsfillAlignmentprintRowprintRowsMergeCellsprintRowMergeCellsSetHeaderColorSetColumnColorSetFooterColorcolorAddcolorFootercolorHeaderscolorRemoveappendCallsheaderLenSetHeaderssetFooterAppendDiffappendBufferLineredRowgreenRowprependdbrpURLCreateConfigDeleteConfigListConfigsSwitchActiveUpdateConfigcmdSwitchActiveRunEFnregisterConfigSettingFlagsregisterFilepathprintConfigsgetValidHostURLnewConfigSVCFlagValueSetafterTimebeforeTimecmdPasswordcmdPasswordRunEFnprintUsernoPasswordRetentionPolicyUpdaterpuSetDurationSetReplicaNSetShardGroupDurationOwnerIDsGetOwnerIDsGetOwnersGetStartTimeGetEndTimeGetDeletedAtGetShardsGetTruncatedAtGetDestinationsGetShardGroupDurationGetReplicaNGetShardGroupsGetSubscriptionsRetentionPolicySpecNewRetentionPolicyInfoNodeInfoTCPHostGetHostGetTCPHostGetDefaultRetentionPolicyGetRetentionPoliciesGetContinuousQueriesGetDatabaseGetPrivilegeGetHashGetAdminGetPrivilegesMaxNodeIDGetTermGetIndexGetClusterIDGetNodesGetMaxNodeIDGetMaxShardGroupIDGetMaxShardIDGetDataNodesGetMetaNodesBucketHandlerbucketSvclabelSvchandlePostBuckethandleGetBuckethandleDeleteBuckethandleGetBucketshandlePatchBucketlookupOrgByBucketIDKeyValuessortedHashKeyneedsEscapeAppendHashKeyExpirationOrgHandlerhandlePostOrghandleGetOrghandleGetOrgshandlePatchOrghandleDeleteOrglookupOrgByIDkeyValueLogBoundsStopTimereadOptionsIndexDiffPresentInIndexMissingFromIndexMissingFromSourceaddMissingSourceaddMissingIndexCorruptVisitFuncValidateFuncLoopHideDefaultHideOrderHideMaskDefaultMaskValvalidateFuncreadOptsUserHandlerpasswordSvcMeResourceHandlerUserResourceHandlerhandlePostUserPasswordputPasswordhandlePutUserPasswordhandlePostUserhandleGetMehandleGetUserhandleGetPermissionshandleDeleteUserhandleGetUsershandlePatchUserresourceHandler/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/backup.goboltClientlogconftenantStoreshsggwpolicyrunEusegzipinfluxloggertenantcompress/gzip"compress/gzip"github.com/influxdata/influxdb/v2/bolt"github.com/influxdata/influxdb/v2/bolt"github.com/influxdata/influxdb/v2/http"github.com/influxdata/influxdb/v2/http"github.com/influxdata/influxdb/v2/logger"github.com/influxdata/influxdb/v2/logger"github.com/influxdata/influxdb/v2/tenant"github.com/influxdata/influxdb/v2/tenant"github.com/influxdata/influxdb/v2/v1/services/meta"github.com/influxdata/influxdb/v2/v1/services/meta"bucket-id"bucket-id"The ID of the bucket to backup"The ID of the bucket to backup""b"The name of the bucket to backup"The name of the bucket to backup"backup [flags] path"backup [flags] path"must specify output path"must specify output path"too many args specified"too many args specified"Backup database"Backup database"
Backs up InfluxDB to a directory.

Examples:
	# backup all data
	influx backup /path/to/backup
`
Backs up InfluxDB to a directory.

Examples:
	# backup all data
	influx backup /path/to/backup
`%s.manifest"%s.manifest"%s.bolt"%s.bolt"%s.s%d"%s.s%d".tar.gz".tar.gz"SuppressLogoNewConfig5110777StoreOptionRetentionAutoCreateLoggingEnabledDiagnosticsBackup complete"Backup complete"Backing up KV store"Backing up KV store"Backing up organization"Backing up organization"Backing up bucket"Backing up bucket"bucket database not found: %s"bucket database not found: %s"Shard removed during backup"Shard removed during backup"shard_id"shard_id"Backing up shard"Backing up shard"CommentOScompressorcompressionLevelgoodlazynicechainfastSkipHashinghuffmanBitWriterhuffmanEncoderhcodeliteralNodebyLiteralbyFreqcodesfreqcachebitCountlnslfsbitLengthbitCountsassignEncodingAndSizegeneratecodegenFreqnbytesliteralFreqoffsetFreqcodegenliteralEncodingoffsetEncodingcodegenEncodingwriteBitsgenerateCodegendynamicSizefixedSizestoredSizewriteCodewriteDynamicHeaderwriteStoredHeaderwriteFixedHeaderwriteBlockwriteBlockDynamicindexTokenswriteBlockHuffdeflateFasttableEntry16384matchLenshiftOffsets13107232768257bulkHasherbestSpeedchainHeadhashHeadhashPrevhashOffsetwindowEndblockStartbyteAvailablemaxInsertIndexhashMatchfillDeflatefillWindowfindMatchwriteStoredBlockencSpeedinitDeflatedeflatefillStorestoreHuffsyncFlushdictdigestwriteStringWriting manifest"Writing manifest"  "  "create manifest: %w"create manifest: %w"
'\n'WriteFile Create top level logger Determine a base Ensure directory exsits. Back up Bolt database to file. Open bolt DB. Open meta store so we can iterate over meta data. Filter through organizations & buckets to backup appropriate shards. backupKVStore streams the bolt KV file to a file at path. Open writer to output file. Stream bolt file from server, sync, and ensure file closes correctly. Lookup file size. Build a filter if org ID or org name were specified. Retrieve a list of all matching organizations. Back up buckets in each matching organization. Build a filter if bucket ID or bucket name were specified. Back up shards in each matching bucket. Lookup matching database from the meta store. Iterate over and backup each shard. backupShard streams a tar of TSM data for shard. Wrap file writer with a gzip writer. Stream file from server, sync, and ensure file closes correctly. Determine file size. Update manifest. writeManifest writes the manifest file out.RowsAddRow/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/bucket.gosvcsFnbktSVCorgSVCprintOptgithub.com/influxdata/influxdb/v2/cmd/internal"github.com/influxdata/influxdb/v2/cmd/internal"Bucket management commands"Bucket management commands"Create bucket"Create bucket"'n'BUCKET_NAME"BUCKET_NAME"New bucket name"New bucket name"Description of bucket that will be created"Description of bucket that will be created""retention""r"Duration bucket will retain data. 0 is infinite. Default is 0."Duration bucket will retain data. 0 is infinite. Default is 0."RawDurationToTimeDurationfailed to create bucket: %v"failed to create bucket: %v"Delete bucket"Delete bucket"The bucket ID, required if name isn't provided"The bucket ID, required if name isn't provided""n"The bucket name, org or org-id will be required by choosing this"The bucket name, org or org-id will be required by choosing this"failed to decode bucket id %q: %v"failed to decode bucket id %q: %v"failed to find bucket with id %q: %v"failed to find bucket with id %q: %v"failed to delete bucket with id %q: %v"failed to delete bucket with id %q: %v"List buckets"List buckets"The bucket name"The bucket name"The bucket ID"The bucket ID"failed to decode org id %q: %v"failed to decode org id %q: %v"failed to retrieve buckets: %s"failed to retrieve buckets: %s""update"Update bucket"Update bucket"The bucket ID (required)"The bucket ID (required)"failed to update bucket: %v"failed to update bucket: %v""Name"Retention"Retention"Organization ID"Organization ID"/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/completion.gorootCmdwriteZSH
compdef _influx influx
"\ncompdef _influx influx\n"completion [bash|zsh]"completion [bash|zsh]"Generates completion scripts"Generates completion scripts"ExactValidArgsbash"bash"zsh"zsh"powershell"powershell"
	Outputs shell completion for the given shell (bash or zsh)

	OS X:
		$ source $(brew --prefix)/etc/bash_completion	# for bash users
		$ source <(influx completion bash)		# for bash users
		$ source <(influx completion zsh)		# for zsh users

	Ubuntu:
		$ source /etc/bash-completion	   # for bash users
		$ source <(influx completion bash) # for bash users
		$ source <(influx completion zsh)  # for zsh users

	Additionally, you may want to add this to your .bashrc/.zshrc
`
	Outputs shell completion for the given shell (bash or zsh)

	OS X:
		$ source $(brew --prefix)/etc/bash_completion	# for bash users
		$ source <(influx completion bash)		# for bash users
		$ source <(influx completion zsh)		# for zsh users

	Ubuntu:
		$ source /etc/bash-completion	   # for bash users
		$ source <(influx completion bash) # for bash users
		$ source <(influx completion zsh)  # for zsh users

	Additionally, you may want to add this to your .bashrc/.zshrc
`/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/config/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/config/config.goDefaultConfigNewLocalConfigSVCParseActiveConfigbadNamesbaseRWblockBadNameioStorelocalConfigsSVCnewConfigsSVCparsePreviousActivewriteConfigsb2parseActiveConfigp0b1activatedcurrentOrPrevioushasActivepreviousTexttomlgithub.com/BurntSushi/toml"github.com/BurntSushi/toml"toml:"-" json:"-"`toml:"-" json:"-"`toml:"url" json:"url"`toml:"url" json:"url"`toml:"token" json:"token"`toml:"token" json:"token"`toml:"org" json:"org"`toml:"org" json:"org"`toml:"active,omitempty" json:"active,omitempty"`toml:"active,omitempty" json:"active,omitempty"`toml:"previous,omitempty" json:"previous,omitempty"`toml:"previous,omitempty" json:"previous,omitempty"`config %q is not found`config %q is not found`-"-""set"switch"switch"%q is not a valid config name`%q is not a valid config name`IndenthasWrittensafeEncodeeElementwriteQuotedeArrayOrSliceElementeArrayOfTableseTableeMapOrStructeMapeStructnewlinekeyEqElementwfindentStr# 
"# \n"us-central"us-central"https://us-central1-1.gcp.cloud2.influxdata.com"https://us-central1-1.gcp.cloud2.influxdata.com"XXX"XXX"us-west"us-west"https://us-west-2-1.aws.cloud2.influxdata.com"https://us-west-2-1.aws.cloud2.influxdata.com"eu-central"eu-central"https://eu-central-1-1.aws.cloud2.influxdata.com"https://eu-central-1-1.aws.cloud2.influxdata.com"# "# ""\n"MetaDatatomlTypetypeStringmaybeQuotedAllmaybeQuoteddecodedPrimitiveDecodeunifyunifyStructunifyMapunifyArrayunifySliceunifySliceArrayunifyDatetimeunifyStringunifyFloat64unifyIntunifyBoolunifyAnythingunifyTextIsDefinedUndecodedDecodeReaderconfig name is empty"config name is empty"config %q already exists"config %q already exists""config %q is not found"ModePermprevious "previous "more than one "more than one "activated configs found"activated configs found"activated config is not found"activated config is not found" Config store the crendentials of influxdb host and token. Token is base64 encoded sequence. DefaultConfig is default config without token Configs is map of configs indexed by name. Service is the service to list and write configs. store is the embedded store of the Config service. Switch to another config. localConfigsSVC has the path and dir to write and parse configs. newConfigsSVC create a new localConfigsSVC. NewLocalConfigSVC create a new local config svc. ListConfigs from the local path. parsePreviousActive from the local path. a list cloud 2 clusters, commented out ListConfigs decodes configs from io readers CreateConfig create new config. DeleteConfig will delete a config. SwitchActive will active the config by name, if name is "-", active the previous one. UpdateConfig will update the config. writeConfigs to the path. parsePreviousActive return the previous active config from the reader ParseActiveConfig returns the active config from the reader.Primitiveundecoded/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/config.godeletedConfigsgithub.com/influxdata/influxdb/v2/cmd/influx/config"github.com/influxdata/influxdb/v2/cmd/influx/config"config [config name]"config [config name]"ArbitraryArgsConfig management commands"Config management commands"
	Providing no argument to the config command will print the active configuration. When
	an argument is provided, the active config will be switched to the config with a name
	matching that of the argument provided.

	Examples:
		# show active config
		influx config

		# set active config to previously active config
		influx config -

		# set active config
		influx config $CONFIG_NAME

	The influx config command displays the active InfluxDB connection configuration and
	manages multiple connection configurations stored, by default, in ~/.influxdbv2/configs.
	Each connection includes a URL, token, associated organization, and active setting.
	InfluxDB reads the token from the active connection configuration, so you don't have
	to manually enter a token to log into InfluxDB.

	For information about the config command, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config
`
	Providing no argument to the config command will print the active configuration. When
	an argument is provided, the active config will be switched to the config with a name
	matching that of the argument provided.

	Examples:
		# show active config
		influx config

		# set active config to previously active config
		influx config -

		# set active config
		influx config $CONFIG_NAME

	The influx config command displays the active InfluxDB connection configuration and
	manages multiple connection configurations stored, by default, in ~/.influxdbv2/configs.
	Each connection includes a URL, token, associated organization, and active setting.
	InfluxDB reads the token from the active connection configuration, so you don't have
	to manually enter a token to log into InfluxDB.

	For information about the config command, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config
`Create config"Create config"
	The influx config create command creates a new InfluxDB connection configuration
	and stores it in the configs file (by default, stored at ~/.influxdbv2/configs).

	Examples:
		# create a config and set it active
		influx config create -a -n $CFG_NAME -u $HOST_URL -t $TOKEN -o $ORG_NAME

		# create a config and without setting it active
		influx config create -n $CFG_NAME -u $HOST_URL -t $TOKEN -o $ORG_NAME

	For information about the config command, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config/create`
	The influx config create command creates a new InfluxDB connection configuration
	and stores it in the configs file (by default, stored at ~/.influxdbv2/configs).

	Examples:
		# create a config and set it active
		influx config create -a -n $CFG_NAME -u $HOST_URL -t $TOKEN -o $ORG_NAME

		# create a config and without setting it active
		influx config create -n $CFG_NAME -u $HOST_URL -t $TOKEN -o $ORG_NAME

	For information about the config command, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config/create`host-url"host-url"rm [cfg_name]"rm [cfg_name]""remove"Delete config"Delete config"
	The influx config delete command deletes an InfluxDB connection configuration from
	the configs file (by default, stored at ~/.influxdbv2/configs).

	Examples:
		# delete a config
		influx config rm $CFG_NAME

		# delete multiple configs
		influx config rm $CFG_NAME_1 $CFG_NAME_2

	For information about the config command, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config/remove`
	The influx config delete command deletes an InfluxDB connection configuration from
	the configs file (by default, stored at ~/.influxdbv2/configs).

	Examples:
		# delete a config
		influx config rm $CFG_NAME

		# delete multiple configs
		influx config rm $CFG_NAME_1 $CFG_NAME_2

	For information about the config command, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config/remove`The config name (required)"The config name (required)"provide the name as an arg; example: influx config rm $CFG_NAME"provide the name as an arg; example: influx config rm $CFG_NAME"Update config"Update config"
	The influx config set command updates information in an InfluxDB connection
	configuration in the configs file (by default, stored at ~/.influxdbv2/configs).

	Examples:
		# update a config and set active
		influx config set -a -n $CFG_NAME -u $HOST_URL -t $TOKEN -o $ORG_NAME

		# update a config and do not set to active
		influx config set -n $CFG_NAME -u $HOST_URL -t $TOKEN -o $ORG_NAME

	For information about the config command, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config/set`
	The influx config set command updates information in an InfluxDB connection
	configuration in the configs file (by default, stored at ~/.influxdbv2/configs).

	Examples:
		# update a config and set active
		influx config set -a -n $CFG_NAME -u $HOST_URL -t $TOKEN -o $ORG_NAME

		# update a config and do not set to active
		influx config set -n $CFG_NAME -u $HOST_URL -t $TOKEN -o $ORG_NAME

	For information about the config command, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config/set`List configs"List configs"
	The influx config ls command lists all InfluxDB connection configurations
	in the configs file (by default, stored at ~/.influxdbv2/configs). Each
	connection configuration includes a URL, authentication token, and active
	setting. An asterisk (*) indicates the active configuration.

	Examples:
		# list configs
		influx config ls

		# list configs with long alias
		influx config list

	For information about the config command, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config/list`
	The influx config ls command lists all InfluxDB connection configurations
	in the configs file (by default, stored at ~/.influxdbv2/configs). Each
	connection configuration includes a URL, authentication token, and active
	setting. An asterisk (*) indicates the active configuration.

	Examples:
		# list configs
		influx config ls

		# list configs with long alias
		influx config list

	For information about the config command, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/config/list`config-name"config-name""a"Set as active config"Set as active config"The host url (required)"The host url (required)""o"The optional organization name"The optional organization name"The token for host (required)"The token for host (required)"use the --config-name flag"use the --config-name flag"use the --host-url flag"use the --host-url flag"skip-verify"skip-verify"trace-debug-id"trace-debug-id""Active""URL""Org"a scheme of HTTP(S) must be provided for host url"a scheme of HTTP(S) must be provided for host url" name is required everywhere deprecated moving forward, not explicit enough based on feedback the short flags will still be respected but their long form is different./Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/dashboard.gorawIDdashSVCgithub.com/influxdata/influxdb/v2/dashboards/transport"github.com/influxdata/influxdb/v2/dashboards/transport"List Dashboard(s)."List Dashboard(s)."
	List Dashboard(s).

	Examples:
		# list all known Dashboards
		influx dashboards

		# list all known Dashboards matching ids
		influx dashboards --id $ID1 --id $ID2

		# list all known Dashboards matching ids shorts
		influx dashboards -i $ID1 -i $ID2
`
	List Dashboard(s).

	Examples:
		# list all known Dashboards
		influx dashboards

		# list all known Dashboards matching ids
		influx dashboards --id $ID1 --id $ID2

		# list all known Dashboards matching ids shorts
		influx dashboards -i $ID1 -i $ID2
`Dashboard ID to retrieve."Dashboard ID to retrieve."unprocessable entityat least one of org, org-id, or id must be provided"at least one of org, org-id, or id must be provided""OrgID"Num Cells"Num Cells"/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/delete.gosignalsgithub.com/influxdata/influxdb/v2/kit/signals"github.com/influxdata/influxdb/v2/kit/signals"Delete points from influxDB"Delete points from influxDB"Delete points from influxDB, by specify start, end time
	and a sql like predicate string.`Delete points from influxDB, by specify start, end time
	and a sql like predicate string.`org-id"org-id"The ID of the organization that owns the bucket"The ID of the organization that owns the bucket"'o'The name of the organization that owns the bucket"The name of the organization that owns the bucket"The ID of the destination bucket"The ID of the destination bucket"The name of destination bucket"The name of destination bucket""start"the start time in RFC3339Nano format, exp 2009-01-02T23:00:00Z"the start time in RFC3339Nano format, exp 2009-01-02T23:00:00Z""stop"the stop time in RFC3339Nano format, exp 2009-01-02T23:00:00Z"the stop time in RFC3339Nano format, exp 2009-01-02T23:00:00Z"predicate"predicate""p"sql like predicate string, exp 'tag1="v1" and (tag2=123)'"sql like predicate string, exp 'tag1=\"v1\" and (tag2=123)'"please specify one of org or org-id"please specify one of org or org-id"please specify one of bucket or bucket-id"please specify one of bucket or bucket-id"both start and stop are required"both start and stop are required"WithStandardSignalsCanceledfailed to delete data: %v"failed to delete data: %v"/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/internal/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/internal/errorfmt.goErrorFmtformatStringTypeunicode"unicode"Trim
 .!?"\n .!?"rune."." ErrorFmt formats errors presented to the user such that the first letter in the error is capitalized and ends with an appropriate punctuation./Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/internal/tabwriter.goformatString"\t"%s TabWriter wraps tab writer headers logic. NewTabWriter creates a new tab writer. HideHeaders will set the hideHeaders flag. WriteHeaders will write headers. Write will write the map into embed tab writer. Flush should be called after the last call to Write to ensure that any data buffered in the Writer is written to output. Any incomplete escape sequence at the end is considered complete for formatting purposes./Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/main.gocanWrapRunEuseRunEMiddlewarefOptsskipFlagsskipsoptFnoptFnschildCmdchildCmdFnsconfigsPathtokBtokenFileisOnboardingsetupErrpersistentinfluxOrgIDgetOrgByNameheadersPjsonOutPcligithub.com/influxdata/influxdb/v2/internal/fs"github.com/influxdata/influxdb/v2/internal/fs"github.com/influxdata/influxdb/v2/kit/cli"github.com/influxdata/influxdb/v2/kit/cli"github.com/spf13/viper"github.com/spf13/viper"dev"dev"influx/%s (%s) Sha/%s Date/%s"influx/%s (%s) Sha/%s Date/%s"WithUserAgentHeaderWithHeaderjaeger-debug-id"jaeger-debug-id"NewHTTPClientNoArgsErr: active config %q was not found
"Err: active config %q was not found\n"global flags are not set: <nil>"global flags are not set: <nil>"'t'Authentication token"Authentication token"HTTP address of InfluxDB"HTTP address of InfluxDB"configs-path"configs-path"Path to the influx CLI configurations"Path to the influx CLI configurations"active-config"active-config"Config name to use for command"Config name to use for command"'c'Skip TLS certificate chain and host name verification."Skip TLS certificate chain and host name verification."StdinInflux Client"Influx Client""help""h"Help for the %s command "Help for the %s command "Print the influx CLI version"Print the influx CLI version"Influx CLI %s (git: %s) build_date: %s
"Influx CLI %s (git: %s) build_date: %s\n"cobra.test"cobra.test"See '%s -h' for help
"See '%s -h' for help\n"InfluxDirDefaultConfigsFileDefaultTokenFilecredentialsTrimSpaceOnboardClientServicethe instance at %q has not been setup. Please run `influx setup` before issuing any additional commands"the instance at %q has not been setup. Please run `influx setup` before issuing any additional commands"Error: %s
"Error: %s\n"The ID of the organization"The ID of the organization"The name of the organization"The name of the organization"invalid org ID provided: %s"invalid org ID provided: %s"failed to locate organization criteria"failed to locate organization criteria"must specify org-id, or org name"must specify org-id, or org name"must specify org-id, or org name not both"must specify org-id, or org name not both"%s; Maps to env var $INFLUX_%s"%s; Maps to env var $INFLUX_%s""_"BindOptionshide-headers"hide-headers"HIDE_HEADERS"HIDE_HEADERS"Hide the table headers; defaults false"Hide the table headers; defaults false""json"OUTPUT_JSON"OUTPUT_JSON"Output data as json; defaults false"Output data as json; defaults false"INFLUX"INFLUX" This is useful for forcing tracing on a given endpoint. this is unrecoverable enforce that viper options only ever get set once migration credential token this is after the flagOpts register b/c we don't want to show the default value in the usage display. This will add it as the config, then if a token flag is provided too, the flag will take precedence. we have some indirection here b/c of how the Config is embedded on the global flags type. For the time being, we check to see if there was a value set on flags registered (via env vars), and override the host/token values if they are. Update help description for all commands in command tree completion command goes last, after the walk, so that all commands have every flag listed in the bash|zsh completions. Workaround FAIL with "go test -v" or "cobra.test -test.v", see #155 return nil if any errsreturn here, since cobra already handles the error no need for migration ignore the remove err walk calls f for c and all of its children. last check is for the org set in the CLI config. This will be last in priority./Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/organization.goerrCsemtwursCOrganization management commands"Organization management commands"Create organization"Create organization"The name of organization that will be created"The name of organization that will be created"The description of the organization that will be created"The description of the organization that will be created"failed to initialize org service client: %v"failed to initialize org service client: %v"failed to create organization: %v"failed to create organization: %v"Delete organization"Delete organization"'i'ORG_ID"ORG_ID"The organization ID"The organization ID"failed to decode org id %s: %v"failed to decode org id %s: %v"failed to find org with id %q: %v"failed to find org with id %q: %v"failed to delete org with id %q: %v"failed to delete org with id %q: %v"List organizations"List organizations"ORG"ORG"The organization name"The organization name"failed find orgs: %v"failed find orgs: %v"Update organization"Update organization"The organization ID (required)"The organization ID (required)"'d'ORG_DESCRIPTION"ORG_DESCRIPTION"failed to update org: %v"failed to update org: %v"members"members"Organization membership commands"Organization membership commands"List organization members"List organization members"must specify exactly one of id and name"must specify exactly one of id and name"failed to find org: %v"failed to find org: %v""add"Add organization member"Add organization member""m"The member ID"The member ID"failed to decode member id %s: %v"failed to decode member id %s: %v"Remove organization member"Remove organization member"failed to find organization: %v"failed to find organization: %v"UserResourceMappingClientSpecificURMSvcUserClientServiceFindMefailed to find members: %v"failed to find members: %v"failed to retrieve user details: %v"failed to retrieve user details: %v"Timeout retrieving user details"Timeout retrieving user details"User Type"User Type""Status"failed to add member: %v"failed to add member: %v"user %s has been added as a %s of %s: %s
"user %s has been added as a %s of %s: %s\n"failed to remove member: %v"failed to remove member: %v"userID %s has been removed from ResourceID %s
"userID %s has been removed from ResourceID %s\n"/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/ping.gohealthResponsegithub.com/influxdata/influxdb/v2/kit/check"github.com/influxdata/influxdb/v2/kit/check"/health"/health"got %d from '%s'"got %d from '%s'"ResponsesHasCheckStatusPasspass"OK"health check failed: '%s'"health check failed: '%s'"Check the InfluxDB /health endpoint"Check the InfluxDB /health endpoint"Checks the health of a running InfluxDB instance by querying /health. Does not require valid token.`Checks the health of a running InfluxDB instance by querying /health. Does not require valid token.`/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/query.goojcpykikjihttpgithub.com/influxdata/flux"github.com/influxdata/flux"github.com/influxdata/flux/csv"github.com/influxdata/flux/csv"github.com/influxdata/flux/values"github.com/influxdata/flux/values"query [query literal or -f /path/to/query.flux]"query [query literal or -f /path/to/query.flux]"Execute a Flux query"Execute a Flux query"Execute a Flux query provided via the first argument or a file or stdin`Execute a Flux query provided via the first argument or a file or stdin`MaximumNArgs"file""f"Path to Flux query file"Path to Flux query file""raw"Display raw query results"Display raw query results"failed to load query: %v"failed to load query: %v"unable to parse host: %s"unable to parse host: %s"api/v2/query"api/v2/query"dialect"dialect""group"datatype"datatype"delimiter"delimiter""header"Token "Token "DefaultClientCheckErrorMultiResultDecoderResultDecoderConfigNoHeaderMaxBufferCountNewMultiResultDecoderResultIteratorTableIteratorTablesStatisticsAddAllTotalDurationCompileDurationQueueDurationPlanDurationRequeueDurationExecuteDurationMaxAllocatedTotalAllocatedRuntimeErrorsquery decode error: %s"query decode error: %s"Result:"Result:"2006-01-02T15:04:05.000000000Z"2006-01-02T15:04:05.000000000Z"TBoolTIntTUIntTFloatTStringTTimeTInvalidTable: keys: ["Table: keys: ["'.'' ''-'':'AppendFloat readFluxQuery returns first argument, file contents or stdin backward compatibility It is safe and appropriate to call Release multiple times and must be called before checking the error on the next line. Below is a copy and trimmed version of the execute/format.go file from flux. It is copied here to avoid requiring a dependency on the execute package which may pull in the flux runtime as a dependency. In the future, the formatters and other primitives such as the csv parser should probably be separated out into user libraries anyway. formatter writes a table to a Writer. fmtBuf is used to format values newFormatter creates a formatter for a given table. WriteTo writes the formatted table data to w. Sort cols Compute header widths Column header is "<label>:<type>" Write table header Check err and return early Write rowsTODO make unicode friendly TODO allow specifying format and precision orderedCols sorts a list of columns: * time * common tags sorted by label * other tags sorted by label * value/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/restore.gonewOrgnewIDnewBucketgrnewShardIDmanifestsrestore"restore""full"Fully restore and replace all data on server"Fully restore and replace all data on server"The ID of the bucket to restore"The ID of the bucket to restore"The name of the bucket to restore"The name of the bucket to restore"new-bucket"new-bucket"The name of the bucket to restore to"The name of the bucket to restore to"new-org"new-org"The name of the organization to restore to"The name of the organization to restore to""input"Local backup data path (required)"Local backup data path (required)"restore [flags] path"restore [flags] path"must specify path to backup directory"must specify path to backup directory"Restores a backup directory to InfluxDB."Restores a backup directory to InfluxDB."
Restore influxdb.

Examples:
	# restore all data
	influx restore /path/to/restore
`
Restore influxdb.

Examples:
	# restore all data
	influx restore /path/to/restore
`must specify source org id or name when renaming restored org"must specify source org id or name when renaming restored org"must specify source bucket id or name when renaming restored bucket"must specify source bucket id or name when renaming restored bucket"restore failed while processing manifest files: %s"restore failed while processing manifest files: %s"no manifest files found in: %s"no manifest files found in: %s"Full metadata restored."Full metadata restored."Restore complete"Restore complete"Restoring organization"Restoring organization"cannot create organization: %w"cannot create organization: %w"cannot find existing organization: %#v"cannot find existing organization: %#v"Restoring bucket"Restoring bucket"cannot create bucket: %w"cannot create bucket: %w"cannot marshal database info: %w"cannot marshal database info: %w"cannot restore bucket: %w"cannot restore bucket: %w"Meta info not found, skipping file"Meta info not found, skipping file"shard"shard"bucket_id"bucket_id""filename"Restoring shard live from backup"Restoring shard live from backup"decompressormultistreamMultistreamreadHeaderGlob*.manifest"*.manifest"Reverseread manifest: %v"read manifest: %v" Ensure org/bucket filters are set if a new org/bucket name is specified. Read in set of KV data & shard data to restore. restoreFull completely replaces the bolt metadata file and restores all shard data. Restore each shard for the bucket. restorePartial restores shard data to a server without deleting existing data. Organizations & buckets are created as needed. Cannot overwrite an existing bucket. Filter through organizations & buckets to restore appropriate shards. Restore matching organizations. Create organization on server, if it doesn't already exist. match on backup's org ID Retrieve a list of all buckets for the organization in the local backup. Restore each matching bucket. Skip internal buckets. Create bucket on server. Search using bucket ID from backup. Serialize to protobufs. Skip if shard metadata was not imported. loadIncremental loads multiple manifest files from a given directory. Read all manifest files from path, sort in descending time. Skip file if it is a directory. Read manifest file for backup. Save latest KV entry. Load most recent backup per shard./Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/secret.gogetSecretFnscrSVCsecretValplatformSecretsisecretgithub.com/influxdata/influxdb/v2/secret"github.com/influxdata/influxdb/v2/secret"github.com/tcnksm/go-input"github.com/tcnksm/go-input""secret"Secret management commands"Secret management commands"Update secret"Update secret""key""k"The secret key (required)"The secret key (required)""value""v"Optional secret value for scripting convenience, using this might expose the secret to your local history"Optional secret value for scripting convenience, using this might expose the secret to your local history"failed to update secret with key %q: %v"failed to update secret with key %q: %v"Delete secret"Delete secret"failed to delete secret with key %q: %v"failed to delete secret with key %q: %v"List secrets"List secrets"failed to retrieve secret keys: %s"failed to retrieve secret keys: %s""Key"GetSecret/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/setup.godPathexistingConfigslocalConfigSVCrpStrconfirmedinternal2"setup"Setup instance with initial user, org, bucket"Setup instance with initial user, org, bucket"primary username"primary username""password"password for username"password for username"token for username, else auto-generated"token for username, else auto-generated"primary organization name"primary organization name"primary bucket name"primary bucket name"config name, only required if you already have existing configs"config name, only required if you already have existing configs""force"skip confirmation prompt"skip confirmation prompt"Setup instance with user, org, bucket"Setup instance with user, org, bucket"failed to retrieve data to setup instance: %v"failed to retrieve data to setup instance: %v"failed to setup instance: %v"failed to setup instance: %v""User""Organization""Bucket"a valid configurations path must be provided"a valid configurations path must be provided"failed to determine if instance has been configured: %v"failed to determine if instance has been configured: %v"instance at %q has already been setup"instance at %q has already been setup"flag name is required if you already have existing configs"flag name is required if you already have existing configs"config name %q already existed"config name %q already existed"failed to write config to path %q: %v"failed to write config to path %q: %v"PromptWithColorConfig %s has been stored in %s."Config %s has been stored in %s."ColorCyanMinPasswordLenErrPasswordIsTooShortWelcome to InfluxDB 2.0!`Welcome to InfluxDB 2.0!`ColorYellowGetInputPlease type your primary username"Please type your primary username"Please type your primary organization name"Please type your primary organization name"Please type your primary bucket name"Please type your primary bucket name"Please type your retention period in hours.
Or press ENTER for infinite."Please type your retention period in hours.\r\nOr press ENTER for infinite."GetConfirminfinite"infinite"%d hrs"%d hrs"
You have entered:
  Username:          %s
  Organization:      %s
  Bucket:            %s
  Retention Period:  %s
`
You have entered:
  Username:          %s
  Organization:      %s
  Bucket:            %s
  Retention Period:  %s
`setup was canceled"setup was canceled" check if setup is allowed ignore the error if found nothing else auto-generated by service/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/task.gofinishedAtrequestedAtstartedAtrunsnewRun"task"Task management commands"Task management commands"create [script literal or -f /path/to/script.flux]"create [script literal or -f /path/to/script.flux]"Create task"Create task"Create a task with a Flux script provided via the first argument or a file or stdin`Create a task with a Flux script provided via the first argument or a file or stdin`Path to Flux script file"Path to Flux script file"error parsing flux script: %s"error parsing flux script: %s"error parsing organization ID: %s"error parsing organization ID: %s"List tasks"List tasks"task ID"task ID"task owner ID"task owner ID"the number of tasks to find"the number of tasks to find""headers"To print the table headers; defaults true"To print the table headers; defaults true"limit must be between 1 and %d"limit must be between 1 and %d"Update task"Update task"Update task status or script. Provide a Flux script via the first argument or a file. Use '-' argument to read from stdin.`Update task status or script. Provide a Flux script via the first argument or a file. Use '-' argument to read from stdin.`task ID (required)"task ID (required)"update task status"update task status"Delete task"Delete task"task id (required)"task id (required)""Every""Cron"Log related commands"Log related commands"List logs for task"List logs for task"task-id"task-id"run-id"run-id"run id"run id""RunID""Time""Message""run"List runs for a task"List runs for a task""after"after time for filtering"after time for filtering""before"before time for filtering"before time for filtering"limit the results; default is 100"limit the results; default is 100""TaskID""ScheduledFor""StartedAt""FinishedAt""RequestedAt"retry"retry"retry a run"retry a run"run id (required)"run id (required)"Retry for task %s's run %s queued as run %s.
"Retry for task %s's run %s queued as run %s.\n" update flux script only if first arg or file is supplied guarantee we never return a null value from CLI/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/telegraf.gonewTelegrafteleCfgupdatedCfgbbstdInList Telegraf configuration(s). Subcommands manage Telegraf configurations."List Telegraf configuration(s). Subcommands manage Telegraf configurations."
	List Telegraf configuration(s). Subcommands manage Telegraf configurations.

	Examples:
		# list all known Telegraf configurations
		influx telegrafs

		# list Telegraf configuration corresponding to specific ID
		influx telegrafs --id $ID

		# list Telegraf configuration corresponding to specific ID shorts
		influx telegrafs -i $ID
`
	List Telegraf configuration(s). Subcommands manage Telegraf configurations.

	Examples:
		# list all known Telegraf configurations
		influx telegrafs

		# list Telegraf configuration corresponding to specific ID
		influx telegrafs --id $ID

		# list Telegraf configuration corresponding to specific ID shorts
		influx telegrafs -i $ID
`Telegraf configuration ID to retrieve."Telegraf configuration ID to retrieve."Create a Telegraf configuration"Create a Telegraf configuration"
	The telegrafs create command creates a new Telegraf configuration.

	Examples:
		# create new Telegraf configuration
		influx telegrafs create --name $CFG_NAME --description $CFG_DESC --file $PATH_TO_TELE_CFG

		# create new Telegraf configuration using shorts
		influx telegrafs create -n $CFG_NAME -d $CFG_DESC -f $PATH_TO_TELE_CFG

		# create a new Telegraf config with a config provided via STDIN
		cat $CONFIG_FILE | influx telegrafs create -n $CFG_NAME -d $CFG_DESC
`
	The telegrafs create command creates a new Telegraf configuration.

	Examples:
		# create new Telegraf configuration
		influx telegrafs create --name $CFG_NAME --description $CFG_DESC --file $PATH_TO_TELE_CFG

		# create new Telegraf configuration using shorts
		influx telegrafs create -n $CFG_NAME -d $CFG_DESC -f $PATH_TO_TELE_CFG

		# create a new Telegraf config with a config provided via STDIN
		cat $CONFIG_FILE | influx telegrafs create -n $CFG_NAME -d $CFG_DESC
`rm"rm"Remove Telegraf configuration(s)"Remove Telegraf configuration(s)"
	The telegrafs rm command removes Telegraf configuration(s).

	Examples:
		# remove a single Telegraf configuration
		influx telegrafs rm --id $ID

		# remove multiple Telegraf configurations
		influx telegrafs rm --id $ID1 --id $ID2

		# remove using short flags
		influx telegrafs rm -i $ID1
`
	The telegrafs rm command removes Telegraf configuration(s).

	Examples:
		# remove a single Telegraf configuration
		influx telegrafs rm --id $ID

		# remove multiple Telegraf configurations
		influx telegrafs rm --id $ID1 --id $ID2

		# remove using short flags
		influx telegrafs rm -i $ID1
`Telegraf configuration ID(s) to remove."Telegraf configuration ID(s) to remove."Update a Telegraf configuration"Update a Telegraf configuration"
	The telegrafs update command updates a Telegraf configuration to match the
	specified parameters. If a name or description is not provided, then are set
	to an empty string.

	Examples:
		# update new Telegraf configuration
		influx telegrafs update --id $ID --name $CFG_NAME --description $CFG_DESC --file $PATH_TO_TELE_CFG

		# update new Telegraf configuration using shorts
		influx telegrafs update -i $ID -n $CFG_NAME -d $CFG_DESC -f $PATH_TO_TELE_CFG

		# update a Telegraf config with a config provided via STDIN
		cat $CONFIG_FILE | influx telegrafs update -i $ID  -n $CFG_NAME -d $CFG_DESC
`
	The telegrafs update command updates a Telegraf configuration to match the
	specified parameters. If a name or description is not provided, then are set
	to an empty string.

	Examples:
		# update new Telegraf configuration
		influx telegrafs update --id $ID --name $CFG_NAME --description $CFG_DESC --file $PATH_TO_TELE_CFG

		# update new Telegraf configuration using shorts
		influx telegrafs update -i $ID -n $CFG_NAME -d $CFG_DESC -f $PATH_TO_TELE_CFG

		# update a Telegraf config with a config provided via STDIN
		cat $CONFIG_FILE | influx telegrafs update -i $ID  -n $CFG_NAME -d $CFG_DESC
`Telegraf configuration id to update"Telegraf configuration id to update"Path to Telegraf configuration"Path to Telegraf configuration"Name of Telegraf configuration"Name of Telegraf configuration"Description for Telegraf configuration"Description for Telegraf configuration"a Telegraf config must be provided"a Telegraf config must be provided"TelegrafServiceNewTelegrafService/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/template.gopromptenvRefsecretKeyskipDefaultconfirmactionOptsdryRunImpactimpactisForcedisTTYprovidedEnvRefsprovidedSecretsmetaNamepairrawActnewOptstdinInptstdinresKindresTypeOptresTypestemplateSVCorgOptresourceKindsfakeUserIDstacksprintStackidRawfailedStringoutPathfffilePathsmFilesrawTemplatesmURLsrawURLremotesstdinTemplateurlTemplatestrimmedstdinInputurlBaseidStrsresourcesnormedArgerrscommonHeadersprinternewRowoldRowappendValuesdashestelestimingargTypediffdiffPrinterGentablePrintFnrowFnlatestcolorHeaderhasBorderhasColorheaderColorsslcaddColorshasDifflenAddlenRemovepreppedAddpreppedRemoveremoveColorsfooterColorheaderColoralignmentsdescrColhasTableBordersqValrFilesfilePassigndirFilesfilePathpieceskvPairshaystackneedleierrortablewritergithub.com/fatih/color"github.com/fatih/color"github.com/influxdata/influxdb/v2/kit/errors"github.com/influxdata/influxdb/v2/kit/errors"github.com/olekukonko/tablewriter"github.com/olekukonko/tablewriter""apply"Apply a template to manage resources"Apply a template to manage resources"
	The apply command applies InfluxDB template(s). Use the command to create new
	resources via a declarative template. The apply command can consume templates
	via file(s), url(s), stdin, or any combination of the 3. Each run of the apply
	command ensures that all templates applied are applied in unison as a transaction.
	If any unexpected errors are discovered then all side effects are rolled back.

	Examples:
		# Apply a template via a file
		influx apply -f $PATH_TO_TEMPLATE/template.json

		# Apply a stack that has associated templates. In this example the stack has a remote
		# template associated with it.
		influx apply --stack-id $STACK_ID

		# Apply a template associated with a stack. Stacks make template application idempotent.
		influx apply -f $PATH_TO_TEMPLATE/template.json --stack-id $STACK_ID

		# Apply multiple template files together (mix of yaml and json)
		influx apply \
			-f $PATH_TO_TEMPLATE/template_1.json \
			-f $PATH_TO_TEMPLATE/template_2.yml

		# Apply a template from a url
		influx apply -f https://raw.githubusercontent.com/influxdata/community-templates/master/docker/docker.yml

		# Apply a template from STDIN
		cat $TEMPLATE.json | influx apply --encoding json

		# Applying a directory of templates, takes everything from provided directory
		influx apply -f $PATH_TO_TEMPLATE_DIR

		# Applying a directory of templates, recursively descending into child directories
		influx apply -R -f $PATH_TO_TEMPLATE_DIR

		# Applying directories from many sources, file and URL
		influx apply -f $PATH_TO_TEMPLATE/template.yml -f $URL_TO_TEMPLATE

		# Applying a template with actions to skip resources applied. The
		# following example skips all buckets and the dashboard whose 
		# metadata.name field matches the provided $DASHBOARD_TMPL_NAME.
		# format for filters:
		#	--filter=kind=Bucket
		#	--filter=resource=Label:$Label_TMPL_NAME
		influx apply \
			-f $PATH_TO_TEMPLATE/template.yml \
			--filter kind=Bucket \
			--filter resource=Dashboard:$DASHBOARD_TMPL_NAME

	For information about finding and using InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/apply/.

	For more templates created by the community, see
	https://github.com/influxdata/community-templates.
`
	The apply command applies InfluxDB template(s). Use the command to create new
	resources via a declarative template. The apply command can consume templates
	via file(s), url(s), stdin, or any combination of the 3. Each run of the apply
	command ensures that all templates applied are applied in unison as a transaction.
	If any unexpected errors are discovered then all side effects are rolled back.

	Examples:
		# Apply a template via a file
		influx apply -f $PATH_TO_TEMPLATE/template.json

		# Apply a stack that has associated templates. In this example the stack has a remote
		# template associated with it.
		influx apply --stack-id $STACK_ID

		# Apply a template associated with a stack. Stacks make template application idempotent.
		influx apply -f $PATH_TO_TEMPLATE/template.json --stack-id $STACK_ID

		# Apply multiple template files together (mix of yaml and json)
		influx apply \
			-f $PATH_TO_TEMPLATE/template_1.json \
			-f $PATH_TO_TEMPLATE/template_2.yml

		# Apply a template from a url
		influx apply -f https://raw.githubusercontent.com/influxdata/community-templates/master/docker/docker.yml

		# Apply a template from STDIN
		cat $TEMPLATE.json | influx apply --encoding json

		# Applying a directory of templates, takes everything from provided directory
		influx apply -f $PATH_TO_TEMPLATE_DIR

		# Applying a directory of templates, recursively descending into child directories
		influx apply -R -f $PATH_TO_TEMPLATE_DIR

		# Applying directories from many sources, file and URL
		influx apply -f $PATH_TO_TEMPLATE/template.yml -f $URL_TO_TEMPLATE

		# Applying a template with actions to skip resources applied. The
		# following example skips all buckets and the dashboard whose 
		# metadata.name field matches the provided $DASHBOARD_TMPL_NAME.
		# format for filters:
		#	--filter=kind=Bucket
		#	--filter=resource=Label:$Label_TMPL_NAME
		influx apply \
			-f $PATH_TO_TEMPLATE/template.yml \
			--filter kind=Bucket \
			--filter resource=Dashboard:$DASHBOARD_TMPL_NAME

	For information about finding and using InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/apply/.

	For more templates created by the community, see
	https://github.com/influxdata/community-templates.
`"quiet"Disable output printing"Disable output printing"TTY input, if template will have destructive changes, proceed if set "true"`TTY input, if template will have destructive changes, proceed if set "true"`stack-id"stack-id"Stack ID to associate template application"Stack ID to associate template application"Secrets to provide alongside the template; format should --secret=SECRET_KEY=SECRET_VALUE --secret=SECRET_KEY_2=SECRET_VALUE_2"Secrets to provide alongside the template; format should --secret=SECRET_KEY=SECRET_VALUE --secret=SECRET_KEY_2=SECRET_VALUE_2"env-ref"env-ref"Environment references to provide alongside the template; format should --env-ref=REF_KEY=REF_VALUE --env-ref=REF_KEY_2=REF_VALUE_2"Environment references to provide alongside the template; format should --env-ref=REF_KEY=REF_VALUE --env-ref=REF_KEY_2=REF_VALUE_2""filter"Resources to skip when applying the template. Filter out by â€˜kindâ€™ or by â€˜resourceâ€™"Resources to skip when applying the template. Filter out by â€˜kindâ€™ or by â€˜resourceâ€™"NoColorPlease provide environment reference value for key "Please provide environment reference value for key "ApplyWithTemplateApplyWithEnvRefsApplyWithStackID$$skip-this-key$$"$$skip-this-key$$"Please provide secret value for key "Please provide secret value for key " (optional, press enter to skip)" (optional, press enter to skip)"ParseBool"conflict"Confirm application of the above resources (y/n)"Confirm application of the above resources (y/n)"aborted application of template"aborted application of template"template has conflicts with existing resources and cannot safely apply"template has conflicts with existing resources and cannot safely apply"ApplyWithSecrets"kind"ActionSkipKindApplyWithKindSkip"resource"invalid skipResource action provided: %q; 
	Expected format --action=skipResource=Label:$LABEL_ID`invalid skipResource action provided: %q; 
	Expected format --action=skipResource=Label:$LABEL_ID`ApplyWithResourceSkipinvalid action provided: %q; 
	Expected format --action=skipResource=Label:$LABEL_ID
	or
	Expected format --action=skipKind=Bucket`invalid action provided: %q; 
	Expected format --action=skipResource=Label:$LABEL_ID
	or
	Expected format --action=skipKind=Bucket`export"export"Export existing resources as a template"Export existing resources as a template"
	The export command provides a mechanism to export existing resources to a
	template. Each template resource kind is supported via flags.

	Examples:
		# export buckets by ID
		influx export --buckets=$ID1,$ID2,$ID3

		# export buckets, labels, and dashboards by ID
		influx export \
			--buckets=$BID1,$BID2,$BID3 \
			--labels=$LID1,$LID2,$LID3 \
			--dashboards=$DID1,$DID2,$DID3

		# export all resources for a stack
		influx export --stack-id $STACK_ID

		# export a stack with resources not associated with the stack
		influx export --stack-id $STACK_ID --buckets $BUCKET_ID

	All of the resources are supported via the examples provided above. Provide the
	resource flag and then provide the IDs.

	For information about exporting InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/export/
`
	The export command provides a mechanism to export existing resources to a
	template. Each template resource kind is supported via flags.

	Examples:
		# export buckets by ID
		influx export --buckets=$ID1,$ID2,$ID3

		# export buckets, labels, and dashboards by ID
		influx export \
			--buckets=$BID1,$BID2,$BID3 \
			--labels=$LID1,$LID2,$LID3 \
			--dashboards=$DID1,$DID2,$DID3

		# export all resources for a stack
		influx export --stack-id $STACK_ID

		# export a stack with resources not associated with the stack
		influx export --stack-id $STACK_ID --buckets $BUCKET_ID

	All of the resources are supported via the examples provided above. Provide the
	resource flag and then provide the IDs.

	For information about exporting InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/export/
`Output file for created template; defaults to std out if no file provided; the extension of provided file (.yml/.json) will dictate encoding"Output file for created template; defaults to std out if no file provided; the extension of provided file (.yml/.json) will dictate encoding"ID for stack to include in export"ID for stack to include in export"resource-type"resource-type"Resource type provided will be associated with all IDs via stdin."Resource type provided will be associated with all IDs via stdin."List of bucket ids comma separated"List of bucket ids comma separated"List of check ids comma separated"List of check ids comma separated"List of dashboard ids comma separated"List of dashboard ids comma separated""endpoints"List of notification endpoint ids comma separated"List of notification endpoint ids comma separated"List of label ids comma separated"List of label ids comma separated""rules"List of notification rule ids comma separated"List of notification rule ids comma separated"List of task ids comma separated"List of task ids comma separated"telegraf-configs"telegraf-configs"List of telegraf config ids comma separated"List of telegraf config ids comma separated"List of variable ids comma separated"List of variable ids comma separated"bucket-names"bucket-names"List of bucket names comma separated"List of bucket names comma separated"check-names"check-names"List of check names comma separated"List of check names comma separated"dashboard-names"dashboard-names"List of dashboard names comma separated"List of dashboard names comma separated"endpoint-names"endpoint-names"List of notification endpoint names comma separated"List of notification endpoint names comma separated"label-names"label-names"List of label names comma separated"List of label names comma separated"rule-names"rule-names"List of notification rule names comma separated"List of notification rule names comma separated"task-names"task-names"List of task names comma separated"List of task names comma separated"telegraf-config-names"telegraf-config-names"List of telegraf config names comma separated"List of telegraf config names comma separated"variable-names"variable-names"List of variable names comma separated"List of variable names comma separated"KindBucketKindCheckKindDashboardKindLabelKindNotificationEndpointKindNotificationRuleKindTaskKindTelegrafKindVariableWrapinvalid stack ID provided"invalid stack ID provided"ExportWithStackIDresource type is invalid; got: "resource type is invalid; got: "Export all existing resources for an organization as a template"Export all existing resources for an organization as a template"
	The export all command will export all resources for an organization. The
	command also provides a mechanism to filter by label name or resource kind.

	Examples:
		# Export all resources for an organization
		influx export all --org $ORG_NAME

		# Export all bucket resources
		influx export all --org $ORG_NAME --filter=kind=Bucket

		# Export all resources associated with label Foo
		influx export all --org $ORG_NAME --filter=labelName=Foo

		# Export all bucket resources and filter by label Foo
		influx export all --org $ORG_NAME \
			--filter=kind=Bucket \
			--filter=labelName=Foo

		# Export all bucket or dashboard resources and filter by label Foo.
		# note: like filters are unioned and filter types are intersections.
		#		This example will export a resource if it is a dashboard or
		#		bucket and has an associated label of Foo.
		influx export all --org $ORG_NAME \
			--filter=kind=Bucket \
			--filter=kind=Dashboard \
			--filter=labelName=Foo

	For information about exporting InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/export
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/export/all
`
	The export all command will export all resources for an organization. The
	command also provides a mechanism to filter by label name or resource kind.

	Examples:
		# Export all resources for an organization
		influx export all --org $ORG_NAME

		# Export all bucket resources
		influx export all --org $ORG_NAME --filter=kind=Bucket

		# Export all resources associated with label Foo
		influx export all --org $ORG_NAME --filter=labelName=Foo

		# Export all bucket resources and filter by label Foo
		influx export all --org $ORG_NAME \
			--filter=kind=Bucket \
			--filter=labelName=Foo

		# Export all bucket or dashboard resources and filter by label Foo.
		# note: like filters are unioned and filter types are intersections.
		#		This example will export a resource if it is a dashboard or
		#		bucket and has an associated label of Foo.
		influx export all --org $ORG_NAME \
			--filter=kind=Bucket \
			--filter=kind=Dashboard \
			--filter=labelName=Foo

	For information about exporting InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/export
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/export/all
`output file for created template; defaults to std out if no file provided; the extension of provided file (.yml/.json) will dictate encoding"output file for created template; defaults to std out if no file provided; the extension of provided file (.yml/.json) will dictate encoding"Filter exported resources by labelName or resourceKind (format: --filter=labelName=example)"Filter exported resources by labelName or resourceKind (format: --filter=labelName=example)"labelName"labelName"resourceKind"resourceKind"invalid filter provided %q; filter must be 1 in [labelName, resourceKind]"invalid filter provided %q; filter must be 1 in [labelName, resourceKind]"ExportWithAllOrgResourcesstack $STACK_ID"stack $STACK_ID"Export all existing resources for associated with a stack as a template"Export all existing resources for associated with a stack as a template"
	The export stack command exports the resources associated with a stack as
	they currently exist in the platform. All the same metadata.name fields will be
	reused.

	Example:
		# Export by a stack
		influx export stack $STACK_ID

	For information about exporting InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/export
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/export/stack/
`
	The export stack command exports the resources associated with a stack as
	they currently exist in the platform. All the same metadata.name fields will be
	reused.

	Example:
		# Export by a stack
		influx export stack $STACK_ID

	For information about exporting InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/export
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/export/stack/
`"template"Summarize the provided template"Summarize the provided template""validate"Validate the provided template"Validate the provided template"stacks [flags]"stacks [flags]"Stack ID to filter by"Stack ID to filter by"stack-name"stack-name"Stack name to filter by"Stack name to filter by"List stack(s) and associated templates. Subcommands manage stacks."List stack(s) and associated templates. Subcommands manage stacks."
	List stack(s) and associated templates. Subcommands manage stacks.

	Examples:
		# list all known stacks
		influx stacks

		# list stacks filtered by stack name
		# output here are stacks that have match at least 1 name provided
		influx stacks --stack-name=$STACK_NAME_1 --stack-name=$STACK_NAME_2

		# list stacks filtered by stack id
		# output here are stacks that have match at least 1 ids provided
		influx stacks --stack-id=$STACK_ID_1 --stack-id=$STACK_ID_2
		
		# list stacks filtered by stack id or stack name
		# output here are stacks that have match the id provided or
		# matches of the name provided
		influx stacks --stack-id=$STACK_ID --stack-name=$STACK_NAME

	For information about Stacks and how they integrate with InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/stacks
`
	List stack(s) and associated templates. Subcommands manage stacks.

	Examples:
		# list all known stacks
		influx stacks

		# list stacks filtered by stack name
		# output here are stacks that have match at least 1 name provided
		influx stacks --stack-name=$STACK_NAME_1 --stack-name=$STACK_NAME_2

		# list stacks filtered by stack id
		# output here are stacks that have match at least 1 ids provided
		influx stacks --stack-id=$STACK_ID_1 --stack-id=$STACK_ID_2
		
		# list stacks filtered by stack id or stack name
		# output here are stacks that have match the id provided or
		# matches of the name provided
		influx stacks --stack-id=$STACK_ID --stack-name=$STACK_NAME

	For information about Stacks and how they integrate with InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/stacks
`"init"Initialize a stack"Initialize a stack"
	The stack init command creates a new stack to associated templates with. A
	stack is used to make applying templates idempotent. When you apply a template
	and associate it with a stack, the stack can manage the created/updated resources
	from the template back to the platform. This enables a multitude of useful features.
	Any associated template urls will be applied when applying templates via a stack.

	Examples:
		# Initialize a stack with a name and description
		influx stacks init -n $STACK_NAME -d $STACK_DESCRIPTION

		# Initialize a stack with a name and urls to associate with stack.
		influx stacks init -n $STACK_NAME -u $PATH_TO_TEMPLATE

	For information about how stacks work with InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/stacks/
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/stacks/init/
`
	The stack init command creates a new stack to associated templates with. A
	stack is used to make applying templates idempotent. When you apply a template
	and associate it with a stack, the stack can manage the created/updated resources
	from the template back to the platform. This enables a multitude of useful features.
	Any associated template urls will be applied when applying templates via a stack.

	Examples:
		# Initialize a stack with a name and description
		influx stacks init -n $STACK_NAME -d $STACK_DESCRIPTION

		# Initialize a stack with a name and urls to associate with stack.
		influx stacks init -n $STACK_NAME -u $PATH_TO_TEMPLATE

	For information about how stacks work with InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/stacks/
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/stacks/init/
`Name given to created stack"Name given to created stack"stack-description"stack-description"Description given to created stack"Description given to created stack"template-url"template-url"Template urls to associate with new stack"Template urls to associate with new stack"rm [--stack-id=ID1 --stack-id=ID2]"rm [--stack-id=ID1 --stack-id=ID2]"Remove a stack(s) and all associated resources"Remove a stack(s) and all associated resources"uninstall"uninstall"Stack IDs to be removed"Stack IDs to be removed"Remove stack without confirmation prompt"Remove stack without confirmation prompt"Confirm removal of the stack[%s] and all associated resources (y/n)"Confirm removal of the stack[%s] and all associated resources (y/n)"Update a stack"Update a stack"
	The stack update command updates a stack.

	Examples:
		# Update a stack with a name and description
		influx stacks update -i $STACK_ID -n $STACK_NAME -d $STACK_DESCRIPTION

		# Update a stack with a name and urls to associate with stack.
		influx stacks update --stack-id $STACK_ID --stack-name $STACK_NAME --template-url $PATH_TO_TEMPLATE

		# Update stack with new resources to manage
		influx stacks update \
			--stack-id $STACK_ID \
			--addResource=Bucket=$BUCKET_ID \
			--addResource=Dashboard=$DASH_ID

		# Update stack with new resources to manage and export stack
		# as a template
		influx stacks update \
			--stack-id $STACK_ID \
			--addResource=Bucket=$BUCKET_ID \
			--export-file /path/to/file.yml

	For information about how stacks work with InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/stacks
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/stacks/update/
`
	The stack update command updates a stack.

	Examples:
		# Update a stack with a name and description
		influx stacks update -i $STACK_ID -n $STACK_NAME -d $STACK_DESCRIPTION

		# Update a stack with a name and urls to associate with stack.
		influx stacks update --stack-id $STACK_ID --stack-name $STACK_NAME --template-url $PATH_TO_TEMPLATE

		# Update stack with new resources to manage
		influx stacks update \
			--stack-id $STACK_ID \
			--addResource=Bucket=$BUCKET_ID \
			--addResource=Dashboard=$DASH_ID

		# Update stack with new resources to manage and export stack
		# as a template
		influx stacks update \
			--stack-id $STACK_ID \
			--addResource=Bucket=$BUCKET_ID \
			--export-file /path/to/file.yml

	For information about how stacks work with InfluxDB templates, see
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/stacks
	and
	https://v2.docs.influxdata.com/v2.0/reference/cli/influx/stacks/update/
`ID of stack"ID of stack"Name for stack"Name for stack"Description for stack"Description for stack"Template urls to associate with stack"Template urls to associate with stack"addResource"addResource"Additional resources to associate with stack"Additional resources to associate with stack"export-file"export-file"Destination for exported template"Destination for exported template"required stack id is invalid"required stack id is invalid"%s resource id %q is invalid"%s resource id %q is invalid"influxdata.com/v2alpha1invalid 'resourceType=resourceID' key value pair[s]: "invalid 'resourceType=resourceID' key value pair[s]: ";";"
Your stack now differs from your template. Applying an outdated template will revert
these updates. Export a new template with these updates to prevent accidental changes? (y/n)`
Your stack now differs from your template. Applying an outdated template will revert
these updates. Export a new template with these updates to prevent accidental changes? (y/n)`disable-color"disable-color"Disable color in output"Disable color in output"disable-table-borders"disable-table-borders"Disable table borders"Disable table borders"Path to template file; Supports HTTP(S) URLs or file paths."Path to template file; Supports HTTP(S) URLs or file paths."yaml"yaml"yml"yml"jsonnet"jsonnet""recurse"R"R"Process the directory used in -f, --file recursively. Useful when you want to manage related templates organized within the same directory."Process the directory used in -f, --file recursively. Useful when you want to manage related templates organized within the same directory."URL to template file"URL to template file"use the --file flag; example: influx apply --file $URL_TO_TEMPLATE"use the --file flag; example: influx apply --file $URL_TO_TEMPLATE""encoding""e"Encoding for the input stream. If a file is provided will gather encoding type from file extension. If extension provided will override."Encoding for the input stream. If a file is provided will gather encoding type from file extension. If extension provided will override."ReaderFnFromFileValidSkipParseErrorFromHTTPRequestfailed to parse url[%s]"failed to parse url[%s]"CombineFromReader.jsonnet".jsonnet"EncodingJsonnetEncodingJSON.yml".yml".yaml".yaml"EncodingSourceExportWithExistingResourcesarg must provide a valid 16 length ID; got: "arg must provide a valid 16 length ID; got: "
	"\n\t"HTTPRemoteServiceMetadata Name"Metadata Name"Resource Name"Resource Name""Labels""Color"IsRemoval"Buckets"Retention Period"Retention Period""Checks""Dashboards"Num Charts"Num Charts"Notification Endpoints"Notification Endpoints"Notification Rules"Notification Rules""Offset"Endpoint Name"Endpoint Name"Endpoint ID"Endpoint ID"Endpoint Type"Endpoint Type"Telegraf Configurations"Telegraf Configurations""Tasks"Cycle"Cycle"every: %s offset: %s"every: %s offset: %s""Variables"Arg Type"Arg Type"Arg Values"Arg Values"Label Associations"Label Associations"Resource Type"Resource Type"Resource Meta Name"Resource Meta Name"Resource ID"Resource ID"Label Package Name"Label Package Name"Label Name"Label Name"Label ID"Label ID"json:"stackID"`json:"stackID"`Package Name"Package Name"LABELS"LABELS"BUCKETS"BUCKETS"CHECKS"CHECKS"DASHBOARDS"DASHBOARDS"NOTIFICATION ENDPOINTS"NOTIFICATION ENDPOINTS"NOTIFICATION RULES"NOTIFICATION RULES"TASKS"TASKS"TELEGRAF CONFIGS"TELEGRAF CONFIGS"VARIABLES"VARIABLES"LABEL ASSOCIATIONS"LABEL ASSOCIATIONS"Secret Key"Secret Key"MISSING SECRETS"MISSING SECRETS"Stack ID: "Stack ID: "Num Resources"Num Resources""URLs"Created At"Created At"Updated At"Updated At"StackEventUninstalledFgHiGreenColorBoldFgHiBlueColorFgCyanColorFgRedColorAttributenoColorunsetsetWriterunsetWriterFprintSprintlnFprintFuncPrintFuncFprintfFuncPrintfFuncFprintlnFuncPrintlnFuncSprintFuncSprintfFuncSprintlnFuncunformatDisableColorEnableColorisNoColorSetattrExistsFgYellowFgHiGreen+add"+add"FgRed-remove"-remove"%s    %s | %s | unchanged
"%s    %s | %s | unchanged\n"+/-"+/-"TOTAL"TOTAL"TOTAL: "TOTAL: "+"+"ALIGN_CENTERALIGN_LEFTFgHiCyanColor<nil>"<nil>"{}"{}"[]"[]"%q"%q"[%s]"[%s]"language=%q query=%q"language=%q query=%q"unknown variable argument"unknown variable argument"inf"inf"KindUnknownKindsEqualFoldinput not stdIn"input not stdIn"ModeCharDevice2097152 is 0 because user is pulled from token... add a breather line between confirm and printout the pkger.ValidSkipParseError option allows our server to be the one to validate the the template is accurate. If a user has an older version of the CLI and cloud gets updated with new validation rules,they'll get immediate access to that change without having to rol their CLI build. set the title and the add/remove legend offset to skip prepended +/- column/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/transpile.gotranspile [InfluxQL query]"transpile [InfluxQL query]"ExactArgsTranspile an InfluxQL query to Flux source code"Transpile an InfluxQL query to Flux source code"Transpile an InfluxQL query to Flux source code.


The transpiled query assumes that the bucket name is the of the form '<database>/<retention policy>'.

The transpiled query will be written for absolute time ranges using the provided now() time.`Transpile an InfluxQL query to Flux source code.


The transpiled query assumes that the bucket name is the of the form '<database>/<retention policy>'.

The transpiled query will be written for absolute time ranges using the provided now() time.`An RFC3339Nano formatted time to use as the now() time. Defaults to the current time"An RFC3339Nano formatted time to use as the now() time. Defaults to the current time"invalid now time"invalid now time"/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/user.godeppassSvcurmSvcUser management commands"User management commands"Update user password"Update user password"Your password has been successfully updated."Your password has been successfully updated."Update user"Update user"The user ID (required)"The user ID (required)"Create user"Create user"The user name (required)"The user name (required)"The user password"The user password"an org id is required when providing a user password"an org id is required when providing a user password"List users"List users"Delete user"Delete user"PasswordClientService/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/v1_authorization.gocinternalgithub.com/influxdata/influxdb/v2/v1/authorization"github.com/influxdata/influxdb/v2/v1/authorization"Authorization management commands for v1 APIs"Authorization management commands for v1 APIs"The username to identify this token"The username to identify this token"no-password"no-password"Don't prompt for a password. You must use v1 auth set-password command before using the token."Don't prompt for a password. You must use v1 auth set-password command before using the token."authorization with username %q exists"authorization with username %q exists"error setting password: %w"error setting password: %w"set-active"set-active"Change the status of an authorization to active"Change the status of an authorization to active"set-inactive"set-inactive"Change the status of an authorization to inactive"Change the status of an authorization to inactive"multiple authorizations found"multiple authorizations found"The ID of the authorization"The ID of the authorization"The username of the authorization"The username of the authorization"specify id or username, not both"specify id or username, not both"id or username required"id or username required"set-password"set-password"Set a password for an existing authorization"Set a password for an existing authorization"Name / Token"Name / Token" verify an existing token with the same username doesn't already exist required when set to true determines whether validate expects either id or username to be set/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/v1_commands.gov1"v1"InfluxDB v1 management commands"InfluxDB v1 management commands"/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/v1_dbrp.godefaultBooldefaultFlgdbrpsdbrpToDeletedbrpUpdatenewDBRPoldDBRPgithub.com/influxdata/influxdb/v2/dbrp"github.com/influxdata/influxdb/v2/dbrp"Commands to manage database and retention policy mappings for v1 APIs"Commands to manage database and retention policy mappings for v1 APIs"List database and retention policy mappings"List database and retention policy mappings"IDVarLimit results to the matching bucket id"Limit results to the matching bucket id"Limit results to a single mapping"Limit results to a single mapping"Limit results to the matching database name"Limit results to the matching database name"Limit results to the matching retention policy name"Limit results to the matching retention policy name"Limit results to default mappings"Limit results to default mappings"Create a database and retention policy mapping to an existing bucket"Create a database and retention policy mapping to an existing bucket"The ID of the bucket to be mapped"The ID of the bucket to be mapped"The name of the database"The name of the database"Identify this retention policy as the default for the database"Identify this retention policy as the default for the database"The name of the retention policy"The name of the retention policy""Database"Bucket ID"Bucket ID"Retention Policy"Retention Policy"Delete a database and retention policy mapping"Delete a database and retention policy mapping"The ID of the mapping to delete"The ID of the mapping to delete"Update a database and retention policy mapping"Update a database and retention policy mapping"The ID of the mapping to be updated"The ID of the mapping to be updated"The updated name of the retention policy"The updated name of the retention policy"Set this mapping's retention policy as the default for the mapping's database"Set this mapping's retention policy as the default for the mapping's database" Specifies the bucket ID to filter on Specifies the database to filter on Specifies filtering on default Specifies the mapping ID to filter on required  // Specifies the organization ID to filter on Specifies the retention policy to filter on Specifies the bucket ID to associate with the mapping Specifies the database of the database Specifies the organization ID to filter on Specifies the mapping ID to update Specifies the organization ID A nil value means that Default is unset in the Flags Updated name of the retention policy note for update we only care about update flags that the user set we do the lookup again, because Update doesn't give us all fields/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx/write.gocmdDryRunstringReaderlineErrorrowcsvReaderthrottledReadercloserserrorsFilerateLimitreadersrowSkippedListenercloserint64ValstrValcsv2lpmodelsshapeioencoding/csv"encoding/csv"github.com/fujiwara/shapeio"github.com/fujiwara/shapeio"github.com/influxdata/influxdb/v2/models"github.com/influxdata/influxdb/v2/models"github.com/influxdata/influxdb/v2/pkg/csv2lp"github.com/influxdata/influxdb/v2/pkg/csv2lp"github.com/influxdata/influxdb/v2/write"github.com/influxdata/influxdb/v2/write""lp"Write points to InfluxDB"Write points to InfluxDB"Write data to InfluxDB via stdin, or add an entire file specified with the -f flag`Write data to InfluxDB via stdin, or add an entire file specified with the -f flag`The ID of destination bucket"The ID of destination bucket"'b'precision"precision"'p'Precision of the timestamps of the lines"Precision of the timestamps of the lines""format"Input format, either lp (Line Protocol) or csv (Comma Separated Values). Defaults to lp unless '.csv' extension"Input format, either lp (Line Protocol) or csv (Comma Separated Values). Defaults to lp unless '.csv' extension"Header prepends lines to input data; Example --header HEADER1 --header HEADER2"Header prepends lines to input data; Example --header HEADER1 --header HEADER2"The path to the file to import"The path to the file to import"The URL to import data from"The URL to import data from"Log CSV columns to stderr before reading data rows"Log CSV columns to stderr before reading data rows"skipRowOnError"skipRowOnError"Log CSV data errors to stderr and continue with CSV processing"Log CSV data errors to stderr and continue with CSV processing"skipHeader"skipHeader"Skip the first <n> rows from input data"Skip the first <n> rows from input data"max-line-length"max-line-length"1600000016_000_000Specifies the maximum number of bytes that can be read for a single line"Specifies the maximum number of bytes that can be read for a single line""1"xIgnoreDataTypeInColumnName"xIgnoreDataTypeInColumnName"Ignores dataType which could be specified after ':' in column name"Ignores dataType which could be specified after ':' in column name"UTF-8"UTF-8"Character encoding of input files or stdin"Character encoding of input files or stdin"errors-file"errors-file"The path to the file to write rejected rows to"The path to the file to write rejected rows to"rate-limit"rate-limit"Throttles write, examples: "5 MB / 5 min" , "17kBs". "" (default) disables throttling."Throttles write, examples: \"5 MB / 5 min\" , \"17kBs\". \"\" (default) disables throttling."dryrun"dryrun"Write to stdout instead of InfluxDB"Write to stdout instead of InfluxDB"Write protocol lines to stdout instead of InfluxDB. Troubleshoot conversion from CSV to line protocol.`Write protocol lines to stdout instead of InfluxDB. Troubleshoot conversion from CSV to line protocol.`WriteFlags%+v args:%v"WriteFlags%+v args:%v"'@'MultiCloserunsupported input format: %s"unsupported input format: %s"CreateDecoderfailed to open %q: %v"failed to open %q: %v".csv".csv"NewRequestWithContextMethodGetfailed to open %q: response status_code=%d"failed to open %q: response status_code=%d"text/csv"text/csv"SkipHeaderLinesReaderCommaUseCRLFWriteAllfieldNeedsQuotesCsvToLineReaderpositioncolFieldsPerRecordLazyQuotesTrimLeadingSpaceReuseRecordTrailingCommanumLinerawBufferrecordBufferfieldIndexesfieldPositionslastRecordFieldPosreadLineLineReaderLineNumberLastLineNumberbufSizelrCsvTableCsvTableColumnDataFormatLinePartTimeZoneParseFComputeValueescapedLabelLineLabelsetupDataTypepartBitsreadTableDatalpColumnsValidextraColumnsignoreDataTypeInColumnNametimeZonevalidatorscachedMeasurementcachedTimecachedFieldNamecachedFieldValuecachedFieldscachedTagsDataColumnsInfoNextTablecomputeLineProtocolColumnsrecomputeLineProtocolColumnsCreateLineAppendLineColumnLabelsFieldValuelineReaderlogTableDataColumnsdataRowAddedRowSkippedlineBufferLogTableColumnsfailed to create %q: %v"failed to create %q: %v"# error : %v"# error : %v"Unable to write to error-file: %v
"Unable to write to error-file: %v\n"MultiReaderCsvToLineProtocol0.0NewLineReaderLimiterdurationFromTokenstokensFromDurationburstlastEventlimBurstAllowAllowNReserveNWaitNSetLimitSetLimitAtSetBurstSetBurstAtreserveNlimiterSetRateLimitNewReaderWithContextValidPrecisioninvalid precision"invalid precision"failed to decode bucket-id: %v"failed to decode bucket-id: %v"failed to decode org-id id: %v"failed to decode org-id id: %v"BatcherMaxFlushBytesMaxFlushIntervalfailed to write data: %v"failed to write data: %v"failed: %v"failed: %v"^(\d*\.?\d*)(B|kB|MB)/?(\d*)?(s|sec|m|min)$`^(\d*\.?\d*)(B|kB|MB)/?(\d*)?(s|sec|m|min)$`"B"kB"kB"1024MB"MB"10485761_048_576"s""sec"invalid rate limit %q: it does not match format COUNT(B|kB|MB)/TIME(s|sec|m|min) with / and TIME being optional, rexpexp: %v"invalid rate limit %q: it does not match format COUNT(B|kB|MB)/TIME(s|sec|m|min) with / and TIME being optional, rexpexp: %v"ParseFloatinvalid rate limit %q: '%v' is not count of bytes: %v"invalid rate limit %q: '%v' is not count of bytes: %v"invalid rate limit %q: time is out of range: %v"invalid rate limit %q: time is out of range: %v"invalid rate limit %q: positive time expected but %v supplied"invalid rate limit %q: positive time expected but %v supplied" skipHeader flag value is optional, skip the first header when unspecified should be used only upon explicit advice createLineReader uses writeFlags and cli arguments to create a reader that produces line protocol backward compatibility: @ in arg denotes a file validate input format validate and setup decoding of files/stdin if encoding is supplied prepend header lines add files #18349 allow URL data sources, a simple alternative to `curl -f -s http://... | influx write ...` add stdin or a single argument use also stdIn if it is a terminal "-" also means stdin skipHeader lines when set find the last non-string reader (stdin or file) ignore headers and new lines create writer for errors-file, if supplied flush is required concatenate readers change LineNumber to report file/stdin line numbers properly throttle reader if requested LineReader ensures that original reader is consumed in the smallest possible units (at most one protocol line) to avoid bigger pauses in throttling print flags when in Debug mode validate InfluxDB flags create line reader write to InfluxDB dry run IsCharacterDevice returns true if the supplied reader is a character device (a terminal) ToBytesPerSecond converts rate from string to number. The supplied string value format must be COUNT(B|kB|MB)/TIME(s|sec|m|min) with / and TIME being optional. All spaces are ignored, they can help with formatting. Examples: "5 MB / 5 min", 17kbs. 5.1MB5m. ignore all spaces number is not specified, for example 5kbs or 1Mb/sReservationtimeToActDelayDelayFromCancelAt/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx_inspect/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx_inspect/buildtsi/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx_inspect/buildtsi/buildtsi.goIndexShardIndexTSMFileNewCommandcollectTSMFilescollectWALFilesdefaultBatchSizeisRootVerboseconcurrencydatabaseFilterretentionFiltershardFiltercompactSeriesFilemaxLogFileSizemaxCacheSizecompactDatabaseSeriesFilecompactSeriesFilePartitionseriesFilePartitionPathsprocessDatabaseprocessRetentionPolicydataDirwalDiranswerfiscompactordbNamepathChpathssfilesfilePathsegmentdstindexPathpartitionIDsegmentPathstmpExtrpNamemaxishardsseriesKeykeysBatchloadernamesBatchtagsBatchtmpPathtsiIndextsmPathsverboseLoggingwalPathstiatomicerrgrouptsdbtsi1tsm1buildtsios/user"os/user"sync/atomic"sync/atomic"github.com/influxdata/influxdb/v2/pkg/file"github.com/influxdata/influxdb/v2/pkg/file"github.com/influxdata/influxdb/v2/tsdb"github.com/influxdata/influxdb/v2/tsdb"github.com/influxdata/influxdb/v2/tsdb/engine/tsm1"github.com/influxdata/influxdb/v2/tsdb/engine/tsm1"github.com/influxdata/influxdb/v2/tsdb/index/tsi1"github.com/influxdata/influxdb/v2/tsdb/index/tsi1"golang.org/x/sync/errgroup"golang.org/x/sync/errgroup"10000NewNopGOMAXPROCSNewFlagSet"buildtsi"ExitOnErrordatadir"datadir"data directory"data directory"waldir"waldir"WAL directory"WAL directory""concurrency"Number of workers to dedicate to shard index building. Defaults to GOMAXPROCS"Number of workers to dedicate to shard index building. Defaults to GOMAXPROCS"optional: database name"optional: database name"optional: retention policy"optional: retention policy"optional: shard id"optional: shard id"compact-series-file"compact-series-file"optional: compact existing series file. Do not rebuilt index."optional: compact existing series file. Do not rebuilt index."max-log-file-size"max-log-file-size"DefaultMaxIndexLogFileSizeoptional: maximum log file size"optional: maximum log file size"max-cache-size"max-cache-size"DefaultCacheMaxMemorySize1073741824optional: maximum cache size"optional: maximum cache size"batch-size"batch-size"optional: set the size of the batches we write to the index. Setting this can have adverse affects on performance and heap requirements"optional: set the size of the batches we write to the index. Setting this can have adverse affects on performance and heap requirements"verbose"verbose"You are currently running as root. This will build your"You are currently running as root. This will build your"index files with root ownership and will be inaccessible"index files with root ownership and will be inaccessible"if you run influxd as a non-root user. You should run"if you run influxd as a non-root user. You should run"buildtsi as the same user you are running influxd."buildtsi as the same user you are running influxd."Are you sure you want to continue? (y/N): "Are you sure you want to continue? (y/N): "Scanlnoperation aborted"operation aborted"cannot specify retention policy when compacting series file"cannot specify retention policy when compacting series file"cannot specify shard ID when compacting series file"cannot specify shard ID when compacting series file"SeriesFileDirectory_serieserrOnceGoSeriesFileSeriesPartitionSeriesSegmentInitForWriteCloseForWriteWriteLogEntryCanWriteAppendSeriesIDsMaxSeriesIDForEachEntryCompactToPathSeriesIndexHashMaphashElemsetKeyrhhTrackerMetricsGaugeVecLoadFactorLastGetDurationInsertDurationLastInsertDurationLastGrowDurationMeanProbeCountGetsPutsPrometheusCollectorsenabledbaseLabelshitIncLabelsmissIncLabelsSetLoadFactorSetSizeObserveGetObservePutSetGrowDurationSetProbeCountincGetIncGetHitIncGetMissincPutIncPutHitIncPutMisshasheselemscapacityloadFactortmpKeytrackerPutQuietAverageProbeCountmaxSeriesIDmaxOffsetkeyIDDataidOffsetDatakeyIDMapidOffsetMaptombstonesRecoverOnDiskCountInMemCountIsDeletedexecEntryFindIDBySeriesKeyFindIDByNameTagsFindIDListByNameTagsFindOffsetByIDFixedIdleCapacityTryTakeTakesegmentscompactingcompactionLimitercompactionsDisabledCompactThresholdopenSegmentsIndexPathSegmentsFileSizeCreateSeriesListIfNotExistsCompactingDeleteSeriesIDSeriesKeySeriesCountDisableCompactionsEnableCompactionscompactionsEnabledactiveSegmentwriteLogEntrycreateSegmentseriesKeyByOffsetpartitionsmaxSnapshotConcurrencyWithMaxCompactionConcurrencySeriesPartitionPathPartitionsSeriesKeysSeriesIDHasSeriesSeriesIDIteratorSeriesIDPartitionIDSeriesIDPartitionSeriesKeysPartitionIDsSeriesKeyPartitionIDSeriesKeyPartitionNewSeriesFileSeriesPartitionCompactorCompactcompactIndexToinsertKeyIDMapinsertIDOffsetMapNewSeriesPartitionCompactorcompacted "compacted "processing partition for %q
"processing partition for %q\n"cannot parse partition id from path: %s"cannot parse partition id from path: %s"NewSeriesPartitioncannot open partition: path=%s err=%s"cannot open partition: path=%s err=%s"processing segment %q %d
"processing segment %q %d\n"renaming new segment %q to %q
"renaming new segment %q to %q\n"RenameFileserious failure. Please rebuild index and series file: %v"serious failure. Please rebuild index and series file: %v"removing index file"removing index file"Rebuilding database"Rebuilding database"Rebuilding retention policy"Rebuilding retention policy"ShardRebuilding shard"Rebuilding shard""index"Checking index path"Checking index path"tsi1 index already exists, skipping"tsi1 index already exists, skipping"Opening shard"Opening shard".index".index"Cleaning up partial index from previous run, if any"Cleaning up partial index from previous run, if any"PartitionLogFileSeriesIDSetBitmaproaringArraycontainercontypeDecodeMsgEncodeMsgMarshalMsgUnmarshalMsgMsgsizemanyIterablenextManyshortIterablehasNextaddOffsetandCardinalitycontainerTypeequalsfillLeastSignificant16bitsgetCardinalitygetManyIteratorgetReverseIteratorgetShortIteratorgetSizeInBytesiaddiaddRangeiaddReturnMinimizediandiandNotinotintersectsioriremoveiremoveRangeiremoveReturnMinimizedisFulllazyIORlazyORmaximumminimumnumberOfRunsorCardinalityrankselectIntserializedSizeInBytestoEfficientContainercontainerSerzcontainersneedCopyOnWritecopyOnWriteconserzrunOptimizeraappendContainerappendWithoutCopyappendCopyappendWithoutCopyManyappendCopyManyappendCopiesUntilappendCopiesAfterremoveIndexRangegetContainergetContainerAtIndexgetFastContainerAtIndexgetWritableContainerAtIndexgetIndexgetKeyAtIndexinsertNewKeyValueAtremoveAtIndexsetContainerAtIndexreplaceKeyAndContainerAtIndexbinarySearchheaderSizefromBufferhasRunCompressionwriteToMsgpackreadFromMsgpackadvanceUntilmarkAllAsNeedingCopyOnWriteneedsCopyOnWritesetNeedsCopyOnWritehighlowcontainerx1repairAfterLazyToBase64rbFromBase64ToBytesWriteToMsgpackFromBufferRunOptimizeHasRunCompressionReadFromMsgpackToArrayGetSerializedSizeInBytesIteratorReverseIteratorManyIteratorMinimumMaximumContainsIntaddwithptrCheckedAddCheckedRemoveIsEmptyGetCardinalityRankOrCardinalityAndCardinalityIntersectsAddManyFlipFlipIntAddRangeRemoveRangeSetCopyOnWriteGetCopyOnWritebitmapAddNoLockContainsNoLockRemoveNoLockCardinalityMergeInPlaceForEachNoLockCloneNoLockUnmarshalBinaryUnsafeClearNoLocklogMeasurementslogMeasurementlogTagKeylogTagValueseriesSettvaddSeriesIDremoveSeriesIDcardinalityseriesIDSettkTagValueIteratorcreateTagValueIfNotExiststagSetforEachseriesIDscreateTagSetIfNotExistsmmsbufferSizenosynckeyBuftombstoneSeriesIDSetopenFlushAndSyncSetPathTombstoneSeriesIDSetMeasurementHasSeriesMeasurementNamesmeasurementNamesDeleteMeasurementTagKeySeriesIDIteratorTagKeyIteratorTagValueDeleteTagKeyTagValueSeriesIDSetMeasurementNTagKeyNTagValueNDeleteTagValueAddSeriesListSeriesNappendEntryexecDeleteMeasurementEntryexecDeleteTagKeyEntryexecDeleteTagValueEntryexecSeriesEntrycreateMeasurementIfNotExistsMeasurementIteratorMeasurementSeriesIDIteratorCompactTowriteTagsetsTowriteTagsetTowriteMeasurementBlockToMeasurementsSketchesmeasurementsSketchesSeriesSketchesseriesSketchesFileSetCompactionLevelMMeasurementElemSeriesIDElemSketchBinaryMarshalerBinaryUnmarshalerTagKeyElemTagValueElemlevelsmanifestSizePrependLogFileMustReplaceMaxIDLogFilesIndexFilesLastContiguousIndexFilesByLevelMeasurementTagKeysByExprtagKeysByFilterHasTagKeyHasTagValueTagValueSeriesIDIteratorMeasurementFieldSetMeasurementFieldsFieldKeysCreateFieldIfNotExistsFieldNHasFieldFieldBytesFieldSetForEachFieldFieldsByStringCreateFieldsIfNotExistsDeleteWithLocksaveNoLockactiveLogFilefileSetlevelCompactingfieldsetcurrentCompactionNMaxLogFileSizelogbufferSizecompactionInterruptopenLogFileopenIndexFiledeleteNonManifestFilesbuildSeriesSetCurrentCompactionNisClosingnextSequenceManifestPathSetFieldSetRetainFileSetretainFileSetFileNprependActiveLogFileForEachMeasurementNameMeasurementExistsMeasurementNamesByRegexDropMeasurementcreateSeriesListIfNotExistsDropSeriesForEachMeasurementTagKeyTagKeyCardinalitySetFieldNameRemoveShardAssignShardcompactcompactToLevelRebuildCheckLogFilecheckLogFilecompactLogFileTagValueSeriesIDCacheevictoraddToSetmeasurementContainsSetscheckEvictiontagValueCachetagValueCacheSizedisableCompactionslogfileBufferSizedisableFsyncmSketchmTSketchsSketchsTSketchPartitionNUniqueReferenceIDPartitionAtpartitionIdxavailableThreadsupdateMeasurementSketchesupdateSeriesSketchesfetchByteValuesCreateSeriesIfNotExistsInitializeSeriesDropSeriesGlobalDropMeasurementIfSeriesNotExistDiskSizeBytesIndexOptionNewIndexWithPathWithMaximumLogFileSizeDisableFsyncWithLogFileBufferSizeOpening tsi index in temporary location"Opening tsi index in temporary location"Iterating over tsm files"Iterating over tsm files"Processing tsm file"Processing tsm file"Building cache from wal files"Building cache from wal files"CachestorerinternalOnlyorderedassertOrderedDeduplicateExcludeIncludeFindRangeInfluxQLTypevtypededuplicateapplySerialCacheStatisticsMemSizeBytesSnapshotCountCacheAgeMsCachedBytesWALCompactionTimeMsWriteOKWriteErrWriteDroppedsnapshotSizesnapshotsnapshottingsnapshotAttemptslastSnapshotlastWriteTimeinitializedCountWriteMultiSnapshotClearSnapshotincreaseSizedecreaseSizeMaxSizeDeleteRangeSetMaxSizeApplyEntryFnLastWriteTimeUpdateAgeUpdateCompactTimeupdateCachedBytesupdateMemSizeupdateSnapshotsNewCacheCacheLoaderNewCacheLoaderIterating over cache"Iterating over cache"SeriesAndFieldFromCompositeKeyParseKeyBytes"Series""tags"problem creating series: (%s)"problem creating series: (%s)"compacting index"compacting index"Closing tsi index"Closing tsi index"Moving tsi to permanent location"Moving tsi to permanent location"TSMReaderblockAccessorindirectIndexoffsetsminKeymaxKeyminTimemaxTimesearchOffsetContainsKeyreadEntriesAtReadEntriesKeyAtKeyCountTombstoneRangeContainsValueOverlapsTimeRangeOverlapsKeyRangeKeyRangeIndexEntryBooleanArrayTimestampsBooleanValueunixnanoFloatArrayFloatValueIntegerArrayIntegerValueStringValueUnsignedArrayUnsignedValuereadAllreadBlockreadBooleanArrayBlockreadBooleanBlockreadBytesreadFloatArrayBlockreadFloatBlockreadIntegerArrayBlockreadIntegerBlockreadStringArrayBlockreadStringBlockreadUnsignedArrayBlockreadUnsignedBlockrenameTSMIndexTombstonerFileStatHasTombstoneMinKeyMaxKeyTombstoneFileStoreObserverFileFinishingFileUnlinkingfileStatsstatsLoadedgzpendingFilelastAppliedOffsetobsWithObserverHasTombstonesTombstoneFileswriteTombstoneV3prepareV4readTombstoneV1readTombstoneV2readTombstoneV3readTombstoneV4tombstonePathwriteTombstonerefsWGmadviseWillNeedaccessortombstonerlastModifieddeleteMuReadFloatBlockAtReadFloatArrayBlockAtReadIntegerBlockAtReadIntegerArrayBlockAtReadUnsignedBlockAtReadUnsignedArrayBlockAtReadStringBlockAtReadStringArrayBlockAtReadBooleanBlockAtReadBooleanArrayBlockAtapplyTombstonesUnrefInUseIndexSizeBlockIteratorBatchDeletetsmReaderOptionNewTSMReaderUnable to read, skipping"Unable to read, skipping"ParseKeyBytesWithTagsTSMFileExtensiontsm.tsmErrNotExistWALFileExtensionwal.walHomeDirGroupIdsCurrent"root" Package buildtsi reads an in-memory index and exports it as a TSI index. Command represents the program execution for "influx_inspect buildtsi". Number of goroutines to dedicate to shard index building. NewCommand returns a new instance of Command. Run executes the command. Verify the user actually wants to run as root. compactDatabaseSeriesFile compacts the series file segments associated with the series file for the provided database. Build input channel. Concurrently process each partition in the series file Build new series file indexes Open partition so index can recover from entries not in the snapshot. Loop over segments and compact. Close partition. Remove the old segment files and replace with new ones. Remove index file so it will be rebuilt when reopened. index won't exist for low cardinality seriesFilePartitionPaths returns the paths to each partition in the series file. index of maximum shard being worked on. Get next partition to work on. No more work. Check for error Check if shard already has a TSI index. Remove temporary index files if this is being re-run. Open TSI index in temporary path. Each new series entry in a log file is ~12 bytes so this should roughly equate to one flush to the file for every batch. Write out tsm1 files. Find shard files. Write out wal files. Flush batch? Flush any remaining series in the batches Attempt to compact the index & wait for all compactions to complete. Close TSI index. Rename TSI to standard path. Reset tags.IntIterableHasNextmorenoEOFbufferedBufferSizeskipSeekscratchpeekExtensionTypeReadExtensionWriteToJSONCopyNextNextTypeReadMapHeaderReadMapKeyReadMapKeyPtrReadArrayHeaderReadNilReadFloat64ReadFloat32ReadBoolReadInt64ReadInt32ReadInt16ReadInt8ReadIntReadUint64ReadUint32ReadUint16ReadUint8ReadUintReadBytesHeaderReadExactBytesReadStringAsBytesReadStringHeaderReadComplex64ReadComplex128ReadMapStrIntfReadTimeReadIntfIndexFileTagBlockvalueDatakeyDatahashDatablkDecodeTagKeyElemDecodeTagValueElemMeasurementBlocksketchDatatSketchDataSketchestblksmblkseriesIDSetDatatombstoneSeriesIDSetDataDecSetToCurrentTimewlocWriteExtensionmwavailbufsizerequireprefix8prefix16prefix32prefix64WriteMapHeaderWriteArrayHeaderWriteNilWriteFloat64WriteFloat32WriteInt64WriteInt8WriteInt16WriteInt32WriteIntWriteUint64WriteUint8WriteUint16WriteUint32WriteUintWriteBytesWriteBytesHeaderWriteBoolWriteStringHeaderWriteStringFromBytesWriteComplex64WriteComplex128WriteMapStrStrWriteMapStrIntfWriteIntfwriteMapwriteSlicewriteStructwriteVallogFileCompactInfologFileMeasurementCompactInfoLogEntryChecksumbatchidxSeriesIDSetIterableentriesPeekNextBatchDeleterContainersArrayContainersArrayContainerBytesArrayContainerValuesBitmapContainersBitmapContainerBytesBitmapContainerValuesRunContainersRunContainerBytesRunContainerValuesLevelsHasFileStatisticManyIntIterableNextManyMeasurementBlockElemtagBlockTagBlockOffsetTagBlockSizeSeriesDataSeriesIDsForEachSeriesIDTagBlockKeyElemhashIndexMarshalBinaryToTagBlockValueElem/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx_inspect/verify/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx_inspect/verify/seriesfile/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx_inspect/verify/seriesfile/verify.goIDDataNewVerifynewBufferverifyResultpartitionPathpartitionInfoconcurrentpartitionInfosConcurrentVerifySeriesFileVerifyPartitionVerifySegmentVerifyIndexsegmentIDsegmentPathsegmentInfosegmentInfoskeyCopyhasKeyszfirstIDprevIDsegmentNamegotDeletedgotOffsetgotIDidsListseriesfileVerifying series file"Verifying series file"Panic verifying file"Panic verifying file"recovered"recovered"Series file does not exist"Series file does not exist""partition"Verifying partition"Verifying partition"Panic verifying partition"Panic verifying partition"ParseSeriesSegmentFilenameNewSeriesSegment"segment"Verifying segment"Verifying segment"Error opening segment"Error opening segment"Panic verifying segment"Panic verifying segment"SeriesSegmentHeaderSizeUnable to advance buffer"Unable to advance buffer"ReadSeriesEntrySeriesEntryInsertFlagID is not monotonically increasing"ID is not monotonically increasing"prev_id"prev_id"JoinSeriesOffsetSeriesEntryTombstoneFlagInvalid flag"Invalid flag"Panic parsing key"Panic parsing key"%x"%x"ParseSeriesKeyVerifying index"Verifying index"Panic verifying index"Panic verifying index"NewSeriesIndexError opening index"Error opening index"Error recovering index"Error recovering index"Index inconsistency"Index inconsistency"got_deleted"got_deleted"expected_deleted"expected_deleted"got_offset"got_offset"expected_offset"expected_offset"got_id"got_id"expected_id"expected_id"unable to advance %d bytes: %d remaining"unable to advance %d bytes: %d remaining" verifyResult contains the result of a Verify... call Verify contains configuration for running verification of series files. NewVerify constructs a Verify with good defaults. VerifySeriesFile performs verifications on a series file. The error is only returned if there was some fatal problem with operating, not if there was a problem with the series file. Check every partition in concurrently. Make sure all the workers are cleaned up when we return. Set up cancellation. Any return will cause the workers to be cancelled. send off the work and read the results. VerifyPartition performs verifications on a partition of a series file. The error is only returned if there was some fatal problem with operating, not if there was a problem with the partition. check every segment open the segment for verifying the index. we want it to be open outside the for loop as well, so the defer is ok. check the index IDData keeps track of data about a series ID. VerifySegment performs verifications on a segment of a series file. The error is only returned The ids map is populated with information about the ids stored in the segment. Open up the segment and grab it's data. Skip the header: it has already been verified by the Open call. Check the flag is valid and for id monotonicity. if zero, there are no more entries Ensure the key parses. This may panic, but our defer handler should make the error message more usable by providing the key. Advance past the entry. VerifyIndex performs verification on an index in a series file. The error is only returned The ids map must be built from verifying the passed in segments. we check all the ids in a consistent order to get the same errors if there is a problem do not perform any other checks if the id is deleted. otherwise, check both that the offset is right and that we get the right id for the key buffer allows one to safely advance a byte slice and keep track of how many bytes were advanced. newBuffer constructs a buffer with the provided data. advance will consume n bytes from the data slice and return an error if there is not enough data to do so./Users/austinjaybecker/projects/abeck-go-testing/cmd/influx_inspect/verify/tombstone/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx_inspect/verify/tombstone/verify.goverifierveryVerboseveryVeryVerbosevvvvvverbosityloadFilestotalEntriestombstoneverify-tombstone"verify-tombstone"GetenvHOME"HOME"/.influxdb"/.influxdb"path to find tombstone files"path to find tombstone files"verbose: emit periodic progress"verbose: emit periodic progress""vv"very verbose: emit every tombstone entry key and time range"very verbose: emit every tombstone entry key and time range""vvv"very very verbose: emit every tombstone entry key and RFC3339Nano time range"very very verbose: emit every tombstone entry key and RFC3339Nano time range"iotaWalkFuncTombstoneFileExtension.tombstoneVerifying: %q
"Verifying: %q\n"NewTombstoner%s has no tombstone entries"%s has no tombstone entries"1e610000000Verified %d tombstone entries
"Verified %d tombstone entries\n"key: %q, min: %v, max: %v
"key: %q, min: %v, max: %v\n"%q failed to walk tombstone entries: %v. Last okay entry: %d
"%q failed to walk tombstone entries: %v. Last okay entry: %d\n"Completed verification for %q in %v.
Verified %d entries

"Completed verification for %q in %v.\nVerified %d entries\n\n"failed tombstone verification"failed tombstone verification" Package tombstone verifies integrity of tombstones. Command represents the program execution for "influx_inspect verify-tombstone"./Users/austinjaybecker/projects/abeck-go-testing/cmd/influx_inspect/verify/tsm/Users/austinjaybecker/projects/abeck-go-testing/cmd/influx_inspect/verify/tsm/verify.goverifyChecksumsverifyTSMverifyUTF8dataPathcheckUTF8printUsageElapsedexpectedblockItrfileErrorstotalErrorscrc32utf8hash/crc32"hash/crc32"unicode/utf8"unicode/utf8"github.com/pkg/errors"github.com/pkg/errors""verify""dir"Root storage path. [$HOME/.influxdb]"Root storage path. [$HOME/.influxdb]"check-utf8"check-utf8"Verify series keys are valid UTF-8"Verify series keys are valid UTF-8""data"Verifies the integrity of TSM files.

Usage: influx_inspect verify [flags]

    -dir <path>
            The root storage path.
            Must be changed if you are using a non-default storage directory.
            Defaults to "%[1]s/.influxdb".
    -check-utf8 
            Verify series keys are valid UTF-8.
            This check skips verification of block checksums.
 `Verifies the integrity of TSM files.

Usage: influx_inspect verify [flags]

    -dir <path>
            The root storage path.
            Must be changed if you are using a non-default storage directory.
            Defaults to "%[1]s/.influxdb".
    -check-utf8 
            Verify series keys are valid UTF-8.
            This check skips verification of block checksums.
 `could not load storage files (use -dir for custom storage root)"could not load storage files (use -dir for custom storage root)"O_RDONLY%s: could not get checksum for key %v block %d due to error: %q
"%s: could not get checksum for key %v block %d due to error: %q\n"ChecksumIEEE%s: got %d but expected %d for key %v, block %d
"%s: got %d but expected %d for key %v, block %d\n"%s: healthy
"%s: healthy\n"Broken Blocks: %d / %d, in %vs
"Broken Blocks: %d / %d, in %vs\n"%s: key #%d is not valid UTF-8
"%s: key #%d is not valid UTF-8\n"Invalid Keys: %d / %d, in %vs
"Invalid Keys: %d / %d, in %vs\n"check-utf8: failed"check-utf8: failed" Package tsm verifies integrity of TSM files. Command represents the program execution for "influx_inspect verify". printUsage prints the usage message to STDERR./Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/inspect/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/inspect/export_index.goNewExportIndexCommandseriesFilePathinspectexport-index`export-index`Exports TSI index data"Exports TSI index data"
This command will export all series in a TSI index to
SQL format for easier inspection and debugging.`
This command will export all series in a TSI index to
SQL format for easier inspection and debugging.`series-path"series-path"Path to series file"Path to series file"index-path"index-path"Path to the index directory of the data engine"Path to the index directory of the data engine"SQLIndexExporterShowSchemaExportIndexexportMeasurementexportMeasurementSeriesexportTagKeyexportTagValueNewSQLIndexExporter Initialize series file. Open index. Dump out index data./Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/inspect/inspect.gosubCommands"inspect"Commands for inspecting on-disk database data"Commands for inspecting on-disk database data" NewCommand creates the new command. List of available sub-commands If a new sub-command is created, it must be added hereNewBuildTSICommand(),NewCompactSeriesFileCommand(),NewExportBlocksCommand(),NewReportTSMCommand(),NewVerifyTSMCommand(),NewVerifyWALCommand(),NewReportTSICommand(),NewVerifySeriesFileCommand(),NewDumpWALCommand(),NewDumpTSICommand(),/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/internal/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/internal/profile/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/internal/profile/profile.goCPUMemorynoProfilesprofpprofprofileruntime/pprof"runtime/pprof"cpucpuprofile: %v"cpuprofile: %v"StartCPUProfilememprofile: %v"memprofile: %v"MemProfileRate4096StopCPUProfileProfileheap"heap" CPU, if set, specifies the file name of the CPU profile to capture Memory, if set, specifies the file name of the CPU profile to capture Start starts a CPU and / or Memory profile if configured and returns a function that should be called to terminate the profiles./Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/launcher/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/launcher/engine.goBoltStoreEngineJaegerTracingLauncherLogTracingMaxIntMemoryStoreNewInfluxdCommandNewLauncherNewTemporaryEngineNewTestLauncherNewTestLauncherServerQueryResultQueryResultsRunTestLauncherOrFailTemporaryEngineTestLauncherWithInfluxQLMaxSelectBucketsNWithInfluxQLMaxSelectSeriesNWithVipercheckForPriorVersioncmdRunEflusherslauncherOptionlauncherOptssetLauncherCMDOptsstoppingSchedulertemporaryTSDBStorevaultConfigWALDirWALFsyncDelayValidateKeysQueryLogEnabledCacheMaxMemorySizeCacheSnapshotMemorySizeCacheSnapshotWriteColdDurationCompactFullWriteColdDurationCompactThroughputCompactThroughputBurstMaxSeriesPerDatabaseMaxValuesPerTagMaxConcurrentCompactionsMaxIndexLogFileSizeSeriesIDSetCacheSizeSeriesFileMaxConcurrentSnapshotCompactionsTraceLoggingEnabledTSMWillNeedCheckIntervalAdvancePeriodRetentionServicePrecreatorConfigEngineOptionsCompactionPlannerCreatorSeriesIDSetsFieldValidatorFieldIteratorAddTagForEachTagHasTagHashIDPrecisionStringRoundedStringSetTagsSetTimeStringSizeCursorIteratorCursorRequestCursorStatsScannedValuesScannedBytesIteratorOptionsAuxOrderedMaxSeriesNInterruptChMergeSortedSeekTimeWindowDerivativeIntervalElapsedIntervalIntegralIntervalGetDimensionsIteratorStatsPointNSeriesIteratorSeriesElemIteratorCostNumShardsNumSeriesCachedValuesNumFilesBlocksReadWriterToCreateCursorIteratorCreateIteratorCreateSnapshotDeleteSeriesRangeDeleteSeriesRangeWithPredicateDigestDiskSizeIsIdleLoadMetadataIndexScheduleFullCompactionSetCompactionsEnabledSetEnabledWritePointsEngineVersionIndexVersionInmemIndexOpenLimiterCompactionDisabledCompactionLimiterCompactionThroughputLimiterWALEnabledMonitorDisabledDatabaseFilterRetentionPolicyFilterShardFilterOnNewEngineShardStatisticsWriteReqWriteReqOKWriteReqErrFieldsCreatedWritePointsErrWritePointsDroppedWritePointsOKBytesWrittenDiskBytesStatisticTagswalPathretentionPolicy_enginedefaultTagsbaseLoggerEnableOnOpenIndexTypevalidateSeriesAndFieldscreateFieldsAndMeasurementsMeasurementTagKeyValuesByExprMeasurementNamesByPredicateCreateSeriesCursormapTypeexpandSourcesengineNoLockdatabaseStateindexTypesaddIndexTyperemoveIndexTypehasMultipleIndexTypesepochTrackerepochDeleteStateguardexprGuardtagGuardmeastagMatchestagExistslargestdeletesStartWriteEndWriteWaitDeletesfilesSeriesFileMaxSizeindexespendingShardDeletesepochsIndexBytesloadShardsepochsForShardsopenSeriesFileseriesFilecreateIndexIfNotExistsShardGroupShardNShardDigestCreateShardCreateShardSnapshotSetShardEnabledDeleteShardsDeleteShardDeleteDatabaseDeleteRetentionPolicyfilterShardswalkShardsshardIDsshardsSlicesketchesForDatabaseSeriesCardinalityMeasurementsCardinalityExportShardImportShardShardRelativePathDeleteSeriesExpandSourcesWriteToShardMeasurementSeriesCountsTagKeysTagValuesmonitorShardsConsistencyLevelTSDBStorecheckIntervaladvancePeriodrunPrecreationprecreatetsdbStorepointsWriterretentionServiceprecreatorServicedefaultMetricLabelswritePointsValidationEnabledUpdateBucketRetentionPeriodDeleteBucketRangeFieldKeysByMeasurementMeasurementsByRegexKeyValueenginepromstoragelaunchergithub.com/influxdata/influxdb/v2/kit/prom"github.com/influxdata/influxdb/v2/kit/prom"github.com/influxdata/influxdb/v2/storage"github.com/influxdata/influxdb/v2/storage"PointsWriterEngineSchemaPrometheusCollectore2e"e2e"NewEngine"service"temporary_engine"temporary_engine"unable to close engine"unable to close engine"unable to open engine"unable to open engine" Engine defines the time-series storage engine.  Wraps *storage.Engine to facilitate testing. TemporaryEngine creates a time-series storage engine backed by a temporary directory that is removed on Close. NewTemporaryEngine creates a new engine that places the storage engine files into a temporary directory; used for testing. Open creates a temporary directory and opens the engine. Close will remove the directory containing the time-series files. WritePoints stores points into the storage engine. SeriesCardinality returns the number of series in the engine. DeleteBucketRangePredicate will delete a bucket from the range and predicate. DeleteBucket deletes a bucket from the time-series data. WithLogger sets the logger on the engine. It must be called before Open. PrometheusCollectors returns all the prometheus collectors associated with the engine and its components. Flush will remove the time-series files and re-open the engine.FlaggerExposeMaxConcurrentQueriesLogQueriesAfterMaxSelectPointNMaxSelectSeriesNMaxSelectBucketsNControllerDependencyConcurrencyQuotaInitialMemoryBytesQuotaPerQueryMemoryBytesQuotaPerQueryMaxMemoryBytesQueueSizeMetricLabelKeysExecutorDependencieshistProgramManagerFreeMemoryRequestMemoryallocationLimitbytesAllocatedmaxAllocatedtotalAllocatedAllocatedrequestMemoryallocatorProfilerResultsqueryMemoryManagermemoryManagerinitialBytesQuotaPerQuerymemoryBytesQuotaPerQueryunusedMemoryBytesunlimitedgetUnusedMemoryBytestrySetUnusedMemoryBytesaddUnusedMemoryBytesgivengiveMemorylabelValuescompileLabelValuesstateMuruntimeErrsparentCtxparentSpancurrentSpandoneChprogramrecordUnusedMemorytransitionToaddRuntimeErrorpumptryCompiletryQueuetryExeccontrollerMetricsrequestsfunctionscompilingqueueingexecutingmemoryUnusedallDurcompilingDurqueueingDurexecutingDurlastIDqueriesMuqueryQueueabortOnceabortmemorylabelKeysdependenciescreateQuerycountQueryRequestcompileQueryenqueueQueryprocessQueryQueueexecuteQuerywaitForQueryGetUnusedMemoryBytesGetUsedMemoryBytescreateAllocatorSubjectPermissionDenyPublishSubscribeClusterOptsRoutePermissionsAuthTimeoutTLSTimeoutListenStrAdvertiseNoAdvertiseConnectRetriesAuthenticationClientAuthenticationclientOptsEchoPedanticTLSRequiredLangProtocolGetOptsGetTLSConnectionStateRegisterUserConfigFileClientAdvertiseNoLogNoSigsLogtimeMaxConnMaxSubsPingIntervalMaxPingsOutHTTPHostHTTPPortHTTPSPortMaxControlLineMaxPayloadMaxPendingProfPortPidFilePortsFileDirSyslogRemoteSyslogRoutesStrTLSVerifyTLSCertTLSKeyTLSCaCertWriteDeadlineRQSubsSweepMaxClosedClientsCustomClientAuthenticationCustomRouterAuthenticationProcessConfigFileStanServerServerInfoDiscoveryUnsubscribeSubCloseAcksSubsMarshalToinMsgsoutMsgsinBytesoutBytesslowConsumersGitCommitGoVersionAuthRequiredCIDClientConnectURLsSublistsubscriptionoutboundconsumenbswspmfspmpwdllftallowdenypubpcachereadCacheSublistResultpsubsqsubsaddSubToResultgenidprandmsgssubsrszsrspinfotmrpubArgreplysidszbdropargBufmsgBufRouteTyperemoteIDdidSolicitrouteTypeauthRequiredtlsRequiredconnectURLsclientFlagcfsetIfNotSetmpaymsubsncncspcdatmrmsgbrttStartechoinitClientsetPermissionscollapsePtoNBhandlePartialWriteflushOutboundflushSignaltraceMsgtraceInOptraceOutOptraceOpprocessInfoprocessErrprocessConnectauthTimeoutauthViolationmaxConnExceededmaxSubsExceededmaxPayloadViolationqueueOutboundsendProtosendPongsendPinggenerateClientInfoJSONsendInfosendErrsendOKprocessPingprocessPongprocessMsgArgsprocessPubprocessSubcanSubscribeunsubscribeprocessUnsubmsgHeaderdeliverMsgprunePubPermsCachepubAllowedprepMsgHeaderprocessMsgpubPermissionViolationprocessPingTimersetPingTimerclearPingTimersetAuthTimerclearAuthTimerisAuthTimerSetclearConnectioncloseConnectionsetRouteNoReconnectOnCloseNoticefTracefgetRTTclonePubArgreRouteQMsgprocessRoutedMsgsendConnectprocessRouteInfocanImportcanExportsetRoutePermissionsisSolicitedRoutenmplistpwcfwcpruneNodenumNodestryCompareAndSwapunexpungeLockedswapLockedtryLoadOrStoretrySwaptryExpungeLockedmissesloadReadOnlyLoadOrStoreLoadAndDeleteCompareAndDeletemissLockeddirtyLockedcacheHitsinsertsremovescacheNumccSweepaddToCacheremoveFromCachereduceCacheCountRemoveBatchremoveFromNodeCacheCountnumLevelsaddNodeToSubscollectLocalSubslocalSubsclosedRingBufferclosedClientConnInfoCidLastActivityRTTUptimePendingInMsgsOutMsgsInBytesOutBytesNumSubsTLSVersionTLSCipherSubscitotalConnsclosedClientsrqsubatimegcidsloptsMuclientstotalClientshttpHandlerprofilerhttpReqStatsrouteListenerrouteInforouteInfoJSONquitChrqsMurqsubsrqsubsTimergrMugrTmpClientsgrRunninggrWGcprotoconfigTimeloggingclientConnectURLsclientConnectURLsMaplastCURLsUpdateclientActualPortclusterActualPortmonitoringServerprofilingServerconfigureAuthorizationcheckAuthorizationhasUsersisClientAuthorizedisRouterAuthorizedremoveUnauthorizedSubsConfigureLoggerSetLoggerReOpenLogFileexecuteLogCallConnzHandleConnzRoutezHandleRoutezSubszHandleSubszHandleStackszHandleRootVarzHandleVarzReloadreloadOptionsdiffOptionsapplyOptionsreloadAuthorizationclearRemoteQSubspurgeRemoteQSubslookupRemoteQGroupholdRemoteQSubsendAsyncInfoToClientsprocessImplicitRoutehasThisRouteConfiguredforwardNewRouteInfoToKnownServerssendLocalSubsToRoutecreateRouterouteSidQueueSubscriberbroadcastInterestToRoutesbroadcastSubscribebroadcastUnSubscriberouteAcceptLoopsetRouteInfoHostPortAndIPStartRoutingreConnectToRouterouteStillValidconnectToRoutesolicitRoutesnumRoutesgetOptssetOptsgenerateRouteInfoJSONisRunninglogPidAcceptLoopsetInfoHostPortAndGenerateJSONStartProfilerStartHTTPMonitoringStartHTTPSMonitoringStartMonitoringstartMonitoringHTTPHandlercopyInfocreateClientsaveClosedClientaddClientConnectURLsAndSendINFOToClientsremoveClientConnectURLsAndSendINFOToClientsupdateServerINFOAndSendINFOToClientsremoveClientNumRoutesNumRemotesNumClientsgetClientNumSubscriptionsNumSlowConsumersConfigTimeMonitorAddrClusterAddrProfilerAddrReadyForConnectionsstartGoRoutinenumClosedConnstotalClosedConnsgetClientConnectURLsPortsInfoportFiledeletePortsFilelogPortsreadyForListenersserviceListenershandleSignalsFileStoreOptionsCompactEnabledCompactIntervalCompactFragmentationCompactMinFileSizeDoCRCCRCPolynomialDoSyncSliceMaxMsgsSliceMaxBytesSliceMaxAgeSliceArchiveScriptFileDescriptorsLimitParallelRecoveryTruncateUnexpectedEOFSQLStoreOptionsDriverNoCachingMaxOpenConnsStoreLimitsChannelLimitsMsgStoreLimitsMaxMsgsMaxBytesSubStoreLimitsMaxSubscriptionsMaxInactivityMaxChannelsPerChannelClonePerChannelMapAddPerChannelapplyInheritanceinheritLimitscheckGlobalLimitsClusteringOptionsClusteredBootstrapPeersRaftLogPathLogCacheSizeLogSnapshotsTrailingLogsRaftLoggingRaftHeartbeatTimeoutRaftElectionTimeoutRaftLeaseTimeoutRaftCommitTimeoutDiscoverPrefixStoreTypeFilestoreDirFileStoreOptsSQLStoreOptsEnableLoggingCustomLoggerHandleSignalsClientCertClientKeyClientCAIOBatchSizeIOSleepTimeNATSServerURLClientHBIntervalClientHBTimeoutClientHBFailCountFTGroupNamePartitioningSyslogNameClusteringReconnectsConnHandlerErrHandlerMsgHandlerbarrierInfoReplySubscriptionTypeQueuedeliveredmcbmchconnClosedpHeadpTailpCondpMsgspBytespMsgsMaxpBytesMaxpMsgsLimitpBytesLimitdroppedNextMsgWithContextAutoUnsubscribeNextMsgvalidateNextMsgStateprocessNextMsgDeliveredQueuedMsgsClearMaxPendingPendingLimitsSetPendingLimitsDeliveredDroppedUserJWTHandlerSignatureHandlerAuthTokenHandlerCustomDialerUrlNoRandomizeNoEchoAllowReconnectMaxReconnectReconnectWaitDrainTimeoutFlusherTimeoutClosedCBDisconnectedCBReconnectedCBDiscoveredServersCBAsyncErrorCBReconnectBufSizeSubChanLenUserJWTNkeySignatureCBTokenHandlerUseOldRequestStyledidConnectreconnectslastAttemptisImplicittlsNameserverInfoConnectURLsNonceasyncCallbacksHandlerasyncCBasyncCBDispatcherpushOrClosemsgArgmaOptssrvPoolfchssidsubsMuachpongsinitcptmrpoutrespSubrespMuxrespMaprespSetuprespRandRequestWithContextoldRequestWithContextSetDisconnectHandlerSetReconnectHandlerSetDiscoveredServersHandlerSetClosedHandlerSetErrorHandlercurrentServerselectNextServerpickServersetupServerPoolconnSchemeaddURLToPoolshufflePoolcreateConnmakeTLSConnwaitForExitsConnectedUrlConnectedServerIdprocessConnectInitcheckForSecureprocessExpectedInfoconnectProtoreadProtoflushReconnectPendingItemsstopPingTimerdoReconnectprocessOpErrwaitForMsgsprocessPermissionsViolationprocessAuthorizationViolationprocessOKprocessAsyncInfoLastErrorkickFlusherPublishMsgPublishRequestpublishrespHandlercreateRespMuxoldRequestinitNewRespnewRespInboxNewRespInboxChanSubscribeChanQueueSubscribeSubscribeSyncQueueSubscribeQueueSubscribeSyncQueueSubscribeSyncWithChansubscriberemoveSubcheckDrainedremoveFlushEntryFlushTimeoutresendSubscriptionsclearPendingFlushCallsclearPendingRequestCallsIsClosedIsReconnectingIsConnecteddrainConnectionIsDraininggetServersDiscoveredServersisClosedisConnectingisReconnectingisConnectedisDrainingisDrainingPubsBarriercloneMsgArgclientStoreClientInfoHbInboxConnIDPingMaxOutsubStateSubStateQGroupInboxAckInboxMaxInFlightAckWaitInSecsDurableNameLastSentIsDurablequeueStatelastSentshadowstalledSubCountnewOnHoldSubStoreAckSeqPendingAddSeqPendingCreateSubDeleteSubUpdateSubsubSentAndAcksentinFlusherqstateackWaitackTimerackSubacksPendingsavedClientIDreplicatestalledhasFailedHBdeleteFromListclearAckTimeradjustAckTimerstartAckSubstopAckSubdurableKeyisQueueSubscriberisQueueDurableSubscriberisShadowQueueDurableisDurableSubscriberisOfflineDurableSubscriberhbtfhbgetSubsCopyMsgStoreMsgProtoRedeliveredCRC32FirstAndLastSequenceFirstMsgFirstSequenceGetSequenceFromTimestampLastMsgLastSequenceMsgsRecoveredStateRecoveredChannelRecoveredSubscriptionPendingAcksClientsChannelsAddClientCreateChannelDeleteChannelDeleteClientGetChannelLimitsGetExclusiveLockSetLimitsconnIDswaitOnRegisterunregisterisValidWithTimeoutlookupByConnIDOrIDlookupByConnIDgetSubsaddSubrecoverClientssetClientHBremoveClientHBgetClientschannelStoresubStoredurablesacksstangetAllSubshasActiveSubsLookupByDurableLookupByAckInboxchannelActivitymaxInactivitydeleteInProgresstimerSetlTimestampsnapshotSubactivitystartDeleteTimerstopDeleteTimerresetDeleteTimerpubMsgToMsgProtosubToSnapshotRestoreRequestsdelMuchannelscreateChannelgetAllmsgsStatelockDeleteunlockDeletemaybeStartChannelDeleteTimerioPendingMsgPubMsgGuidSha256PubAcksdcsubStartInfoisDurableelementsNumLevelssendListSubjectprocessChanSubinboxSubisShutdowncreateChannelsMapAndSublisttopologyChangedinitSubscriptionscheckChannelsUniqueInClusterprocessChannelsListRequestsStanLoggerltimelfileGetLoggerReopenLogFileraftNodeRaftraftStateRaftStatecurrentTermcommitIndexlastAppliedlastLocklastSnapshotIndexlastSnapshotTermlastLogIndexlastLogTermroutinesGroupgetCurrentTermsetCurrentTermgetLastLogsetLastLoggetLastSnapshotsetLastSnapshotgetCommitIndexsetCommitIndexgetLastAppliedsetLastAppliedgoFuncwaitShutdowngetLastIndexgetLastEntrylogFuturedeferErrorerrChrespondedrespondLogTypedispatchServerIDHeartbeatTimeoutElectionTimeoutCommitTimeoutMaxAppendEntriesShutdownOnRemoveSnapshotIntervalSnapshotThresholdLeaderLeaseTimeoutStartAsLeaderLocalIDNotifyChLogOutputFSMFSMSnapshotSnapshotSinkPersistreqSnapshotFutureServerAddressleaderStatecommitmentcommitChmatchIndexesstartIndexsetConfigurationrecalculatefollowerReplicationServerSuffrageSuffrageverifyFuturenotifyChquorumSizevotesvoteLockvotepeerstopChtriggerChnextIndexlastContactlastContactLockfailuresnotifyLockstepDownallowPipelinenotifyAllLastContactsetLastContactinflightreplStateLogStoreFirstIndexGetLogLastIndexStoreLogStoreLogsconfigurationChangeFutureconfigurationChangeRequestConfigurationChangeCommandserverAddressprevIndexconfigurationsConfigurationcommittedcommittedIndexlatestIndexRPCRPCResponseRespChanSnapshotStoreSnapshotVersionAppendEntriesRequestRPCHeaderPrevLogEntryPrevLogTermLeaderCommitIndexGetRPCHeaderAppendEntriesResponseLastLogSuccessNoRetryBackoffAppendPipelineAppendFutureFutureAppendEntriesConsumerInstallSnapshotRequestLastLogIndexLastLogTermConfigurationIndexInstallSnapshotResponseRequestVoteRequestCandidateRequestVoteResponseGrantedAppendEntriesPipelineDecodePeerEncodePeerInstallSnapshotRequestVoteSetHeartbeatHandlerSnapshotMetauserSnapshotFutureopeneruserRestoreFutureStableStoreconfigurationsFuturebootstrapFutureconfigurationObservationnumObservednumDroppedblockingGetNumObservedGetNumDroppedprotocolVersionapplyChfsmfsmMutateChfsmSnapshotChleaderleaderLockleaderChlocalIDconfigurationChangeChrpcChshutdownChshutdownLocksnapshotsuserSnapshotChuserRestoreChstableverifyChconfigurationsChbootstrapChobserversLockobserversrestoreSnapshotBootstrapClusterVerifyLeaderGetConfigurationAddPeerRemovePeerAddVoterAddNonvoterRemoveServerDemoteVoterLeaderChAppliedIndexrunFSMRegisterObserverDeregisterObserverobservegetRPCHeadercheckRPCHeadersetLeaderrequestConfigChangerunFollowerliveBootstraprunCandidaterunLeaderstartStopReplicationconfigurationChangeChIfStableleaderLoopverifyLeadercheckLeaderLeaserestoreUserSnapshotappendConfigurationEntrydispatchLogsprocessLogsprocessLogprocessRPCprocessHeartbeatappendEntriesprocessConfigurationLogEntryrequestVoteinstallSnapshotelectSelfpersistVotereplicateTosendLatestSnapshotheartbeatpipelineReplicatepipelineSendpipelineDecodesetupAppendEntriessetPreviousLogsetNewLogshandleStaleTermrunSnapshotsshouldSnapshottakeSnapshotcompactLogsraftLoglockfileMsgpackHandleBasicHandleextHandleextTypeTagFnrtidencFndecFnAddExtgetExtgetExtForTaggetDecodeExtForTaggetDecodeExtgetEncodeExtEncodeOptionsAsSymbolFlagStructToArrayAsSymbolsDecodeOptionsSliceTypeErrorIfNoFieldRawToStringWriteExtnewEncDrivernewDecDriverwriteExtgetBasicHandletrailingLogsratioThresholdsimpleDelThresholdHighsimpleDelThresholdLowopenAndSetOptionsencodeRaftLogdecodeRaftLogsimpleDeleteRangetransferLogsNetworkTransportnetConndecReaderreadUint16readUint32readUint64readbreadnreadn1decDrivervalueTypecurrentEncodedTypedecodeBooldecodeBuiltindecodeBytesdecodeExtdecodeFloatdecodeIntdecodeNakeddecodeStringdecodeUintinitReadNextisBuiltinTypereadArrayLenreadMapLentryDecodeAsNildecFnInfotypeInfostructFieldInfoencNameomitEmptytoArraysfisfipbaseIdbaseIndirmbsunmmIndirunmIndirindexForEncNamexfFnxfTagbuiltinrawExtbinaryMarshalkErrkStringkBoolkIntkInt64kInt32kInt8kInt16kFloat32kFloat64kUint8kUint64kUintkUint32kUint16kInterfacekStructkSlicekArraykMapdecodeValuechkPtrValuedecEmbeddedFielddecSliceIntfdecSliceInt64decSliceUint64decSliceStrdecMapIntfIntfdecMapInt64IntfdecMapUint64IntfdecMapStrIntfencWriteratEndOfEncodewriteUint64writebwriten1writen2writestrencDrivercharEncodingencodeArrayPreambleencodeBoolencodeBuiltinencodeExtPreambleencodeFloat32encodeFloat64encodeIntencodeMapPreambleencodeNilencodeStringencodeStringBytesencodeSymbolencodeUintencFnInfoeekInvalidhhencodeValueencRawExtencSliceIntfencSliceStrencSliceInt64encSliceUint64encMapStrStrencMapStrIntfencMapInt64IntfencMapUint64IntfencMapIntfIntfServerAddressProviderServerAddrStreamLayerconnPoolconnPoolLockconsumeChheartbeatFnheartbeatFnLockmaxPoolserverAddressProviderTimeoutScaleIsShutdowngetPooledConngetConnFromAddressProvidergetProviderAddressOrFallbackreturnConngenericRPClistenhandleConnhandleCommandraftFSMsnapshotsOnInitrestoreClientsFromSnapshotrestoreChannelsFromSnapshotrestoreMsgsFromSnapshotlogInputjoinSublazyReplicationioChannelStatsMaxBatchSizenatsServernatsOptsncancrncsrdupCIDTimeoutcliDupCIDsMucliDipCIDsMapmonMunumSubsioChannelioChannelQuitioChannelWGcloseMutmpBufsubStartChsubStartQuitftncftSubjectftHBIntervalftHBMissedIntervalftHBChftQuitlastErrorraftraftLoggingisClusteredlazyReplconnectSubcloseSubpubSubsubSubsubCloseSubsubUnsubSubcliPingSubpingResponseOKBytespingResponseInvalidClientBytescreateServerRaftNodedetectBootstrapMisconfigcreateRaftNodebootstrapClustergetClusteringAddrgetClusteringPeerAddrftStartftGetStoreLockftSendHBLoopftSetuphandleRootzhandleServerzhandleStorezhandleClientszhandleChannelszhandleOneChannelsendResponseinitPartitionsisLeaderlookupOrCreateChannelcreateSubStorestanDisconnectedHandlerstanReconnectedHandlerstanClosedHandlerstanErrorHandlerbuildServerURLscreateNatsClientConncreateNatsConnectionsconfigureLoggerstartRaftNodesendSynchronziationRequestleadershipAcquiredleadershipLostconfigureClusterOptsconfigureNATSServerTLSstartNATSServerensureRunningStandAloneprocessRecoveredClientsprocessRecoveredChannelsrecoverOneSubpostRecoveryProcessingperformRedeliveryOnStartupinitInternalSubsunsubscribeInternalSubscreateSubconnectCBhandleConnectisDuplicateConnectreplicateDeleteChannelhandleChannelDeleteprocessDeleteChannelreplicateConnectfinishConnectRequestsendConnectErrcheckClientHealthcloseClientprocessCloseRequestreplicateConnClosesendCloseResponseprocessClientPublishprocessClientPingsprocessCtrlMsgsendPublishErrsendMsgToQueueGroupperformDurableRedeliveryperformAckExpirationRedeliverygetMsgForRedeliveryreplicateSentOrAckreplicateSentAndAckSeqsflushReplicatedSentAndAckSeqslazyReplicationOfSentAndAckprocessReplicatedSendAndAcksendMsgToSubsetupAckTimerstartIOLoopioLooplogErrAndSendPublishErrsendDeleteChannelRequestackPublisherremoveAllNonDurableSubscribersprocessUnsubscribeRequestprocessSubCloseRequestperformmUnsubOrCloseSubscriptionunsubscribeSubreplicateRemoveSubscriptionreplicateCloseSubscriptionreplicateUnsubscribesendSubscriptionResponseErrreplicateSubaddSubscriptionupdateDurableprocessSubscriptionRequestprocessSubscriptionsStartprocessAckMsgprocessAcksendAvailableMessagesToQueuesendAvailableMessagesgetNextMsgsetSubStartSequencesetLastErrorserverOptsSchedulableSchedulelowhighdomldowmonthldomdowntcountEveryyearIsZerosetYearysyearIneveryYearsetEveryYeareveryMonthsetEveryMontheveryDaysetEveryDayeverySecondsaddEveryDureveryZeromonthInhourInminuteInnextYearnextMonthnextDaynextHournextMinutenextSecondprepDaysLastScheduledSchedulerExecutorTaskControlServiceQueryServiceCompilerRuntimeASTHandleExecOptsConfigConfigureNowConfigureProfilerEvalIsPreludePackageJSONToHandleLookupBuiltinTypeMergePackagesCompilerTypeCompileCompilerMappingsCreateCompilercompilerMappingsWithOptionWithReturnNoContentApplyOptionsWithCompilerMappingsCheckerPermissionServiceExecutorMetricsSummaryVectotalRunsCompleteactiveRunsqueueDeltarunDurationerrorsCountermanualRunsCounterresumeRunsCounterunrecoverableCounterrunLatencyemStartRunLogErrorLogUnrecoverableErrorpromisecreatedAtcancelFuncLimitFuncCompilerBuilderFuncCompilerBuilderTimestampsExterntcscurrentPromisespromiseQueuelimitFuncworkerPoolworkerLimitnonSystemBuildCompilersystemBuildCompilerflaggerSetLimitFuncPromisedExecuteManualRunResumeCurrentRunstartWorkercreatePromiseRunsActiveWorkersBusyPromiseQueueUsageRegistrycollectorsByIDdescIDsdimHashesByNameuncheckedCollectorspedanticChecksEnabledGatherapplyConfigapplyInitAPIBackendProxyQueryServiceDialectDialectTypeMultiResultEncoderDialectMappingsCreateDialectdialectMappingsWithDialectMappingsEventRecorderRequestBytesResponseBytesFeatureProxyHandlerEncodingFormatChunkedStatementCountAddingLogToSpanAssetsPathSessionRenewDisabledMaxBatchSizeBytesWriteParserMaxBytesWriteParserMaxLinesWriteParserMaxValuesNewQueryServiceWriteEventRecorderQueryEventRecorderAlgoWProxyDBRPServiceInfluxQLServiceInfluxqldServiceFluxServiceChronografServiceOrgLookupServiceFlagsHandlerstoreTypeassetsPathtestingAlwaysAllowSetupsessionLengthsessionRenewDisabledlogLeveltracingTypereportingDisabledhttpBindAddressenginePathsecretStorefeatureFlagsconcurrencyQuotainitialMemoryBytesQuotaPerQuerymaxMemoryBytesqueueSizeStorageConfigCoordinatorConfigqueryControllerhttpPorthttpTLSCerthttpTLSKeyhttpTLSMinVersionhttpTLSStrictCiphersnatsPortnoTasksschedulerexecutortaskControlServicejaegerTracerCloserapibackendRunningNatsURLQueryControllerKeyValueServicerealServerShutdownOrFailSetupSetupOrFailOnBoardOnBoardOrFailWriteOrFailWritePointsOrFailMustExecuteQueryExecuteQueryQueryAndConsumeQueryAndNopConsumeFluxQueryOrFailQueryFluxMustNewHTTPRequestNewHTTPRequestNewHTTPRequestOrFailFluxQueryServiceNotificationRuleServicePkgerServiceTaskServiceKVNumReadsTBapplyInitFnapplyConfigFnFieldCreateSeriesCursorRequestSeriesCursorSeriesCursorRowHasTableCountSortedNamesHasTablesWithColsTablesNepochWaiterCACertCAPathTLSServerNameAgentAddressClientTimeoutMaxRetriesClosedStatereasonIndexFuturevoteResultvoterIDAddrPortSubscriptionRequestStartPositionEnumDescriptorStartSequenceStartTimeDeltaRaftSnapshotChannelSnapshotSubscriptionSnapshotAcksPendingPaddingPromisechannelLimitInfoisLiteralisProcessedUnsubscribeRequestConnectRequestHeartbeatInboxApplyFutureamendedPortsNatsMonitoringConfigurationFutureSubSentAndAckSentAckSnapshotFutureRawExtVarzOptionsMemCoresConnectionsTotalConnectionsRemotesSlowConsumersHTTPReqStatsConfigLoadTimemodifyLockHttpClientOutputCurlStringConfigureTLSReadEnvironmentSubszOptionsTestSublistStatsNumCacheNumInsertsNumRemovesNumMatchesCacheHitRateMaxFanoutAvgFanoutSubDetailSidrequestsLabelIsAuthChangeIsLoggingChangeCloseRequestConnzOptionsSortOptNumConnsConnsMetricFamilyMetricTypeEnumGetHelpGetMetriccontrolRaftOperation_TypeRoutezOptionsRouteInfoRidDidSolicitIsConfiguredInsecure/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/launcher/flusher.go/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/launcher/launcher.goassignDescsrunCmdtracerauthStorescheduledAtsmcombinedTaskServicecoordLoggerexecutorMetricsschschLoggertaskCoordcoordinatorlabelsStoredashboardServiceauthedOrgSVCauthedUrmSVCpkgerLoggertLoggerauthLoggerpassServicelabelHandlerurmHandlerhttpLoggerplatformHandlercipherConfigstrictCipherstlsMinVersionauthAgentauthHTTPServerauthSvcV1authorizerV1bucketHTTPServerbucketLogSvccercheckSvcchronografSvcdashboardLogSvcdashboardServerdashboardSvcdbrpSvcdeleteServicedepslnmappermigratornotificationEndpointSvcnotificationRuleSvconboardHTTPServeronboardOptsonboardSvcopLogSvcorgHTTPServerorgLogSvcpasswordV1pkgSVCpublisherqeresourceResolverscraperSchedulerscraperTargetSvcsesecretSvcserviceConfigsessionHTTPServersessionSvcsourceSvcstacksHTTPServerstorageQueryServicesubscribertaskSvctelegrafSvctemplatesHTTPServeruserHTTPServeruserLogSvcv1AuthHTTPServervariableSvchasErrorsauthv1dashboardTransportendpointservicefeaturefluxinitgatherinfprominmemiqlcontroliqlcoordinatoriqlqueryjaegerconfignatsnethttpopentracingoverrideflaggerpzapreadserviceruleservicestorage2storagefluxtaskbackendtelegrafservicetelemetryvaultzapcoregithub.com/influxdata/influxdb/v2/checks"github.com/influxdata/influxdb/v2/checks"github.com/influxdata/influxdb/v2/dashboards"github.com/influxdata/influxdb/v2/dashboards"github.com/influxdata/influxdb/v2/fluxinit"github.com/influxdata/influxdb/v2/fluxinit"github.com/influxdata/influxdb/v2/gather"github.com/influxdata/influxdb/v2/gather"github.com/influxdata/influxdb/v2/influxql/control"github.com/influxdata/influxdb/v2/influxql/control"github.com/influxdata/influxdb/v2/influxql/query"github.com/influxdata/influxdb/v2/influxql/query"github.com/influxdata/influxdb/v2/inmem"github.com/influxdata/influxdb/v2/inmem"github.com/influxdata/influxdb/v2/internal/resource"github.com/influxdata/influxdb/v2/internal/resource"github.com/influxdata/influxdb/v2/kit/feature"github.com/influxdata/influxdb/v2/kit/feature"github.com/influxdata/influxdb/v2/kit/feature/override"github.com/influxdata/influxdb/v2/kit/feature/override"github.com/influxdata/influxdb/v2/kv/migration"github.com/influxdata/influxdb/v2/kv/migration"github.com/influxdata/influxdb/v2/kv/migration/all"github.com/influxdata/influxdb/v2/kv/migration/all"github.com/influxdata/influxdb/v2/label"github.com/influxdata/influxdb/v2/label"github.com/influxdata/influxdb/v2/nats"github.com/influxdata/influxdb/v2/nats"github.com/influxdata/influxdb/v2/notification/endpoint/service"github.com/influxdata/influxdb/v2/notification/endpoint/service"github.com/influxdata/influxdb/v2/notification/rule/service"github.com/influxdata/influxdb/v2/notification/rule/service"github.com/influxdata/influxdb/v2/prometheus"github.com/influxdata/influxdb/v2/prometheus"github.com/influxdata/influxdb/v2/query"github.com/influxdata/influxdb/v2/query"github.com/influxdata/influxdb/v2/query/control"github.com/influxdata/influxdb/v2/query/control"github.com/influxdata/influxdb/v2/query/stdlib/influxdata/influxdb"github.com/influxdata/influxdb/v2/query/stdlib/influxdata/influxdb"github.com/influxdata/influxdb/v2/session"github.com/influxdata/influxdb/v2/session"github.com/influxdata/influxdb/v2/source"github.com/influxdata/influxdb/v2/source"github.com/influxdata/influxdb/v2/storage/flux"github.com/influxdata/influxdb/v2/storage/flux"github.com/influxdata/influxdb/v2/storage/readservice"github.com/influxdata/influxdb/v2/storage/readservice"github.com/influxdata/influxdb/v2/task/backend"github.com/influxdata/influxdb/v2/task/backend"github.com/influxdata/influxdb/v2/task/backend/coordinator"github.com/influxdata/influxdb/v2/task/backend/coordinator"github.com/influxdata/influxdb/v2/task/backend/executor"github.com/influxdata/influxdb/v2/task/backend/executor"github.com/influxdata/influxdb/v2/task/backend/middleware"github.com/influxdata/influxdb/v2/task/backend/middleware"github.com/influxdata/influxdb/v2/task/backend/scheduler"github.com/influxdata/influxdb/v2/task/backend/scheduler"github.com/influxdata/influxdb/v2/telegraf/service"github.com/influxdata/influxdb/v2/telegraf/service"github.com/influxdata/influxdb/v2/telemetry"github.com/influxdata/influxdb/v2/telemetry"github.com/influxdata/influxdb/v2/v1/coordinator"github.com/influxdata/influxdb/v2/v1/coordinator"github.com/influxdata/influxdb/v2/v1/services/storage"github.com/influxdata/influxdb/v2/v1/services/storage"github.com/influxdata/influxdb/v2/vault"github.com/influxdata/influxdb/v2/vault"github.com/influxdata/influxdb/v2/zap"github.com/influxdata/influxdb/v2/zap"github.com/opentracing/opentracing-go"github.com/opentracing/opentracing-go"github.com/uber/jaeger-client-go/config"github.com/uber/jaeger-client-go/config"go.uber.org/zap/zapcore"go.uber.org/zap/zapcore""bolt""memory"jaeger"jaeger"IntSize92233720368547758089223372036854775807influxd"influxd"Start the influxd server (default)"Start the influxd server (default)"
	Start up the daemon configured with flags/env vars/config file.

	The order of precedence for config options are as follows (1 highest, 3 lowest):
		1. flags
		2. env vars
		3. config file

	A config file can be provided via the INFLUXD_CONFIG_PATH env var. If a file is
	not provided via an env var, influxd will look in the current directory for a
	config.{json|toml|yaml|yml} file. If one does not exist, then it will continue unchanged.`
	Start up the daemon configured with flags/env vars/config file.

	The order of precedence for config options are as follows (1 highest, 3 lowest):
		1. flags
		2. env vars
		3. config file

	A config file can be provided via the INFLUXD_CONFIG_PATH env var. If a file is
	not provided via an env var, influxd will look in the current directory for a
	config.{json|toml|yaml|yml} file. If one does not exist, then it will continue unchanged.`FluxInitthe daemon is already running"the daemon is already running"ReporterGathererPushFormatNewReporter28800000000000failed to determine influx directory: %v"failed to determine influx directory: %v"log-level"log-level"InfoLevelsupported log levels are debug, info, and error"supported log levels are debug, info, and error"tracing-type"tracing-type"supported tracing types are %s, %s"supported tracing types are %s, %s"http-bind-address"http-bind-address":8086":8086"bind address for the REST HTTP API"bind address for the REST HTTP API"bolt-path"bolt-path"path to boltdb database"path to boltdb database"assets-path"assets-path"override default assets by serving from a specific directory (developer mode)"override default assets by serving from a specific directory (developer mode)""store"backing store for REST resources (bolt or memory)"backing store for REST resources (bolt or memory)"e2e-testing"e2e-testing"add /debug/flush endpoint to clear stores; used for end-to-end tests"add /debug/flush endpoint to clear stores; used for end-to-end tests"testing-always-allow-setup"testing-always-allow-setup"ensures the /api/v2/setup endpoint always returns true to allow onboarding"ensures the /api/v2/setup endpoint always returns true to allow onboarding"engine-path"engine-path""engine"path to persistent engine files"path to persistent engine files"secret-store"secret-store"data store for secrets (bolt or vault)"data store for secrets (bolt or vault)"reporting-disabled"reporting-disabled"disable sending telemetry data to https://telemetry.influxdata.com every 8 hours"disable sending telemetry data to https://telemetry.influxdata.com every 8 hours"session-length"session-length"ttl in minutes for newly created sessions"ttl in minutes for newly created sessions"session-renew-disabled"session-renew-disabled"disables automatically extending session ttl on request"disables automatically extending session ttl on request"vault-addr"vault-addr"address of the Vault server expressed as a URL and port, for example: https://127.0.0.1:8200/."address of the Vault server expressed as a URL and port, for example: https://127.0.0.1:8200/."vault-client-timeout"vault-client-timeout"timeout variable. The default value is 60s."timeout variable. The default value is 60s."vault-max-retries"vault-max-retries"maximum number of retries when a 5xx error code is encountered. The default is 2, for three total attempts. Set this to 0 or less to disable retrying."maximum number of retries when a 5xx error code is encountered. The default is 2, for three total attempts. Set this to 0 or less to disable retrying."vault-cacert"vault-cacert"path to a PEM-encoded CA certificate file on the local disk. This file is used to verify the Vault server's SSL certificate. This environment variable takes precedence over VAULT_CAPATH."path to a PEM-encoded CA certificate file on the local disk. This file is used to verify the Vault server's SSL certificate. This environment variable takes precedence over VAULT_CAPATH."vault-capath"vault-capath"path to a directory of PEM-encoded CA certificate files on the local disk. These certificates are used to verify the Vault server's SSL certificate."path to a directory of PEM-encoded CA certificate files on the local disk. These certificates are used to verify the Vault server's SSL certificate."vault-client-cert"vault-client-cert"path to a PEM-encoded client certificate on the local disk. This file is used for TLS communication with the Vault server."path to a PEM-encoded client certificate on the local disk. This file is used for TLS communication with the Vault server."vault-client-key"vault-client-key"path to an unencrypted, PEM-encoded private key on disk which corresponds to the matching client certificate."path to an unencrypted, PEM-encoded private key on disk which corresponds to the matching client certificate."vault-skip-verify"vault-skip-verify"do not verify Vault's presented certificate before communicating with it. Setting this variable is not recommended and voids Vault's security model."do not verify Vault's presented certificate before communicating with it. Setting this variable is not recommended and voids Vault's security model."vault-tls-server-name"vault-tls-server-name"name to use as the SNI host when connecting via TLS."name to use as the SNI host when connecting via TLS."vault-token"vault-token"vault authentication token"vault authentication token"tls-cert"tls-cert"TLS certificate for HTTPs"TLS certificate for HTTPs"tls-key"tls-key"TLS key for HTTPs"TLS key for HTTPs"tls-min-version"tls-min-version"1.2"1.2"Minimum accepted TLS version"Minimum accepted TLS version"tls-strict-ciphers"tls-strict-ciphers"Restrict accept ciphers to: ECDHE_RSA_WITH_AES_256_GCM_SHA384, ECDHE_RSA_WITH_AES_256_CBC_SHA, RSA_WITH_AES_256_GCM_SHA384, RSA_WITH_AES_256_CBC_SHA"Restrict accept ciphers to: ECDHE_RSA_WITH_AES_256_GCM_SHA384, ECDHE_RSA_WITH_AES_256_CBC_SHA, RSA_WITH_AES_256_GCM_SHA384, RSA_WITH_AES_256_CBC_SHA"no-tasks"no-tasks"disables the task scheduler"disables the task scheduler"query-concurrency"query-concurrency"the number of queries that are allowed to execute concurrently"the number of queries that are allowed to execute concurrently"query-initial-memory-bytes"query-initial-memory-bytes"the initial number of bytes allocated for a query when it is started. If this is unset, then query-memory-bytes will be used"the initial number of bytes allocated for a query when it is started. If this is unset, then query-memory-bytes will be used"query-memory-bytes"query-memory-bytes"maximum number of bytes a query is allowed to use at any given time. This must be greater or equal to query-initial-memory-bytes"maximum number of bytes a query is allowed to use at any given time. This must be greater or equal to query-initial-memory-bytes"query-max-memory-bytes"query-max-memory-bytes"the maximum amount of memory used for queries. If this is unset, then this number is query-concurrency * query-memory-bytes"the maximum amount of memory used for queries. If this is unset, then this number is query-concurrency * query-memory-bytes"query-queue-size"query-queue-size"the number of queries that are allowed to be awaiting execution before new queries are rejected"the number of queries that are allowed to be awaiting execution before new queries are rejected"feature-flags"feature-flags"feature flag overrides"feature flag overrides"storage-wal-fsync-delay"storage-wal-fsync-delay"The amount of time that a write will wait before fsyncing. A duration greater than 0 can be used to batch up multiple fsync calls. This is useful for slower disks or when WAL write contention is seen."The amount of time that a write will wait before fsyncing. A duration greater than 0 can be used to batch up multiple fsync calls. This is useful for slower disks or when WAL write contention is seen."storage-validate-keys"storage-validate-keys"Validates incoming writes to ensure keys only have valid unicode characters."Validates incoming writes to ensure keys only have valid unicode characters."storage-cache-max-memory-size"storage-cache-max-memory-size"The maximum size a shard's cache can reach before it starts rejecting writes."The maximum size a shard's cache can reach before it starts rejecting writes."storage-cache-snapshot-memory-size"storage-cache-snapshot-memory-size"The size at which the engine will snapshot the cache and write it to a TSM file, freeing up memory."The size at which the engine will snapshot the cache and write it to a TSM file, freeing up memory."storage-cache-snapshot-write-cold-duration"storage-cache-snapshot-write-cold-duration"The length of time at which the engine will snapshot the cache and write it to a new TSM file if the shard hasn't received writes or deletes."The length of time at which the engine will snapshot the cache and write it to a new TSM file if the shard hasn't received writes or deletes."storage-compact-full-write-cold-duration"storage-compact-full-write-cold-duration"The duration at which the engine will compact all TSM files in a shard if it hasn't received a write or delete."The duration at which the engine will compact all TSM files in a shard if it hasn't received a write or delete."storage-compact-throughput-burst"storage-compact-throughput-burst"The rate limit in bytes per second that we will allow TSM compactions to write to disk."The rate limit in bytes per second that we will allow TSM compactions to write to disk."storage-max-concurrent-compactions"storage-max-concurrent-compactions"The maximum number of concurrent full and level compactions that can run at one time.  A value of 0 results in 50% of runtime.GOMAXPROCS(0) used at runtime.  Any number greater than 0 limits compactions to that value.  This setting does not apply to cache snapshotting."The maximum number of concurrent full and level compactions that can run at one time.  A value of 0 results in 50% of runtime.GOMAXPROCS(0) used at runtime.  Any number greater than 0 limits compactions to that value.  This setting does not apply to cache snapshotting."storage-max-index-log-file-size"storage-max-index-log-file-size"The threshold, in bytes, when an index write-ahead log file will compact into an index file. Lower sizes will cause log files to be compacted more quickly and result in lower heap usage at the expense of write throughput."The threshold, in bytes, when an index write-ahead log file will compact into an index file. Lower sizes will cause log files to be compacted more quickly and result in lower heap usage at the expense of write throughput."storage-series-id-set-cache-size"storage-series-id-set-cache-size"The size of the internal cache used in the TSI index to store previously calculated series results."The size of the internal cache used in the TSI index to store previously calculated series results."storage-series-file-max-concurrent-snapshot-compactions"storage-series-file-max-concurrent-snapshot-compactions"The maximum number of concurrent snapshot compactions that can be running at one time across all series partitions in a database."The maximum number of concurrent snapshot compactions that can be running at one time across all series partitions in a database."storage-tsm-use-madv-willneed"storage-tsm-use-madv-willneed"Controls whether we hint to the kernel that we intend to page in mmap'd sections of TSM files."Controls whether we hint to the kernel that we intend to page in mmap'd sections of TSM files."storage-retention-check-interval"storage-retention-check-interval"The interval of time when retention policy enforcement checks run."The interval of time when retention policy enforcement checks run."storage-shard-precreator-check-interval"storage-shard-precreator-check-interval"The interval of time when the check to pre-create new shards runs."The interval of time when the check to pre-create new shards runs."storage-shard-precreator-advance-period"storage-shard-precreator-advance-period"The default period ahead of the endtime of a shard group that its successor group is created."The default period ahead of the endtime of a shard group that its successor group is created."influxql-max-select-point"influxql-max-select-point"The maximum number of points a SELECT can process. A value of 0 will make the maximum point count unlimited. This will only be checked every second so queries will not be aborted immediately when hitting the limit."The maximum number of points a SELECT can process. A value of 0 will make the maximum point count unlimited. This will only be checked every second so queries will not be aborted immediately when hitting the limit."influxql-max-select-series"influxql-max-select-series"The maximum number of series a SELECT can run. A value of 0 will make the maximum series count unlimited."The maximum number of series a SELECT can run. A value of 0 will make the maximum series count unlimited."influxql-max-select-buckets"influxql-max-select-buckets"The maximum number of group by time bucket a SELECT can create. A value of zero will max the maximum number of buckets unlimited."The maximum number of group by time bucket a SELECT can create. A value of zero will max the maximum number of buckets unlimited."http://127.0.0.1:%d"http://127.0.0.1:%d"Stopping"Stopping""nats"Failed closing bolt"Failed closing bolt"Failed closing query service"Failed closing query service"storage-engine"storage-engine"Failed to close engine"Failed to close engine"Failed to closer Jaeger tracer"Failed to closer Jaeger tracer"WithCancelunknown log level; supported levels are debug, info, and error"unknown log level; supported levels are debug, info, and error"Welcome to InfluxDB"Welcome to InfluxDB""commit"build_date"build_date"Tracing via zap logging"Tracing via zap logging"NewTracerSetGlobalTracerTracing via Jaeger"Tracing via Jaeger"SamplerConfigSamplingServerURLMaxOperationsSamplingRefreshIntervalNewSamplerReporterConfigBufferFlushIntervalLogSpansLocalAgentHostPortCollectorEndpointnewTransportHeadersConfigJaegerDebugHeaderJaegerBaggageHeaderTraceContextHeaderNameTraceBaggageHeaderPrefixApplyDefaultsBaggageRestrictionsConfigDenyBaggageOnInitializationFailureHostPortRefreshIntervalThrottlerConfigSynchronousInitializationDisabledRPCMetricsSamplerBaggageRestrictionsThrottlerInitGlobalTracerFromEnvFailed to get Jaeger client config from environment variables"Failed to get Jaeger client config from environment variables"FactoryHistogramOptionsNSOptionsTimerOptionsTraceIDHighLowIsSampledTracesStartedSampledTracesStartedNotSampledTracesJoinedSampledTracesJoinedNotSampledSpansStartedSampledSpansStartedNotSampledSpansFinishedDecodingErrorsReporterSuccessReporterFailureReporterDroppedReporterQueueLengthSamplerRetrievedSamplerQueryFailureSamplerUpdatedSamplerUpdateFailureBaggageUpdateSuccessBaggageUpdateFailureBaggageTruncateBaggageRestrictionsUpdateSuccessBaggageRestrictionsUpdateFailureThrottledDebugSpansThrottlerUpdateSuccessThrottlerUpdateFailurepoolSpansgen128BitzipkinSharedRPCSpanhighTraceIDGeneratormaxTagValueLengthInjectorSpanIDtraceIDspanIDparentIDbaggagedebugIDIsDebugParentIDCopyFromWithBaggageItemisDebugIDContainerOnlyExtractorcompositeObserverContribObserverContribSpanObserverOnFinishOnSetOperationNameOnSetTagOnStartSpanProcessRestrictionManagerRestrictionkeyAllowedmaxValueLengthKeyAllowedMaxValueLengthGetRestrictionbaggageSetterrestrictionManagersetBaggagelogFieldshostIPv4samplertimeNowrandomNumberspanPoolinjectorsextractorsobserverprocessbaggageRestrictionManagerdebugThrottleraddCodecstartSpanWithOptionsnewSpanstartSpanInternalreportSpanrandomIDisDebugAllowedoperationNamefirstInProcesssetTagNoLockinglogFieldsNoLockingappendLogOperationNameSpanObservercontribObserversFailed to instantiate Jaeger tracer"Failed to instantiate Jaeger tracer"Failed opening bolt"Failed opening bolt"kvstore-bolt"kvstore-bolt"BTreeitemsinsertAtremoveAttruncatecopyOnWriteContextFreeListnewNodefreeNodecowmutableFormutableChildmaybeSplitChildgrowChildAndRemoveiterateprintdegreemaxItemsminItemsReplaceOrInsertDeleteMinDeleteMaxdeleteItemAscendRangeAscendLessThanAscendGreaterOrEqualAscendDescendRangeDescendLessOrEqualDescendGreaterThanDescendbtreeascenddescendunknown store type %s; expected bolt or memory"unknown store type %s; expected bolt or memory"MigratorMigrationNameSpecsAddMigrationslogMigrationEventputMigrationdeleteMigrationNewMigrator"migrations"Failed to initialize kv migrator"Failed to initialize kv migrator"Failed to apply migrations"Failed to apply migrations"NewRegistryprom_registry"prom_registry"NewGoCollectorNewInfluxCollectorNewSystem"new"WithSuffix"kv"OpLogServiceOpLogStoreopLogStoreNewOpLogServiceFailed creating new authorization store"Failed creating new authorization store"StorageListSecretFailed creating new meta store"Failed creating new meta store"SecreteServiceNewMetricServicesecretService"vault"WrappingLookupFuncwrappingLookupFuncmfaCredspolicyOverrideSetAddressSetLimiterSetMaxRetriesSetClientTimeoutSetOutputCurlStringCurrentWrappingLookupFuncSetWrappingLookupFuncSetMFACredsSetNamespacesetNamespaceSetTokenClearTokenSetBackoffSetPolicyOverrideRawRequestRawRequestWithContextLogicalNewRenewerSSHSSHWithMountPointSSHHelperSSHHelperWithMountPointloadSecretsputSecretsConfigOptFnWithConfigFailed initializing vault secret service"Failed initializing vault secret service"unknown secret service %q, expected "bolt" or "vault""unknown secret service %q, expected \"bolt\" or \"vault\""Failed setting secret service"Failed setting secret service"Failed creating chronograf service"Failed creating chronograf service"Failed to open meta client"Failed to open meta client"WithMetaClientFailed to open engine"Failed to open engine"DependenciesStorageDependenciesFromDependenciesStorageReaderReadFilterSpecNode_TypeisNode_ValueChildrenGetNodeTypeGetChildrenGetStringValueGetBooleanValueGetIntegerValueGetUnsignedValueGetFloatValueGetRegexValueGetTagRefValueGetFieldRefValueGetLogicalGetComparisonXXX_OneofWrappersMarshalToSizedBufferGetRootReadGroupSpecGroupModeGroupKeysAggregateMethodspecReadTagKeysSpecReadTagValuesSpecReadWindowAggregateSpecProcedureKindgetTruncateFuncGetEarliestBoundsGetOverlappingBoundsWindowEveryAggregatesCreateEmptyTimeColumnReadFilterReadGroupReadTagKeysReadTagValuesReadWindowAggregateBucketLookupOrganizationLookupctxLabelKeysrequestDurgetLabelValuesrecordMetricsBucketDependenciesAllBucketLookupFindAllBucketsToDependenciesFromDepsBucketDepsToDepsReadWriteCloserValidatorFilesystemServiceURLValidatorStorageDepsFluxDepsNewDependenciesReadFilterRequestXXX_MessageNameProtoSizeTimestampRangeReadSourceResultSetReadGroupRequestReadGroupRequest_GroupAggregateAggregate_AggregateTypeHintFlagsNoPointsSetNoPointsNoSeriesSetNoSeriesHintSchemaAllTimeSetHintSchemaAllTimeGroupResultSetGroupCursorPartitionKeyValsTagKeysRequestTagsSourceStringIteratorTagValuesRequestReadWindowAggregateRequestNsecsNegativeWindowAggregatefindShardIDsvalidateArgstagKeysWithFieldPredicatemeasurementFieldstagValuesSlowFailed to get query controller dependencies"Failed to get query controller dependencies"storage-reads"storage-reads"Failed to create query controller"Failed to create query controller"NewProxyQueryServiceAnalyticalStorageRunRecordercombineRunsNewAnalyticalStoragetask-analytical-store"task-analytical-store"QueryServiceBridgeAsyncQueryServiceexecutorOptionexecutorConfigmaxWorkersNewExecutortask-executor"task-executor"WithFlaggertask-scheduler"task-scheduler"NoopSchedulerSchedulerMetricsexecutingTasksTreeSchedulerErrorFuncWhenupdateNextSchedulableServiceUpdateLastScheduledtoInserttoDeletepriorityQueuenextTimeonErrworkchanscheckpointerresetTimeriteratorworktotalExecuteCallstotalExecuteFailurescheduleCallsscheduleFailsreleaseCallsscheduleDelayexecuteDeltaschedulescheduleFailreportScheduleDelayreportExecutiontreeSchedulerOptFuncNewSchedulerSchedulableTaskServiceUpdateTaskServiceNewSchedulableTaskServiceWithOnErrorFnerror in scheduler run"error in scheduler run""taskID""scheduledAt"could not start task scheduler"could not start task scheduler"task-coordinator"task-coordinator"CoordinatorexTaskCreatedTaskUpdatedTaskDeletedRunCancelledRunRetriedRunForcedCoordinatorOptionNewCoordinatorCoordinatingTaskServiceTaskResumerTaskNotifyCoordinatorOfExistingFailed to resume existing tasks"Failed to resume existing tasks"AuthorizedServiceNewAuthorizedServiceControllerMetricsRequestsNotImplementedRequestsLatencyExecutingDurationNewControllerMetricsLocalShardMapperMapShardsmapShardsConfiguring InfluxQL statement executor (zeros indicate unlimited)."Configuring InfluxQL statement executor (zeros indicate unlimited)."max_select_point"max_select_point"max_select_series"max_select_series"max_select_buckets"max_select_buckets"StatementExecutorExecutionContextRowPartialSameSeriestagsHashtagsKeysStatementIDStatisticsGathererStatisticsCollectorExecutionOptionsQuietstatementIDectxExecuteStatementStatementNormalizerNormalizeStatementShardMapperSelectOptionsMaxPointNMaxBucketsNIteratorCreatorexecuteExplainStatementexecuteExplainAnalyzeStatementexecuteSelectStatementcreateIteratorsexecuteShowDatabasesStatementgetDefaultRPexecuteDeleteSeriesStatementexecuteDropMeasurementStatementexecuteShowMeasurementsStatementexecuteShowRetentionPoliciesStatementexecuteShowTagKeysexecuteShowTagValuesnormalizeMeasurement"svc"CoordinatingCheckServicetaskServiceendpointStoresecretSVCcreateNotificationEndpointupdateNotificationEndpointpatchNotificationEndpointPutNotificationEndpointfindNotificationEndpointByIDfindNotificationEndpointsdeleteNotificationEndpointRuleServiceinitializeNotificationRulenotificationRuleBucketcreateNotificationRulecreateNotificationTaskupdateNotificationTaskPutNotificationRuleputNotificationRulefindNotificationRuleByIDfindNotificationRulesforEachNotificationRuledeleteNotificationRuleCoordinatingNotificationRuleStorebyOrganisationIndexfindTelegrafConfigByIDfindTelegrafConfigsPutTelegrafConfigputTelegrafConfigputTelegrafConfigStatscreateTelegrafConfigupdateTelegrafConfigdeleteTelegrafConfigdeleteTelegrafConfigStatsNewDefaultServerOptionsRandomPortFailed to start nats streaming server"Failed to start nats streaming server"AsyncPublisherAckHandlerSubscriptionOptionSubscriptionOptionsMaxInflightAckWaitStartAtManualAcksNatsConnPublishAsyncNewAsyncPublishernats-publisher-%d"nats-publisher-%d"Failed to connect to streaming server"Failed to connect to streaming server"QueueSubscriberNewQueueSubscribernats-subscriber-%d"nats-subscriber-%d"MetricsSubject"metrics"RecorderHandlerRecorderMetricsCollectionMetricsSlicePointsNewRecorderHandlerPointWriterPublisherTargetsdoGatherSubscriberFailed to create scraper subscriber"Failed to create scraper subscriber"scraper"scraper"Failed scraper service"Failed scraper service"DefaultFlaggerByKeyFnoverridesbyKeycoerceMakeByKeyFailed to configure feature flag overrides"Failed to configure feature flag overrides""overrides"Running with feature flag overrides"Running with feature flag overrides"ExpireAtFindSessionByKeyFindSessionByIDRefreshSessionDeleteSessionuserServiceurmServiceidGentokenGendisableAuthorizationsForMaxPermissionsWithMaxPermissionFuncgetPermissionSetServiceOptionNewStorageSessionStoretimerExpireFuncNewSessionStoreWithSessionLengthSessionMetricsNewSessionMetricsSessionLoggersessionServiceNewSessionLoggerFailed creating new labels store"Failed creating new labels store"OnboardServiceOptionFnOnboardServicealwaysAllowonboardUserWithAlwaysAllowInitialUserNewOnboardServiceAuthedOnboardSvcNewAuthedOnboardSvcOnboardingMetricsonboardingServiceNewOnboardingMetricsOnboardingLoggerNewOnboardingLoggeronboard"onboard"SetPasswordHashCachingPasswordsServiceinnerNewCachingPasswordsServiceAuthTokenFinderPasswordComparerUserFinderAuthV1AuthV2ComparercheckAuthErrortryV1AuthorizationtryV2AuthorizationnormalizeErroropLogfindDashboardByIDFindDashboardfindOrganizationDashboardsfindDashboardscreateCellViewaddDashboardCellfindDashboardCellViewdeleteDashboardCellViewputDashboardCellViewPutDashboardputOrganizationDashboardIndexremoveOrganizationDashboardIndexputDashboardputDashboardWithMetaforEachDashboarddeleteDashboardappendDashboardEventToLogAuthorizationFinderBucketFinderOrganizationFinderDashboardFinderSourceFinderTaskFinderTelegrafConfigFinderVariableFinderTargetFinderCheckFinderNotificationEndpointFinderNotificationRuleFinderLoggingPointsWriterUnderlyingLogBucketNameNoopProxyHandlerProxyExecutorNewProxyExecutorrequestBytesresponseBytesNewEventRecorderNewFlagsHandler"pkger"NameGeneratorCreateStackReadStackByIDapplyReqLimitnameGentimeGenbucketSVCcheckSVClabelSVCendpointSVCruleSVCtaskSVCteleSVCvarSVCuninstallStackapplyStackUpdatecloneOrgResourcescloneOrgBucketscloneOrgCheckscloneOrgDashboardscloneOrgLabelscloneOrgNotificationEndpointscloneOrgNotificationRulescloneOrgTaskscloneOrgTelegrafscloneOrgVariablesfilterOrgResourceKindsdryRundryRunBucketsdryRunChecksdryRunDashboardsdryRunLabelsdryRunNotificationEndpointsdryRunNotificationRulesdryRunSecretsdryRunTasksdryRunTelegrafConfigsdryRunVariablesdryRunLabelMappingsdryRunResourceLabelMappingaddStackStateapplyStateapplyBucketsrollbackBucketsapplyBucketapplyChecksrollbackChecksapplyCheckapplyDashboardsapplyDashboardrollbackDashboardsapplyLabelsrollbackLabelsapplyLabelapplyNotificationEndpointsapplyNotificationEndpointrollbackNotificationEndpointsapplyNotificationGeneratorapplyNotificationRulesapplyNotificationRulerollbackNotificationRulesapplyTasksapplyTaskrollbackTasksapplyTelegrafsapplyTelegrafConfigrollbackTelegrafConfigsapplyVariablesrollbackVariablesapplyVariableremoveLabelMappingsrollbackRemoveLabelMappingsapplyLabelMappingsrollbackLabelMappingstemplateFromApplyOptsgetStackRemoteTemplatesupdateStackAfterSuccessupdateStackAfterRollbackfindLabelgetAllPlatformVariablesgetAllChecksgetNotificationRulesgetAllTasksServiceSetterFnserviceOptStoreKVindexBaselistStacksByIDentStoreBaseindexStoreBaseviewNewStoreKVWithBucketSVCWithCheckSVCWithDashboardSVCWithLabelSVCAuthedLabelServiceNewAuthedLabelServiceWithNotificationEndpointSVCWithNotificationRuleSVCWithOrganizationServiceWithSecretSVCWithTaskSVCWithTelegrafSVCWithVariableSVCSVCMiddlewareMWTracingMWMetricsMWLoggingMWAuthHTTPServerStackslistStackscreateStackdeleteStackreadStackupdateStack"stacks"NewHTTPServerStacksHTTPServerTemplatesencResp"templates"NewHTTPServerTemplatesOnboardHandleronboardingSvchandleIsOnboardinghandleInitialOnboardRequesthandleOnboardRequestNewHTTPOnboardHandlerLabelHandlerhandlePostLabelhandleGetLabelhandleGetLabelshandlePatchLabelhandleDeleteLabelLabelLoggerlabelServiceNewLabelLoggerLabelMetricsNewLabelMetricsNewHTTPLabelHandlerv1_authorization"v1_authorization"AuthedPasswordServiceAuthFinderNewAuthedPasswordServiceSessionHandlerSignInResourceHandlerSignOutResourceHandlerhandleSigninhandleSignoutNewSessionHandlerAuthedSvcNewAuthedServiceDashboardHandlerhandleGetDashboardshandlePostDashboardhandleGetDashboardhandleDeleteDashboardhandlePatchDashboardhandlePostDashboardCellhandlePutDashboardCellshandleGetDashboardCellViewhandlePatchDashboardCellViewhandleDeleteDashboardCellhandlePatchDashboardCelllookupOrgByDashboardIDNewURMHandler"urm"AuthedURMServiceNewAuthedURMServiceLabelEmbeddedHandlerhandlePostLabelMappinghandleFindResourceLabelshandleDeleteLabelMappingNewHTTPEmbeddedHandlerNewDashboardHandlerPlatformHandlerAssetHandlerDocsHandlerAPIHandlerLegacyHandlerAPIHandlerOptFnNewPlatformHandlerResourceHandlerWithResourceHandlerinitMetricsHandlerOptFnhandlerOptsapiHandlerdebugHandlerhealthHandlermetricsHandlerreadyHandlerNewHandlerFromRegistry"platform"WithAPIHandlerDebugLevelMiddlewareLoggingMWDebugFlushfailed http listener"failed http listener"failed to load x509 key pair"failed to load x509 key pair"VersionTLS127711.0"1.0"Setting the minimum version of TLS to 1.0 - this is discouraged. Please use 1.2 or 1.3"Setting the minimum version of TLS to 1.0 - this is discouraged. Please use 1.2 or 1.3"VersionTLS107691.1"1.1"Setting the minimum version of TLS to 1.1 - this is discouraged. Please use 1.2 or 1.3"Setting the minimum version of TLS to 1.1 - this is discouraged. Please use 1.2 or 1.3"VersionTLS117701.3"1.3"VersionTLS13772TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA38449200TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA49172TLS_RSA_WITH_AES_256_GCM_SHA384TLS_RSA_WITH_AES_256_CBC_SHACurveP521CurveP384CurveP256Listening"Listening""transport""addr""port"ErrServerClosedFailed https service"Failed https service"Failed http service"Failed http service"Failed to retrieve buckets"Failed to retrieve buckets"Checking InfluxDB metadata for prior version."Checking InfluxDB metadata for prior version."bolt_path"bolt_path"Missing metadata for bucket."Missing metadata for bucket."Incompatible InfluxDB 2.0 metadata found. File must be moved before influxd will start."Incompatible InfluxDB 2.0 metadata found. File must be moved before influxd will start.""_series"Found directory that is incompatible with this version of InfluxDB."Found directory that is incompatible with this version of InfluxDB."Incompatible InfluxDB 2.0 version found. Move all files outside of engine_path before influxd will start."Incompatible InfluxDB 2.0 version found. Move all files outside of engine_path before influxd will start."engine_path"engine_path"incompatible InfluxDB version"incompatible InfluxDB version" needed to add pprof to our binary. needed for tsm1 needed for tsi1 BoltStore stores all REST resources in boltdb. MemoryStore stores all REST resources in memory (useful for testing). LogTracing enables tracing via zap logs JaegerTracing enables tracing via the Jaeger client library Max Integer exit with SIGINT and SIGTERM Attempt clean shutdown. 60 minutes storage configuration limits InfluxQL Coordinator Config Launcher represents the main program execution. in minutes Query options. storage engine InfluxQL query engine NewLauncher returns a new instance of Launcher connected to standard in/out/err. Running returns true if the main Launcher has started running. ReportingDisabled is true if opted out of usage stats. Registry returns the prometheus metrics registry. Log returns the launchers logger. URL returns the URL to connect to the HTTP server. NatsURL returns the URL to connection to the NATS server. Engine returns a reference to the storage engine. It should only be called for end-to-end testing purposes. Shutdown shuts down the HTTP server and waits for all services to clean up. Cancel executes the context cancel on the program. Used for testing. Run executes the program with the given CLI arguments. apply migrations to metadata store If it is bolt, then we already set it above. The vault secret service is configured using the standard vault environment variables. https://www.vaultproject.io/docs/commands/index.html#environment-variables the testing engine will write/read into a temporary directory check for 2.x data / state from a prior 2.x The Engine's metrics must be registered after it opens. create the task stack tasks service notification middleware which keeps task service up to date with persisted changes to notification rules. NATS streaming server updated with random port TODO(jm): this is an example of using a subscriber to consume from the channel. It should be removed. basic service with auth with metrics with logging resourceResolver is a deprecated type which combines the lookups of multiple resources into one type, used to resolve the resources associated org ID or name . It is a stop-gap while we move this behaviour off of *kv.Service to aid in reducing the coupling on this type. Wrap the BucketService in a storage backed one that will ensure deleted buckets are removed from the storage engine. feature flagging for new labels service feature flagging for new authorization service If we are in testing mode we allow all data to be flushed and removed. Sensible default nil uses the default cipher suite TLS 1.3 does not support configuring the Cipher suites if there are no buckets, we will be fine see if there are existing files which match the old directory structure OrganizationService returns the internal organization service. QueryController returns the internal query service. BucketService returns the internal bucket service. UserService returns the internal user service. UserResourceMappingService returns the internal user resource mapping service. AuthorizationService returns the internal authorization service. SecretService returns the internal secret service. TaskService returns the internal task service. TaskControlService returns the internal store service. CheckService returns the internal check service. KeyValueService returns the internal key-value service.stateTaskstateLabelstateStatusparserLabelexistingdiffLabelshouldApplytoInfluxLabellabelAssociationsparserTaskdiffTaskstateIdentitymetaqueryAttributesstateDashboardparserDashdiffDashboardItemIteratorstateCheckparserCheckdiffCheckstateCoordinatorstateBucketparserBktdiffBucketstateEndpointparserEndpointdiffEndpointstateRuleparserRuleendpointAssociationdiffRuleendpointIDendpointTemplateNameendpointTypestateTelegrafparserTelegrafdiffTelegrafstateVariableparserVardiffVariablestateLabelMappingdiffLabelMappinglmstateLabelMappingForRemovalmEndpointsmRuleslabelMappingsToRemovetelegrafConfigssummarygetLabelByMetaNametemplateToStateLabelsreconcileStackResourcesreconcileLabelMappingsreconcileNotificationDependenciessetObjectIDaddObjectForRemovalgetObjectIDSetterClientTokenMFAHeaderValsWrapTTLObjBodyBytesBodySizePolicyOverrideSetJSONBodyResetJSONBodyToHTTPtoRetryableHTTPappliercreaterapplyErrBodyrollbackerShowTagValuesStatementTagKeyExprMigrationStateresClonecloneResFncloneFnAuditHashListAuditEnableAuditEnableAuditWithOptionsDisableAuditListAuthEnableAuthEnableAuthWithOptionsDisableAuthCapabilitiesSelfCapabilitiesCORSStatusConfigureCORSDisableCORSGenerateRootStatusGenerateDROperationTokenStatusgenerateRootStatusCommonGenerateRootInitGenerateDROperationTokenInitgenerateRootInitCommonGenerateRootCancelGenerateDROperationTokenCancelgenerateRootCancelCommonGenerateRootUpdateGenerateDROperationTokenUpdategenerateRootUpdateCommonHealthInitStatusRenewRevokePrefixRevokeForceRevokeWithOptionsListMountsUnmountRemountTuneMountMountConfigListPluginsGetPluginRegisterPluginDeregisterPluginListPoliciesGetPolicyPutPolicyDeletePolicyRekeyStatusRekeyRecoveryKeyStatusRekeyVerificationStatusRekeyRecoveryKeyVerificationStatusRekeyInitRekeyRecoveryKeyInitRekeyCancelRekeyRecoveryKeyCancelRekeyVerificationCancelRekeyRecoveryKeyVerificationCancelRekeyUpdateRekeyRecoveryKeyUpdateRekeyRetrieveBackupRekeyRetrieveRecoveryBackupRekeyDeleteBackupRekeyDeleteRecoveryBackupRekeyVerificationUpdateRekeyRecoveryKeyVerificationUpdateRotateKeyStatusSealStatusResetUnsealProcessUnsealUnsealWithOptionsStepDownfreeTypeMountPointCredentialSignKeyDeleteSeriesStatementtruncateFuncSubsettoRemovedirectionRenewerInputSecretAuthAccessorPoliciesTokenPoliciesIdentityPoliciesOrphanLeaseDurationRenewableSecretWrapInfoCreationTimeCreationPathWrappedAccessorLeaseIDWarningsWrapInfoTokenIDTokenAccessorTokenRemainingUsesTokenMetadataTokenIsRenewableTokenTTLGraceRenewBufferRenewerRenewOutputRenewedAtgracerenewChDoneChRenewChrenewAuthrenewLeasesleepDurationcalculateGraceNode_ComparisonSeeAlsoNode_LogicalLocalShardMappingShardMapReadWithDataUnwrapencoderPairrollbackCoordinatorrollbacksrunTilEndGenerateRootStatusResponseStartedProgressEncodedTokenEncodedRootTokenPGPFingerprintOTPOTPLengthTokenAuthCreateOrphanCreateWithRoleLookupAccessorLookupSelfRenewSelfRenewTokenAsSelfRevokeAccessorRevokeOrphanRevokeSelfRevokeTreeRekeyStatusResponsePGPFingerprintsVerificationRequiredVerificationNonceRekeyRetrieveResponseKeysB64CORSResponseAllowedOriginsRekeyVerificationStatusResponseMountInputMountConfigInputMaxLeaseTTLForceNoCacheAuditNonHMACRequestKeysAuditNonHMACResponseKeysListingVisibilityPassthroughRequestHeadersAllowedResponseHeadersSealWrapReaderFuncAuditSSHVerifyResponseRoleNameSealStatusResponseInitializedSealedClusterNameRecoverySealRekeyInitRequestSecretSharesSecretThresholdStoredSharesPGPKeysRequireVerificationRekeyVerificationUpdateResponseInstallTimeLeaderResponseHAEnabledIsSelfLeaderAddressLeaderClusterAddressPerfStandbyPerfStandbyLastRemoteWALLastWALHealthResponseStandbyPerformanceStandbyReplicationPerformanceModeReplicationDRModeServerTimeUTCEnableAuditOptionsDeregisterPluginInputPluginTypeMountConfigOutputCORSRequestRegisterPluginInputSHA256RevokeOptionsForceListPluginsInputListPluginsResponsePluginsByTypeUnsealOptsMountOutputInitRequestRecoverySharesRecoveryThresholdRecoveryPGPKeysRootTokenPGPKeyInitResponseRecoveryKeysRecoveryKeysB64RootTokenGetPluginInputGetPluginResponseBuiltinRekeyUpdateResponseTokenCreateRequestExplicitMaxTTLNoParentNoDefaultPolicyNumUses/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/launcher/launcher_helpers.gotblargstofqgotErrrawurlmetricNamemfsystemColsdtoexpfmtinfluxdbcontextgithub.com/influxdata/flux/lang"github.com/influxdata/flux/lang"github.com/influxdata/influxdb/v2/mock"github.com/influxdata/influxdb/v2/mock"github.com/prometheus/client_model/go"github.com/prometheus/client_model/go"github.com/prometheus/common/expfmt"github.com/prometheus/common/expfmt"MultiWriter--store"--store"--e2e-testing"--e2e-testing"--testing-always-allow-setup"--testing-always-allow-setup"--bolt-path"--bolt-path"--engine-path"--engine-path"--http-bind-address"--http-bind-address"127.0.0.1:0"127.0.0.1:0"--log-level"--log-level"USER"USER"PASSWORD"PASSWORD"BUCKET"BUCKET"/api/v2/write?org=%s&bucket=%s"/api/v2/write?org=%s&bucket=%s"unexpected status code: %d, body: %s, headers: %v"unexpected status code: %d, body: %s, headers: %v"SetAuthorizerAllowAllNewMockAuthorizerAnnotateFluxCompilerSimpleQuery"\r"LabelClientServiceNewNotificationRuleService/metrics"/metrics"unexpected status code: %d %s"unexpected status code: %d %s"TextParsermetricFamiliesByNamelineCountcurrentBytecurrentTokencurrentMFcurrentMetriccurrentLabelPaircurrentLabelssummariescurrentQuantilehistogramscurrentBucketcurrentIsSummaryCountcurrentIsSummarySumcurrentIsHistogramCountcurrentIsHistogramSumTextToMetricFamiliesstartOfLinestartCommentreadingMetricNamereadingLabelsstartLabelNamestartLabelValuereadingValuestartTimestampreadingHelpreadingTypeparseErrorskipBlankTabskipBlankTabIfCurrentBlankTabreadTokenUntilWhitespacereadTokenUntilNewlinereadTokenAsMetricNamereadTokenAsLabelNamereadTokenAsLabelValuesetOrCreateCurrentMFquery_influxdb_source_read_request_duration_seconds"query_influxdb_source_read_request_duration_seconds"%v
"%v\n""op"DeepEqualgot %v, expected %v"got %v, expected %v"result has %d tables, expected %d. Tables: %s"result has %d tables, expected %d. Tables: %s" TestLauncher is a test wrapper for launcher.Launcher. Root temporary directory for all data. Initialized after calling the Setup() helper. Standard in/out/err buffers. Flag to act as standard server: disk store, no-e2e testing flag NewTestLauncher returns a new instance of TestLauncher. NewTestLauncherServer returns a new instance of TestLauncher configured as real server (disk store, no e2e flag). RunTestLauncherOrFail initializes and starts the server. Run executes the program with additional arguments to set paths and ports. Passed arguments will overwrite/add to the default ones. Shutdown stops the program and cleans up temporary paths. ShutdownOrFail stops the program and cleans up temporary paths. Fail on error. Setup creates a new user, bucket, org, and auth token. SetupOrFail creates a new user, bucket, org, and auth token. Fail on error. OnBoard attempts an on-boarding request. The on-boarding status is also reset to allow multiple user/org/buckets to be created. OnBoardOrFail attempts an on-boarding request or fails on error. WriteOrFail attempts a write to the organization and bucket identified by to or fails if there is an error. WritePoints attempts a write to the organization and bucket used during setup. WritePointsOrFail attempts a write to the organization and bucket used during setup or fails if there is an error. MustExecuteQuery executes the provided query panicking if an error is encountered. Callers of MustExecuteQuery must call Done on the returned QueryResults. ExecuteQuery executes the provided query against the ith query node. Callers of ExecuteQuery must call Done on the returned QueryResults. QueryAndConsume queries InfluxDB using the request provided. It uses a function to consume the results obtained. It returns the first error encountered when requesting the query, consuming the results, or executing the query. iterate over results to populate res.Err() QueryAndNopConsume does the same as QueryAndConsume but consumes results with a nop function. FluxQueryOrFail performs a query to the specified organization and returns the results or fails if there is an error. QueryFlux returns the csv response from a flux query. It also removes all the \r to make it easier to write tests. remove all \r as well as the extra terminating \n MustNewHTTPRequest returns a new nethttp.Request with base URL and auth attached. Fail on error. NewHTTPRequest returns a new nethttp.Request with base URL and auth attached. NewHTTPRequestOrFail returns a new nethttp.Request with base URL and auth attached. Fail on error. Services QueryResult wraps a single flux.Result with some helper methods. HasTableWithCols checks if the desired number of tables and columns exist, ignoring any system columns. If the result is not as expected then the testing.T fails. _start, _stop, _time, _f TablesN returns the number of tables for the result. QueryResults wraps a set of query results with some helper methods. First returns the first QueryResult. When there are not exactly 1 table First will fail. HasTableCount asserts that there are n tables in the result. Names returns the sorted set of result names for the query results. SortedNames returns the sorted set of table names for the query results./Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/launcher/launcher_option.go WithInfluxQLMaxSelectSeriesN configures the maximum number of series returned by a select statement. WithInfluxQLMaxSelectBucketsN configures the maximum number of buckets returned by a select statement./Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/main.goversionCmdupgradegithub.com/influxdata/influxdb/v2/cmd/influxd/inspect"github.com/influxdata/influxdb/v2/cmd/influxd/inspect"github.com/influxdata/influxdb/v2/cmd/influxd/launcher"github.com/influxdata/influxdb/v2/cmd/influxd/launcher"github.com/influxdata/influxdb/v2/cmd/influxd/upgrade"github.com/influxdata/influxdb/v2/cmd/influxd/upgrade"Print the influxd server version"Print the influxd server version"InfluxDB %s (git: %s) build_date: %s
"InfluxDB %s (git: %s) build_date: %s\n" upgrade binds options to env variables, so it must be added after rootCmd is initialized/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/upgrade/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/upgrade/config.goCopyDirDirSizeconfigMapRulesconfigUpgraderconfigV1configValueTransformsfluxInitializedhomeOrAnyDirinfluxConfigPathV1influxDBv1influxDBv2influxDirV1loadV1ConfignewInfluxDBv1newInfluxDBv2newSecurityUpgradeHelperopenV1MetaoptionsV1optionsV2runUpgradeEsaveLocalConfigsecurityUpgradeHelpersetupAdminupgradeConfigupgradeDatabasesupgradeUsersv1DumpMetaCommandv1DumpMetaOptionsv2DumpMetaCommandv2DumpMetaOptionsvalidatePathsretcAnybomcTransformedconfigFileV2targetOptionsv1ConfigvalueTransformsupdateV2Configsavechildgolang.org/x/text/encoding/unicode"golang.org/x/text/encoding/unicode"golang.org/x/text/transform"golang.org/x/text/transform"data.dir"data.dir"data.wal-fsync-delay"data.wal-fsync-delay"data.validate-keys"data.validate-keys"data.cache-max-memory-size"data.cache-max-memory-size"data.cache-snapshot-memory-size"data.cache-snapshot-memory-size"data.cache-snapshot-write-cold-duration"data.cache-snapshot-write-cold-duration"data.compact-full-write-cold-duration"data.compact-full-write-cold-duration"data.compact-throughput-burst"data.compact-throughput-burst"data.max-concurrent-compactions"data.max-concurrent-compactions"data.max-index-log-file-size"data.max-index-log-file-size"data.series-id-set-cache-size"data.series-id-set-cache-size"data.series-file-max-concurrent-snapshot-compactions"data.series-file-max-concurrent-snapshot-compactions"data.tsm-use-madv-willneed"data.tsm-use-madv-willneed"retention.check-interval"retention.check-interval"shard-precreation.check-interval"shard-precreation.check-interval"shard-precreation.advance-period"shard-precreation.advance-period"coordinator.max-concurrent-queries"coordinator.max-concurrent-queries"coordinator.max-select-point"coordinator.max-select-point"coordinator.max-select-series"coordinator.max-select-series"coordinator.max-select-buckets"coordinator.max-select-buckets"logging.level"logging.level"http.bind-address"http.bind-address"http.https-certificate"http.https-certificate"http.https-private-key"http.https-private-key"BindAddressHttpsEnabledAuthEnabledHttpdbURL1.x config file '%s' does not exist"1.x config file '%s' does not exist"TransformerBOMOverrideSpanningTransformerNopcliConfigsPathcqPathconfigPathorgName4380666Could not save upgraded config to %s, trying to save it to user's home."Could not save upgraded config to %s, trying to save it to user's home." Configuration file upgrade implementation. The strategy is to transform only those entries for which rule exists. configMapRules is a map of transformation rules configValueTransforms is a map from 2.x config keys to transformation functions that should run on the 1.x values before they're written into the 2.x config. Transform config values of 0 into 10 (the new default). query-concurrency used to accept 0 as a representation of infinity, but the 2.x controller now forces a positive value to be chosen for the parameter. load 1.x config content into byte array parse it into simplified v1 config used as return value parse into a generic config map From master-1.x/cmd/influxd/run/config.go: Handle any potential Byte-Order-Marks that may be in the config file. This is for Windows compatibility only. See https://github.com/influxdata/telegraf/issues/1378 and https://github.com/influxdata/influxdb/issues/8965. upgradeConfig upgrades existing 1.x configuration file to 2.x influxdb.toml file. create and initialize helper update new config with upgrade command options configUpgrader is a helper used by `upgrade-config` command. permission issue possible - try to save the file to home or current dir Credits: @rogpeppe (Roger Peppe)metaDirdbDirpopulateDirsauthSvcV2logPathcheckDbBucketssortUserInfo/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/upgrade/database.gotargetPathdbv2shardsNumsourcePathcqheaderPaddingmaxNameLencqFiledb2BucketIdsdirFilterFuncdiskInfosize2targetDataPathtargetWalPathv1optsv2v2dirv2optshumanizegithub.com/dustin/go-humanize"github.com/dustin/go-humanize""wal"'_'No database found in the 1.x meta"No database found in the 1.x meta"Checking space"Checking space"error getting size of %s: %w"error getting size of %s: %w"DiskStatusUsedAvailerror getting info of disk %s: %w"error getting info of disk %s: %w"Disk space info"Disk space info"Free space"Free space"Requested space"Requested space"not enough space on target disk of %s: need %d, available %d "not enough space on target disk of %s: need %d, available %d "O_APPEND520O_WRONLY5214200644error opening file for CQ export %s: %w"error opening file for CQ export %s: %w"Upgrading databases"Upgrading databases"_internal"_internal"Skipping _internal "Skipping _internal "Upgrading database "Upgrading database "Upgraded from v1 database %s with retention policy %s"Upgraded from v1 database %s with retention policy %s"Creating bucket "Creating bucket "error creating bucket %s: %w"error creating bucket %s: %w"Creating database with retention policy"Creating database with retention policy"DefaultRetentionPolicyNameerror creating database %s: %w"error creating database %s: %w"Creating mapping"Creating mapping"retention policy"retention policy""bucketID"error creating mapping  %s/%s -> Org %s, bucket %s: %w"error creating mapping  %s/%s -> Org %s, bucket %s: %w"Creating shard group"Creating shard group"Copying data"Copying data""target"error copying v1 data from %s to %s: %w"error copying v1 data from %s to %s: %w"Copying wal"Copying wal"Empty retention policy"Empty retention policy"name: %s
"name: %s\n"name%[1]squery
----%[1]s-----
"name%[1]squery\n----%[1]s-----\n"RepeatExporting CQ"Exporting CQ"cq_name"cq_name"%s%s%s
"%s%s%s\n"error exporting continuous query %s from DB %s: %w"error exporting continuous query %s from DB %s: %w" upgradeDatabases creates databases, buckets, retention policies and shard info according to 1.x meta and copies dataskip internal databases Check space read each database / retention policy from v1.meta and create bucket db-name/rp-name create database in v2.meta copy shard info from v1.meta export any continuous queries db to buckets IDs mappingempty retention policy doesn't have data Output CQs in the same format as SHOW CONTINUOUS QUERIES 4 == len("name"), the column header/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/upgrade/fs.godstPathentryNamesrcPathdirRenameFuncfileFilterFunc513source is not a directory"source is not a directory"destination '%s' already exists"destination '%s' already exists"ModeSymlink134217728 DirSize returns total size in bytes of containing files CopyFile copies the contents of the file named src to the file named by dst. The file will be created if it does not already exist. If the destination file exists, all it's contents will be replaced by the contents of the source file. The file mode will be copied from the source and the copied data is synced/flushed to stable storage.out, err := os.Create(dst) CopyDir recursively copies a directory tree, attempting to preserve permissions. Source directory must exist, destination directory must *not* exist. Symlinks are ignored and skipped. dirRenameFunc is a mapping function that transforms path to a new name. Returning the path specifies the directory should not be renamed. dirFilterFunc ignores all directories where dirFilterFunc(path) is true. Passing nil for dirFilterFunc includes all directories. fileFilterFunc ignores all files where fileFilterFunc(path) is true. Passing nil for fileFilterFunc includes all files. Skip symlinks./Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/upgrade/security.gopermissiondbListdbBucketshelpernumUpgradedproceedv1metaThere are no users in 1.x, nothing to upgrade."There are no users in 1.x, nothing to upgrade."upgrade: there were errors/warnings, please fix them and run the command again"upgrade: there were errors/warnings, please fix them and run the command again"User is admin and will not be upgraded."User is admin and will not be upgraded."User has no privileges and will not be upgraded."User has no privileges and will not be upgraded."ReadPrivilegeWritePrivilege's Legacy Token"'s Legacy Token"Failed to create authorization."Failed to create authorization."Failed to set user's password."Failed to set user's password."User upgraded."User upgraded."No buckets for database [%s] exist in 2.x."No buckets for database [%s] exist in 2.x." Security upgrade implementation. Creates tokens representing v1 users. upgradeUsers creates tokens representing v1 users. check if there any 1.x users at all get helper instance check if target buckets exists in 2.x upgrade users securityUpgradeHelper is a helper used by `upgrade` command. newSecurityUpgradeHelper returns new security script helper instance for `upgrade` command./Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/upgrade/setup.gosourceOptionsonboarding error: %w"onboarding error: %w"Welcome to InfluxDB 2.0 upgrade!`Welcome to InfluxDB 2.0 upgrade!`failed to save CLI config"failed to save CLI config"CLI config has been stored."CLI config has been stored."/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/upgrade/upgrade.gov1dirv2ConfigPathcanOnboardgenericV1opslvlusersUpgradedmetaDbsourceOptstargetOptsauthStoreV1authStoreV2wdhomegithub.com/influxdata/influxdb/v2/v1/services/meta/filestore"github.com/influxdata/influxdb/v2/v1/services/meta/filestore"toml:"dir"`toml:"dir"`toml:"meta"`toml:"meta"`toml:"wal-dir"`toml:"wal-dir"`toml:"data"`toml:"data"`toml:"bind-address"`toml:"bind-address"`toml:"https-enabled"`toml:"https-enabled"`toml:"auth-enabled"`toml:"auth-enabled"`toml:"http"`toml:"http"`localhost"localhost""meta"error fetching default InfluxDB 2.0 dir: "error fetching default InfluxDB 2.0 dir: ""upgrade"Upgrade a 1.x version of InfluxDB"Upgrade a 1.x version of InfluxDB"
    Upgrades a 1.x version of InfluxDB by performing the following actions:
      1. Reads the 1.x config file and creates a 2.x config file with matching options. Unsupported 1.x options are reported.
      2. Copies 1.x database files.
      3. Creates influx CLI configurations.
      4. Exports any 1.x continuous queries to disk.

    If --config-file is not passed, 1.x db folder (--v1-dir options) is taken as an input. If neither option is given,
    the CLI will search for config under ${HOME}/.influxdb/ and /etc/influxdb/. If config can't be found, the CLI assumes
    a standard V1 directory structure under ${HOME}/.influxdb/.

    Target 2.x database dir is specified by the --engine-path option. If changed, the bolt path should be changed as well.
`
    Upgrades a 1.x version of InfluxDB by performing the following actions:
      1. Reads the 1.x config file and creates a 2.x config file with matching options. Unsupported 1.x options are reported.
      2. Copies 1.x database files.
      3. Creates influx CLI configurations.
      4. Exports any 1.x continuous queries to disk.

    If --config-file is not passed, 1.x db folder (--v1-dir options) is taken as an input. If neither option is given,
    the CLI will search for config under ${HOME}/.influxdb/ and /etc/influxdb/. If config can't be found, the CLI assumes
    a standard V1 directory structure under ${HOME}/.influxdb/.

    Target 2.x database dir is specified by the --engine-path option. If changed, the bolt path should be changed as well.
`v1-dir"v1-dir"path to source 1.x db directory containing meta, data and wal sub-folders"path to source 1.x db directory containing meta, data and wal sub-folders"verbose output"verbose output"'v'path for boltdb database"path for boltdb database"'m'influx-configs-path"influx-configs-path""configs"path for 2.x CLI configurations file"path for 2.x CLI configurations file"path for persistent engine files"path for persistent engine files"'e'continuous-query-export-path"continuous-query-export-path"continuous_queries.txt"continuous_queries.txt"path for exported 1.x continuous queries"path for exported 1.x continuous queries"'u'optional: duration bucket will retain data. 0 is infinite. The default is 0."optional: duration bucket will retain data. 0 is infinite. The default is 0."'r'optional: token for username, else auto-generated"optional: token for username, else auto-generated"config-file"config-file"optional: Custom InfluxDB 1.x config file path, else the default config file"optional: Custom InfluxDB 1.x config file path, else the default config file"supported log levels are debug, info, warn and error"supported log levels are debug, info, warn and error"log-path"log-path"upgrade.log"upgrade.log"optional: custom log file path"optional: custom log file path"skip the confirmation prompt"skip the confirmation prompt"unknown log level; supported levels are debug, info, warn and error"unknown log level; supported levels are debug, info, warn and error"only one of --v1-dir or --config-file may be specified"only one of --v1-dir or --config-file may be specified"error fetching default InfluxDB 1.x dir: %w"error fetching default InfluxDB 1.x dir: %w"AtomicLevelCASSetLevelSamplingConfigInitialThereafterEncoderConfigLevelEncoderTimeEncoderDurationEncoderCallerEncoderNameEncoderMessageKeyLevelKeyTimeKeyNameKeyCallerKeyStacktraceKeyLineEndingEncodeLevelEncodeTimeEncodeDurationEncodeCallerEncodeNameDevelopmentDisableCallerDisableStacktraceSamplingOutputPathsErrorOutputPathsInitialFieldsbuildOptionsopenSinksbuildEncoderNewProductionConfigNewAtomicLevelAtconfig.toml"config.toml"Starting InfluxDB 1.x upgrade"Starting InfluxDB 1.x upgrade"Upgrading config file"Upgrading config file"Config file upgraded."Config file upgraded."1.x config"1.x config"2.x config"2.x config"No InfluxDB 1.x config file specified, skipping its upgrade"No InfluxDB 1.x config file specified, skipping its upgrade"Upgrade source paths"Upgrade source paths"Upgrade target paths"Upgrade target paths"Failed to close 2.0 services."Failed to close 2.0 services."InfluxDB has been already set up"InfluxDB has been already set up"Database upgrade error, removing data"Database upgrade error, removing data"Unable to remove bolt database."Unable to remove bolt database."Unable to remove time series data."Unable to remove time series data."V1 users were upgraded, but V1 auth was not enabled. Existing clients will fail authentication against V2 if using invalid credentials."V1 users were upgraded, but V1 auth was not enabled. Existing clients will fail authentication against V2 if using invalid credentials."Upgrade successfully completed. Start service now"Upgrade successfully completed. Start service now"1.x DB dir '%s' does not exist"1.x DB dir '%s' does not exist"1.x DB dir '%s' is not a directory"1.x DB dir '%s' is not a directory"meta.db"meta.db"1.x meta.db '%s' does not exist"1.x meta.db '%s' does not exist"file present at target path for upgraded 2.x config file '%s'"file present at target path for upgraded 2.x config file '%s'"file present at target path for upgraded 2.x bolt DB: '%s'"file present at target path for upgraded 2.x bolt DB: '%s'"upgraded 2.x engine path '%s' is not a directory"upgraded 2.x engine path '%s' is not a directory"upgraded 2.x engine directory '%s' must be empty"upgraded 2.x engine directory '%s' must be empty"file present at target path for 2.x CLI configs '%s'"file present at target path for 2.x CLI configs '%s'"file present at target path for exported continuous queries '%s'"file present at target path for exported continuous queries '%s'"error opening 1.x meta.db: %w"error opening 1.x meta.db: %w"checkKeyGetwd.influxdb".influxdb"INFLUXDB_CONFIG_PATH"INFLUXDB_CONFIG_PATH"ExpandEnv${HOME}/.influxdb/influxdb.conf"${HOME}/.influxdb/influxdb.conf"/etc/influxdb/influxdb.conf"/etc/influxdb/influxdb.conf"USERPROFILE"USERPROFILE" Simplified 1.x config. fallback to default address is just :port cmd option populateDirs sets values for expected sub-directories of o.dbDir flags for source InfluxDB flags for target InfluxDB verbose output logging target flags add sub commands This command is executed multiple times by test code. Initialization can happen only once. Try finding config at usual paths If not found, try loading a V1 dir under HOME. If config is present, use it to set data paths. Otherwise, assume a standard directory layout.remove all files validatePaths ensures that all filesystem paths provided as input are usable by the upgrade command Create BoltDB store and K/V service ensure migrator is run Create Tenant service (orgs, buckets, ) DB/RP service on-boarding service (influx setup) v1 auth service influxDirV1 retrieves the influxdb directory. By default, store meta and data files in current users home directory influxConfigPathV1 returns default 1.x config file path or empty path if not found. homeOrAnyDir retrieves user's home directory, current working one or just none.AppendByteTrimNewlineEncodeEntry/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/upgrade/v1_dump_meta.goshowBoolv1-dump-meta"v1-dump-meta"Dump InfluxDB 1.x meta.db"Dump InfluxDB 1.x meta.db"âœ“"âœ“""Databases"---------"---------"%s	%s	%s
"%s\t%s\t%s\n"Default RP"Default RP""Shards"%s	%s	"%s\t%s\t"Retention policies"Retention policies"%s	%s	%s	%s
"%s\t%s\t%s\t%s\n""Duration"Shard Group duration"Shard Group duration"Shard groups"Shard groups"Database/RP"Database/RP"Start Time"Start Time"End Time"End Time"%s/%s	%s	%s	"%s/%s\t%s\t%s\t""Users"-----"-----"%s	%s
"%s\t%s\n""Admin"error fetching default InfluxDB 1.x dir: "error fetching default InfluxDB 1.x dir: "v1-meta-dir"v1-meta-dir"Path to meta.db directory"Path to meta.db directory"/Users/austinjaybecker/projects/abeck-go-testing/cmd/influxd/upgrade/v2_dump_meta.goshowCheckv2-dump-meta"v2-dump-meta"Dump InfluxDB 2.x influxd.bolt"Dump InfluxDB 2.x influxd.bolt"error opening InfluxDB 2.0: %w"error opening InfluxDB 2.0: %w""Orgs"----"----"-------"-------""Mappings"%s	%s	%s	%s	%s
"%s\t%s\t%s\t%s\t%s\n""RP"yes"yes"no"no"v2-bolt-path"v2-bolt-path"Path to 2.0 metadata"Path to 2.0 metadata"/Users/austinjaybecker/projects/abeck-go-testing/cmd/internal/Users/austinjaybecker/projects/abeck-go-testing/cmd/internal/input.goColorRedErrPasswordNotMatchKeyEscapeKeyResetpromptFuncnewStrshowNewdefaultValuemagdayweekgithub.com/influxdata/influxdb/v2/task/options"github.com/influxdata/influxdb/v2/task/options"passwords do not match"passwords do not match"password is too short"password is too short"'[''3''1''6''0'Confirm? (y/n)"Confirm? (y/n)"Please type your secret"Please type your secret"ErrInterrupted new" new"enterPasswordPlease type your"Please type your" password" password"Password too short - minimum length is 8 characters.
"Password too short - minimum length is 8 characters.\n\r" password again" password again"Passwords do not match.
"Passwords do not match.\n"ParseSignedDurationmust be greater than 0"must be greater than 0""w""us"Microsecondduration must be week(w), day(d), hour(h), min(m), sec(s), millisec(ms), microsec(us), or nanosec(ns)"duration must be week(w), day(d), hour(h), min(m), sec(s), millisec(ms), microsec(us), or nanosec(ns)" vt100EscapeCodes Nothing./Users/austinjaybecker/projects/abeck-go-testing/cmd/telemetryd/Users/austinjaybecker/projects/abeck-go-testing/cmd/telemetryd/main.goexitCodetelemetryd"telemetryd"bind-addr"bind-addr":8080":8080"binding address for telemetry server"binding address for telemetry server"Command returned error"Command returned error"Error syncing logs: %v
"Error syncing logs: %v\n"WriteMessageNewLogStorePushGatewayTransformersNewPushGatewayLineProtocolStarting telemetryd server"Starting telemetryd server"NewStdLog Print data as line protocol/Users/austinjaybecker/projects/abeck-go-testing/context/Users/austinjaybecker/projects/abeck-go-testing/context/token.goauthorizerCtxKeyinflux/authorizer/v1"influx/authorizer/v1"authorizer not found on context"authorizer not found on context"unexpected invalid authorizer"unexpected invalid authorizer"authorizer not an authorization but a %T"authorizer not an authorization but a %T" SetAuthorizer sets an authorizer on context. GetAuthorizer retrieves an authorizer from context. GetToken retrieves a token from the context; errors if no token. GetUserID retrieves the user ID from the authorizer on the context./Users/austinjaybecker/projects/abeck-go-testing/credentials.goUnauthorized"Unauthorized"basic"basic" ErrCredentialsUnauthorized is the error returned when CredentialsV1 cannot be authorized. SchemeV1 is an enumeration of supported authorization types SchemeV1Basic indicates the credentials came from an Authorization header using the BASIC scheme SchemeV1Token indicates the credentials came from an Authorization header using the Token scheme SchemeV1URL indicates the credentials came from the u and p query parameters CredentialsV1 encapsulates the required credentials to authorize a v1 HTTP request./Users/austinjaybecker/projects/abeck-go-testing/crud_log.go CRUDLogSetter is the interface to set the crudlog. CRUDLog is the struct to store crud related ops. SetCreatedAt set the created time. SetUpdatedAt set the updated time. TimeGenerator represents a generator for now. Now creates the generated time. RealTimeGenerator will generate the real time. Now returns the current time./Users/austinjaybecker/projects/abeck-go-testing/dashboard.gosorterrawJSONnewCellpropscvxyvssvgvmvlvhvsvevvisaliascopyCfgfunctionTypecell not found"cell not found"view not found"view not found""FindDashboardByID""FindDashboards""CreateDashboard""UpdateDashboard""AddDashboardCell""RemoveDashboardCell""UpdateDashboardCell""GetDashboardCellView""UpdateDashboardCellView""DeleteDashboard""ReplaceDashboardCells"json:"owner,omitempty"`json:"owner,omitempty"`"CreatedAt""UpdatedAt"json:"properties,omitempty"`json:"properties,omitempty"`owner"owner"must update at least one attribute"must update at least one attribute"expected at least one attribute to be updated"expected at least one attribute to be updated"heatmap"heatmap"histogram"histogram"log-viewer"log-viewer"markdown"markdown"scatter"scatter"mosaic"mosaic"band"band"Shapejson:"shape"`json:"shape"`chronograf-v2"chronograf-v2""empty"unknown shape %v"unknown shape %v"json:"note"`json:"note"`json:"showNoteWhenEmpty"`json:"showNoteWhenEmpty"`json:"xColumn"`json:"xColumn"`json:"generateXAxisTicks"`json:"generateXAxisTicks"`json:"xTotalTicks"`json:"xTotalTicks"`json:"xTickStart"`json:"xTickStart"`json:"xTickStep"`json:"xTickStep"`json:"yColumn"`json:"yColumn"`json:"generateYAxisTicks"`json:"generateYAxisTicks"`json:"yTotalTicks"`json:"yTotalTicks"`json:"yTickStart"`json:"yTickStart"`json:"yTickStep"`json:"yTickStep"`json:"shadeBelow"`json:"shadeBelow"`json:"hoverDimension"`json:"hoverDimension"`json:"legendColorizeRows"`json:"legendColorizeRows"`json:"legendOpacity"`json:"legendOpacity"`json:"legendOrientationThreshold"`json:"legendOrientationThreshold"`json:"geom"`json:"geom"`json:"upperColumn"`json:"upperColumn"`json:"mainColumn"`json:"mainColumn"`json:"lowerColumn"`json:"lowerColumn"`json:"checkID"`json:"checkID"`json:"tickPrefix"`json:"tickPrefix"`json:"tickSuffix"`json:"tickSuffix"`json:"fillColumns"`json:"fillColumns"`json:"xDomain,omitempty"`json:"xDomain,omitempty"`json:"xAxisLabel"`json:"xAxisLabel"`json:"binCount"`json:"binCount"`json:"binSize"`json:"binSize"`json:"yDomain,omitempty"`json:"yDomain,omitempty"`json:"yAxisLabel"`json:"yAxisLabel"`json:"xPrefix"`json:"xPrefix"`json:"xSuffix"`json:"xSuffix"`json:"yPrefix"`json:"yPrefix"`json:"ySuffix"`json:"ySuffix"`json:"symbolColumns"`json:"symbolColumns"`json:"ySeriesColumns"`json:"ySeriesColumns"`json:"tableOptions"`json:"tableOptions"`json:"settings"`json:"settings"`json:"editMode"`json:"editMode"`json:"builderConfig"`json:"builderConfig"`json:"buckets"`json:"buckets"`json:"key"`json:"key"`json:"aggregateFunctionType"`json:"aggregateFunctionType"`json:"functions"`json:"functions"`json:"period"`json:"period"`json:"fillValues"`json:"fillValues"`json:"aggregateWindow"`json:"aggregateWindow"` ErrDashboardNotFound is the error msg for a missing dashboard. ErrCellNotFound is the error msg for a missing cell. ErrViewNotFound is the error msg for a missing View. ops for dashboard service. DashboardService represents a service for managing dashboard data. FindDashboardByID returns a single dashboard by ID. FindDashboards returns a list of dashboards that match filter and the total count of matching dashboards. CreateDashboard creates a new dashboard and sets b.ID with the new identifier. UpdateDashboard updates a single dashboard with changeset. Returns the new dashboard state after update. AddDashboardCell adds a cell to a dashboard. RemoveDashboardCell removes a dashboard. UpdateDashboardCell replaces the dashboard cell with the provided ID. GetDashboardCellView retrieves a dashboard cells view. UpdateDashboardCellView retrieves a dashboard cells view. DeleteDashboard removes a dashboard by ID. ReplaceDashboardCells replaces all cells in a dashboard Dashboard represents all visual and query data for a dashboard. DashboardMeta contains meta information about dashboards DefaultDashboardFindOptions are the default find options for dashboards SortDashboards sorts a slice of dashboards by a field. Cell holds positional information about a cell on dashboard and a reference to a cell. Marshals the cell CellProperty contains the properties of a cell. DashboardFilter is a filter for dashboards. QueryParams turns a dashboard filter into query params It implements PagingFilter. DashboardUpdate is the patch structure for a dashboard. Apply applies an update to a dashboard. Valid returns an error if the dashboard update is invalid. AddDashboardCellOptions are options for adding a dashboard. CellUpdate is the patch structure for a cell. Apply applies an update to a Cell. Valid returns an error if the cell update is invalid. ViewUpdate is a struct for updating Views. Valid validates the update struct. It expects minimal values to be set. Apply updates a view with the view updates properties. ViewContentsUpdate is a struct for updating the non properties content of a View. ViewFilter represents a set of filter that restrict the returned results. View holds positional and visual information for a View. ViewContents is the id and name of a specific view. Values for all supported view property types. ViewProperties is used to mark other structures as conforming to a View. EmptyViewProperties is visualization that has no values UnmarshalViewPropertiesJSON unmarshals JSON bytes into a ViewProperties. Then there wasn't any visualization field, so there's no need unmarshal it happens in log viewer stays in log viewer. MarshalViewPropertiesJSON encodes a view into JSON bytes. MarshalJSON encodes a view to JSON bytes. UnmarshalJSON decodes JSON bytes into the corresponding view type (those that implement ViewProperties). UnmarshalJSON decodes JSON bytes into the corresponding view update type (those that implement ViewProperties). LinePlusSingleStatProperties represents options for line plus single stat view in Chronograf XYViewProperties represents options for line, bar, step, or stacked view in Chronograf Either "line", "step", "stacked", or "bar" BandViewProperties represents options for the band view CheckViewProperties represents options for a view representing a check SingleStatViewProperties represents options for single stat view in Chronograf HistogramViewProperties represents options for histogram view in Chronograf HeatmapViewProperties represents options for heatmap view in Chronograf ScatterViewProperties represents options for scatter view in Chronograf MosaicViewProperties represents options for mosaic view in Chronograf GaugeViewProperties represents options for gauge view in Chronograf TableViewProperties represents options for table view in Chronograf LogViewProperties represents options for log viewer in Chronograf. LogViewerColumn represents a specific column in a Log Viewer. LogColumnSetting represent the settings for a specific column of a Log Viewer./////////////////////////// Old Chronograf Types DashboardQuery represents a query used in a dashboard cell Either "builder" or "advanced" Term or phrase that refers to the query MarshalJSON is necessary for the time being. UI keeps breaking b/c it relies on these slices being populated/not nil. Other consumers may have same issue. NewBuilderTag is a constructor for the builder config types. This isn't technically required, but working with struct literals with embedded struct tags is really painful. This is to get around that bit. Would be nicer to have these as actual types maybe. bounds are an arbitrary list of client-defined strings that specify the viewport for a View ViewColor represents the encoding of data into visualizations ID is the unique id of the View color TableOptions is a type of options for a DashboardView with type Table RenamableField is a column/row field in a DashboardView of type Table/Users/austinjaybecker/projects/abeck-go-testing/dashboards/Users/austinjaybecker/projects/abeck-go-testing/dashboards/service.godashboardCellAddedEventdashboardCellRemovedEventdashboardCellUpdatedEventdashboardCellViewBucketdashboardCellsReplacedEventdashboardCreatedEventdashboardOperationLogKeyPrefixdashboardRemovedEventdashboardUpdatedEventdecodeOrgDashboardIndexKeyencodeDashboardCellViewIDencodeDashboardOperationLogKeyencodeOrgDashboardIndexfilterDashboardsFnorgDashboardIndexpedashsdashIDorgDashboardsenforceOrgPaginationvborgsdashboardsv1"orgsdashboardsv1"dashboardcellviewsv1"dashboardcellviewsv1"Dashboard Created"Dashboard Created"Dashboard Updated"Dashboard Updated"Dashboard Removed"Dashboard Removed"Dashboard Cells Replaced"Dashboard Cells Replaced"Dashboard Cell Added"Dashboard Cell Added"Dashboard Cell Removed"Dashboard Cell Removed"Dashboard Cell Updated"Dashboard Cell Updated"WithCursorPrefixmalformed org dashboard index key (please report this error)"malformed org dashboard index key (please report this error)"bad org id"bad org id"bad dashboard id"bad dashboard id"BoolFlagLifetimeUnmarshalYAMLexposeEnforceOrganizationDashboardLimitscannot provide empty cell id"cannot provide empty cell id"cannot replace cells that were not already present"cannot replace cells that were not already present"CursorAscendingWithCursorDirectionErrKeyValueLogBoundsNotFound TODO(desa): what do we want these to be? OpLogStore is a type which persists and reports operation log entries on a backing kv store transaction. NewService constructs and configures a new dashboard service. FindDashboardByID retrieves a dashboard by id. FindDashboard retrieves a dashboard using an arbitrary dashboard filter. FindDashboards retrieves all dashboards that match an arbitrary dashboard filter. TODO(desa): support find options. CreateDashboard creates a influxdb dashboard and sets d.ID. If not view exists create the view TODO: this is temporary until we can fully remove the view service. ReplaceDashboardCells updates the positions of each cell in a dashboard concurrently. AddDashboardCell adds a cell to a dashboard and sets the cells ID. RemoveDashboardCell removes a cell from a dashboard. GetDashboardCellView retrieves the view for a dashboard cell. UpdateDashboardCellView updates the view for a dashboard cell. UpdateDashboardCell udpates a cell on a dashboard. PutDashboard will put a dashboard without setting an ID. TODO(desa): don't populate this here. use the first/last methods of the oplog to get meta fields. forEachDashboard will iterate through all dashboards while fn returns true. UpdateDashboard updates a dashboard according the parameters set on upd. DeleteDashboard deletes a dashboard and prunes it from the index. GetDashboardOperationLog retrieves a dashboards operation log. TODO(desa): might be worthwhile to allocate a slice of size opts.Limit TODO(desa): this is fragile and non explicit since it requires an authorizer to be on context. It should be             replaced with a higher level transaction so that adding to the log can take place in the http handler             where the userID will exist explicitly. Add the user to the log if you can, but don't error if its not there./Users/austinjaybecker/projects/abeck-go-testing/dashboards/testing/Users/austinjaybecker/projects/abeck-go-testing/dashboards/testing/dashboards.goDashboardFieldsErrorsEqualFloatPtrMustCreateLabelsMustCreateMappingsMustCreateOrgsMustCreateUsersMustIDBase16MustIDBase16PtrMustMakeUsersOrgMemberMustMakeUsersOrgOwnerMustNewPermissionAtIDdashFourIDdashOneIDdashThreeIDdashTwoIDdashboardCmpOptionsdiffPlatformErrorsidPtrint32PtrownerOneIDownerTwoIDtttestsopPrefixcmpoptsgithub.com/google/go-cmp/cmp/cmpopts"github.com/google/go-cmp/cmp/cmpopts"020f755c3c082000"020f755c3c082000"020f755c3c082001"020f755c3c082001"020f755c3c082002"020f755c3c082002"020f755c3c082003"020f755c3c082003"020f755c3c0820a0"020f755c3c0820a0"020f755c3c0820a1"020f755c3c0820a1"EquateEmptybasic create dashboard"basic create dashboard"IDFnFakeValue2009Novemberdashboard1"dashboard1"dashboard2"dashboard2"00000000000000aa"00000000000000aa"create dashboard with missing id"create dashboard with missing id"failed to retrieve dashboards: %v"failed to retrieve dashboards: %v"dashboards are different -got/+want
diff %s"dashboards are different -got/+want\ndiff %s"basic add cell"basic add cell"add cell with no id"add cell with no id"add cell with id not exist"add cell with id not exist"basic find dashboard by id"basic find dashboard by id"find dashboard by id not exists"find dashboard by id not exists"dashboard is different -got/+want
diff %s"dashboard is different -got/+want\ndiff %s"organizationIDownerIDfindOptionsfind all dashboards"find all dashboards"abc"abc"xyz"xyz"find all dashboards by offset and limit"find all dashboards by offset and limit"321"321"find all dashboards with limit"find all dashboards with limit"find all dashboards by descending"find all dashboards by descending"find all dashboards by org 10"find all dashboards by org 10"hello"hello"world"world"find all dashboards by offset and limit and org 1"find all dashboards by offset and limit and org 1"find all dashboards sorted by created at"find all dashboards sorted by created at"2004find all dashboards sorted by updated at"find all dashboards sorted by updated at"2010find dashboard by id"find dashboard by id"find multiple dashboards by id"find multiple dashboards by id"find multiple dashboards by owner"find multiple dashboards by owner""def"find multiple dashboards by id not exists"find multiple dashboards by id not exists"NewFlaggerdelete dashboards using exist id"delete dashboards using exist id""A"delete dashboards using id that does not exist"delete dashboards using id that does not exist"update name"update name""changed"update description"update description"update description and name"update description and name"update description name and cells"update description name and cells"buckets() |> count()"buckets() |> count()"update with id not exist"update with id not exist"basic remove cell"basic remove cell"cellUpdatebasic update cell"basic update cell"invalid cell update without attribute"invalid cell update without attribute"invalid cell update cell id not exist"invalid cell update cell id not exist"basic replace cells"basic replace cells"try to add a cell that didn't previously exist"try to add a cell that didn't previously exist"get view for cell that exists"get view for cell that exists"get view for cell that does not exist"get view for cell that does not exist"dashboard cell views are different -got/+want
diff %s"dashboard cell views are different -got/+want\ndiff %s"update view name"update view name"update view type"update view type"rfc3339"rfc3339"update view type and name"update view type and name"update view for cell that does not exist"update view for cell that does not exist" DashboardFields will include the IDGenerator, and dashboards DashboardService tests all the service functions. CreateDashboard testing AddDashboardCell testing FindDashboardByID testing FindDashboards testing ownerless dashboard added to similar nil owner pointer scenario DeleteDashboard testing UpdateDashboard testing RemoveDashboardCell testing UpdateDashboardCell testing ReplaceDashboardCells testing GetDashboardCellView is the conformance test for the retrieving a dashboard cell. UpdateDashboardCellView is the conformance test for the updating a dashboard cell./Users/austinjaybecker/projects/abeck-go-testing/dashboards/testing/util.gouidsunexpected error %s"unexpected error %s"expected error %s but received nil"expected error %s but received nil"
expected: %v
actual: %v

"\nexpected: %v\nactual: %v\n\n"expected error code %q but received %q"expected error code %q but received %q"expected error message %q but received %q"expected error message %q but received %q" TODO(goller): remove opPrefix argument ErrorsEqual checks to see if the provided errors are equivalent. FloatPtr takes the ref of a float number. MustIDBase16 is an helper to ensure a correct ID is built during testing. MustIDBase16Ptr is an helper to ensure a correct ID ptr ref is built during testing./Users/austinjaybecker/projects/abeck-go-testing/dashboards/transport/Users/austinjaybecker/projects/abeck-go-testing/dashboards/transport/http.gocellPathcellViewPathdashboardCellIDPathdashboardCellViewResponsedashboardCellsResponsedashboardIDPathdecodeDeleteDashboardCellRequestdecodeDeleteDashboardRequestdecodeGetDashboardCellViewRequestdecodeGetDashboardRequestdecodeGetDashboardsRequestdecodePatchDashboardCellRequestdecodePatchDashboardCellViewRequestdecodePatchDashboardRequestdecodePostDashboardCellRequestdecodePutDashboardCellRequestdeleteDashboardCellRequestdeleteDashboardRequestgetDashboardCellViewRequestgetDashboardRequestgetDashboardsRequestnewDashboardCellResponsenewDashboardCellViewResponsenewDashboardCellsResponsenewGetDashboardsResponsepatchDashboardCellRequestpatchDashboardCellViewRequestpatchDashboardRequestpostDashboardCellRequestprefixDashboardsputDashboardCellRequestviewLinksmountableRoutertoinfluxdbMembersorgNameFilterdashboardFilterinitialIDUpddrqueryPairsdcv/api/v2/dashboards"/api/v2/dashboards"/cells"/cells"/{cellID}"/{cellID}"/view"/view"ValidResource/members"/members"/owners"/owners"/labels"/labels"json:"members"`json:"members"`json:"owners"`json:"owners"`json:"labels"`json:"labels"`/api/v2/dashboards/%s"/api/v2/dashboards/%s"/api/v2/dashboards/%s/members"/api/v2/dashboards/%s/members"/api/v2/dashboards/%s/owners"/api/v2/dashboards/%s/owners"/api/v2/dashboards/%s/cells"/api/v2/dashboards/%s/cells"/api/v2/dashboards/%s/labels"/api/v2/dashboards/%s/labels"/api/v2/orgs/%s"/api/v2/orgs/%s"/api/v2/dashboards/%s/cells/%s"/api/v2/dashboards/%s/cells/%s""view"/api/v2/dashboards/%s/cells/%s/view"/api/v2/dashboards/%s/cells/%s/view"List Dashboards"List Dashboards""ownerID"include"include""properties"Get Dashboard"Get Dashboard"url missing id"url missing id"Dashboard deleted"Dashboard deleted""dashboardID"Dashboard updated"Dashboard updated"missing dashboard ID"missing dashboard ID"UsingViewjson:"usingView"`json:"usingView"`bad request json body"bad request json body"req body is empty"req body is empty"Dashboard cell created"Dashboard cell created""cell"Dashboard cell replaced"Dashboard cell replaced""cells""cellID"url missing cellID"url missing cellID"Dashboard cell view retrieved"Dashboard cell view retrieved"Dashboard cell view updated"Dashboard cell view updated"Dashboard cell deleted"Dashboard cell deleted"Dashboard cell updated"Dashboard cell updated" DashboardHandler is the handler for the dashboard service NewDashboardHandler returns a new instance of DashboardHandler. setup routing mount embedded resources Prefix returns the mounting prefix for the handler handleGetDashboards returns all dashboards within the store. handlePostDashboard creates a new dashboard. handleGetDashboard retrieves a dashboard by ID. handleDeleteDashboard removes a dashboard by ID. handlePatchDashboard updates a dashboard. Valid validates that the dashboard ID is non zero valued and update has expected values set. handlePostDashboardCell creates a dashboard cell. load the view handlePutDashboardCells replaces a dashboards cells. handleDeleteDashboardCell deletes a dashboard cell. handlePatchDashboardCell updates a dashboard cell. DashboardService is a dashboard service over HTTP to the influxdb server. TODO(@jsteenb2): decipher why this is doing this? TODO: previous implementation did not do anything with the response except validate it is valid json.  seems likely we should have to overwrite (:sadpanda:) the incoming cs.../Users/austinjaybecker/projects/abeck-go-testing/dbrp/Users/austinjaybecker/projects/abeck-go-testing/dbrp/bucket_service.goErrDBRPAlreadyExistsErrDBRPNotFoundErrInternalServiceErrInvalidDBRPErrInvalidDBRPIDErrNotUniqueIDNewHTTPHandlerPrefixDBRPcomposeForeignKeycreateDBRPRequestdefaultBucketfilterFuncgetBucketIDFromHTTPRequestgetDBRPIDFromHTTPRequestgetDBRPResponsegetDBRPsResponsegetIDFromHTTPRequestindexForeignKeydbrpServiceFailed to lookup DBRP mappings for Bucket."Failed to lookup DBRP mappings for Bucket."Failed to delete DBRP mapping for Bucket."Failed to delete DBRP mapping for Bucket."byOrgAndDatabasegetDefaultgetDefaultIDisDefaultisDefaultSetsetAsDefaultunsetDefaultgetFirstButisDBRPUniquehandlePostDBRPhandleGetDBRPshandleGetDBRPhandlePatchDBRPhandleDeleteDBRPgetFilterFromHTTPRequestmustGetOrgIDFromHTTPRequest/Users/austinjaybecker/projects/abeck-go-testing/dbrp/error.goDBRP ID is invalid"DBRP ID is invalid"unable to find DBRP"unable to find DBRP"DBRP provided is invalid"DBRP provided is invalid"DBRP already exists"DBRP already exists" ErrInvalidDBRPID is used when the ID of the DBRP cannot be encoded. ErrDBRPNotFound is used when the specified DBRP cannot be found. ErrNotUniqueID is used when the ID of the DBRP is not unique. ErrInvalidDBRP is used when a service was provided an invalid DBRP. ErrInternalService is used when the error comes from an internal system. ErrDBRPAlreadyExists is used when there is a conflict in creating a new DBRP./Users/austinjaybecker/projects/abeck-go-testing/dbrp/http_client_dbrp.go/api/v2/dbrpsplease filter by orgID"please filter by orgID"FormatBool Client connects to Influx via HTTP using tokens to manage DBRPs./Users/austinjaybecker/projects/abeck-go-testing/dbrp/http_server_dbrp.gobodyRequestrawDBrawDefaultrawRP"/api/v2/dbrps"json:"retention_policy"`json:"retention_policy"`json:"content"`json:"content"`invalid default parameter"invalid default parameter" NewHTTPHandler constructs a new http server. Always provide OrgID. mustGetOrgIDFromHTTPRequest returns the org ID parameter from the request, falling back to looking up the org ID by org name if the ID parameter is not present./Users/austinjaybecker/projects/abeck-go-testing/dbrp/middleware_auth.go/Users/austinjaybecker/projects/abeck-go-testing/dbrp/mocks/Users/austinjaybecker/projects/abeck-go-testing/dbrp/mocks/bucket_service.goMockBucketServiceMockBucketServiceMockRecorderMockDBRPMappingServiceV2MockDBRPMappingServiceV2MockRecorderNewMockBucketServiceNewMockDBRPMappingServiceV2TestHelperTestReportercallSetcallSetKeyreceiverfnameMatchermethodTypeoriginpreReqsminCallsmaxCallsnumCallsAnyTimesMinTimesMaxTimesDoAndReturnSetArgisPreReqsatisfiedexhausteddropPrereqsaddActionFindMatchFailuresexpectedCallsRecordCallRecordCallWithMethodTyperecorderEXPECTarg0arg1ret0ret1arg2ret2varargsgomockgithub.com/golang/mock/gomock"github.com/golang/mock/gomock""FindBucketByName" Code generated by MockGen. DO NOT EDIT. Source: github.com/influxdata/influxdb/v2 (interfaces: BucketService) Package mocks is a generated GoMock package. MockBucketService is a mock of BucketService interface MockBucketServiceMockRecorder is the mock recorder for MockBucketService NewMockBucketService creates a new mock instance EXPECT returns an object that allows the caller to indicate expected use CreateBucket mocks base method CreateBucket indicates an expected call of CreateBucket DeleteBucket mocks base method DeleteBucket indicates an expected call of DeleteBucket FindBucket mocks base method FindBucket indicates an expected call of FindBucket FindBucketByID mocks base method FindBucketByID indicates an expected call of FindBucketByID FindBucketByName mocks base method FindBucketByName indicates an expected call of FindBucketByName FindBuckets mocks base method FindBuckets indicates an expected call of FindBuckets UpdateBucket mocks base method UpdateBucket indicates an expected call of UpdateBucket/Users/austinjaybecker/projects/abeck-go-testing/dbrp/mocks/dbrp_mapping_service_v2.go"Create""Delete""FindByID""FindMany""Update" Source: github.com/influxdata/influxdb/v2 (interfaces: DBRPMappingServiceV2) MockDBRPMappingServiceV2 is a mock of DBRPMappingServiceV2 interface MockDBRPMappingServiceV2MockRecorder is the mock recorder for MockDBRPMappingServiceV2 NewMockDBRPMappingServiceV2 creates a new mock instance Create mocks base method Create indicates an expected call of Create Delete mocks base method Delete indicates an expected call of Delete FindByID mocks base method FindByID indicates an expected call of FindByID FindMany mocks base method FindMany indicates an expected call of FindMany Update mocks base method Update indicates an expected call of Update/Users/austinjaybecker/projects/abeck-go-testing/dbrp/service.goencIDcompKeydefIDskipIDdefsdefSetferrdbrpv1"dbrpv1"dbrpbyorganddbindexv1"dbrpbyorganddbindexv1"dbrpdefaultv1"dbrpdefaultv1"IndexSourceOnFuncNewIndexMappingWithIndexReadPathEnabledanother DBRP mapping with same orgID, db, and rp exists"another DBRP mapping with same orgID, db, and rp exists"dbrp already exist for this particular ID. If you are trying an update use the right function .Update"dbrp already exist for this particular ID. If you are trying an update use the right function .Update" The DBRP Mapping `Service` maps database, retention policy pairs to buckets. Every `DBRPMapping` stored is scoped to an organization ID. The service must ensure the following invariants are valid at any time:  - each orgID, database, retention policy triple must be unique;  - for each orgID and database there must exist one and only one default mapping (`mapping.Default` set to `true`). The service does so using three kv buckets:  - one for storing mappings;  - one for storing an index of mappings by orgID and database;  - one for storing the current default mapping for an orgID and a database. On *create*, the service creates the mapping. If another mapping with the same orgID, database, and retention policy exists, it fails. If the mapping is the first one for the specified orgID-database couple, it will be the default one. On *find*, the service find mappings. Every mapping returned uses the kv bucket where the default is specified to update the `mapping.Default` field. On *update*, the service updates the mapping. If the update causes another bucket to have the same orgID, database, and retention policy, it fails. If the update unsets `mapping.Default`, the first mapping found is set as default. On *delete*, the service updates the mapping. If the deletion deletes the default mapping, the first mapping found is set as default. getDefault gets the default mapping ID inside of a transaction. getDefaultID returns the default mapping ID for the given orgID and db. isDefault tells whether a mapping is the default one. isDefaultSet tells if there is a default mapping for the given composite key. setAsDefault sets the given id as default for the given composite key. unsetDefault un-sets the default for the given composite key. Useful when a db/rp pair does not exist anymore. getFirstBut returns the first element in the db/rp index (not accounting for the `skipID`). If the length of the returned ID is 0, it means no element was found. The skip value is useful, for instance, if one wants to delete an element based on the result of this operation. isDBRPUnique verifies if the triple orgID-database-retention-policy is unique. Corner case. This is the very same DBRP, just skip it! FindBy returns the mapping for the given ID. If the given orgID is wrong, it is as if we did not found a mapping scoped to this org. Update the default value for this mapping. FindMany returns a list of mappings that match filter and the total count of matching dbrp mappings. TODO(affo): find a smart way to apply FindOptions to a list of items. Memoize default IDs. Still need to store a not-found result. Updating the Default field must be done before filtering. Optimized path, use index. The index performs a prefix search. The foreign key is `orgID + db`. If you want to look by orgID only, just pass orgID as prefix. Even more optimized, looking for the default given an orgID and database. No walking index needed. Create creates a new mapping. If another mapping with same organization ID, database, and retention policy exists, an error is returned. If the mapping already contains a valid ID, that one is used for storing the mapping. If a dbrp with this particular ID already exists an error is returned. If a dbrp with this orgID, db, and rp exists an error is returned. Updates a mapping. Un-setting `Default` for a mapping will cause the first one to become the default. Overwrite fields that cannot change. This means default was unset. Need to find a new default. If no first was found, then this will remain the default. Delete removes a mapping. Deleting a mapping that does not exists is not an error. Deleting the default mapping will cause the first one (if any) to become the default. If this was the default, we need to set a new default. This means no other mapping is in the index. Unset the default filterFunc is capable to validate if the dbrp is valid from a given filter. it runs true if the filtering data are contained in the dbrp./Users/austinjaybecker/projects/abeck-go-testing/dbrp_mapping.godbrpMapdatabase must contain at least one character and only be letters, numbers, '_', '-', and '.'"database must contain at least one character and only be letters, numbers, '_', '-', and '.'"retentionPolicy must contain at least one character and only be letters, numbers, '_', '-', and '.'"retentionPolicy must contain at least one character and only be letters, numbers, '_', '-', and '.'"organizationID is required"organizationID is required"bucketID is required"bucketID is required"{ id:"{ id:" org_id:" org_id:" bucket_id:" bucket_id:" db:" db:" rp:" rp:" default:" default:""}"json:"organization_id"`json:"organization_id"`json:"bucket_id"`json:"bucket_id"`cluster must contain at least one character and only be letters, numbers, '_', '-', and '.'"cluster must contain at least one character and only be letters, numbers, '_', '-', and '.'"IsPrint..".."ContainsAny/\`/\`{"{"cluster:"cluster:" DBRPMappingServiceV2 provides CRUD to DBRPMappingV2s. Requires orgID because every resource will be org-scoped. DBRPMappingV2 represents a mapping of a database and retention policy to an organization ID and bucket ID. Default indicates if this mapping is the default for the cluster and database. Validate reports any validation errors for the mapping. Equal checks if the two mappings are identical. DBRPMappingFilterV2 represents a set of filters that restrict the returned results. DBRPMappingService provides a mapping of cluster, database and retention policy to an organization ID and bucket ID. FindBy returns the dbrp mapping the for cluster, db and rp. Find returns the first dbrp mapping the matches the filter. DBRPMapping represents a mapping of a cluster, database and retention policy to an organization ID and bucket ID. validName checks to see if the given name can would be valid for DB/RP name DBRPMappingFilter represents a set of filters that restrict the returned results by cluster, database and retention policy./Users/austinjaybecker/projects/abeck-go-testing/delete.go Predicate is something that can match on a series key. DeleteService will delete a bucket from the range and predict./Users/austinjaybecker/projects/abeck-go-testing/document.godocument not found"document not found"json:"content,omitempty"`json:"content,omitempty"`json:"labels,omitempty"`json:"labels,omitempty"`json:"version,omitempty"`json:"version,omitempty"` ErrDocumentNotFound is the error msg for a missing document. DocumentService is used to create/find instances of document stores. Document is a generic structure for stating data. TODO(desa): maybe this needs to be json.Marshaller & json.Unmarshaler read only This is needed for authorization. The service that passes documents around will take care of filling it via request parameters or others, as the kv store will take care of filling it once it returns a document. This is not stored in the kv store neither required in the API. DocumentMeta is information that is universal across documents. Ideally data in the meta should be indexed and queryable. DocumentStore is used to perform CRUD operations on documents. It follows an options pattern that allows users to perform actions related to documents in a transactional way./Users/austinjaybecker/projects/abeck-go-testing/duration.goinvalid duration"invalid duration" Duration is based on time.Duration to embed in any struct. MarshalJSON implements json.Marshaler interface. UnmarshalJSON implements json.Unmarshaler interface./Users/austinjaybecker/projects/abeck-go-testing/error.go ChronografError is a domain error encountered while processing chronograf requests. ChronografError returns the string of an error./Users/austinjaybecker/projects/abeck-go-testing/errors.goerrStrinternalErrinternalErrMap"internal error""not found""unprocessable entity"empty value"empty value"unavailable"unavailable""forbidden"too many requests"too many requests"method not allowed"method not allowed"request too large"request too large": ": "<%s>"<%s>"An internal error has occurred."An internal error has occurred."json:"message,omitempty"`json:"message,omitempty"`json:"op,omitempty"`json:"op,omitempty"` Some error code constant, ideally we want define common platform codes here projects on use platform's error, should have their own central place like this. Any time this set of constants changes, you must also update the swagger for Error.properties.code.enum. action cannot be performed validation failed data type is correct, but out of range Error is the error struct of platform. Errors may have error codes, human-readable messages, and a logical stack trace. The Code targets automated handlers so that recovery can occur. Msg is used by the system operator to help diagnose and fix the problem. Op and Err chain errors together in a logical stack trace to further help operators. To create a simple error,     &Error{         Code:ENotFound,     } To show where the error happens, add Op.         Code: ENotFound,         Op: "bolt.FindUserByID" To show an error with a unpredictable value, add the value in Msg.        Code: EConflict,        Message: fmt.Sprintf("organization with name %s already exist", aName), To show an error wrapped with another error.         Code:EInternal,         Err: err,     }. NewError returns an instance of an error. WithErrorErr sets the err on the error. WithErrorCode sets the code on the error. WithErrorMsg sets the message on the error. WithErrorOp sets the message on the error. Error implements the error interface by writing out the recursive messages. ErrorCode returns the code of the root error, if available; otherwise returns EINTERNAL. ErrorOp returns the op of the error, if available; otherwise return empty string. ErrorMessage returns the human-readable message of the error, if available. Otherwise returns a generic error message. errEncode an JSON encoding helper that is needed to handle the recursive stack of errors. Code is the machine-readable error code. Msg is a human-readable message. Op describes the logical code operation during error. Err is a stack of additional errors. MarshalJSON recursively marshals the stack of Err. UnmarshalJSON recursively unmarshals the error stack. HTTPErrorHandler is the interface to handle http error./Users/austinjaybecker/projects/abeck-go-testing/flux/Users/austinjaybecker/projects/abeck-go-testing/flux/client.go Shared transports for all clients to prevent leaking connections. Client is how we interact with Flux. Ping checks the connection of a Flux./Users/austinjaybecker/projects/abeck-go-testing/fluxinit/Users/austinjaybecker/projects/abeck-go-testing/fluxinit/init.gogithub.com/influxdata/flux/runtime"github.com/influxdata/flux/runtime"github.com/influxdata/flux/stdlib"github.com/influxdata/flux/stdlib"github.com/influxdata/influxdb/v2/query/stdlib"github.com/influxdata/influxdb/v2/query/stdlib"FinalizeBuiltIns This package imports all the influxdb-specific query builtins. From influxdb we must use this package and not the init package provided by flux. This package is used for initializing with a function call. As a convenience, the fluxinit/static package can be imported for use cases where static initialization is okay, such as tests. Import the stdlib The FluxInit() function prepares the runtime for compilation and execution of Flux. This is a costly step and should only be performed if the intention is to compile and execute flux code./Users/austinjaybecker/projects/abeck-go-testing/fluxinit/static/Users/austinjaybecker/projects/abeck-go-testing/fluxinit/static/static.go The fluxinit/static package can be imported in test cases and other uses cases where it is okay to always initialize flux./Users/austinjaybecker/projects/abeck-go-testing/gather/Users/austinjaybecker/projects/abeck-go-testing/gather/handler.goMetricTypeCounterMetricTypeGaugeMetricTypeHistogrmMetricTypeSummaryMetricTypeUntypedScrapergetNameAndValuemakeBucketsmakeLabelsmakeQuantilesmetricTypeNamemetricTypeValuenewPrometheusScraperpromTargetSubjectprometheusScraperrequestScrapeUnable to unmarshal json"Unable to unmarshal json"Unable to gather"Unable to gather"Unable to marshal json"Unable to marshal json"Unable to publish scraper metrics"Unable to publish scraper metrics" handler implements nats Handler interface. Process consumes scraper target from scraper target queue, call the scraper to gather, and publish to metrics queue. send metrics to recorder queueinsecureHttp/Users/austinjaybecker/projects/abeck-go-testing/gather/metrics.gojson:"metrics"`json:"metrics"`json:"timestamp"`json:"timestamp"`NewPointNewTagsCOUNTER"COUNTER"GAUGE"GAUGE"SUMMARY"SUMMARY"UNTYPED"UNTYPED"HISTOGRAM"HISTOGRAM"UnmarshalJSONEnum"MetricType" MetricsCollection is the struct including metrics and other requirements. Metrics is the default influx based metrics. MetricsSlice is a slice of Metrics Points convert the MetricsSlice to model.Points Reader returns an io.Reader that enumerates the metrics. All metrics are allocated into the underlying buffer. MetricType is prometheus metrics type. the set of metric types Valid returns whether the metrics type is valid. String returns the string value of MetricType. UnmarshalJSON implements the unmarshaler interface./Users/austinjaybecker/projects/abeck-go-testing/gather/prometheus.gocustomTransportcollectedmediatypemetricFamiliesmimepbutil"mime"github.com/matttproud/golang_protobuf_extensions/pbutil"github.com/matttproud/golang_protobuf_extensions/pbutil"DefaultTransportParseMediaTypeapplication/vnd.google.protobuf"application/vnd.google.protobuf"delimited"delimited""proto"io.prometheus.client.MetricFamily"io.prometheus.client.MetricFamily"ReadDelimitedreading metric family protocol buffer failed: %s"reading metric family protocol buffer failed: %s"reading text format failed: %s"reading text format failed: %s"MetricType_SUMMARYMetricType_HISTOGRAMIsNaNcounter"counter" prometheusScraper handles parsing prometheus metrics. implements Scraper interfaces. newPrometheusScraper create a new prometheusScraper. Gather parse metrics from a scraper target url. Prepare output read metrics reading tags reading fields summary metric histogram metric standard metric Get labels from metric Get Buckets from histogram metric Get name and value from metric Get Quantiles from summary metric/Users/austinjaybecker/projects/abeck-go-testing/gather/recorder.goRecorder handler error"Recorder handler error" PointWriter will use the storage.PointWriter interface to record metrics. Record the metrics and write using storage.PointWriter interface. Recorder record the metrics of a time based. Subscriber nats.Subscriber RecorderHandler implements nats.Handler interface. Process consumes job queue, and use recorder to record./Users/austinjaybecker/projects/abeck-go-testing/gather/scheduler.gointervalnumScraperspromTarget"promTarget"Cannot list targets"Cannot list targets"JSON encoding error"JSON encoding error"unsupported target scrape type: %s"unsupported target scrape type: %s" nats subjects Scheduler is struct to run scrape jobs. Interval is between each metrics gathering event. Timeout is the maximum time duration allowed by each TCP request Publisher will send the gather requests and gathered metrics to the queue. NewScheduler creates a new Scheduler and subscriptions for scraper jobs. Run will retrieve scraper targets from the target storage, and publish them to nats job queue for gather. TODO: change to ticker because of garbage collection/Users/austinjaybecker/projects/abeck-go-testing/gather/scraper.go Scraper gathers metrics from a scraper target./Users/austinjaybecker/projects/abeck-go-testing/go.modmodulego1.15v0.3.1v1.0.1github.com/RoaringBitmap/roaringv0.4.16github.com/andreyvit/diffv0.0.0-20170406064948-c7f18ee00883github.com/apache/arrow/go/arrowv0.0.0-20200923215132-ac86123a3f01github.com/benbjohnson/clockv0.0.0-20161215174838-7dc76406b6d3github.com/benbjohnson/tmplv1.0.0github.com/boltdb/boltv1.3.1// indirectv0.0.0-20160817010721-ee8b3818a7f5v0.0.0-20191004114745-ee4c978eae7egithub.com/cespare/xxhashv1.1.0github.com/codahale/hdrhistogramv0.0.0-20161010025455-3a0bb77429bdgithub.com/davecgh/go-spewv1.1.1v3.2.0+incompatiblegithub.com/dgryski/go-bitstreamv0.0.0-20180413035011-3522498ce2c8github.com/docker/dockerv1.13.1github.com/editorconfig-checker/editorconfig-checkerv0.0.0-20190819115812-1474bdeaf2a2v1.9.0v0.0.0-20170602072123-c073257dd745github.com/getkin/kin-openapiv0.2.0github.com/ghodss/yamlgithub.com/glycerine/go-unsnap-streamv0.0.0-20181221182339-f9677308dec2github.com/glycerine/goconveyv0.0.0-20180728074245-46e3a41ad493v4.1.0+incompatiblegithub.com/go-stack/stackv1.8.0github.com/gogo/protobufgithub.com/golang/gddov0.0.0-20181116215533-9bd4a3295021github.com/golang/mockv1.4.4github.com/golang/protobufv1.3.3github.com/golang/snappyv0.0.1github.com/google/btreegithub.com/google/go-cmpv0.5.0github.com/google/go-githubv17.0.0+incompatiblegithub.com/google/go-jsonnetv0.14.0github.com/google/go-querystringgithub.com/google/martianv2.1.1-0.20190517191504-25dcb96d9e51+incompatiblegithub.com/hashicorp/go-msgpackv0.0.0-20150518234257-fa3f63826f7cgithub.com/hashicorp/go-retryablehttpv0.6.4github.com/hashicorp/raftgithub.com/hashicorp/vault/apiv1.0.2github.com/imdario/mergov0.3.9github.com/influxdata/cronv0.0.0-20191203200038-ded12750aac6v0.95.0v1.3.1-0.20191122104820-ee83e2772f69v0.0.0-20180925231337-1cbfca8e56b6github.com/influxdata/pkg-configv0.2.5github.com/influxdata/usage-clientv0.0.0-20160829180054-6d3895376368v1.4.0github.com/jsternberg/zap-logfmtv1.2.0github.com/jwilder/encodingv0.0.0-20170811194829-b4e1701a28efgithub.com/k0kubun/colorstringv0.0.0-20150214042306-9440f1994b88github.com/kevinburke/go-bindatav3.11.0+incompatiblegithub.com/lib/pqgithub.com/mattn/go-isattyv0.0.11github.com/matttproud/golang_protobuf_extensionsgithub.com/mileusna/useragentv0.0.0-20190129205925-3e331f0949a5github.com/mna/pigeonv1.0.1-0.20180808201053-bb0192cfc2aegithub.com/mschoch/smatv0.0.0-20160514031455-90eadee771aegithub.com/nats-io/gnatsdv1.3.0github.com/nats-io/go-natsv1.7.0github.com/nats-io/go-nats-streamingv0.4.0github.com/nats-io/nats-streaming-serverv0.11.2github.com/nats-io/nkeysv0.0.2github.com/nats-io/nuidv0.0.4github.com/onsi/ginkgov1.11.0github.com/onsi/gomegav1.8.1github.com/philhofer/fwdv0.9.1github.com/prometheus/client_golangv1.5.1github.com/prometheus/client_modelgithub.com/prometheus/commonv1.2.1-0.20181028125025-b2ce2384e17bgithub.com/spf13/castgithub.com/spf13/pflagv1.0.5v1.6.1github.com/stretchr/testifyv0.0.0-20180404061846-548a7d7a8ee8github.com/testcontainers/testcontainers-gov0.0.0-20190108154635-47c0da630f72github.com/tinylib/msgpv1.2.15github.com/uber-go/atomicv1.3.2github.com/uber/jaeger-client-gov2.16.0+incompatiblegithub.com/uber/jaeger-libv2.2.0+incompatiblegithub.com/willf/bitsetv1.1.9github.com/xlab/treeprintgithub.com/yudai/gojsondiffgithub.com/yudai/golcsv0.0.0-20170316035057-ecda9a501e82github.com/yudai/ppv2.0.1+incompatiblev1.3.5go.uber.org/multierrv1.5.0v1.14.1golang.org/x/cryptov0.0.0-20200622213623-75b288015ac9golang.org/x/netv0.0.0-20200625001655-4c5254603344v0.0.0-20200107190931-bf48bf16ab8dgolang.org/x/syncv0.0.0-20200625203802-6e8e738ad208golang.org/x/sysv0.0.0-20200323222414-85ca7c5b95cdgolang.org/x/textv0.3.2golang.org/x/timev0.0.0-20191024005414-555d28b269f0golang.org/x/toolsv0.0.0-20200721032237-77f530d86f9agoogle.golang.org/apiv0.17.0gopkg.in/vmihailenco/msgpack.v2v2.9.1gopkg.in/yaml.v2v2.3.0gopkg.in/yaml.v3v3.0.0-20200121175148-a6ecf24a6d71honnef.co/go/toolsv0.0.1-2020.1.4istio.io/pkgv0.0.0-20200606170016-70c5172b9cdflabix.org/v2/mgov0.0.0-20140701140051-000000000287launchpad.net/gocheckv0.0.0-20140225173054-000000000087replacev0.0.0-20191024131854-af6fa24be0db=>github.com/influxdata/arrow/go/arrowv0.0.0-20200917142114-986e413c1705// Arrow has been taking too long to merge our PR that addresses some checkptr fixes.// We are using our own fork, which specifically applies the change in// https://github.com/apache/arrow/pull/8112, on top of the commit of Arrow that flux uses.//// The next time Flux updates its Arrow dependency, we will see checkptr test failures,// if that version does not include PR 8112. In that event, someone (perhaps Mark R again)// will need to apply the change in 8112 on top of the newer version of Arrow.github.com/influxdata/nats-streaming-serverv0.11.3-0.20201112040610-c277f7560803/Users/austinjaybecker/projects/abeck-go-testing/http/Users/austinjaybecker/projects/abeck-go-testing/http/api_handler.goAuthenticationHandlerAuthorizationBackendAuthorizationHandlerBackupBackendBackupHandlerCheckBackendCheckBuilderConfigCheckErrorStatusCheckHandlerCheckLinksCheckQueryCheckThresholdChronografHandlerDebugPathDefaultShutdownTimeoutDefaultTransportInsecureDeleteBackendDeleteHandlerDocumentBackendDocumentHandlerErrAuthBadSchemeErrAuthHeaderMissingErrInvalidDurationFluxBackendFluxHandlerFormatDurationGetQueryResponseGetQueryResponseBodyHTTPDialectHealthHandlerHealthPathInactiveUserErrorLabelBackendMemberBackendMetricsPathNewAPIHandlerNewAssetHandlerNewAuthenticationHandlerNewAuthorizationBackendNewAuthorizationHandlerNewBackupBackendNewBackupHandlerNewBaseChiRouterNewBucketResponseNewCheckBackendNewCheckHandlerNewChronografHandlerNewDeleteBackendNewDeleteHandlerNewDocumentBackendNewDocumentHandlerNewFluxBackendNewFluxHandlerNewFrontEndTaskNewLabelHandlerNewNotificationEndpointBackendNewNotificationEndpointHandlerNewNotificationRuleBackendNewNotificationRuleHandlerNewRestoreBackendNewRestoreHandlerNewScraperBackendNewScraperHandlerNewSessionBackendNewSourceBackendNewSourceHandlerNewTaskBackendNewTaskHandlerNewTelegrafBackendNewTelegrafHandlerNewURLNewUserBackendNewUserHandlerNewVariableBackendNewVariableHandlerNewWriteBackendNewWriteHandlerNewWriteUsageRecorderNotificationEndpointBackendNotificationEndpointHandlerNotificationRuleBackendNotificationRuleHandlerProbeAuthSchemeQueryAnalysisQueryDialectQueryHealthCheckQueryRequestFromProxyRequestReadyHandlerReadyPathRestoreBackendRestoreHandlerScraperBackendScraperHandlerScraperServiceSessionBackendSetCookieSessionSourceBackendSourceHandlerSourceProxyQueryServiceSpanTransportTaskBackendTaskHandlerTelegrafBackendTelegrafHandlerUnauthorizedErrorUserBackendUserResponseVariableBackendVariableHandlerWithDebugHandlerWithHealthHandlerWithMaxBatchSizeBytesWithMetricsHandlerWithReadyHandlerWriteBackendWriteHandlerWriteHandlerOptionWriteUsageRecorderapiLinksbackupKVStorePathbackupShardPathbaseHandlerblacklistEndpointsbodyEchoerbucketIDPathbucketResponsebucketsResponsebuildDocumentsPathcancelPathcancelRunRequestcheckBucketWritePermissionscheckIDPathcheckLinkscheckResponsechecksIDLabelsIDPathchecksIDLabelsPathchecksIDMembersIDPathchecksIDMembersPathchecksIDOwnersIDPathchecksIDOwnersPathchecksIDPathchecksIDQueryPathchecksResponsecolumnFromCharactercompressWithGzipconvertRunconvertTaskcookieSessionNamecountReadercustomParseDurationdecodeCancelRunRequestdecodeCheckFilterdecodeCookieSessiondecodeDeleteAuthorizationRequestdecodeDeleteLabelMappingRequestdecodeDeleteLabelRequestdecodeDeleteMemberRequestdecodeDeleteRequestdecodeDeleteSourceRequestdecodeDeleteTaskRequestdecodeDeleteUserRequestdecodeForceRunRequestdecodeGetAuthorizationRequestdecodeGetBucketsRequestdecodeGetCheckRequestdecodeGetDocumentsRequestdecodeGetLabelMappingsRequestdecodeGetLabelRequestdecodeGetLabelsRequestdecodeGetLogsRequestdecodeGetMembersRequestdecodeGetNotificationEndpointRequestdecodeGetNotificationRuleRequestdecodeGetRunRequestdecodeGetRunsRequestdecodeGetSourceBucketsRequestdecodeGetSourceRequestdecodeGetSourcesRequestdecodeGetTaskRequestdecodeGetTasksRequestdecodeGetTelegrafRequestdecodeGetUserRequestdecodeGetUsersRequestdecodeGetVariablesRequestdecodeIDFromCtxdecodeLabelsdecodeNotificationEndpointFilterdecodeNotificationRuleFilterdecodePasswordResetRequestdecodePatchCheckRequestdecodePatchLabelRequestdecodePatchNotificationEndpointRequestdecodePatchNotificationRuleRequestdecodePatchSourceRequestdecodePatchUserRequestdecodePatchVariableRequestdecodePostCheckRequestdecodePostLabelMappingRequestdecodePostLabelRequestdecodePostMemberRequestdecodePostNotificationEndpointRequestdecodePostNotificationRuleRequestdecodePostSourceRequestdecodePostTaskRequestdecodePostUserRequestdecodePostVariableRequestdecodeProxyQueryRequestdecodePutCheckRequestdecodePutNotificationEndpointRequestdecodePutNotificationRuleRequestdecodePutTelegrafRequestdecodePutVariableRequestdecodeQueryRequestdecodeRetryRunRequestdecodeScraperTargetAddRequestdecodeScraperTargetIDRequestdecodeScraperTargetUpdateRequestdecodeScraperTargetsRequestdecodeSigninRequestdecodeSignoutRequestdecodeSourceQueryRequestdecodeStatusdecodeTelegrafConfigFilterdecodeUpdateTaskRequestdecodeUserResourceMappingFilterdecodeWriteRequestdeleteAuthorizationRequestdeleteLabelMappingRequestdeleteLabelRequestdeleteMemberRequestdeleteRequestdeleteRequestDecodedeleteSourceRequestdeleteTaskRequestdeleteUserRequestdocumentResponsedocumentServicedocumentsPathdocumentsResponseencodeCookieSessionencodeResponsefindSwaggerPathfirstLineAsErrorfluxParamsfluxRespforceRunRequestgetAuthorizationRequestgetBucketsRequestgetDocumentsRequestgetLabelMappingsRequestgetLabelRequestgetLabelsRequestgetLogsRequestgetLogsResponsegetMembersRequestgetNotificationRulesIDPathgetPanicLoggergetRunRequestgetRunsRequestgetScraperTargetsRequestgetSourceBucketsRequestgetSourceRequestgetSourcesRequestgetTargetsLinksgetTargetsResponsegetTaskRequestgetTasksRequestgetTelegrafPluginsgetUserRequestgetUsersRequestgetVariablesRequestgetVariablesResponsehttpClientTimeouthttpRunignoreMethodinfluxqlParseErrorREisDigitisValidMethodFnlabelIDPathlabelResponselabelsIDPathlabelsResponselangRequestlogEncodingErrormapURLPathmePasswordPathmsgInvalidGzipHeadermsgInvalidPrecisionnewBucketsResponsenewDeleteLabelHandlernewDeleteMemberHandlernewDocumentResponsenewDocumentsResponsenewFluxResponsenewGetLabelsHandlernewGetMembersHandlernewGetVariablesResponsenewLabelResponsenewLabelsResponsenewLegacyBackendnewLegacyHandlernewNotificationEndpointResponsenewNotificationEndpointsResponsenewPostLabelHandlernewPostMemberHandlernewResourceUserResponsenewResourceUsersResponsenewRunResponsenewRunsResponsenewSourceResponsenewSourcesResponsenewSwaggerLoadernewTaskResponsenewTasksPagingLinksnewTasksResponsenewTelegrafResponsenewTelegrafResponsesnewVariableResponsenotificationEndpointDecodernotificationEndpointEncodernotificationEndpointLinksnotificationEndpointResponsenotificationEndpointsIDLabelsIDPathnotificationEndpointsIDLabelsPathnotificationEndpointsIDMembersIDPathnotificationEndpointsIDMembersPathnotificationEndpointsIDOwnersIDPathnotificationEndpointsIDOwnersPathnotificationEndpointsIDPathnotificationEndpointsResponsenotificationRuleCreateEncodernotificationRuleDecodernotificationRuleLinksnotificationRuleResponsenotificationRulesIDLabelsIDPathnotificationRulesIDLabelsPathnotificationRulesIDMembersIDPathnotificationRulesIDMembersPathnotificationRulesIDOwnersIDPathnotificationRulesIDOwnersPathnotificationRulesIDPathnotificationRulesIDQueryPathnotificationRulesResponseopWriteHandlerorganizationIDPathorganizationsIDSecretsDeletePathorganizationsIDSecretsPathpanicLoggerpanicLoggerOncepanicMWpasswordResetRequestpasswordResetRequestBodypasswordSetRequestpatchCheckRequestpatchLabelRequestpatchNotificationEndpointRequestpatchNotificationRuleRequestpatchSourceRequestpatchUserRequestpatchVariableRequestpostCheckRequestpostFluxASTResponsepostLabelMappingRequestpostLabelRequestpostMemberRequestpostNotificationEndpointRequestpostNotificationRuleRequestpostSourceRequestpostTaskRequestpostUserRequestpostVariableRequestprefixBackupprefixBucketsprefixChecksprefixChronografprefixDeleteprefixDocumentsprefixLabelsprefixMeprefixNotificationEndpointsprefixNotificationRulesprefixOrganizationsprefixQueryprefixRestoreprefixSetupprefixSignInprefixSignOutprefixSourcesprefixTargetsprefixTasksprefixTelegrafprefixTelegrafPluginsprefixUsersprefixVariablesprefixWriteproxyHandlerputVariableRequestqueryBucketqueryOrganizationqueryParseErrorrequestVariableIDresourceIDMappingPathresourceIDPathresourceIDUserPathresourceUserResponseresourceUsersResponserestoreBucketPathrestoreKVPathrestoreShardPathretryRunRequestroutingQueryServiceruleResponseMetarunResponserunsResponseserveLinksHandlersessionAuthSchemesigninRequestsignoutRequestsourceResponsesourcesResponsestatusDecodesuggestionResponsesuggestionsResponseswaggerLoadertargetIDPathtargetLinkstargetResponsetargetsIDLabelsIDPathtargetsIDLabelsPathtargetsIDMembersIDPathtargetsIDMembersPathtargetsIDOwnersIDPathtargetsIDOwnersPathtaskIDPathtaskIDRunIDPathtaskIDRunsPathtaskResponsetasksIDLabelsIDPathtasksIDLabelsPathtasksIDLogsPathtasksIDMembersIDPathtasksIDMembersPathtasksIDOwnersIDPathtasksIDOwnersPathtasksIDPathtasksIDRunsIDLogsPathtasksIDRunsIDPathtasksIDRunsIDRetryPathtasksIDRunsPathtasksResponsetelegrafLinkstelegrafPluginsPathtelegrafResponsetelegrafResponsestelegrafsIDLabelsIDPathtelegrafsIDLabelsPathtelegrafsIDMembersIDPathtelegrafsIDMembersPathtelegrafsIDOwnersIDPathtelegrafsIDOwnersPathtelegrafsIDPathtokenAuthSchemetokenSchemetraceIDHeaderupdateTaskRequestusersIDPathusersPasswordPathvariableLinksvariableResponsewithFeatureProxyresHandlerbackupBackendcheckBackenddeleteBackenddocumentBackendfluxBackendnotificationEndpointBackendnotificationRuleBackendrestoreBackendscraperBackendsourceBackendtaskBackendtaskHandlertaskLoggertelegrafBackendvariableBackendwriteBackenderrorHandlergithub.com/influxdata/influxdb/v2/http/metric"github.com/influxdata/influxdb/v2/http/metric"github.com/influxdata/influxdb/v2/influxql"github.com/influxdata/influxdb/v2/influxql"/api/v2"/api/v2"/api/v2/checksfullPathparamsPoolGlobalOPTIONSglobalAllowedAddMatchedRouteToContextgetParamsputParamsnewCheckResponsenewChecksResponsehandleGetCheckshandleGetCheckQueryhandleGetCheckhandlePostCheckmapNewCheckLabelshandlePutCheckhandlePatchCheckhandleDeleteCheck/chronograf/api/v2/deletehandleDeletedocument"document"/api/v2/documentshandleGetDocuments/api/v2/queryhandleQuerypostFluxASTpostQueryAnalyzegetFluxSuggestionsgetFluxSuggestion"notificationEndpoint"/api/v2/notificationEndpointshandleGetNotificationEndpointshandleGetNotificationEndpointhandlePostNotificationEndpointmapNewNotificationEndpointLabelshandlePutNotificationEndpointhandlePatchNotificationEndpointhandleDeleteNotificationEndpointnotification_rule"notification_rule"/api/v2/notificationRulesnewNotificationRuleResponsenewNotificationRulesResponsehandleGetNotificationRuleshandleGetNotificationRuleQueryhandleGetNotificationRulehandlePostNotificationRulemapNewNotificationRuleLabelshandlePutNotificationRulehandlePatchNotificationRulehandleDeleteNotificationRuleScraperStorageService/api/v2/scrapershandlePostScraperTargethandleDeleteScraperTargethandlePatchScraperTargethandleGetScraperTargethandleGetScraperTargetsnewListTargetsResponsenewTargetResponse/api/v2/sourceshandlePostSourceQueryhandleGetSourcesBucketshandlePostSourcehandleGetSourcehandleGetSourceHealthhandleDeleteSourcehandleGetSourceshandlePatchSource/api/v2/swagger.json"/api/v2/swagger.json"loadErrassetswagger-loader"swagger-loader"handleGetTaskshandlePostTaskhandleGetTaskhandleUpdateTaskhandleDeleteTaskhandleGetLogshandleGetRunshandleForceRunhandleGetRunhandleCancelRunhandleRetryRunpopulateTaskCreateOrggetAuthorizationForTask/api/v2/tasks"telegraf"/api/v2/telegrafhandleGetTelegrafPluginshandleGetTelegrafshandleGetTelegrafhandlePostTelegrafhandlePutTelegrafhandleDeleteTelegraf/api/v2/telegrafs/api/v2/flags"/api/v2/flags""variable"/api/v2/variableshandleGetVariableshandleGetVariablehandlePostVariablehandlePatchVariablehandlePutVariablehandleDeleteVariable/api/v2/backuphandleBackupKVStorehandleBackupShard/api/v2/restorehandleRestoreKVStorehandleRestoreBuckethandleRestoreShard/api/v2/writemaxBatchSizeBytesfindBuckethandleWrite"/api/v2/backup"/api/v2/buckets"/api/v2/buckets"external"external"statusFeed"statusFeed"https://www.influxdata.com/feed/json"https://www.influxdata.com/feed/json""flags"/api/v2/labels"/api/v2/labels""/api/v2/variables"/api/v2/me"/api/v2/me""/api/v2/notificationRules""/api/v2/notificationEndpoints"/api/v2/orgs"/api/v2/orgs""/api/v2/query""ast"/api/v2/query/ast"/api/v2/query/ast"analyze"analyze"/api/v2/query/analyze"/api/v2/query/analyze"suggestions"suggestions"/api/v2/query/suggestions"/api/v2/query/suggestions""/api/v2/restore"/api/v2/setup"/api/v2/setup"signin"signin"/api/v2/signin"/api/v2/signin"signout"signout"/api/v2/signout"/api/v2/signout""/api/v2/sources""/api/v2/scrapers""swagger"/debug/pprof"/debug/pprof"health"health""/api/v2/tasks""/api/v2/checks""/api/v2/telegrafs"plugins"plugins"/api/v2/telegraf/plugins"/api/v2/telegraf/plugins"/api/v2/users"/api/v2/users""/api/v2/write""/api/v2/delete"url missing "url missing " APIHandler is a collection of all the service handlers. APIBackend is all services and associated parameters required to construct an APIHandler. if empty then assets are served from bindata. MaxBatchSizeBytes is the maximum number of bytes which can be written in a single points batch WriteParserMaxBytes specifies the maximum number of bytes that may be allocated when processing a single write request. A value of zero specifies there is no limit. WriteParserMaxLines specifies the maximum number of lines that may be parsed when processing a single WriteParserMaxValues specifies the maximum number of values that may be parsed when processing a single PrometheusCollectors exposes the prometheus collectors associated with an APIBackend. APIHandlerOptFn is a functional input param to set parameters on the APIHandler. WithResourceHandler registers a resource handler on the APIHandler. NewAPIHandler constructs all api handlers beneath it and returns an APIHandlerWithParserOptions(	models.WithParserMaxBytes(b.WriteParserMaxBytes),	models.WithParserMaxLines(b.WriteParserMaxLines),	models.WithParserMaxValues(b.WriteParserMaxValues),), when adding new links, please take care to keep this list alphabetical as this makes it easier to verify values against the swagger document.BackendInfluxqldQueryServicetoPlatformDocumentsEverySecondsHandlerConfigDefaultRoutingKeyfindMappingPingHandlerInfluxDBVersionInfluxqlHandlerInfluxQLBackendhandleInfluxqldQueryPointsWriterHandlerInfluxQLHandlerThresholdConfigBaseCheckLevelAllValuesGetLevelWithinTimeSinceStaleTimeReportZeroStatusMessageTemplateThresholdsMemberIDSourceIDteedRnenrcPasswordOldPasswordNewCharacterGetDocumentsShutdownTimeoutListenForSignalsnotifyOnSignalsDelimiterCommentPrefixDateTimeFormatPreferNoContentPreferNoContentWithErrorWithDefaultsanalyzeFluxQueryanalyzeInfluxQLQueryproxyRequestRunsbytesReadToinfluxdbrulequeryFluxqueryInfluxQLToInfluxdbTelegrafPluginsPluginfindPluginByNameTokenParserKeyStoreValidMethodsUseJSONNumberSkipClaimsValidationParseUnverifiedkeyStorenoAuthRouterRegisterNoAuthRouteisUserActiveextractAuthorizationextractSessionStatusResponseWriterstatusCodeStatusCodeClassvariableUpdate/Users/austinjaybecker/projects/abeck-go-testing/http/assets.go../../ui/build"../../ui/build"../../ui/build/index.html"../../ui/build/index.html"index.html"index.html" TODO: use platform version of the code AssetHandler is an http handler for serving chronograf assets. NewAssetHandler is the constructor an asset handler. ServeHTTP implements the http handler interface for serving assets./Users/austinjaybecker/projects/abeck-go-testing/http/auth_service.goplatcontext/api/v2/authorizations/:id"/api/v2/authorizations/:id""PATCH""DELETE"Failed to get organization"Failed to get organization" AuthorizationBackend is all services and associated parameters required to construct the AuthorizationHandler. NewAuthorizationBackend returns a new instance of AuthorizationBackend. AuthorizationHandler represents an HTTP API handler for authorizations. NewAuthorizationHandler returns a new instance of AuthorizationHandler. handleGetAuthorization is the HTTP handler for the GET /api/v2/authorizations/:id route. AuthorizationService connects to Influx via HTTP using tokens to manage authorizations FindAuthorizationByID finds the authorization against a remote influx server. FindAuthorizationByToken returns a single authorization by Token./Users/austinjaybecker/projects/abeck-go-testing/http/authentication_middleware.gosessErrtokenErrjsonwebgithub.com/influxdata/influxdb/v2/jsonweb"github.com/influxdata/influxdb/v2/jsonweb"NewTokenParserKeyStoreFuncEmptyKeyStoretoken required"token required"invalid auth scheme"invalid auth scheme"SpanFromContextUser is inactive"User is inactive"IsMalformedError AuthenticationHandler is a middleware for authenticating incoming requests. This is only really used for it's lookup method the specific http handler used to register routes does not matter. NewAuthenticationHandler creates an authentication handler. RegisterNoAuthRoute excludes routes from needing authentication. the handler specified here does not matter. ProbeAuthScheme probes the http request for the requests for token or cookie session. ServeHTTP extracts the session or token from the http request and places the resulting authorizer on the request context. TODO: this error will be nil if it gets here, this should be remedied with some  sentinel error I'm thinking jwt based auth is permission based rather than identity based and therefor has no associated user. if the user ID is invalid disregard the user active check if the error returned signifies ths token is not a well formed JWT then use it as a lookup key for its associated authorization otherwise return the error if the session is not expired, renew the session/Users/austinjaybecker/projects/abeck-go-testing/http/backup_service.go/kv"/kv"/api/v2/backup/kv/shards/:shardID"/shards/:shardID"/api/v2/backup/shards/:shardIDExtractFromHTTPRequestBackupHandler.handleBackupKVStore"BackupHandler.handleBackupKVStore"BackupHandler.handleBackupShard"BackupHandler.handleBackupShard""shardID"ParseInLocation/shards/%d"/shards/%d"/api/v2/backup/shards/%d "github.com/influxdata/influxdb/v2/bolt" BackupBackend is all services and associated parameters required to construct the BackupHandler. NewBackupBackend returns a new instance of BackupBackend. BackupHandler is http handler for backup service. NewBackupHandler creates a new handler at /api/v2/backup to receive backup requests. BackupService is the client implementation of influxdb.BackupService./Users/austinjaybecker/projects/abeck-go-testing/http/check_service.golabelBackendmemberBackendownerBackendorgNameStrorgIDStrdllidpctx/api/v2/checks/:id"/api/v2/checks/:id"/api/v2/checks/:id/query"/api/v2/checks/:id/query"/api/v2/checks/:id/members"/api/v2/checks/:id/members"/api/v2/checks/:id/members/:userID"/api/v2/checks/:id/members/:userID"/api/v2/checks/:id/owners"/api/v2/checks/:id/owners"/api/v2/checks/:id/owners/:userID"/api/v2/checks/:id/owners/:userID"/api/v2/checks/:id/labels"/api/v2/checks/:id/labels"/api/v2/checks/:id/labels/:lid"/api/v2/checks/:id/labels/:lid""PUT"json:"latestCompleted,omitempty"`json:"latestCompleted,omitempty"`json:"latestScheduled,omitempty"`json:"latestScheduled,omitempty"`json:"LastRunStatus,omitempty"`json:"LastRunStatus,omitempty"`json:"LastRunError,omitempty"`json:"LastRunError,omitempty"`json:"lastRunStatus,omitempty"`json:"lastRunStatus,omitempty"`json:"lastRunError,omitempty"`json:"lastRunError,omitempty"`json:"checks"`json:"checks"`/api/v2/checks/%s"/api/v2/checks/%s"/api/v2/checks/%s/labels"/api/v2/checks/%s/labels"/api/v2/checks/%s/members"/api/v2/checks/%s/members"/api/v2/checks/%s/owners"/api/v2/checks/%s/owners"/api/v2/checks/%s/query"/api/v2/checks/%s/query"Failed to retrieve task associated with check"Failed to retrieve task associated with check"checkID"checkID"Checks retrieved"Checks retrieved"Check query retrieved"Check query retrieved"check query"check query"Check retrieved"Check retrieved"orgID is invalid"orgID is invalid"invalid check id format"invalid check id format"unable to read HTTP body"unable to read HTTP body"malformed check body"malformed check body"Check replaced"Check replaced"Check patch"Check patch"Check deleted"Check deleted"check %q not found"check %q not found"json:"ownerID,omitempty"`json:"ownerID,omitempty"`json:"createdAt,omitempty"`json:"createdAt,omitempty"`json:"updatedAt,omitempty"`json:"updatedAt,omitempty"`json:"latestCompleted"`json:"latestCompleted"`json:"lastRunStatus"`json:"lastRunStatus"`json:"lastRunError"`json:"lastRunError"`json:"timeSince"`json:"timeSince"`json:"staleTime"`json:"staleTime"`json:"reportZero"`json:"reportZero"`json:"level"`json:"level"`json:"offset"`json:"offset"`json:"statusMessageTemplate"`json:"statusMessageTemplate"`json:"thresholds"`json:"thresholds"`json:"min,omitempty"`json:"min,omitempty"`json:"max,omitempty"`json:"max,omitempty"`json:"within"`json:"within"` CheckBackend is all services and associated parameters required to construct the CheckBackendHandler. NewCheckBackend returns a new instance of CheckBackend. CheckHandler is the handler for the check service NewCheckHandler returns a new instance of CheckHandler. TODO(desa): this should be handled in the check and not exposed in http land, but is currently blocking the FE. https://github.com/influxdata/influxdb/issues/15259 Ensure that we don't expose that this creates a task behind the scene handlePostCheck is the HTTP handler for the POST /api/v2/checks route. mapNewCheckLabels takes label ids from create check and maps them to the newly created check handlePutCheck is the HTTP handler for the PUT /api/v2/checks route. handlePatchCheck is the HTTP handler for the PATCH /api/v2/checks/:id route. CheckService is a client to interact with the handlers in this package over HTTP. It does not implement influxdb.CheckService because it returns a concrete representation of the API response and influxdb.Check as returned by that interface is not appropriate for this use case. FindCheckByID returns the Check matching the ID. FindCheck returns the first check matching the filter. CreateCheck creates a new check. UpdateCheck updates a check. PatchCheck changes the status, description or name of a check. DeleteCheck removes a check. TODO(gavincabbage): These structures should be in a common place, like other models, 		but the common influxdb.Check is an interface that is not appropriate for an API client./Users/austinjaybecker/projects/abeck-go-testing/http/chronograf_handler.gohe"/chronograf" ChronografHandler is an http handler for serving chronograf chronografs. NewChronografHandler is the constructor an chronograf handler. TODO(desa): what to do here?h.HandlerFunc("PUT", "/chronograf/v1/me", h.Service.UpdateMe(opts.Auth)) TODO(desa): what to do herelogger:      opts.logger,CustomLinks: opts.CustomLinks,/Users/austinjaybecker/projects/abeck-go-testing/http/client.godefaultOptsWithAddrWithContentTypeWithInsecureSkipVerifyWithStatusFnWithAuthToken NewHTTPClient creates a new httpc.Client type. This call sets all the options that are important to the http pkg on the httpc client. The default status fn and so forth will all be set for the caller. In addition, some options can be specified. Those will be added to the defaults. Service connects to an InfluxDB via HTTP. NewService returns a service that is an HTTP client to a remote. Address and token are needed for those services that do not use httpc.Client, but use those for configuring. Usually one would do: ``` c := NewHTTPClient(addr, token, insecureSkipVerify) s := NewService(c, addr token) So one should provide the same `addr` and `token` to both calls to ensure consistency in the behavior of the returned service. NewURL concats addr and path. NewClient returns an http.Client that pools connections and injects a span. SpanTransport injects the http.RoundTripper.RoundTrip() request with a span. RoundTrip implements the http.RoundTripper, intercepting the base round trippers call and injecting a span. DefaultTransport wraps http.DefaultTransport in SpanTransport to inject tracing headers into all outgoing requests. DefaultTransportInsecure is identical to DefaultTransport, with the exception that tls.Config is configured with InsecureSkipVerify set to true./Users/austinjaybecker/projects/abeck-go-testing/http/debug.go/debug/flush"/debug/flush" Flusher flushes data from a store to reset; used for testing. DebugFlush clears all services for testing./Users/austinjaybecker/projects/abeck-go-testing/http/delete_handler.godrdpcontextgithub.com/influxdata/influxdb/v2/predicate"github.com/influxdata/influxdb/v2/predicate""DeleteHandler"http/handleDelete"http/handleDelete"unable to create permission for bucket: %v"unable to create permission for bucket: %v"insufficient permissions to delete"insufficient permissions to delete"Not implemented"Not implemented"invalid request; error parsing request json"invalid request; error parsing request json"json:"start"`json:"start"`json:"stop"`json:"stop"`json:"predicate"`json:"predicate"`Invalid delete predicate node request"Invalid delete predicate node request"http/Delete"http/Delete"invalid RFC3339Nano for field start, please format your time with RFC3339Nano format, example: 2009-01-02T23:00:00Z"invalid RFC3339Nano for field start, please format your time with RFC3339Nano format, example: 2009-01-02T23:00:00Z"invalid RFC3339Nano for field stop, please format your time with RFC3339Nano format, example: 2009-01-01T23:00:00Z"invalid RFC3339Nano for field stop, please format your time with RFC3339Nano format, example: 2009-01-01T23:00:00Z"ToDataTypeapplication/json; charset=utf-8"application/json; charset=utf-8" DeleteBackend is all services and associated parameters required to construct the DeleteHandler. NewDeleteBackend returns a new instance of DeleteBackend DeleteHandler receives a delete request with a predicate and sends it to storage. NewDeleteHandler creates a new handler at /api/v2/delete to receive delete requests. DeleteRequest is the request send over http to delete points. org name DeleteService sends data over HTTP to delete points. DeleteBucketRangePredicate send delete request over http to delete points./Users/austinjaybecker/projects/abeck-go-testing/http/document_service.godocdocsoidStr"/api/v2/documents"/api/v2/documents/:ns"/api/v2/documents/:ns"/api/v2/documents/%s/%s"/api/v2/documents/%s/%s"json:"documents"`json:"documents"`Documents retrieved"Documents retrieved"url missing namespace"url missing namespace"Invalid orgID"Invalid orgID" DocumentService is an interface HTTP-exposed portion of the document service. DocumentBackend is all services and associated parameters required to construct the DocumentHandler. NewDocumentBackend returns a new instance of DocumentBackend. DocumentHandler represents an HTTP API handler for documents. NewDocumentHandler returns a new instance of DocumentHandler. TODO(desa): this should probably take a namespace handleGetDocuments is the HTTP handler for the GET /api/v2/documents/:ns route. NewDocumentService creates a client to connect to Influx via HTTP to manage documents. GetDocuments returns the documents for a `namespace` and an `orgID`. Returned documents do not  contain their content./Users/austinjaybecker/projects/abeck-go-testing/http/duration.goisNegativemeasureunit's'Âµ'Âµ''h''w''y'365overflowed duration %d%s: choose a smaller duration or INF"overflowed duration %d%s: choose a smaller duration or INF"0s"0s"876031536000000000000%dy"%dy"7202592000000000000%dmo"%dmo"%dw"%dw"%dd"%dd"%dh"%dh"%dm"%dm"%ds"%ds"%dms"%dms"%dus"%dus"%dns"%dns"'9' ErrInvalidDuration is returned when parsing a malformatted duration. ParseDuration parses a time duration from a string. This is needed instead of time.ParseDuration because this will support the full syntax that InfluxQL supports for specifying durations including weeks and days. Return an error if the string is blank or one character Split string into individual runes. Start with a zero duration. Check for a negative. Parsing loop. Find the number portion. Scan for the digits. Check if we reached the end of the string prematurely. Parse the numeric part. Extract the unit of measure. Check for `mo` and `ms`; month and millisecond, respectively. ms == milliseconds mo == month TODO(goller): use real duration values: https://github.com/influxdata/platform/issues/657 Check to see if we overflowed a duration FormatDuration formats a duration to a string. split splits a string into a slice of runes. isDigit returns true if the rune is a digit./Users/austinjaybecker/projects/abeck-go-testing/http/errors.gocontentTypeperrkhttpstderrorsunexpected status code: %s"unexpected status code: %s"StatusCodeToErrorCodeStatusUnsupportedMediaType415invalid media type: %q"invalid media type: %q"failed to read error response"failed to read error response"attempted to unmarshal error as JSON but failed: %q"attempted to unmarshal error as JSON but failed: %q"TrimSuffixunauthorized access"unauthorized access" AuthzError is returned for authorization errors. When this error type is returned, the user can be presented with a generic "authorization failed" error, but the system can log the underlying AuthzError() so that operators have insight into what actually failed with authorization. CheckErrorStatus for status and any error in the response. CheckError reads the http.Response and returns an error if one exists. It will automatically recognize the errors returned by Influx services and decode the error into an internal error type. If the error cannot be determined in that way, it will create a generic error message. If there is no error, then this returns nil. We will attempt to parse this error outside of this block. TODO(jsternberg): Figure out what to do here? Assume JSON if there is no content-type. given it was unset during attempt to unmarshal as JSON UnauthorizedError encodes a error message and status code for unauthorized access. InactiveUserError encode a error message and status code for inactive users./Users/austinjaybecker/projects/abeck-go-testing/http/handler.gohandlerSubsystem/ready"/ready"/debug"/debug""api"user_agent"user_agent"response_code"response_code"CounterOptsSubsystemConstLabelsNewCounterVecrequests_total"requests_total"Number of http requests received"Number of http requests received"HistogramOptsNewHistogramVecrequest_duration_seconds"request_duration_seconds"Time taken to respond to HTTP request"Time taken to respond to HTTP request"Error encoding response"Error encoding response" used for debug pprof at the default path. MetricsPath exposes the prometheus metrics over /metrics. ReadyPath exposes the readiness of the service over /ready. HealthPath exposes the health of the service over /health. DebugPath exposes /debug/pprof for go debugging. Handler provides basic handling of metrics, health and debug endpoints. All other requests are passed down to the sub handler. log logs all HTTP requests as they are served NewHandlerFromRegistry creates a new handler with the given name, and sets the /metrics endpoint to use the metrics from the given registry, after self-registering h's metrics. only gather metrics for system handlers gather metrics and traces for everything else ServeHTTP delegates a request to the appropriate subhandler. PrometheusCollectors satisfies prom.PrometheusCollector. If we encounter an error while encoding the response to an http request the best thing we can do is log that error, as we may have already written the headers for the http request in question./Users/austinjaybecker/projects/abeck-go-testing/http/health.go{"name":"influxdb", "message":"ready for queries and writes", "status":"pass", "checks":[], "version": %q, "commit": %q}`{"name":"influxdb", "message":"ready for queries and writes", "status":"pass", "checks":[], "version": %q, "commit": %q}` HealthHandler returns the status of the process./Users/austinjaybecker/projects/abeck-go-testing/http/influxdb/Users/austinjaybecker/projects/abeck-go-testing/http/influxdb/bucket.gonewClientnewTraceClientnewURLtraceClientnot supported"not supported" BucketService connects to Influx via HTTP using tokens to manage buckets TODO(desa): what to do about IDs?fluxQueryinfluxQuery/Users/austinjaybecker/projects/abeck-go-testing/http/influxdb/client.go This is copy of the method from chronograf/influx adapted for platform sources. traceClient always injects any opentracing trace into the client requests. Do injects the trace and then performs the request./Users/austinjaybecker/projects/abeck-go-testing/http/influxdb/source_proxy_query_service.gohreqrequestcompilercsvDialectplatformhttpFluxCompilerTypecompiler type not supported"compiler type not supported"json:"dialect"`json:"dialect"`compiler type not supported: %s"compiler type not supported: %s"ResultEncoderConfig','"organizationID"URL from source cannot be empty if the compiler type is influxql"URL from source cannot be empty if the compiler type is influxql"/query"/query"LogicalOptionlogicalPlannerheuristicPlannerRuleStackEntryFunctionNameProcedureSpecAddPredecessorsAddSuccessorsCallStackClearPredecessorsClearSuccessorsPredecessorsReplaceSpecSetBoundsShallowCopySuccessorsdisabledRulesaddRulesremoveRulesclearRulesmatchRulesdisableIntegrityChecksCreateInitialPlanlogicalPlannerOptionsWithLogicalPlannerOptionspassed compiler is not of type 'influxql'"passed compiler is not of type 'influxql'"unsupported dialect %T"unsupported dialect %T"NewMultiResultEncoderNewResponseIterator TODO(fntlnz): configure authentication methods username/password and stuffOperationOperationIDOperationSpecOperationKindOperationSourceEdgeChildResourceManagementMemoryBytesQuotaOperationsEdgesparentscomputeLookupdetermineParentsChildrenAndRootsvisitplanCheckIntegrityTopDownWalkBottomUpWalkTopologicalWalk/Users/austinjaybecker/projects/abeck-go-testing/http/label_service.gourlPathlabelID/api/v2/labels/:id"/api/v2/labels/:id"Label created"Label created"label requires a name"label requires a name"label requires a valid orgID"label requires a valid orgID"unable to decode label request"unable to decode label request"Labels retrieved"Labels retrieved"Label retrieved"Label retrieved"label id is not valid"label id is not valid"Label deleted"Label deleted""labelID"Label updated"Label updated"/api/v2/labels/%s"/api/v2/labels/%s"Invalid post label map request"Invalid post label map request"url missing resource id"url missing resource id""lid"label id is missing"label id is missing"/api/v2/"/api/v2/" LabelHandler represents an HTTP API handler for labels NewLabelHandler returns a new instance of LabelHandler handlePostLabel is the HTTP handler for the POST /api/v2/labels route. TODO(jm): ensure that the specified org actually exists handleGetLabels is the HTTP handler for the GET /api/v2/labels route. handleGetLabel is the HTTP handler for the GET /api/v2/labels/id route. handleDeleteLabel is the HTTP handler for the DELETE /api/v2/labels/:id route. handlePatchLabel is the HTTP handler for the PATCH /api/v2/labels route. LabelBackend is all services and associated parameters required to construct label handlers. newGetLabelsHandler returns a handler func for a GET to /labels endpoints newPostLabelHandler returns a handler func for a POST to /labels endpoints newDeleteLabelHandler returns a handler func for a DELETE to /labels endpoints LabelService connects to Influx via HTTP using tokens to manage labels FindLabelByID returns a single label by ID. FindLabels is a client for the find labels response from the server. FindResourceLabels returns a list of labels, derived from a label mapping filter. CreateLabel creates a new label. this is super dirty >_< UpdateLabel updates a label and returns the updated label. DeleteLabel removes a label by ID. CreateLabelMapping will create a labbel mapping/Users/austinjaybecker/projects/abeck-go-testing/http/legacy/Users/austinjaybecker/projects/abeck-go-testing/http/legacy/backend.goDefaultChunkSizeInflux1xAuthenticationHandlerNewHandlerConfigNewInflux1xAuthenticationHandlerNewInfluxQLBackendNewInfluxQLHandlerNewPingHandlerNewPointsWriterBackendNewWriterHandlerPointsWriterBackendnewWriteUsageRecorderparseTokenunauthorizedErrorwriteUsageRecorderhttp2legacyinfluxql-default-routing-key"influxql-default-routing-key"defaultQueue"defaultQueue"Default routing key for publishing new query requests"Default routing key for publishing new query requests"/ping"/ping" Handler is a collection of all the service handlers. HandlerConfig provides configuration for the legacy handler. Opts returns the CLI options for use with kit/cli. Currently set values on c are provided as the defaults.parseCredentials/Users/austinjaybecker/projects/abeck-go-testing/http/legacy/common.goinsufficient permissions; session not supported"insufficient permissions; session not supported" getAuthorization extracts authorization information from a context.Context. It guards against non influxdb.Authorization values for authorization and InfluxQL feature flag not enabled./Users/austinjaybecker/projects/abeck-go-testing/http/legacy/influx1x_authentication_handler.goerristrsIndexByteunable to parse authentication credentials"unable to parse authentication credentials" NewInflux1xAuthenticationHandler creates an authentication handler to process InfluxDB 1.x authentication requests. The ping endpoint does not need authorization Token <token> Token <username>:<token> parseCredentials parses a request and returns the authentication credentials. The credentials may be present as URL query params, or as a Basic Authentication header. As params: http://127.0.0.1/query?u=username&p=token As basic auth: http://username:token@127.0.0.1 As Token in Authorization header: Token <username:token> Check for username and password in URL params. Check for the HTTP Authorization header. Check for Bearer token. fallback to only a token Check for basic auth. unauthorizedError encodes a error message and status code for unauthorized access./Users/austinjaybecker/projects/abeck-go-testing/http/legacy/influxql_handler.goinfluxqld InfluxqlHandler mimics the /query handler from influxdb, but, enriches with org and forwards requests to the transpiler service. NewInfluxQLBackend constructs an InfluxQLBackend from a LegacyBackend. NewInfluxQLHandler returns a new instance of InfluxqlHandler to handle influxql v1 queries DefaultChunkSize is the default number of points to write in one chunk./Users/austinjaybecker/projects/abeck-go-testing/http/legacy/influxqld_handler.gofhschunkSizerawParamsrespSizeiocountergithub.com/influxdata/flux/iocounter"github.com/influxdata/flux/iocounter"Trace-Id"Trace-Id""handleInfluxqldQuery"InfoFromSpaninsufficient permissions"insufficient permissions"application/vnd.influxql"application/vnd.influxql""params"error parsing query parameters"error parsing query parameters"error parsing json value"error parsing json value""true"chunk_size"chunk_size"EncodingFormatFromMimeTypeUser-Agent"User-Agent"error writing response to client"error writing response to client" HandleQuery mimics the influxdb 1.0 /query Attempt to read the form value from the "q" form value. If we have a multipart/form-data, try to retrieve a file from 'q'. parse the parameters Convert json.Number into int64 and float64 values Parse chunk size. Use default if not provided or cannot be parsed Only record the error headers IFF nothing has been written to w./Users/austinjaybecker/projects/abeck-go-testing/http/legacy/ping_handler.go"HEAD"cloud2"cloud2" handlePostLegacyWrite is the HTTP handler for the POST /write route./Users/austinjaybecker/projects/abeck-go-testing/http/legacy/router.goruntime/debug"runtime/debug"path not found"path not found""Allow"allow: %s"allow: %s"a panic has occurred"a panic has occurred"%s: %v"%s: %v"ErrorLevel"panic" NewRouter returns a new router with a 404 handler, a 405 handler, and a panic handler. notFound represents a 404 handler that return a JSON response. methodNotAllowed represents a 405 handler that return a JSON response. panic handles panics recovered from http handlers. It returns a json response with http status code 500 and the recovered error message. getPanicLogger returns a logger for panicHandler./Users/austinjaybecker/projects/abeck-go-testing/http/legacy/write_handler.gogithub.com/influxdata/influxdb/v2/http/points"github.com/influxdata/influxdb/v2/http/points"http/v1WriteHandler"http/v1WriteHandler"points_writer"points_writer"MethodPost"WriteHandler"NewStatusResponseWriterParsedPointsRawSizeparsePointsunexpected error writing points to database"unexpected error writing points to database"insufficient permissions for write"insufficient permissions for write"no dbrp mapping found"no dbrp mapping found"missing db"missing db"Content-Encoding"Content-Encoding"BatchReadCloser PointsWriterBackend contains all the services needed to run a PointsWriterHandler. NewPointsWriterBackend creates a new backend for legacy work. PointsWriterHandler represents an HTTP API handler for writing points. NewWriterHandler returns a new instance of PointsWriterHandler. WriteHandlerOption is a functional option for a *PointsWriterHandler WithMaxBatchSizeBytes configures the maximum size for a (decompressed) points batch allowed by the write handler ServeHTTP implements http.Handler handleWrite handles requests for the v1 write endpoint Close around the requestBytes variable to placate the linter. findBucket finds a bucket for the specified database and retention policy combination. checkBucketWritePermissions checks an Authorizer for write permissions to a specific Bucket. findMapping finds a DBRPMappingV2 for the database and retention policy combination. Can't get a direct pointer to `true`... writeRequest is a transport-agnostic write request. It holds all inputs for processing a v1 write request. decodeWriteRequest extracts write request information from an inbound http.Request and returns a writeRequest./Users/austinjaybecker/projects/abeck-go-testing/http/legacy/write_usage_recorder.go/Users/austinjaybecker/projects/abeck-go-testing/http/legacy.goinfluxqlBackendpointsWriterBackendgithub.com/influxdata/influxdb/v2/http/legacy"github.com/influxdata/influxdb/v2/http/legacy" newLegacyBackend constructs a legacy backend from an api backend. TODO(sgc): /write supportMaxBatchSizeBytes:     b.APIBackend.MaxBatchSizeBytes, newLegacyHandler constructs a legacy handler from a backend./Users/austinjaybecker/projects/abeck-go-testing/http/metric/Users/austinjaybecker/projects/abeck-go-testing/http/metric/recorder.goNopEventRecorder EventRecorder records meta-data associated with http requests. Event represents the meta data associated with an API request. NopEventRecorder never records events. Record never records events./Users/austinjaybecker/projects/abeck-go-testing/http/middleware.goerrReferenceerrFielderrReferenceFieldinvalidMethodFnsrwrawHeadrawTailsourceHeadsourceTailcompareRawSourceURLspartsMatchrawPathshiftPathignoreMapignoredMethodsTeeReaderPlatformErrorCodeHeaderX-Platform-Error-Codeerror_code"error_code"status_code"status_code"response_size"response_size"content_length"content_length"referrer"referrer"remote"remote"ByteString"body"/api/v2/orgs/:id/secrets"/api/v2/orgs/:id/secrets"/api/v2/orgs/:id/secrets/delete"/api/v2/orgs/:id/secrets/delete"/api/v2/me/password/api/v2/users/:id/password/api/v2/packages/apply"/api/v2/packages/apply"/api/v2/notificationEndpoints/:id LoggingMW middleware for logging inflight http requests. ugh, should probably make this whole operation use a trie TODO(@jsteenb2): make this a stronger type that handlers can register routes that should not be logged./Users/austinjaybecker/projects/abeck-go-testing/http/mock/Users/austinjaybecker/projects/abeck-go-testing/http/mock/middleware.goNewAuthMiddlewareHandlerauthMiddlewareHandler NewAuthMiddlewareHandler create a mocked middleware handler./Users/austinjaybecker/projects/abeck-go-testing/http/mocks/Users/austinjaybecker/projects/abeck-go-testing/http/mocks/bucket_service.goMockDBRPMappingServiceMockDBRPMappingServiceMockRecorderMockEventRecorderMockEventRecorderMockRecorderMockOrganizationServiceMockOrganizationServiceMockRecorderMockPointsWriterMockPointsWriterMockRecorderNewMockDBRPMappingServiceNewMockEventRecorderNewMockOrganizationServiceNewMockPointsWriter/Users/austinjaybecker/projects/abeck-go-testing/http/mocks/dbrp_mapping_service.goarg3"Find""FindBy" Source: github.com/influxdata/influxdb/v2 (interfaces: DBRPMappingService) MockDBRPMappingService is a mock of DBRPMappingService interface MockDBRPMappingServiceMockRecorder is the mock recorder for MockDBRPMappingService NewMockDBRPMappingService creates a new mock instance Find mocks base method Find indicates an expected call of Find FindBy mocks base method FindBy indicates an expected call of FindBy/Users/austinjaybecker/projects/abeck-go-testing/http/mocks/dbrp_mapping_service_v2.go/Users/austinjaybecker/projects/abeck-go-testing/http/mocks/event_recorder.go"Record" Source: github.com/influxdata/influxdb/v2/http/metric (interfaces: EventRecorder) MockEventRecorder is a mock of EventRecorder interface MockEventRecorderMockRecorder is the mock recorder for MockEventRecorder NewMockEventRecorder creates a new mock instance Record mocks base method Record indicates an expected call of Record/Users/austinjaybecker/projects/abeck-go-testing/http/mocks/organization_service.go"CreateOrganization""DeleteOrganization""FindOrganization""FindOrganizationByID""FindOrganizations""UpdateOrganization" Source: github.com/influxdata/influxdb/v2 (interfaces: OrganizationService) MockOrganizationService is a mock of OrganizationService interface MockOrganizationServiceMockRecorder is the mock recorder for MockOrganizationService NewMockOrganizationService creates a new mock instance CreateOrganization mocks base method CreateOrganization indicates an expected call of CreateOrganization DeleteOrganization mocks base method DeleteOrganization indicates an expected call of DeleteOrganization FindOrganization mocks base method FindOrganization indicates an expected call of FindOrganization FindOrganizationByID mocks base method FindOrganizationByID indicates an expected call of FindOrganizationByID FindOrganizations mocks base method FindOrganizations indicates an expected call of FindOrganizations UpdateOrganization mocks base method UpdateOrganization indicates an expected call of UpdateOrganization/Users/austinjaybecker/projects/abeck-go-testing/http/mocks/points_writer.go"WritePoints" Source: github.com/influxdata/influxdb/v2/storage (interfaces: PointsWriter) MockPointsWriter is a mock of PointsWriter interface MockPointsWriterMockRecorder is the mock recorder for MockPointsWriter NewMockPointsWriter creates a new mock instance WritePoints mocks base method WritePoints indicates an expected call of WritePoints/Users/austinjaybecker/projects/abeck-go-testing/http/no_assets.go/Users/austinjaybecker/projects/abeck-go-testing/http/notification_endpoint.gonrefldsfieldMapughhhnewEndpointgithub.com/influxdata/influxdb/v2/notification/endpoint"github.com/influxdata/influxdb/v2/notification/endpoint""/api/v2/notificationEndpoints/:id"/api/v2/notificationEndpoints/:id/members"/api/v2/notificationEndpoints/:id/members"/api/v2/notificationEndpoints/:id/members/:userID"/api/v2/notificationEndpoints/:id/members/:userID"/api/v2/notificationEndpoints/:id/owners"/api/v2/notificationEndpoints/:id/owners"/api/v2/notificationEndpoints/:id/owners/:userID"/api/v2/notificationEndpoints/:id/owners/:userID"/api/v2/notificationEndpoints/:id/labels"/api/v2/notificationEndpoints/:id/labels"/api/v2/notificationEndpoints/:id/labels/:lid"/api/v2/notificationEndpoints/:id/labels/:lid"json:"notificationEndpoints"`json:"notificationEndpoints"`/api/v2/notificationEndpoints/%s"/api/v2/notificationEndpoints/%s"/api/v2/notificationEndpoints/%s/labels"/api/v2/notificationEndpoints/%s/labels"/api/v2/notificationEndpoints/%s/members"/api/v2/notificationEndpoints/%s/members"/api/v2/notificationEndpoints/%s/owners"/api/v2/notificationEndpoints/%s/owners"NotificationEndpoints retrieved"NotificationEndpoints retrieved"NotificationEndpoint retrieved"NotificationEndpoint retrieved"NotificationEndpoint created"NotificationEndpoint created"NotificationEndpoint replaced"NotificationEndpoint replaced"NotificationEndpoint patch"NotificationEndpoint patch"http/handleDeleteNotificationEndpoint"http/handleDeleteNotificationEndpoint"Bad Secret Key in endpoint "Bad Secret Key in endpoint "NotificationEndpoint deleted"NotificationEndpoint deleted"notificationEndpointID"notificationEndpointID"invalid ID: please provide a valid ID"invalid ID: please provide a valid ID"Endpoints-password"-password"-routing-key"-routing-key""routingKey"-token"-token"-username"-username" NotificationEndpointBackend is all services and associated parameters required to construct the NotificationEndpointBackendHandler. NewNotificationEndpointBackend returns a new instance of NotificationEndpointBackend. NotificationEndpointHandler is the handler for the notificationEndpoint service NewNotificationEndpointHandler returns a new instance of NotificationEndpointHandler. handlePostNotificationEndpoint is the HTTP handler for the POST /api/v2/notificationEndpoints route. handlePutNotificationEndpoint is the HTTP handler for the PUT /api/v2/notificationEndpoints route. handlePatchNotificationEndpoint is the HTTP handler for the PATCH /api/v2/notificationEndpoints/:id route. NotificationEndpointService is an http client for the influxdb.NotificationEndpointService server implementation. NewNotificationEndpointService constructs a new http NotificationEndpointService. FindNotificationEndpointByID returns a single notification endpoint by ID. FindNotificationEndpoints returns a list of notification endpoints that match filter and the total count of matching notification endpoints. CreateNotificationEndpoint creates a new notification endpoint and sets b.ID with the new identifier. TODO(@jsteenb2): this is unsatisfactory, we have no way of grabbing the new notification endpoint without  serious hacky hackertoning. Put it on the list... :sadpanda: UpdateNotificationEndpoint updates a single notification endpoint. Returns the new notification endpoint after update. PatchNotificationEndpoint updates a single  notification endpoint with changeset. Returns the new notification endpoint state after update. DeleteNotificationEndpoint removes a notification endpoint by ID, returns secret fields, orgID for further deletion. TODO: axe this delete design, makes little sense in how its currently being done. Right now, as an http client,  I am forced to know how the store handles this and then figure out what the server does in between me and that store,  then see what falls out :flushed... for now returning nothing for secrets, orgID, and only returning an error. This makes  the code/design smell super obvious imo this makes me queezy and altogether sad/Users/austinjaybecker/projects/abeck-go-testing/http/notification_rule.gokeyvalueresponseMetatppnrrstsnewRulegithub.com/influxdata/influxdb/v2/notification/rule"github.com/influxdata/influxdb/v2/notification/rule"/api/v2/notificationRules/:id"/api/v2/notificationRules/:id"/api/v2/notificationRules/:id/query"/api/v2/notificationRules/:id/query"/api/v2/notificationRules/:id/members"/api/v2/notificationRules/:id/members"/api/v2/notificationRules/:id/members/:userID"/api/v2/notificationRules/:id/members/:userID"/api/v2/notificationRules/:id/owners"/api/v2/notificationRules/:id/owners"/api/v2/notificationRules/:id/owners/:userID"/api/v2/notificationRules/:id/owners/:userID"/api/v2/notificationRules/:id/labels"/api/v2/notificationRules/:id/labels"/api/v2/notificationRules/:id/labels/:lid"/api/v2/notificationRules/:id/labels/:lid"json:"notificationRules"`json:"notificationRules"`/api/v2/notificationRules/%s"/api/v2/notificationRules/%s"/api/v2/notificationRules/%s/labels"/api/v2/notificationRules/%s/labels"/api/v2/notificationRules/%s/members"/api/v2/notificationRules/%s/members"/api/v2/notificationRules/%s/owners"/api/v2/notificationRules/%s/owners"/api/v2/notificationRules/%s/query"/api/v2/notificationRules/%s/query"Notification rules retrieved"Notification rules retrieved"http/handleGetNotificationRuleQuery"http/handleGetNotificationRuleQuery"Notification rule query retrieved"Notification rule query retrieved"notificationRuleQuery"notificationRuleQuery"Notification rule retrieved"Notification rule retrieved""notificationRule""tag""resourceID"Notification rule created"Notification rule created"Notification rule updated"Notification rule updated"Notification rule patch"Notification rule patch"Notification rule deleted"Notification rule deleted"notificationRuleID"notificationRuleID" NotificationRuleBackend is all services and associated parameters required to construct the NotificationRuleBackendHandler. NewNotificationRuleBackend returns a new instance of NotificationRuleBackend. NotificationRuleHandler is the handler for the notification rule service NewNotificationRuleHandler returns a new instance of NotificationRuleHandler. TODO(desa): this should be handled in the rule service and not exposed in http land, but is currently blocking the FE. https://github.com/influxdata/influxdb/issues/15259 ignore malformed tag pairs handlePostNotificationRule is the HTTP handler for the POST /api/v2/notificationRules route. handlePutNotificationRule is the HTTP handler for the PUT /api/v2/notificationRule route. handlePatchNotificationRule is the HTTP handler for the PATCH /api/v2/notificationRule/:id route. NotificationRuleService is an http client that implements the NotificationRuleStore interface NewNotificationRuleService wraps an httpc.Client in a NotificationRuleService CreateNotificationRule creates a new NotificationRule from a NotificationRuleCreate the Status on the NotificationRuleCreate is used to determine the status (active/inactive) of the associated Task FindNotificationRuleByID finds and returns one Notification Rule with a matching ID FindNotificationRules returns a list of notification rules that match filter and the total count of matching notification rules. loop over tags and append a string of format key:value for each UpdateNotificationRule updates a single notification rule. Returns the new notification rule after update. PatchNotificationRule updates a single  notification rule with changeset. Returns the new notification rule state after update. DeleteNotificationRule removes a notification rule by ID./Users/austinjaybecker/projects/abeck-go-testing/http/platform_handler.goassetHandlerlegacyBackendlhwrappedHandlerNewHandlerSetCORSSkipOptions/v1"/v1"/chronograf/"/chronograf/"/private/"/private/" PlatformHandler is a collection of all the service handlers. NewPlatformHandler returns a platform handler that serves the API and associated assets. TODO(affo): change this to be mounted prefixes: https://github.com/influxdata/idpe/issues/6689. Serve the chronograf assets for any basepath that does not start with addressable parts of the platform API./Users/austinjaybecker/projects/abeck-go-testing/http/points/Users/austinjaybecker/projects/abeck-go-testing/http/points/batch_reader.goErrMaxBatchSizeExceededmsgUnableToReadDatamsgWritingRequiresPointsopPointsWriterio2github.com/influxdata/influxdb/v2/kit/io"github.com/influxdata/influxdb/v2/kit/io""gzip"x-gzip"x-gzip"LimitedReadCloserlimitExceededNewLimitedReadCloser BatchReadCloser (potentially) wraps an io.ReadCloser in Gzip decompression and limits the reading to a specific number of bytes./Users/austinjaybecker/projects/abeck-go-testing/http/points/points_parser.gocerristio.io/pkg/log"istio.io/pkg/log"points batch is too large"points batch is too large"http/pointsWriter"http/pointsWriter"unable to read data"unable to read data"writing requires points"writing requires points"write points"write points"ErrHeaderErrChecksumStartSpanFromContextWithOperationNameencoding and parsing"encoding and parsing"ParsePointsWithPrecisionvalues_total"values_total"Error parsing points"Error parsing points"ErrReadLimitExceededread request body"read request body"request_bytes"request_bytes" ErrMaxBatchSizeExceeded is returned when a points batch exceeds the defined upper limit in bytes. This pertains to the size of the batch after inflation from any compression (i.e. ungzipped). ParsedPoints contains the points parsed as well as the total number of bytes after decompression. Parser parses batches of Points.ParserOptions []models.ParserOption Parse parses the points from an io.ReadCloser for a specific Bucket. TODO - backport these if errors.Is(err, models.ErrLimitMaxBytesExceeded) || 	errors.Is(err, models.ErrLimitMaxLinesExceeded) || 	errors.Is(err, models.ErrLimitMaxValuesExceeded) { 	code = influxdb.ETooLarge } NewParser returns a new ParserparserOptions ...models.ParserOptionParserOptions: parserOptions,/Users/austinjaybecker/projects/abeck-go-testing/http/proxy_handler.go withFeatureProxy wraps an HTTP handler in a proxyHandler proxyHandler is a wrapper around an http.Handler that conditionally forwards a request to another HTTP backend using a proxy. If the proxy doesn't decide to forward the request, we fall-back to our normal http.Handler behavior. ServeHTTP implements http.Handler interface. It first FeatureProxyHandler is an HTTP proxy that conditionally forwards requests to another backend. NoopProxyHandler is a no-op FeatureProxyHandler. It should be used if no feature-flag driven proxying is necessary. Do implements FeatureProxyHandler./Users/austinjaybecker/projects/abeck-go-testing/http/query.goerrCountcharcharStrlineStrencConfignoHeaderprtranspilerjson:"extern,omitempty"`json:"extern,omitempty"`json:"ast,omitempty"`json:"ast,omitempty"`json:"now"`json:"now"`json:"bucket,omitempty"`json:"bucket,omitempty"`json:"header"`json:"header"`json:"delimiter"`json:"delimiter"`json:"commentPrefix"`json:"commentPrefix"`json:"dateTimeFormat"`json:"dateTimeFormat"`"RFC3339"request body requires either query or AST`request body requires either query or AST`unknown query type: %s`unknown query type: %s`bucket parameter is required for influxql queries"bucket parameter is required for influxql queries"invalid dialect comment prefix: must be length 0 or 1"invalid dialect comment prefix: must be length 0 or 1"invalid dialect delimeter: must be length 1"invalid dialect delimeter: must be length 1"DecodeRuneInStringRuneError65533invalid dialect delimeter character"invalid dialect delimeter character"unknown dialect annotation type: %s`unknown dialect annotation type: %s`"RFC3339Nano"unknown dialect date time format: %s`unknown dialect date time format: %s`json:"errors"`json:"errors"`json:"line"`json:"line"`json:"character"`json:"character"`unknown query request type %s"unknown query request type %s"CreateVisitorinfluxql query error is not formatted as expected: got %d matches expected 4"influxql query error is not formatted as expected: got %d matches expected 4"failed to parse line number from error mesage: %s -> %v"failed to parse line number from error mesage: %s -> %v"failed to parse character number from error mesage: %s -> %v"failed to parse character number from error mesage: %s -> %v"^(.+) at line (\d+), char (\d+)$`^(.+) at line (\d+), char (\d+)$`ASTCompilerNoContentDialectCompressionFormatCompressionNoContentWithErrorDialectunsupported compiler %T"unsupported compiler %T"#"#"application/vnd.flux"application/vnd.flux"PreferHeaderKeyPreferPreferNoContentHeaderValuereturn-no-contentPreferNoContentWErrHeaderValuereturn-no-content-with-error QueryRequest is a flux query request. Flux fields InfluxQL fields PreferNoContent specifies if the Response to this request should contain any result. This is done for avoiding unnecessary bandwidth consumption in certain cases. For example, when the query produces side effects and the results do not matter. E.g.: 	from(...) |> ... |> to() For example, tasks do not use the results of queries, but only care about their side effects. To obtain a QueryRequest with no result, add the header `Prefer: return-no-content` to the HTTP request. PreferNoContentWithError is the same as above, but it forces the Response to contain an error if that is a Flux runtime error encoded in the response body. To obtain a QueryRequest with no result but runtime errors, add the header `Prefer: return-no-content-with-error` to the HTTP request. QueryDialect is the formatting options for the query response. WithDefaults adds default values to the request. Validate checks the query request and returns an error if the request is invalid. QueryAnalysis is a structured response of errors. Analyze attempts to parse the query request and returns any errors encountered in a structured way. ProxyRequest returns a request to proxy from the flux. Query is preferred over AST Use default transpiler dialect TODO(nathanielc): Use commentPrefix and dateTimeFormat once they are supported. QueryRequestFromProxyRequest converts a query.ProxyRequest into a QueryRequest. The ProxyRequest must contain supported compilers and dialects otherwise an error occurs./Users/austinjaybecker/projects/abeck-go-testing/http/query_handler.goqhhdfilteredParamssuggestioncompleterqreqitrpreq/api/v2/query/suggestions/:name"/api/v2/query/suggestions/:name"http/handlePostQuery"http/handlePostQuery""FluxHandler"TraceFieldsInfoFromContextauthorization is invalid or missing in the query request"authorization is invalid or missing in the query request"failed to decode request body"failed to decode request body"unsupported dialect over HTTP: %T"unsupported dialect over HTTP: %T"Error writing response to client"Error writing response to client"invalid json"invalid json"invalid AST"invalid AST"json:"params"`json:"params"`json:"funcs"`json:"funcs"`DefaultDialectheaders must be key value pairs"headers must be key value pairs"query health"query health"StatusFailfailcould not form URL"could not form URL"error getting response"error getting response"http error %v"http error %v"error decoding JSON response"error decoding JSON response"internal-routingQueryService"internal-routingQueryService" FluxBackend is all services and associated parameters required to construct the FluxHandler. NewFluxBackend returns a new instance of FluxBackend. HTTPDialect is an encoding dialect that can write metadata to HTTP headers FluxHandler implements handling flux queries. Prefix provides the route prefix. NewFluxHandler returns a new handler at /api/v2/query for flux queries. query reponses can optionally be gzip encoded TODO(desa): I really don't like how we're recording the usage metrics here Ideally this will be moved when we solve https://github.com/influxdata/influxdb/issues/13403 This should be sufficient for the time being as it should only be single endpoint. Transform the context into one with the request's authorization. postFluxAST returns a flux AST for provided flux string postQueryAnalyze parses a query and returns any query errors. fluxParams contain flux funciton parameters as defined by the semantic graph suggestionResponse provides the parameters available for a given Flux function suggestionsResponse provides a list of available Flux functions getFluxSuggestions returns a list of available Flux functions for the Flux Builder getFluxSuggestion returns the function parameters for the requested function PrometheusCollectors satisifies the prom.PrometheusCollector interface. TODO: gather and return relevant metrics. FluxService connects to Influx via HTTP using tokens to run queries. Query runs a flux query against a influx server and sends the results to the io.Writer. Will use the token from the context over the token within the service struct. Now that the request is all set, we can apply header mutators. FluxQueryService implements query.QueryService by making HTTP requests to the /api/v2/query API endpoint. Query runs a flux query against a influx server and decodes the result Can't defer resp.Body.Close here because the CSV decoder depends on reading from resp.Body after this function returns. GetQueryResponse runs a flux query with common parameters and returns the response from the query service. Default headers. Apply custom headers. GetQueryResponseBody reads the body of a response from some query service. It also checks for errors in the response. SimpleQuery runs a flux query with common parameters and returns CSV results. routingQueryService routes queries to specific query services based on their compiler type. InfluxQLService handles queries with compiler type of "influxql" DefaultService handles all other queries Produce combined check response/Users/austinjaybecker/projects/abeck-go-testing/http/ready.gogithub.com/influxdata/influxdb/v2/toml"github.com/influxdata/influxdb/v2/toml"json:"started"`json:"started"`json:"up"`json:"up"`"ready"Error encoding status data: %v
"Error encoding status data: %v\n" ReadyHandler is a default readiness handler. The default behaviour is always ready. TODO(jsteenb2): learn why and leave comment for this being a toml.Duration/Users/austinjaybecker/projects/abeck-go-testing/http/redoc.go<!DOCTYPE html>
<html>
  <head>
    <title>Chronograf API</title>
    <!-- needed for adaptive design -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--
    ReDoc doesn't change outer page styles
    -->
    <style>
      body {
        margin: 0;
        padding: 0;
      }
    </style>
  </head>
  <body>
    <redoc spec-url='%s' suppressWarnings=true></redoc>
	<script src="https://cdn.jsdelivr.net/npm/redoc/bundles/redoc.standalone.js"> </script>
  </body>
</html>
`<!DOCTYPE html>
<html>
  <head>
    <title>Chronograf API</title>
    <!-- needed for adaptive design -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--
    ReDoc doesn't change outer page styles
    -->
    <style>
      body {
        margin: 0;
        padding: 0;
      }
    </style>
  </head>
  <body>
    <redoc spec-url='%s' suppressWarnings=true></redoc>
	<script src="https://cdn.jsdelivr.net/npm/redoc/bundles/redoc.standalone.js"> </script>
  </body>
</html>
`/Users/austinjaybecker/projects/abeck-go-testing/http/requests.goreqIDPlease provide either bucketID or bucket"Please provide either bucketID or bucket" OrgID is the http query parameter to specify an organization by ID. Org is the http query parameter that take either the ID or Name interchangeably BucketID is the http query parameter to specify an bucket by ID. Bucket is the http query parameter take either the ID or Name interchangably queryOrganization returns the organization for any http request. It checks the org= and then orgID= parameter of the request. This will try to find the organization using an ID string or the name.  It interprets the &org= parameter as either the name or the ID. queryBucket returns the bucket for any http request. It checks the bucket= and then bucketID= parameter of the request. This will try to find the bucket using an ID string or the name.  It interprets the &bucket= parameter as either the name/Users/austinjaybecker/projects/abeck-go-testing/http/restore_service.go/api/v2/restore/kv/buckets/:bucketID"/buckets/:bucketID"/api/v2/restore/buckets/:bucketID/api/v2/restore/shards/:shardIDRestoreHandler.handleRestoreKVStore"RestoreHandler.handleRestoreKVStore"RestoreHandler.handleRestoreBucket"RestoreHandler.handleRestoreBucket"RestoreHandler.handleRestoreShard"RestoreHandler.handleRestoreShard"/buckets/%s"/buckets/%s"/api/v2/restore/shards/%d RestoreBackend is all services and associated parameters required to construct the RestoreHandler. NewRestoreBackend returns a new instance of RestoreBackend. RestoreHandler is http handler for restore service. NewRestoreHandler creates a new handler at /api/v2/restore to receive restore requests. Read bucket ID. Read serialized DBI data. RestoreService is the client implementation of influxdb.RestoreService./Users/austinjaybecker/projects/abeck-go-testing/http/router.gopanicErr"github.com/go-stack/stack"StripSlashes%+v"%+v"frameTrimBelowTrimAboveTrimRuntime NewBaseChiRouter returns a new chi router with a 404 handler, a 405 handler, and a panic handler./Users/austinjaybecker/projects/abeck-go-testing/http/scraper_service.gotargetsResptargetResp/:id/members"/:id/members"/api/v2/scrapers/:id/members/:id/members/:userID"/:id/members/:userID"/api/v2/scrapers/:id/members/:userID/:id/owners"/:id/owners"/api/v2/scrapers/:id/owners/:id/owners/:userID"/:id/owners/:userID"/api/v2/scrapers/:id/owners/:userID/:id/labels"/:id/labels"/api/v2/scrapers/:id/labels/:id/labels/:lid"/:id/labels/:lid"/api/v2/scrapers/:id/labels/:lid/:id"/:id"/api/v2/scrapers/:idScraper created"Scraper created"Scraper deleted"Scraper deleted"scraperTargetID"scraperTargetID"Scraper updated"Scraper updated"Scraper retrieved"Scraper retrieved"Scrapers retrieved"Scrapers retrieved"provided scraper target ID has invalid format"provided scraper target ID has invalid format"provided organization ID has invalid format"provided organization ID has invalid format"provided bucket ID has invalid format"provided bucket ID has invalid format"json:"configurations"`json:"configurations"`/api/v2/scrapers/%s/members"/api/v2/scrapers/%s/members"/api/v2/scrapers/%s/owners"/api/v2/scrapers/%s/owners" ScraperBackend is all services and associated parameters required to construct the ScraperHandler. NewScraperBackend returns a new instance of ScraperBackend. ScraperHandler represents an HTTP API handler for scraper targets. NewScraperHandler returns a new instance of ScraperHandler. handlePostScraperTarget is HTTP handler for the POST /api/v2/scrapers route. handleDeleteScraperTarget is the HTTP handler for the DELETE /api/v2/scrapers/:id route. handlePatchScraperTarget is the HTTP handler for the PATCH /api/v2/scrapers/:id route. handleGetScraperTargets is the HTTP handler for the GET /api/v2/scrapers route. ScraperService connects to Influx via HTTP using tokens to manage scraper targets. OpPrefix is for update invalid ops ListTargets returns a list of all scraper targets. UpdateTarget updates a single scraper target with changeset. Returns the new target state after update. AddTarget creates a new scraper target and sets target.ID with the new identifier. TODO(jsternberg): Should this check for a 201 explicitly? RemoveTarget removes a scraper target by ID. GetTargetByID returns a single target by ID./Users/austinjaybecker/projects/abeck-go-testing/http/server.gosignalChsigsyscallos/signal"os/signal""syscall"20000000000Shutting down server"Shutting down server""timeout"Initializing hard shutdown"Initializing hard shutdown"NotifyInterruptSIGTERM DefaultShutdownTimeout is the default timeout for shutting down the http server. Server is an abstraction around the http.Server that handles a server process. It manages the full lifecycle of a server by serving a handler on a socket. If signals have been registered, it will attempt to terminate the server using Shutdown if a signal is received and will force a shutdown if a second signal is received. NewServer returns a new server struct that can be used. Serve will run the server using the listener to accept connections. When we return, wait for all pending goroutines to finish. The server has failed and reported an error. We have received an interrupt. Signal the shutdown process. The shutdown needs to succeed in 20 seconds or less. Wait for another signal to cancel the shutdown. ListenForSignals registers the the server to listen for the given signals to shutdown the server. The signals are not captured until Serve is called. Retrieve which signals we want to be notified on. Create the signal channel and mark ourselves to be notified of signals. Allow up to two signals for each signal type we catch. ListenAndServe is a convenience method for opening a listener using the address and then serving the handler on that address. This method sets up the typical signal handlers./Users/austinjaybecker/projects/abeck-go-testing/http/session_handler.goinvalid basic auth"invalid basic auth"SameSiteStrictMode SessionBackend is all services and associated parameters required to construct the SessionHandler. NewSessionBackend creates a new SessionBackend with associated logger. SessionHandler represents an HTTP API handler for authorizations. NewSessionHandler returns a new instance of SessionHandler. handleSignin is the HTTP handler for the POST /signin route. handleSignout is the HTTP handler for the POST /signout route. TODO(desa): not sure what to do here maybe redirect? SetCookieSession adds a cookie for the session to an http request/Users/austinjaybecker/projects/abeck-go-testing/http/source_proxy_service.goToken %s"Token %s"compiler is not of type 'influxql'"compiler is not of type 'influxql'"application/x-www-form-urlencoded"application/x-www-form-urlencoded"/Users/austinjaybecker/projects/abeck-go-testing/http/source_service.gogsrquerySvcgetBucketsReqgetSrcReq%s/%s/query"%s/%s/query"%s/%s/buckets"%s/%s/buckets"%s/%s/health"%s/%s/health"/api/v2/sources/:id"/api/v2/sources/:id"/api/v2/sources/:id/buckets"/api/v2/sources/:id/buckets"/api/v2/sources/:id/query"/api/v2/sources/:id/query"/api/v2/sources/:id/health"/api/v2/sources/:id/health"json:"spec"`json:"spec"`json:"retentionRules"`json:"retentionRules"`expire"expire"json:"everySeconds"`json:"everySeconds"`expiration seconds must be greater than or equal to one second"expiration seconds must be greater than or equal to one second"/api/v2/buckets/%s/labels"/api/v2/buckets/%s/labels""logs"/api/v2/buckets/%s/logs"/api/v2/buckets/%s/logs"/api/v2/buckets/%s/members"/api/v2/buckets/%s/members"owners"owners"/api/v2/buckets/%s/owners"/api/v2/buckets/%s/owners"/api/v2/buckets/%s"/api/v2/buckets/%s"Source created"Source created"Source retrieved"Source retrieved"{"name":"sources","message":"source is %shealthy","status":"%s","checks":[]}`{"name":"sources","message":"source is %shealthy","status":"%s","checks":[]}`"pass"Source deleted"Source deleted"sourceID"sourceID"Sources retrieved"Sources retrieved"Source updated"Source updated" SourceBackend is all services and associated parameters required to construct the SourceHandler. NewSourceBackend returns a new instance of SourceBackend. SourceHandler is a handler for sources TODO(desa): this was done so in order to remove an import cycle and to allow for http mocking. NewSourceHandler returns a new instance of SourceHandler. starts here TODO(desa): support influxql dialect handlePostSourceQuery is the HTTP handler for POST /api/v2/sources/:id/query handleGetSourcesBuckets is the HTTP handler for the GET /api/v2/sources/:id/buckets route. retentionRule is the retention rule action for a bucket. handlePostSource is the HTTP handler for the POST /api/v2/sources route. handleGetSource is the HTTP handler for the GET /api/v2/sources/:id route. handleGetSourceHealth is the HTTP handler for the GET /v1/sources/:id/health route. todo(leodido) > check source is actually healthy and reply with 503 if not w.WriteHeader(http.StatusServiceUnavailable) fmt.Fprintln(w, fmt.Sprintf(msg, "not ", "fail")) handleDeleteSource is the HTTP handler for the DELETE /api/v2/sources/:id route. handleGetSources is the HTTP handler for the GET /api/v2/sources route. handlePatchSource is the HTTP handler for the PATH /api/v2/sources route. SourceService connects to Influx via HTTP using tokens to manage sources FindSourceByID returns a single source by ID. FindSources returns a list of sources that match filter and the total count of matching sources. CreateSource creates a new source and sets b.ID with the new identifier. UpdateSource updates a single source with changeset. Returns the new source state after update. DeleteSource removes a source by ID./Users/austinjaybecker/projects/abeck-go-testing/http/swagger.go"github.com/ghodss/yaml"swagger.yml"swagger.yml"YAMLToJSONthis developer binary not built with assets"this developer binary not built with assets" Only generate an asset for swagger.yml.go:generate env GO111MODULE=on go run github.com/kevinburke/go-bindata/go-bindata -o swagger_gen.go -tags assets -nocompress -pkg http ./swagger.yml swaggerLoader manages loading the swagger asset and serving it as JSON. Ensure we only call initialize once. The swagger converted from YAML to JSON. The error loading the swagger asset./Users/austinjaybecker/projects/abeck-go-testing/http/swagger_noassets.goswaggerDataexecDirexecPaththis developer binary not built with assets, and could not locate swagger.yml on disk"this developer binary not built with assets, and could not locate swagger.yml on disk"Unable to load swagger.yml from disk"Unable to load swagger.yml from disk"this developer binary not built with assets, and unable to load swagger.yml from disk"this developer binary not built with assets, and unable to load swagger.yml from disk"Successfully loaded swagger.yml"Successfully loaded swagger.yml"INFLUXDB_VALID_SWAGGER_PATH"INFLUXDB_VALID_SWAGGER_PATH"INFLUXDB_VALID_SWAGGER_PATH not set; falling back to checking relative paths"INFLUXDB_VALID_SWAGGER_PATH not set; falling back to checking relative paths"ExecutableCan't determine path of currently running executable"Can't determine path of currently running executable"Couldn't guess path to swagger definition"Couldn't guess path to swagger definition" asset returns swaggerData if present; otherwise it looks in the following locations in this order: the location specified in environment variable INFLUXDB_SWAGGER_YML_PATH; <path to executable>/../../http/swagger.yml (binary built with make but without assets tag); <path to executable>/http/swagger.yml (user ran go build ./cmd/influxd && ./influxd); ./http/swagger.yml (user ran go run ./cmd/influxd). None of these lookups happen in production builds, which have the assets build tag. Couldn't find it. findSwaggerPath makes a best-effort to find the path of the swagger file on disk. If it can't find the path, it returns the empty string. First, look for environment variable pointing at swagger. Environment variable set. Get the path to the executable so we can do a relative lookup. Give up. Assume the executable is in bin/$OS/, i.e. the developer built with make, but somehow without assets. Looks like we found it. We didn't build from make... maybe the developer ran something like "go build ./cmd/influxd && ./influxd". Maybe they're in the influxdb root, and ran something like "go run ./cmd/influxd"./Users/austinjaybecker/projects/abeck-go-testing/http/task_service.golatestCompletedupdatedAthoursbasePathpErrauthzafterIDrisess/api/v2/tasks/:id"/api/v2/tasks/:id"/api/v2/tasks/:id/logs"/api/v2/tasks/:id/logs"/api/v2/tasks/:id/members"/api/v2/tasks/:id/members"/api/v2/tasks/:id/members/:userID"/api/v2/tasks/:id/members/:userID"/api/v2/tasks/:id/owners"/api/v2/tasks/:id/owners"/api/v2/tasks/:id/owners/:userID"/api/v2/tasks/:id/owners/:userID"/api/v2/tasks/:id/runs"/api/v2/tasks/:id/runs"/api/v2/tasks/:id/runs/:rid"/api/v2/tasks/:id/runs/:rid"/api/v2/tasks/:id/runs/:rid/logs"/api/v2/tasks/:id/runs/:rid/logs"/api/v2/tasks/:id/runs/:rid/retry"/api/v2/tasks/:id/runs/:rid/retry"/api/v2/tasks/:id/labels"/api/v2/tasks/:id/labels"/api/v2/tasks/:id/labels/:lid"/api/v2/tasks/:id/labels/:lid"json:"ownerID"`json:"ownerID"`json:"every,omitempty"`json:"every,omitempty"`json:"cron,omitempty"`json:"cron,omitempty"`json:"metadata,omitempty"`json:"metadata,omitempty"`%s%dh"%s%dh"%s%dm"%s%dm"%s%ds"%s%ds"/api/v2/tasks/%s"/api/v2/tasks/%s"/api/v2/tasks/%s/members"/api/v2/tasks/%s/members"/api/v2/tasks/%s/owners"/api/v2/tasks/%s/owners"/api/v2/tasks/%s/labels"/api/v2/tasks/%s/labels""runs"/api/v2/tasks/%s/runs"/api/v2/tasks/%s/runs"/api/v2/tasks/%s/logs"/api/v2/tasks/%s/logs"json:"tasks"`json:"tasks"`json:"links,omitempty"`json:"links,omitempty"`json:"taskID"`json:"taskID"`json:"scheduledFor"`json:"scheduledFor"`json:"startedAt,omitempty"`json:"startedAt,omitempty"`json:"finishedAt,omitempty"`json:"finishedAt,omitempty"`json:"requestedAt,omitempty"`json:"requestedAt,omitempty"`json:"log,omitempty"`json:"log,omitempty"`/api/v2/tasks/%s/runs/%s"/api/v2/tasks/%s/runs/%s"/api/v2/tasks/%s/runs/%s/logs"/api/v2/tasks/%s/runs/%s/logs"/api/v2/tasks/%s/runs/%s/retry"/api/v2/tasks/%s/runs/%s/retry"json:"runs"`json:"runs"`failed to decode request"failed to decode request"Tasks retrieved"Tasks retrieved"org not found or unauthorized"org not found or unauthorized"org "org " not found or unauthorized" not found or unauthorized"could not identify organization"could not identify organization"invalid organization id"invalid organization id"Failed authentication"Failed authentication"error messages"error messages"failed to create task"failed to create task"failed to find task"failed to find task"failed to find resource labels"failed to find resource labels"Task retrieved"Task retrieved"failed to update task"failed to update task"Tasks updated"Tasks updated"you must provide a task ID"you must provide a task ID"failed to delete task"failed to delete task"Tasks deleted"Tasks deleted"failed to get authorizer"failed to get authorizer"failed to find task logs"failed to find task logs"json:"events"`json:"events"`"rid"failed to find runs"failed to find runs""afterTime""beforeTime"beforeTime must be later than afterTime"beforeTime must be later than afterTime"failed to force run"failed to force run"failed to find run"failed to find run"you must provide a run ID"you must provide a run ID"failed to cancel run"failed to cancel run"failed to retry run"failed to retry run"missing orgID and organization name"missing orgID and organization name"unable to authorize session"unable to authorize session"task ID unknown or unauthorized"task ID unknown or unauthorized"task ID required"task ID required" TaskBackend is all services and associated parameters required to construct the TaskHandler. NewTaskBackend returns a new instance of TaskBackend. TaskHandler represents an HTTP API handler for tasks. NewTaskHandler returns a new instance of TaskHandler. Task is a package-specific Task format that preserves the expected format for the API, where time values are represented as strings NewFrontEndTask converts a internal task type to a task that we want to display to users parse hours parse minutes parse seconds httpRun is a version of the Run object used to communicate over the API it uses a pointer to a time.Time instead of a time.Time so that we can pass a nil value for empty time values the task api can only create or lookup system tasks. if the error is not already a influxdb.error then make it into one pull auth from ctx, populate OwnerID when creating a task we set the type so we can filter later. Get the authorization for the task, if allowed. We were able to access the authorizer for the task, so reassign that on the context for the rest of this call. prevent attempts to use up memory since r.Body should include at most one item (RunManually) getAuthorizationForTask looks up the authorization associated with taskID, ensuring that the authorizer on ctx is allowed to view the task and the authorization. This method returns a *influxdb.Error, suitable for directly passing to h.HandleHTTPError. First look up the task, if we're allowed. This assumes h.TaskService validates access. TaskService connects to Influx via HTTP using tokens to manage tasks. FindTaskByID returns a single task FindTasks returns a list of tasks that match a filter (limit 100) and the total count of matching tasks. slice of 2-capacity string slices for storing parameter key-value pairs CreateTask creates a new task. UpdateTask updates a single task with changeset. DeleteTask removes a task by ID and purges all associated data and scheduled runs. FindLogs returns logs for a run. FindRuns returns a list of runs that match a filter and the total count of returned runs. FindRunByID returns a single run of a specific task. ErrRunNotFound is expected as part of the FindRunByID contract, so return that actual error instead of a different error that looks like it. TODO cleanup backend error implementation RetryRun creates and returns a new run (which is a retry of another run). ErrRunNotFound is expected as part of the RetryRun contract, TODO cleanup backend task error implementation RequestStillQueuedError is also part of the contract. ForceRun starts a run manually right now. CancelRun stops a longer running run./Users/austinjaybecker/projects/abeck-go-testing/http/telegraf.gotelPluginsdefaultOffermimeTypeoffersteleRespgithub.com/golang/gddo/httputil"github.com/golang/gddo/httputil"github.com/influxdata/influxdb/v2/telegraf/plugins"github.com/influxdata/influxdb/v2/telegraf/plugins"/api/v2/telegrafs/:id"/api/v2/telegrafs/:id"/api/v2/telegrafs/:id/members"/api/v2/telegrafs/:id/members"/api/v2/telegrafs/:id/members/:userID"/api/v2/telegrafs/:id/members/:userID"/api/v2/telegrafs/:id/owners"/api/v2/telegrafs/:id/owners"/api/v2/telegrafs/:id/owners/:userID"/api/v2/telegrafs/:id/owners/:userID"/api/v2/telegrafs/:id/labels"/api/v2/telegrafs/:id/labels"/api/v2/telegrafs/:id/labels/:lid"/api/v2/telegrafs/:id/labels/:lid""/api/v2/telegraf"AvailablePluginsListAvailablePlugins/api/v2/telegrafs/%s"/api/v2/telegrafs/%s"/api/v2/telegrafs/%s/labels"/api/v2/telegrafs/%s/labels"/api/v2/telegrafs/%s/members"/api/v2/telegrafs/%s/members"/api/v2/telegrafs/%s/owners"/api/v2/telegrafs/%s/owners"Telegrafs retrieved"Telegrafs retrieved"Telegraf retrieved"Telegraf retrieved"application/toml"application/toml"application/octet-stream"application/octet-stream"NegotiateContentTypeContent-Disposition"Content-Disposition"attachment; filename="%s.toml""attachment; filename=\"%s.toml\""application/toml; charset=utf-8"application/toml; charset=utf-8"Telegraf created"Telegraf created"Telegraf updated"Telegraf updated"Telegraf deleted"Telegraf deleted"telegrafID"telegrafID" TelegrafBackend is all services and associated parameters required to construct the TelegrafHandler. NewTelegrafBackend returns a new instance of TelegrafBackend. TelegrafHandler is the handler for the telegraf service NewTelegrafHandler returns a new instance of TelegrafHandler. handlePostTelegraf is the HTTP handler for the POST /api/v2/telegrafs route. handlePutTelegraf is the HTTP handler for the POST /api/v2/telegrafs route. TelegrafService is an http client that speaks to the telegraf service via HTTP. NewTelegrafService is a constructor for a telegraf service. FindTelegrafConfigByID returns a single telegraf config by ID. FindTelegrafConfigs returns a list of telegraf configs that match filter and the total count of matching telegraf configs. CreateTelegrafConfig creates a new telegraf config and sets b.ID with the new identifier. UpdateTelegrafConfig updates a single telegraf config. Returns the new telegraf config after update. DeleteTelegrafConfig removes a telegraf config by ID./Users/austinjaybecker/projects/abeck-go-testing/http/tokens.goauthorization Header is missing"authorization Header is missing"authorization Header Scheme is invalid"authorization Header Scheme is invalid" TODO(goller): I'd like this to be Bearer errors GetToken will parse the token from http Authorization Header. SetToken adds the token to the request./Users/austinjaybecker/projects/abeck-go-testing/http/ua.gouauseragent"github.com/mileusna/useragent"unknown"unknown"OSVersionMobileTabletDesktopBotIsWindowsIsAndroidIsMacOSIsIOSIsLinuxIsOperaIsOperaMiniIsChromeIsFirefoxIsInternetExplorerIsSafariIsEdgeIsGooglebotIsTwitterbotIsFacebookbot/Users/austinjaybecker/projects/abeck-go-testing/http/user_resource_mapping_service.gouserTypemidurs/api/v2/%s/%s/%ss"/api/v2/%s/%s/%ss"Member/owner created"Member/owner created""mapping"user id missing or invalid"user id missing or invalid"Members/owners retrieved"Members/owners retrieved"Member deleted"Member deleted""memberID"url missing member id"url missing member id" MemberBackend is all services and associated parameters required to construct member handler. newPostMemberHandler returns a handler func for a POST to /members or /owners endpoints newGetMembersHandler returns a handler func for a GET to /members or /owners endpoints newDeleteMemberHandler returns a handler func for a DELETE to /members or /owners endpoints UserResourceMappingService is the struct of urm service FindUserResourceMappings returns the user resource mappings CreateUserResourceMapping will create a user resource mapping DeleteUserResourceMapping will delete user resource mapping based in criteria. SpecificURMSvc returns a urm service with specific resource and user types. this will help us stay compatible with the existing service contract but also allow for urm deletes to go through the correct api SpecificURMSvc is a URM client that speaks to a specific resource with a specified user type/Users/austinjaybecker/projects/abeck-go-testing/http/user_service.go"/api/v2/me/password"/api/v2/users/:id"/api/v2/users/:id""/api/v2/users/:id/password"invalid user ID provided in route"invalid user ID provided in route"User password updated"User password updated"User created"User created"User retrieved"User retrieved"User deleted"User deleted"/api/v2/users/%s/logs"/api/v2/users/%s/logs"Users retrieved"Users retrieved"Users updated"Users updated"no results found"no results found" UserBackend is all services and associated parameters required to construct the UserHandler. NewUserBackend creates a UserBackend using information in the APIBackend. UserHandler represents an HTTP API handler for users. NewUserHandler returns a new instance of UserHandler. the POST doesn't need to be nested under users in this scheme seems worthwhile to make this a root resource in our HTTP API removes coupling with userid. handlePutPassword is the HTTP handler for the PUT /api/v2/users/:id/password handlePostUser is the HTTP handler for the POST /api/v2/users route. handleGetMe is the HTTP handler for the GET /api/v2/me. handleGetUser is the HTTP handler for the GET /api/v2/users/:id route. handleDeleteUser is the HTTP handler for the DELETE /api/v2/users/:id route. UserResponse is the response of user handleGetUsers is the HTTP handler for the GET /api/v2/users route. handlePatchUser is the HTTP handler for the PATCH /api/v2/users/:id route. UserService connects to Influx via HTTP using tokens to manage users OpPrefix is the ops of not found error. FindMe returns user information about the owner of the token FindUserByID returns a single user by ID. FindUser returns the first user that matches filter. FindUsers returns a list of users that match filter and the total count of matching users. CreateUser creates a new user and sets u.ID with the new identifier. UpdateUser updates a single user with changeset. Returns the new user state after update. DeleteUser removes a user by ID. PasswordService is an http client to speak to the password service. SetPassword sets the user's password. ComparePassword compares the user new password with existing. Note: is not implemented. CompareAndSetPassword compares the old and new password and submits the new password if possible. Note: is not implemented./Users/austinjaybecker/projects/abeck-go-testing/http/variable_service.goentityLabelsIDPathentityLabelsPathentityPathnumurlID%s/:id"%s/:id"%s/labels"%s/labels"%s/:lid"%s/:lid"json:"variables"`json:"variables"`could not read variables"could not read variables"Variables retrieved"Variables retrieved""vars"Variable retrieved"Variable retrieved"var"var"/api/v2/variables/%s"/api/v2/variables/%s"/api/v2/variables/%s/labels"/api/v2/variables/%s/labels"Variable created"Variable created"Variable updated"Variable updated"Variable replaced"Variable replaced"Variable deleted"Variable deleted"variableID"variableID" VariableBackend is all services and associated parameters required to construct the VariableHandler. NewVariableBackend creates a backend used by the variable handler. VariableHandler is the handler for the variable service NewVariableHandler creates a new VariableHandler VariableService is a variable service over HTTP to the influxdb server FindVariableByID finds a single variable from the store by its ID FindVariables returns a list of variables that match filter. CreateVariable creates a new variable and assigns it an influxdb.ID UpdateVariable updates a single variable with a changeset ReplaceVariable replaces a single variable DeleteVariable removes a variable from the store/Users/austinjaybecker/projects/abeck-go-testing/http/write_handler.gogzipped HTTP body contains an invalid header"gzipped HTTP body contains an invalid header"invalid precision; valid precision units are ns, us, ms, and s"invalid precision; valid precision units are ns, us, ms, and s"http/writeHandler"http/writeHandler"org_id"org_id"http/newWriteRequest"http/newWriteRequest"bucket not found"bucket not found"http/Write"http/Write" WriteBackend is all services and associated parameters required to construct the WriteHandler. NewWriteBackend returns a new instance of WriteBackend. WriteHandler receives line protocol and sends to a publish function. parserOptions     []models.ParserOption WriteHandlerOption is a functional option for a *WriteHandlerfunc WithParserOptions(opts ...models.ParserOption) WriteHandlerOption {	return func(w *WriteHandler) {		w.parserOptions = opts NewWriteHandler creates a new handler at /api/v2/write to receive line protocol. TODO: Backport?opts := append([]models.ParserOption{}, h.parserOptions...)opts = append(opts, models.WithParserPrecision(req.Precision)) writeRequest is a request object holding information about a batch of points to be written to a Bucket. decodeWriteRequest extracts information from an http.Request object to produce a writeRequest. WriteService sends data over HTTP to influxdb via line protocol. WriteTo writes to the bucket matching the filter./Users/austinjaybecker/projects/abeck-go-testing/http/write_usage_recorder.go/Users/austinjaybecker/projects/abeck-go-testing/id.gohexunsafeencoding/hex"encoding/hex""unsafe"invalid ID"invalid ID"id must have a length of 16 bytes"id must have a length of 16 bytes"corrupt ID provided"corrupt ID provided" IDLength is the exact length a string (or a byte slice representing it) must have in order to be decoded into a valid ID. ErrInvalidID signifies invalid IDs. ErrInvalidIDLength is returned when an ID has the incorrect number of bytes. ErrCorruptID means the ID stored in the Store is corrupt. ID is a unique identifier. Its zero value is not a valid ID. IDGenerator represents a generator for IDs. ID creates unique byte slice ID. IDFromString creates an ID from a given string. It errors if the input string does not match a valid ID. InvalidID returns a zero ID. Decode parses b as a hex-encoded byte-slice-string. It errors if the input byte slice does not have the correct length or if it contains all zeros. DecodeFromString parses s as a hex-encoded string. Encode converts ID to a hex-encoded byte-slice-string. It errors if the receiving ID holds its zero value. Valid checks whether the receiving ID is a valid one or not. String returns the ID as a hex encoded string. Returns an empty string in the case the ID is invalid. GoString formats the ID the same as the String method. Without this, when using the %#v verb, an ID would be printed as a uint64, so you would see e.g. 0x2def021097c6000 instead of 02def021097c6000 (note the leading 0x, which means the former doesn't show up in searches for the latter). MarshalText encodes i as text. Providing this method is a fallback for json.Marshal, with the added benefit that IDs encoded as map keys will be the expected string encoding, rather than the effective fmt.Sprintf("%d", i) that json.Marshal uses by default for integer types. UnmarshalText decodes i from a byte slice. Providing this method is also a fallback for json.Unmarshal, also relevant when IDs are used as map keys./Users/austinjaybecker/projects/abeck-go-testing/influxql/Users/austinjaybecker/projects/abeck-go-testing/influxql/control/Users/austinjaybecker/projects/abeck-go-testing/influxql/control/prometheus.goLabelGenericErrorLabelInterruptedErrLabelNotExecutedLabelNotImplErrorLabelParseErrLabelRuntimeErrorLabelSuccesssubsystemsuccess"success"generic_err"generic_err"parse_err"parse_err"interrupt_err"interrupt_err"runtime_error"runtime_error"not_implemented"not_implemented"not_executed"not_executed"Count of the query requests"Count of the query requests""result"not_implemented_total"not_implemented_total"Count of the query requests executing unimplemented operations"Count of the query requests executing unimplemented operations"operation"operation"requests_latency_seconds"requests_latency_seconds"Histogram of times spent for end-to-end latency (from issuing query request, to receiving the first byte of the response)"Histogram of times spent for end-to-end latency (from issuing query request, to receiving the first byte of the response)"ExponentialBuckets1e-30.00100000000000000002081152921504606847/1152921504606846976executing_duration_seconds"executing_duration_seconds"Histogram of times spent executing queries"Histogram of times spent executing queries" controllerMetrics holds metrics related to the query controller./Users/austinjaybecker/projects/abeck-go-testing/influxql/errors.goCollectorFnEncodingFormatCSVEncodingFormatJSONEncodingFormatMessagePackEncodingFormatTableErrNotImplementedImmutableCollectorMutableCollectorNewImmutableCollectorNewMutableCollectorNotImplementedErrorProxyModeProxyModeHTTPProxyModeQueueRequestModeRequestModeAllRequestModeHTTPRequestModeQueueproxyModeStringrequestModeStringnot implemented: "not implemented: " NotImplementedError is returned when a specific operation is unavailable. Op is the name of the unimplemented operation ErrNotImplemented creates a NotImplementedError specifying op is unavailable./Users/austinjaybecker/projects/abeck-go-testing/influxql/mock/Users/austinjaybecker/projects/abeck-go-testing/influxql/mock/proxy_query_service.goMock InfluxQL Proxy Query Service"Mock InfluxQL Proxy Query Service" ProxyQueryService mocks the InfluxQL QueryService for testing./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/call_iterator.goAggregateBooleanPointsAggregateFloatPointsAggregateIntegerPointsAggregateStringPointsAggregateUnsignedPointsAuthorizerIsOpenBooleanBulkPointAggregatorBooleanCountReduceBooleanDistinctReducerBooleanElapsedReducerBooleanFirstReduceBooleanFuncFloatReducerBooleanFuncIntegerReducerBooleanFuncReducerBooleanFuncStringReducerBooleanFuncUnsignedReducerBooleanIteratorBooleanLastReduceBooleanMaxReduceBooleanMinReduceBooleanModeReduceSliceBooleanPointBooleanPointAggregatorBooleanPointDecoderBooleanPointEmitterBooleanPointEncoderBooleanReduceFloatFuncBooleanReduceFloatSliceFuncBooleanReduceFuncBooleanReduceIntegerFuncBooleanReduceIntegerSliceFuncBooleanReduceSliceFuncBooleanReduceStringFuncBooleanReduceStringSliceFuncBooleanReduceUnsignedFuncBooleanReduceUnsignedSliceFuncBooleanSampleReducerBooleanSliceFuncFloatReducerBooleanSliceFuncIntegerReducerBooleanSliceFuncReducerBooleanSliceFuncStringReducerBooleanSliceFuncUnsignedReducerCallTypeMapperChandeMomentumOscillatorReducerCompileOptionsDefaultStatsIntervalDefaultTypeMapperDoubleExponentialMovingAverageReducerDrainCursorDrainIteratorDrainIteratorsEmitterErrDatabaseNotFoundErrInvalidQueryErrMaxConcurrentQueriesLimitExceededErrMaxSelectPointsLimitExceededErrNotExecutedErrQueryInterruptedErrUnknownCallExponentialMovingAverageReducerFieldMapFloatBottomReducerFloatBulkPointAggregatorFloatCountReduceFloatCumulativeSumReducerFloatDerivativeReducerFloatDifferenceReducerFloatDistinctReducerFloatElapsedReducerFloatFirstReduceFloatFuncBooleanReducerFloatFuncIntegerReducerFloatFuncReducerFloatFuncStringReducerFloatFuncUnsignedReducerFloatHoltWintersReducerFloatIntegralReducerFloatIteratorFloatLastReduceFloatMaxReduceFloatMeanReducerFloatMedianReduceSliceFloatMinReduceFloatModeReduceSliceFloatMovingAverageReducerFloatPointFloatPointAggregatorFloatPointDecoderFloatPointEmitterFloatPointEncoderFloatReduceBooleanFuncFloatReduceBooleanSliceFuncFloatReduceFuncFloatReduceIntegerFuncFloatReduceIntegerSliceFuncFloatReduceSliceFuncFloatReduceStringFuncFloatReduceStringSliceFuncFloatReduceUnsignedFuncFloatReduceUnsignedSliceFuncFloatSampleReducerFloatSliceFuncBooleanReducerFloatSliceFuncIntegerReducerFloatSliceFuncReducerFloatSliceFuncStringReducerFloatSliceFuncUnsignedReducerFloatSpreadReducerFloatStddevReduceSliceFloatSumReduceFloatTopReducerFunctionTypeMapperGatherResultsIntegerBottomReducerIntegerBulkPointAggregatorIntegerCountReduceIntegerCumulativeSumReducerIntegerDerivativeReducerIntegerDifferenceReducerIntegerDistinctReducerIntegerElapsedReducerIntegerFirstReduceIntegerFuncBooleanReducerIntegerFuncFloatReducerIntegerFuncReducerIntegerFuncStringReducerIntegerFuncUnsignedReducerIntegerIntegralReducerIntegerIteratorIntegerLastReduceIntegerMaxReduceIntegerMeanReducerIntegerMedianReduceSliceIntegerMinReduceIntegerModeReduceSliceIntegerMovingAverageReducerIntegerPointIntegerPointAggregatorIntegerPointDecoderIntegerPointEmitterIntegerPointEncoderIntegerReduceBooleanFuncIntegerReduceBooleanSliceFuncIntegerReduceFloatFuncIntegerReduceFloatSliceFuncIntegerReduceFuncIntegerReduceSliceFuncIntegerReduceStringFuncIntegerReduceStringSliceFuncIntegerReduceUnsignedFuncIntegerReduceUnsignedSliceFuncIntegerSampleReducerIntegerSliceFuncBooleanReducerIntegerSliceFuncFloatReducerIntegerSliceFuncReducerIntegerSliceFuncStringReducerIntegerSliceFuncUnsignedReducerIntegerSpreadReducerIntegerStddevReduceSliceIntegerSumReduceIntegerTopReducerIteratorEncoderIteratorMapIteratorScannerIteratorsKaufmansAdaptiveMovingAverageReducerKaufmansEfficiencyRatioReducerLimitTagSetsMathTypeMapperMathValuerNewBooleanDistinctReducerNewBooleanElapsedReducerNewBooleanFuncFloatReducerNewBooleanFuncIntegerReducerNewBooleanFuncReducerNewBooleanFuncStringReducerNewBooleanFuncUnsignedReducerNewBooleanPointDecoderNewBooleanPointEncoderNewBooleanSampleReducerNewBooleanSliceFuncFloatReducerNewBooleanSliceFuncIntegerReducerNewBooleanSliceFuncReducerNewBooleanSliceFuncStringReducerNewBooleanSliceFuncUnsignedReducerNewCallIteratorNewChandeMomentumOscillatorReducerNewContextWithIteratorsNewDedupeIteratorNewDistinctIteratorNewDoubleExponentialMovingAverageReducerNewEmitterNewExponentialMovingAverageReducerNewFillIteratorNewFilterIteratorNewFloatBottomReducerNewFloatCumulativeSumReducerNewFloatDerivativeReducerNewFloatDifferenceReducerNewFloatDistinctReducerNewFloatElapsedReducerNewFloatFuncBooleanReducerNewFloatFuncIntegerReducerNewFloatFuncReducerNewFloatFuncStringReducerNewFloatFuncUnsignedReducerNewFloatHoltWintersReducerNewFloatIntegralReducerNewFloatMeanReducerNewFloatMovingAverageReducerNewFloatPercentileReduceSliceFuncNewFloatPointDecoderNewFloatPointEncoderNewFloatSampleReducerNewFloatSliceFuncBooleanReducerNewFloatSliceFuncIntegerReducerNewFloatSliceFuncReducerNewFloatSliceFuncStringReducerNewFloatSliceFuncUnsignedReducerNewFloatSpreadReducerNewFloatTopReducerNewIntegerBottomReducerNewIntegerCumulativeSumReducerNewIntegerDerivativeReducerNewIntegerDifferenceReducerNewIntegerDistinctReducerNewIntegerElapsedReducerNewIntegerFuncBooleanReducerNewIntegerFuncFloatReducerNewIntegerFuncReducerNewIntegerFuncStringReducerNewIntegerFuncUnsignedReducerNewIntegerIntegralReducerNewIntegerMeanReducerNewIntegerMovingAverageReducerNewIntegerPercentileReduceSliceFuncNewIntegerPointDecoderNewIntegerPointEncoderNewIntegerSampleReducerNewIntegerSliceFuncBooleanReducerNewIntegerSliceFuncFloatReducerNewIntegerSliceFuncReducerNewIntegerSliceFuncStringReducerNewIntegerSliceFuncUnsignedReducerNewIntegerSpreadReducerNewIntegerTopReducerNewInterruptIteratorNewIntervalIteratorNewIteratorEncoderNewIteratorMapperNewIteratorScannerNewKaufmansAdaptiveMovingAverageReducerNewKaufmansEfficiencyRatioReducerNewLimitIteratorNewMedianIteratorNewMergeIteratorNewModeIteratorNewParallelMergeIteratorNewPointDecoderNewReaderIteratorNewRelativeStrengthIndexReducerNewResponseWriterNewSampleIteratorNewSortedMergeIteratorNewStringDistinctReducerNewStringElapsedReducerNewStringFuncBooleanReducerNewStringFuncFloatReducerNewStringFuncIntegerReducerNewStringFuncReducerNewStringFuncUnsignedReducerNewStringPointDecoderNewStringPointEncoderNewStringSampleReducerNewStringSliceFuncBooleanReducerNewStringSliceFuncFloatReducerNewStringSliceFuncIntegerReducerNewStringSliceFuncReducerNewStringSliceFuncUnsignedReducerNewTagSubsetIteratorNewTripleExponentialDerivativeReducerNewTripleExponentialMovingAverageReducerNewUnsignedBottomReducerNewUnsignedCumulativeSumReducerNewUnsignedDerivativeReducerNewUnsignedDifferenceReducerNewUnsignedDistinctReducerNewUnsignedElapsedReducerNewUnsignedFuncBooleanReducerNewUnsignedFuncFloatReducerNewUnsignedFuncIntegerReducerNewUnsignedFuncReducerNewUnsignedFuncStringReducerNewUnsignedIntegralReducerNewUnsignedMeanReducerNewUnsignedMovingAverageReducerNewUnsignedPercentileReduceSliceFuncNewUnsignedPointDecoderNewUnsignedPointEncoderNewUnsignedSampleReducerNewUnsignedSliceFuncBooleanReducerNewUnsignedSliceFuncFloatReducerNewUnsignedSliceFuncIntegerReducerNewUnsignedSliceFuncReducerNewUnsignedSliceFuncStringReducerNewUnsignedSpreadReducerNewUnsignedTopReducerNullFloatNullMapOpenAuthorizerPanicCrashEnvPointDecoderPreparePreparedStatementReadOnlyWarningRelativeStrengthIndexReducerRewriteStatementRowCursorSkipDefaultStringBulkPointAggregatorStringCountReduceStringDistinctReducerStringElapsedReducerStringFirstReduceStringFuncBooleanReducerStringFuncFloatReducerStringFuncIntegerReducerStringFuncReducerStringFuncUnsignedReducerStringLastReduceStringModeReduceSliceStringPointStringPointAggregatorStringPointDecoderStringPointEmitterStringPointEncoderStringReduceBooleanFuncStringReduceBooleanSliceFuncStringReduceFloatFuncStringReduceFloatSliceFuncStringReduceFuncStringReduceIntegerFuncStringReduceIntegerSliceFuncStringReduceSliceFuncStringReduceUnsignedFuncStringReduceUnsignedSliceFuncStringSampleReducerStringSliceFuncBooleanReducerStringSliceFuncFloatReducerStringSliceFuncIntegerReducerStringSliceFuncReducerStringSliceFuncUnsignedReducerTagMapTagSetTripleExponentialDerivativeReducerTripleExponentialMovingAverageReducerUnsignedBottomReducerUnsignedBulkPointAggregatorUnsignedCountReduceUnsignedCumulativeSumReducerUnsignedDerivativeReducerUnsignedDifferenceReducerUnsignedDistinctReducerUnsignedElapsedReducerUnsignedFirstReduceUnsignedFuncBooleanReducerUnsignedFuncFloatReducerUnsignedFuncIntegerReducerUnsignedFuncReducerUnsignedFuncStringReducerUnsignedIntegralReducerUnsignedIteratorUnsignedLastReduceUnsignedMaxReduceUnsignedMeanReducerUnsignedMedianReduceSliceUnsignedMinReduceUnsignedModeReduceSliceUnsignedMovingAverageReducerUnsignedPointUnsignedPointAggregatorUnsignedPointDecoderUnsignedPointEmitterUnsignedPointEncoderUnsignedReduceBooleanFuncUnsignedReduceBooleanSliceFuncUnsignedReduceFloatFuncUnsignedReduceFloatSliceFuncUnsignedReduceFuncUnsignedReduceIntegerFuncUnsignedReduceIntegerSliceFuncUnsignedReduceSliceFuncUnsignedReduceStringFuncUnsignedReduceStringSliceFuncUnsignedSampleReducerUnsignedSliceFuncBooleanReducerUnsignedSliceFuncFloatReducerUnsignedSliceFuncIntegerReducerUnsignedSliceFuncReducerUnsignedSliceFuncStringReducerUnsignedSpreadReducerUnsignedStddevReduceSliceUnsignedSumReduceUnsignedTopReducerZeroTimeasFloatasFloatsbooleanCloseInterruptIteratorbooleanDedupeIteratorbooleanFillIteratorbooleanFilterIteratorbooleanInterruptIteratorbooleanIntervalIteratorbooleanIteratorMapperbooleanIteratorScannerbooleanLimitIteratorbooleanMergeHeapbooleanMergeHeapItembooleanMergeIteratorbooleanParallelIteratorbooleanPointErrorbooleanPointsbooleanPointsByFuncbooleanPointsByTimebooleanPointsByValuebooleanPointsSortBybooleanReaderIteratorbooleanReduceBooleanIteratorbooleanReduceBooleanPointbooleanReduceFloatIteratorbooleanReduceFloatPointbooleanReduceIntegerIteratorbooleanReduceIntegerPointbooleanReduceStringIteratorbooleanReduceStringPointbooleanReduceUnsignedIteratorbooleanReduceUnsignedPointbooleanSortedMergeHeapbooleanSortedMergeHeapItembooleanSortedMergeIteratorbooleanStreamBooleanIteratorbooleanStreamFloatIteratorbooleanStreamIntegerIteratorbooleanStreamStringIteratorbooleanStreamUnsignedIteratorbooleanTagSubsetIteratorbufBooleanIteratorbufFloatIteratorbufIntegerIteratorbufStringIteratorbufUnsignedIteratorbuildAuxIteratorbuildCursorbuildExprIteratorbuildFieldIteratorcastToBooleancastToFloatcastToIntegercastToStringcastToTypecastToUnsignedcloneAuxcolumnsEqualcompiledFieldcompiledStatementconvertToEpochcsvFormatterdecodeAuxdecodeBooleanPointdecodeFloatPointdecodeIntegerPointdecodeIntervaldecodeIteratorOptionsdecodeIteratorStatsdecodeMeasurementdecodeStringPointdecodeTagsdecodeUnsignedPointdecodeVarRefencodeAuxencodeBooleanPointencodeFloatPointencodeIntegerPointencodeIntervalencodeIteratorOptionsencodeIteratorStatsencodeMeasurementencodeStringPointencodeTagsencodeUnsignedPointencodeVarRefexplainIteratorCreatorexprIteratorBuilderfastDedupeKeyfieldMapperAdapterfilterCursorfloatCloseInterruptIteratorfloatDedupeIteratorfloatFastDedupeIteratorfloatFillIteratorfloatFilterIteratorfloatInterruptIteratorfloatIntervalIteratorfloatIteratorMapperfloatIteratorScannerfloatLimitIteratorfloatMergeHeapfloatMergeHeapItemfloatMergeIteratorfloatParallelIteratorfloatPointErrorfloatPointsfloatPointsByFuncfloatPointsByTimefloatPointsByValuefloatPointsSortByfloatReaderIteratorfloatReduceBooleanIteratorfloatReduceBooleanPointfloatReduceFloatIteratorfloatReduceFloatPointfloatReduceIntegerIteratorfloatReduceIntegerPointfloatReduceStringIteratorfloatReduceStringPointfloatReduceUnsignedIteratorfloatReduceUnsignedPointfloatSortedMergeHeapfloatSortedMergeHeapItemfloatSortedMergeIteratorfloatStreamBooleanIteratorfloatStreamFloatIteratorfloatStreamIntegerIteratorfloatStreamStringIteratorfloatStreamUnsignedIteratorfloatTagSubsetIteratorhasValidTypeheadersEqualhwDefaultEpsilonhwGuessLowerhwGuessStephwGuessUpperhwWeightintegerCloseInterruptIteratorintegerDedupeIteratorintegerFillIteratorintegerFilterIteratorintegerInterruptIteratorintegerIntervalIteratorintegerIteratorMapperintegerIteratorScannerintegerLimitIteratorintegerMergeHeapintegerMergeHeapItemintegerMergeIteratorintegerParallelIteratorintegerPointErrorintegerPointsintegerPointsByFuncintegerPointsByTimeintegerPointsByValueintegerPointsSortByintegerReaderIteratorintegerReduceBooleanIteratorintegerReduceBooleanPointintegerReduceFloatIteratorintegerReduceFloatPointintegerReduceIntegerIteratorintegerReduceIntegerPointintegerReduceStringIteratorintegerReduceStringPointintegerReduceUnsignedIteratorintegerReduceUnsignedPointintegerSortedMergeHeapintegerSortedMergeHeapItemintegerSortedMergeIteratorintegerStreamBooleanIteratorintegerStreamFloatIteratorintegerStreamIntegerIteratorintegerStreamStringIteratorintegerStreamUnsignedIteratorintegerTagSubsetIteratorinterfaceToStringisMathFunctioniteratorsContextKeyjsonFormatterkeysMatchlinearFloatlinearIntegerlinearUnsignedmatchAllRegexmsgpFormattermultiScannerCursornewBooleanCloseInterruptIteratornewBooleanDedupeIteratornewBooleanFillIteratornewBooleanFilterIteratornewBooleanInterruptIteratornewBooleanIntervalIteratornewBooleanIteratorMappernewBooleanIteratorScannernewBooleanIteratorsnewBooleanLimitIteratornewBooleanMergeIteratornewBooleanParallelIteratornewBooleanReaderIteratornewBooleanReduceBooleanIteratornewBooleanReduceFloatIteratornewBooleanReduceIntegerIteratornewBooleanReduceStringIteratornewBooleanReduceUnsignedIteratornewBooleanSortedMergeIteratornewBooleanStreamBooleanIteratornewBooleanStreamFloatIteratornewBooleanStreamIntegerIteratornewBooleanStreamStringIteratornewBooleanStreamUnsignedIteratornewBooleanTagSubsetIteratornewBottomIteratornewBufBooleanIteratornewBufFloatIteratornewBufIntegerIteratornewBufStringIteratornewBufUnsignedIteratornewChandeMomentumOscillatorIteratornewCompilernewCountIteratornewCumulativeSumIteratornewDerivativeIteratornewDifferenceIteratornewDoubleExponentialMovingAverageIteratornewElapsedIteratornewExponentialMovingAverageIteratornewFieldMapperAdapternewFilterCursornewFirstIteratornewFloatCloseInterruptIteratornewFloatDedupeIteratornewFloatFastDedupeIteratornewFloatFillIteratornewFloatFilterIteratornewFloatInterruptIteratornewFloatIntervalIteratornewFloatIteratorMappernewFloatIteratorScannernewFloatIteratorsnewFloatLimitIteratornewFloatMergeIteratornewFloatParallelIteratornewFloatReaderIteratornewFloatReduceBooleanIteratornewFloatReduceFloatIteratornewFloatReduceIntegerIteratornewFloatReduceStringIteratornewFloatReduceUnsignedIteratornewFloatSortedMergeIteratornewFloatStreamBooleanIteratornewFloatStreamFloatIteratornewFloatStreamIntegerIteratornewFloatStreamStringIteratornewFloatStreamUnsignedIteratornewFloatTagSubsetIteratornewHoltWintersIteratornewIntegerCloseInterruptIteratornewIntegerDedupeIteratornewIntegerFillIteratornewIntegerFilterIteratornewIntegerInterruptIteratornewIntegerIntervalIteratornewIntegerIteratorMappernewIntegerIteratorScannernewIntegerIteratorsnewIntegerLimitIteratornewIntegerMergeIteratornewIntegerParallelIteratornewIntegerReaderIteratornewIntegerReduceBooleanIteratornewIntegerReduceFloatIteratornewIntegerReduceIntegerIteratornewIntegerReduceStringIteratornewIntegerReduceUnsignedIteratornewIntegerSortedMergeIteratornewIntegerStreamBooleanIteratornewIntegerStreamFloatIteratornewIntegerStreamIntegerIteratornewIntegerStreamStringIteratornewIntegerStreamUnsignedIteratornewIntegerTagSubsetIteratornewIntegralIteratornewIteratorOptionsStmtnewIteratorOptionsSubstatementnewKaufmansAdaptiveMovingAverageIteratornewKaufmansEfficiencyRatioIteratornewLastIteratornewMaxIteratornewMeanIteratornewMedianIteratornewMinIteratornewMovingAverageIteratornewMultiScannerCursornewNullCursornewParallelIteratornewPercentileIteratornewRelativeStrengthIndexIteratornewSampleIteratornewScannerCursornewScannerCursorBasenewSpreadIteratornewStddevIteratornewStringCloseInterruptIteratornewStringDedupeIteratornewStringFillIteratornewStringFilterIteratornewStringInterruptIteratornewStringIntervalIteratornewStringIteratorMappernewStringIteratorScannernewStringIteratorsnewStringLimitIteratornewStringMergeIteratornewStringParallelIteratornewStringReaderIteratornewStringReduceBooleanIteratornewStringReduceFloatIteratornewStringReduceIntegerIteratornewStringReduceStringIteratornewStringReduceUnsignedIteratornewStringSortedMergeIteratornewStringStreamBooleanIteratornewStringStreamFloatIteratornewStringStreamIntegerIteratornewStringStreamStringIteratornewStringStreamUnsignedIteratornewStringTagSubsetIteratornewSumIteratornewTagsIDnewTopIteratornewTripleExponentialDerivativeIteratornewTripleExponentialMovingAverageIteratornewUnsignedCloseInterruptIteratornewUnsignedDedupeIteratornewUnsignedFillIteratornewUnsignedFilterIteratornewUnsignedInterruptIteratornewUnsignedIntervalIteratornewUnsignedIteratorMappernewUnsignedIteratorScannernewUnsignedIteratorsnewUnsignedLimitIteratornewUnsignedMergeIteratornewUnsignedParallelIteratornewUnsignedReaderIteratornewUnsignedReduceBooleanIteratornewUnsignedReduceFloatIteratornewUnsignedReduceIntegerIteratornewUnsignedReduceStringIteratornewUnsignedReduceUnsignedIteratornewUnsignedSortedMergeIteratornewUnsignedStreamBooleanIteratornewUnsignedStreamFloatIteratornewUnsignedStreamIntegerIteratornewUnsignedStreamStringIteratornewUnsignedStreamUnsignedIteratornewUnsignedTagSubsetIteratornewValueMappernilFloatIteratornilFloatReaderIteratornowKeynullCursornullNormalizernullNormalizerImplopenAuthorizerplanNodepreparedStatementqueryFieldMapperrewriteShowFieldKeyCardinalityStatementrewriteShowFieldKeysStatementrewriteShowMeasurementCardinalityStatementrewriteShowMeasurementsStatementrewriteShowSeriesCardinalityStatementrewriteShowSeriesStatementrewriteShowTagKeyCardinalityStatementrewriteShowTagKeysStatementrewriteShowTagValuesCardinalityStatementrewriteShowTagValuesStatementrewriteSourcesrewriteSources2rewriteSourcesConditionroundrowCursorscannerCursorscannerCursorBasescannerFuncsecToNsstringCloseInterruptIteratorstringDedupeIteratorstringFillIteratorstringFilterIteratorstringInterruptIteratorstringIntervalIteratorstringIteratorMapperstringIteratorScannerstringLimitIteratorstringMergeHeapstringMergeHeapItemstringMergeIteratorstringParallelIteratorstringPointErrorstringPointsstringPointsByFuncstringPointsByTimestringPointsByValuestringPointsSortBystringReaderIteratorstringReduceBooleanIteratorstringReduceBooleanPointstringReduceFloatIteratorstringReduceFloatPointstringReduceIntegerIteratorstringReduceIntegerPointstringReduceStringIteratorstringReduceStringPointstringReduceUnsignedIteratorstringReduceUnsignedPointstringSortedMergeHeapstringSortedMergeHeapItemstringSortedMergeIteratorstringStreamBooleanIteratorstringStreamFloatIteratorstringStreamIntegerIteratorstringStreamStringIteratorstringStreamUnsignedIteratorstringTagSubsetIteratorstringsEqualsubqueryBuildertagsEqualtextFormatterunsignedCloseInterruptIteratorunsignedDedupeIteratorunsignedFillIteratorunsignedFilterIteratorunsignedInterruptIteratorunsignedIntervalIteratorunsignedIteratorMapperunsignedIteratorScannerunsignedLimitIteratorunsignedMergeHeapunsignedMergeHeapItemunsignedMergeIteratorunsignedParallelIteratorunsignedPointErrorunsignedPointsunsignedPointsByFuncunsignedPointsByTimeunsignedPointsByValueunsignedPointsSortByunsignedReaderIteratorunsignedReduceBooleanIteratorunsignedReduceBooleanPointunsignedReduceFloatIteratorunsignedReduceFloatPointunsignedReduceIntegerIteratorunsignedReduceIntegerPointunsignedReduceStringIteratorunsignedReduceStringPointunsignedReduceUnsignedIteratorunsignedReduceUnsignedPointunsignedSortedMergeHeapunsignedSortedMergeHeapItemunsignedSortedMergeIteratorunsignedStreamBooleanIteratorunsignedStreamFloatIteratorunsignedStreamIntegerIteratorunsignedStreamStringIteratorunsignedStreamUnsignedIteratorunsignedTagSubsetIteratorvalidateTypesvalueMapperwillCrashcreateFncurrFreqcurrModecurrTimemostFreqmostModemostTimefalsFreqtrueFreqvariancekeepTagsfloatPercentileReduceSliceintegerPercentileReduceSliceunsignedPercentileReduceSliceisNonNegativenHoldwarmupTypeincludeFitDatagotagithub.com/influxdata/influxdb/v2/influxql/query/internal/gota"github.com/influxdata/influxdb/v2/influxql/query/internal/gota"unsupported function call: %s"unsupported function call: %s"AggregatedNilCopyToAggregateFloatEmit-9223372036854775808peekTimeNextInWindowdimsreduceAggregateIntegerAggregateUnsignedAggregateStringAggregateBooleanunsupported count iterator type: %T"unsupported count iterator type: %T"unsupported min iterator type: %T"unsupported min iterator type: %T"unsupported max iterator type: %T"unsupported max iterator type: %T"unsupported sum iterator type: %T"unsupported sum iterator type: %T"unsupported first iterator type: %T"unsupported first iterator type: %T"unsupported last iterator type: %T"unsupported last iterator type: %T"unsupported distinct iterator type: %T"unsupported distinct iterator type: %T"unsupported mean iterator type: %T"unsupported mean iterator type: %T"AggregateFloatBulkAggregateIntegerBulkAggregateUnsignedBulkunsupported median iterator type: %T"unsupported median iterator type: %T"AggregateStringBulkAggregateBooleanBulkunsupported stddev iterator type: %T"unsupported stddev iterator type: %T"NaNPowunsupported spread iterator type: %T"unsupported spread iterator type: %T"unsupported top iterator type: %T"unsupported top iterator type: %T"unsupported bottom iterator type: %T"unsupported bottom iterator type: %T"unsupported percentile iterator type: %T"unsupported percentile iterator type: %T"Floor100.00.51/2Aggregatorunsupported derivative iterator type: %T"unsupported derivative iterator type: %T"unsupported difference iterator type: %T"unsupported difference iterator type: %T"unitConversionunsupported elapsed iterator type: %T"unsupported elapsed iterator type: %T"unsupported moving average iterator type: %T"unsupported moving average iterator type: %T"WarmupTypeEMAinTimePeriodalphawarmTypeWarmCountemaWarmedholdPeriodaggregateunsupported exponential moving average iterator type: %T"unsupported exponential moving average iterator type: %T"DEMAema1ema2demaunsupported double exponential moving average iterator type: %T"unsupported double exponential moving average iterator type: %T"TEMAema3temaunsupported triple exponential moving average iterator type: %T"unsupported triple exponential moving average iterator type: %T"RSIemaUpemaDownlastVrsiunsupported relative strength index iterator type: %T"unsupported relative strength index iterator type: %T"TRIXtrixunsupported triple exponential derivative iterator type: %T"unsupported triple exponential derivative iterator type: %T"KERkerPointpricenoisekerunsupported kaufmans efficiency ratio iterator type: %T"unsupported kaufmans efficiency ratio iterator type: %T"KAMAkamaunsupported kaufmans adaptive moving average iterator type: %T"unsupported kaufmans adaptive moving average iterator type: %T"AlgSimplecmounsupported chande momentum oscillator iterator type: %T"unsupported chande momentum oscillator iterator type: %T"unsupported cumulative sum iterator type: %T"unsupported cumulative sum iterator type: %T"OptimizerMaxIterationsAlphaBetaGammaOptimizeseasonalhalfIntervaloptimepsilonroundTimeforecastsseconstrainrngunsupported integral iterator type: %T"unsupported integral iterator type: %T"
This file contains iterator implementations for each function call available
in InfluxQL. Call iterators are separated into two groups:

1. Map/reduce-style iterators - these are passed to IteratorCreator so that
   processing can be at the low-level storage and aggregates are returned.

2. Raw aggregate iterators - these require the full set of data for a window.
   These are handled by the select() function and raw points are streamed in
   from the low-level storage.

There are helpers to aid in building aggregate iterators. For simple map/reduce
iterators, you can use the reduceIterator types and pass a reduce function. This
reduce function is passed a previous and current value and the new timestamp,
value, and auxiliary fields are returned from it.

For raw aggregate iterators, you can use the reduceSliceIterators which pass
in a slice of all points to the function and return a point. For more complex
iterator types, you may need to create your own iterators by hand.

Once your iterator is complete, you'll need to add it to the NewCallIterator()
function if it is to be available to IteratorCreators and add it to the select()
function to allow it to be included during planning.
 NewCallIterator returns a new iterator for a Call. newCountIterator returns an iterator for operating on a count() call. FIXME: Wrap iterator in int-type iterator and always output int value. FloatCountReduce returns the count of points. IntegerCountReduce returns the count of points. UnsignedCountReduce returns the count of points. StringCountReduce returns the count of points. BooleanCountReduce returns the count of points. newMinIterator returns an iterator for operating on a min() call. FloatMinReduce returns the minimum value between prev & curr. IntegerMinReduce returns the minimum value between prev & curr. UnsignedMinReduce returns the minimum value between prev & curr. BooleanMinReduce returns the minimum value between prev & curr. newMaxIterator returns an iterator for operating on a max() call. FloatMaxReduce returns the maximum value between prev & curr. IntegerMaxReduce returns the maximum value between prev & curr. UnsignedMaxReduce returns the maximum value between prev & curr. BooleanMaxReduce returns the minimum value between prev & curr. newSumIterator returns an iterator for operating on a sum() call. FloatSumReduce returns the sum prev value & curr value. IntegerSumReduce returns the sum prev value & curr value. UnsignedSumReduce returns the sum prev value & curr value. newFirstIterator returns an iterator for operating on a first() call. FloatFirstReduce returns the first point sorted by time. IntegerFirstReduce returns the first point sorted by time. UnsignedFirstReduce returns the first point sorted by time. StringFirstReduce returns the first point sorted by time. BooleanFirstReduce returns the first point sorted by time. newLastIterator returns an iterator for operating on a last() call. FloatLastReduce returns the last point sorted by time. IntegerLastReduce returns the last point sorted by time. UnsignedLastReduce returns the last point sorted by time. StringLastReduce returns the first point sorted by time. BooleanLastReduce returns the first point sorted by time. NewDistinctIterator returns an iterator for operating on a distinct() call. newMeanIterator returns an iterator for operating on a mean() call. NewMedianIterator returns an iterator for operating on a median() call. newMedianIterator returns an iterator for operating on a median() call. FloatMedianReduceSlice returns the median value within a window. OPTIMIZE(benbjohnson): Use getSortedRange() from v0.9.5.1. Return the middle value from the points. If there are an even number of points then return the mean of the two middle points. IntegerMedianReduceSlice returns the median value within a window. UnsignedMedianReduceSlice returns the median value within a window. newModeIterator returns an iterator for operating on a mode() call. FloatModeReduceSlice returns the mode value within a window. IntegerModeReduceSlice returns the mode value within a window. UnsignedModeReduceSlice returns the mode value within a window. StringModeReduceSlice returns the mode value within a window. BooleanModeReduceSlice returns the mode value within a window. In case either of true or false are mode then retuned mode value wont be of metric with oldest timestamp newStddevIterator returns an iterator for operating on a stddev() call. FloatStddevReduceSlice returns the stddev value within a window. If there is only one point then return NaN. Calculate the mean. Calculate the variance. IntegerStddevReduceSlice returns the stddev value within a window. UnsignedStddevReduceSlice returns the stddev value within a window. newSpreadIterator returns an iterator for operating on a spread() call. newPercentileIterator returns an iterator for operating on a percentile() call. NewFloatPercentileReduceSliceFunc returns the percentile value within a window. NewIntegerPercentileReduceSliceFunc returns the percentile value within a window. NewUnsignedPercentileReduceSliceFunc returns the percentile value within a window. newDerivativeIterator returns an iterator for operating on a derivative() call. newDifferenceIterator returns an iterator for operating on a difference() call. newElapsedIterator returns an iterator for operating on a elapsed() call. newMovingAverageIterator returns an iterator for operating on a moving_average() call. newExponentialMovingAverageIterator returns an iterator for operating on an exponential_moving_average() call. newDoubleExponentialMovingAverageIterator returns an iterator for operating on a double_exponential_moving_average() call. newTripleExponentialMovingAverageIterator returns an iterator for operating on a triple_exponential_moving_average() call. newRelativeStrengthIndexIterator returns an iterator for operating on a triple_exponential_moving_average() call. newTripleExponentialDerivativeIterator returns an iterator for operating on a triple_exponential_moving_average() call. newKaufmansEfficiencyRatioIterator returns an iterator for operating on a kaufmans_efficiency_ratio() call. newKaufmansAdaptiveMovingAverageIterator returns an iterator for operating on a kaufmans_adaptive_moving_average() call. newChandeMomentumOscillatorIterator returns an iterator for operating on a triple_exponential_moving_average() call. newCumulativeSumIterator returns an iterator for operating on a cumulative_sum() call. newHoltWintersIterator returns an iterator for operating on a holt_winters() call. NewSampleIterator returns an iterator for operating on a sample() call (exported for use in test). newSampleIterator returns an iterator for operating on a sample() call. newIntegralIterator returns an iterator for operating on a integral() call.endTimeauxFieldssymbolsrewriteExprCostScanAtValuerEvalIntegerFloatDivisionEvalBoolevalBinaryExprvaluerscannersdriverShowMeasurementCardinalityStatementExactCallTypemonitorEncodeStringPointuseDefaultsGetDataTypeGetSeriesNGetPointNGetTagsGetNilGetAuxGetAggregatedGetTracefastIdxfastdetectFastlastTagsdimensionsDecodeFloatPointInheritedIntervalExtraIntervalsFunctionCallsOnlySelectorsHasDistinctTopBottomFunctionHasAuxiliaryFieldsHasTargetpreprocesscompilecompileFieldscompileDimensionsvalidateFieldsvalidateConditionsubqueryglobalAllowWildcardcompileExprcompileNestedExprcompileSymbolcompileFunctioncompilePercentilecompileSamplecompileDerivativecompileElapsedcompileDifferencecompileCumulativeSumcompileMovingAveragecompileExponentialMovingAveragecompileKaufmanscompileChandeMomentumOscillatorcompileIntegralcompileHoltWinterscompileDistinctcompileTopBottomcompileMathFunctionfmWriteResponseDecodeStringPointcreateRowEncodeBooleanPointStatsIntervalencodeFloatIteratorencodeIntegerIteratorencodeUnsignedIteratorencodeStringIteratorencodeBooleanIteratorEncodeIteratorencodeStatsFiltersAddFilterEncodeUnsignedPointDecodeIntegerPointDecodeBooleanPointShowFieldKeyCardinalityStatementExplainGetRetentionPolicyGetRegexGetIsTargetGetSystemIteratoricselectorwriteModebuildVarRefIteratorbuildCallIteratorcallIteratorShowTagValuesCardinalityStatementDecodeUnsignedPointEncodeIntegerPointGetValmapAuxFieldsmapAuxFieldfilterNonNildataTypeDecodePointformatResultsGetOffsetGetExprGetFieldsGetSourcesGetIntervalGetGroupByGetFillGetFillValueGetConditionGetAscendingGetSLimitGetSOffsetGetStripNameGetDedupeGetMaxSeriesNGetOrderedPrettyEncodeFloatPointShowSeriesStatementShowTagKeyCardinalityStatementShowSeriesCardinalityStatementmaxPointN/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/cast.goUnsigned castToType will coerce the underlying interface type to another interface depending on the type./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/compile.goshardMapperwithFitnestednargsmaxDiffnewTimesopttimeRange9223372036854775806SubQueryunable to use wildcard in a binary expression"unable to use wildcard in a binary expression"unable to use regex in a binary expression"unable to use regex in a binary expression""sample"distinct"distinct"derivative"derivative"non_negative_derivative"non_negative_derivative"difference"difference"non_negative_difference"non_negative_difference"cumulative_sum"cumulative_sum"moving_average"moving_average"exponential_moving_average"exponential_moving_average"double_exponential_moving_average"double_exponential_moving_average"triple_exponential_moving_average"triple_exponential_moving_average"relative_strength_index"relative_strength_index"triple_exponential_derivative"triple_exponential_derivative"kaufmans_efficiency_ratio"kaufmans_efficiency_ratio"kaufmans_adaptive_moving_average"kaufmans_adaptive_moving_average"chande_momentum_oscillator"chande_momentum_oscillator""elapsed"integral"integral"holt_winters"holt_winters"holt_winters_with_fit"holt_winters_with_fit"cannot perform a binary expression on two literals"cannot perform a binary expression on two literals"field must contain at least one variable"field must contain at least one variable"unimplemented"unimplemented"unsupported expression with wildcard: %s()"unsupported expression with wildcard: %s()"unsupported expression with regex field: %s()"unsupported expression with regex field: %s()"expected field argument in %s()"expected field argument in %s()"mode"mode"undefined function %s()"undefined function %s()"invalid number of arguments for %s, expected %d, got %d"invalid number of arguments for %s, expected %d, got %d"invalid number of arguments for percentile, expected %d, got %d"invalid number of arguments for percentile, expected %d, got %d"expected float argument in percentile()"expected float argument in percentile()"invalid number of arguments for sample, expected %d, got %d"invalid number of arguments for sample, expected %d, got %d"sample window must be greater than 1, got %d"sample window must be greater than 1, got %d"expected integer argument in sample()"expected integer argument in sample()"invalid number of arguments for %s, expected at least %d but no more than %d, got %d"invalid number of arguments for %s, expected at least %d but no more than %d, got %d"duration argument must be positive, got %s"duration argument must be positive, got %s"second argument to %s must be a duration, got %T"second argument to %s must be a duration, got %T"%s aggregate requires a GROUP BY interval"%s aggregate requires a GROUP BY interval"aggregate function required inside the call to %s"aggregate function required inside the call to %s"invalid number of arguments for elapsed, expected at least %d but no more than %d, got %d"invalid number of arguments for elapsed, expected at least %d but no more than %d, got %d"second argument to elapsed must be a duration, got %T"second argument to elapsed must be a duration, got %T"elapsed aggregate requires a GROUP BY interval"elapsed aggregate requires a GROUP BY interval"aggregate function required inside the call to elapsed"aggregate function required inside the call to elapsed"invalid number of arguments for %s, expected 1, got %d"invalid number of arguments for %s, expected 1, got %d"invalid number of arguments for cumulative_sum, expected 1, got %d"invalid number of arguments for cumulative_sum, expected 1, got %d"cumulative_sum aggregate requires a GROUP BY interval"cumulative_sum aggregate requires a GROUP BY interval"aggregate function required inside the call to cumulative_sum"aggregate function required inside the call to cumulative_sum"invalid number of arguments for moving_average, expected 2, got %d"invalid number of arguments for moving_average, expected 2, got %d"second argument for moving_average must be an integer, got %T"second argument for moving_average must be an integer, got %T"moving_average window must be greater than 1, got %d"moving_average window must be greater than 1, got %d"moving_average aggregate requires a GROUP BY interval"moving_average aggregate requires a GROUP BY interval"aggregate function required inside the call to moving_average"aggregate function required inside the call to moving_average"invalid number of arguments for %s, expected at least 2 but no more than 4, got %d"invalid number of arguments for %s, expected at least 2 but no more than 4, got %d"%s period must be an integer"%s period must be an integer"%s period must be greater than or equal to 1"%s period must be greater than or equal to 1"%s hold period must be greater than or equal to 1"%s hold period must be greater than or equal to 1"%s hold period must be greater than or equal to 0"%s hold period must be greater than or equal to 0"%s hold period must be an integer"%s hold period must be an integer"exponential"exponential"simple"simple"%s warmup type must be one of: 'exponential' 'simple'"%s warmup type must be one of: 'exponential' 'simple'"%s warmup type must be a string"%s warmup type must be a string"invalid number of arguments for %s, expected at least 2 but no more than 3, got %d"invalid number of arguments for %s, expected at least 2 but no more than 3, got %d"invalid number of arguments for chande_momentum_oscillator, expected at least 2 but no more than 4, got %d"invalid number of arguments for chande_momentum_oscillator, expected at least 2 but no more than 4, got %d"chande_momentum_oscillator period must be an integer"chande_momentum_oscillator period must be an integer"chande_momentum_oscillator period must be greater than or equal to 1"chande_momentum_oscillator period must be greater than or equal to 1"chande_momentum_oscillator hold period must be greater than or equal to 0"chande_momentum_oscillator hold period must be greater than or equal to 0"chande_momentum_oscillator hold period must be an integer"chande_momentum_oscillator hold period must be an integer"chande_momentum_oscillator warmup type must be one of: 'none' 'exponential' 'simple'"chande_momentum_oscillator warmup type must be one of: 'none' 'exponential' 'simple'"chande_momentum_oscillator warmup type must be a string"chande_momentum_oscillator warmup type must be a string"chande_momentum_oscillator aggregate requires a GROUP BY interval"chande_momentum_oscillator aggregate requires a GROUP BY interval"aggregate function required inside the call to chande_momentum_oscillator"aggregate function required inside the call to chande_momentum_oscillator"invalid number of arguments for integral, expected at least %d but no more than %d, got %d"invalid number of arguments for integral, expected at least %d but no more than %d, got %d"second argument must be a duration"second argument must be a duration"expected integer argument as second arg in %s"expected integer argument as second arg in %s"second arg to %s must be greater than 0, got %d"second arg to %s must be greater than 0, got %d"expected integer argument as third arg in %s"expected integer argument as third arg in %s"third arg to %s cannot be negative, got %d"third arg to %s cannot be negative, got %d"must use aggregate function with %s"must use aggregate function with %s"distinct function requires at least one argument"distinct function requires at least one argument"distinct function can only have one argument"distinct function can only have one argument"expected field argument in distinct()"expected field argument in distinct()"selector function %s() cannot be combined with other functions"selector function %s() cannot be combined with other functions"invalid number of arguments for %s, expected at least %d, got %d"invalid number of arguments for %s, expected at least %d, got %d"expected integer as last argument in %s(), found %s"expected integer as last argument in %s(), found %s"limit (%d) in %s function must be at least 1"limit (%d) in %s function must be at least 1"limit (%d) in %s function can not be larger than the LIMIT (%d) in the select statement"limit (%d) in %s function can not be larger than the LIMIT (%d) in the select statement"expected first argument to be a field in %s(), found %s"expected first argument to be a field in %s(), found %s"only fields or tags are allowed in %s(), found %s"only fields or tags are allowed in %s(), found %s"atan2"atan2"pow"pow"time() is a function and expects at least one argument"time() is a function and expects at least one argument"only time() calls allowed in dimensions"only time() calls allowed in dimensions"multiple time dimensions not allowed"multiple time dimensions not allowed"only time and tag dimensions allowed"only time and tag dimensions allowed"at least 1 non-time field must be queried"at least 1 non-time field must be queried"fill(none) must be used with a function"fill(none) must be used with a function"fill(linear) must be used with a function"fill(linear) must be used with a function"GROUP BY requires at least one aggregate function"GROUP BY requires at least one aggregate function"aggregate function distinct() cannot be combined with other functions or fields"aggregate function distinct() cannot be combined with other functions or fields"mixing aggregate and non-aggregate queries is not supported"mixing aggregate and non-aggregate queries is not supported"mixing multiple selector functions with tags or fields is not supported"mixing multiple selector functions with tags or fields is not supported"invalid function call in condition: %s"invalid function call in condition: %s"MultiValuersubqueries must be ordered in the same direction as the query itself"subqueries must be ordered in the same direction as the query itself"MinNanoTimemax-select-buckets limit exceeded: (%d/%d)"max-select-buckets limit exceeded: (%d/%d)" CompileOptions are the customization options for the compiler. Statement is a compiled query statement. Prepare prepares the statement by mapping shards and finishing the creation of the query plan. compiledStatement represents a select statement that has undergone some initial processing to determine if it is valid and to have some initial modifications done on the AST. Condition is the condition used for accessing data. TimeRange is the TimeRange for selecting data. Interval holds the time grouping interval. InheritedInterval marks if the interval was inherited by a parent. If this is set, then an interval that was inherited will not cause a query that shouldn't have an interval to fail. ExtraIntervals is the number of extra intervals that will be read in addition to the TimeRange. It is a multiple of Interval and only applies to queries that have an Interval. It is used to extend the TimeRange of the mapped shards to include additional non-emitted intervals used by derivative and other functions. It will be set to the highest number of extra intervals that need to be read even if it doesn't apply to all functions. The number will always be positive. This value may be set to a non-zero value even if there is no interval for the compiled query. Ascending is true if the time ordering is ascending. FunctionCalls holds a reference to the call expression of every function call that has been encountered. OnlySelectors is set to true when there are no aggregate functions. HasDistinct is set when the distinct() function is encountered. FillOption contains the fill option for aggregates. TopBottomFunction is set to top or bottom when one of those functions are used in the statement. HasAuxiliaryFields is true when the function requires auxiliary fields. Fields holds all of the fields that will be used. TimeFieldName stores the name of the time field's column. The column names generated by the compiler will not conflict with this name. Limit is the number of rows per series this query should be limited to. HasTarget is true if this query is being written into a target. Options holds the configured compiler options. Convert DISTINCT into a call. Remove "time" from fields list. Rewrite any regex conditions that could make use of the index. preprocess retrieves and records the global attributes of the current statement. Verify that the condition is actually ok to use. Read the dimensions of the query, validate them, and retrieve the interval if it exists. Retrieve the fill option for the statement. Resolve the min and max times now that we know if there is an interval or not. If the interval is non-zero, then we have an aggregate query and need to limit the maximum time to now() for backwards compatibility and usability. Look through the sources and compile each of the subqueries (if they exist). We do this after compiling the outside because subqueries may require inherited state. Remove any time selection (it is automatically selected by default) and set the time column name to the alias of the time field if it exists. Such as SELECT time, max(value) FROM cpu will be SELECT max(value) FROM cpu and SELECT time AS timestamp, max(value) FROM cpu will return "timestamp" as the column name for the time. Append this field to the list of processed fields and compile it. This holds the global state from the compiled statement. Field is the top level field that is being compiled. AllowWildcard is set to true if a wildcard or regular expression is allowed. compileExpr creates the node that executes the expression and connects that node to the WriteEdge as the output. A bare variable reference will require auxiliary fields. Wildcards use auxiliary fields. We assume there will be at least one expansion. Register the function call in the list of function calls. Disallow wildcards in binary expressions. RewriteFields, which expands wildcards, is too complicated if we allow wildcards inside of expressions. Check if either side is a literal so we only compile one side if it is. Validate both sides of the expression. compileNestedExpr ensures that the expression is compiled as if it were a nested expression. Intercept the distinct call so we can pass nested as true. Must be a variable reference, wildcard, or regexp. Validate the function call and mark down some meta properties related to the function for query validation. top/bottom are not included here since they are not typical functions. These functions are not considered selectors. If this is a call to count(), allow distinct() to be used as the function argument. If we have count(), the argument may be a distinct() call. Retrieve the duration from the derivative() call, if specified. Must be a variable reference, function, wildcard, or regexp. Retrieve the duration from the elapsed() call, if specified. Add a field for each of the listed dimensions when not writing the results. How many arguments are we expecting? Did we get the expected number of args? Compile all the argument expressions that are not just literals. Reduce the expression before attempting anything. Do not evaluate the call. Ensure the call is time() and it has one or two duration arguments. If we already have a duration Use the evaluated offset to replace the argument. Ideally, we would use the interval assigned above, but the query engine hasn't been changed to use the compiler information yet. If literal looks like a date time then parse it as a time literal. Assign the reduced/changed expression to the dimension. validateFields validates that the fields are mutually compatible with each other. This runs at the end of compilation but before linking. Validate that at least one field has been selected. Ensure there are not multiple calls if top/bottom is present. If a distinct() call is present, ensure there is exactly one function. Validate we are using a selector or raw query if auxiliary fields are required. validateCondition verifies that all elements in the condition are appropriate. For example, aggregate calls don't work in the condition and should throw an error as an invalid expression. Verify each side of the binary expression. We do not need to verify the binary expression itself since that should have been done by influxql.ConditionExpr. Are all the args valid? subquery compiles and validates a compiled statement for the subquery using this compiledStatement as the parent. Substitute now() into the subquery condition. Then use ConditionExpr to validate the expression. Do not store the results. We have no way to store and read those results at the moment. If the ordering is different and the sort field was specified for the subquery, throw an error. Find the intersection between this time range and the parent. If the subquery doesn't have a time range, this causes it to inherit the parent's time range. If the fill option is null, set it to none so we don't waste time on null values with a redundant fill iterator. Inherit the grouping interval if the subquery has none. If this is a query with a grouping, there is a bucket limit, and the minimum time has not been specified, we need to limit the possible time range that can be used when mapping shards but not when actually executing the select statement. Determine the shard time range here. Determine the last bucket using the end time. Determine the time difference using the number of buckets. Determine the maximum difference between the buckets based on the end time. Modify the time range if there are extra intervals and an interval. Create an iterator creator based on the shards in the cluster. Rewrite wildcards, if any exist. Validate if the types are correct now that they have been assigned. Determine base options for iterators. Determine the start and end time matched to the interval (may not match the actual times). Determine the number of buckets by finding the time span and dividing by the interval./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/cursor.goexprstypmapfvcurNamecurTagscurTimeEvalTypeMapValuerExprNames Series represents the metadata about a series. Name is the measurement name. Tags for the series. This is an internal id used to easily compare if a series is the same as another series. Whenever the internal cursor changes to a new series, this id gets incremented. It is not exposed to the user so we can implement this in whatever way we want. If a series is not generated by a cursor, this id is zero and it will instead attempt to compare the name and tags. SameSeries checks if this is the same series as another one. It does not necessarily check for equality so this is different from checking to see if the name and tags are the same. It checks whether the two are part of the same series in the response. Equal checks to see if the Series are identical. If the ids are the same, then we can short-circuit and assume they are the same. If they are not the same, do the long check since they may still be identical, but not necessarily generated from the same cursor. Row represents a single row returned by the query engine. Time returns the time for this row. If the cursor was created to return time as one of the values, the time will also be included as a time.Time in the appropriate column within Values. This ensures that time is always present in the Row structure even if it hasn't been requested in the output. Series contains the series metadata for this row. Values contains the values within the current row. Scan will retrieve the next row and assign the result to the passed in Row. If the Row has not been initialized, the Cursor will initialize the Row. To increase speed and memory usage, the same Row can be used and the previous values will be overwritten while using the same memory. Stats returns the IteratorStats from the underlying iterators. Err returns any errors that were encountered from scanning the rows. Columns returns the column names and types. Close closes the underlying resources that the cursor is using. RowCursor returns a Cursor that iterates over Rows. A special case if the field is time to reduce memory allocations. If the float value is NaN, convert it to a null float so this can be serialized correctly, but not mistaken for a null value that needs to be filled. if a new series, clear the map of previous values fields holds the mapping of field names to the index in the row based off of the column metadata. This only contains the fields we need and will exclude the ones we do not. If the field is not a column, assume it is a tag value. We do not know what the tag values will be, but there really isn't any different between NullMap and a TagMap that's pointed at the wrong location for the purposes described here. Use the field mappings to prepare the map for the valuer. Passes the filter! Return true. We no longer need to search for a suitable value. DrainCursor will read and discard all values from a Cursor and return the error if one happens. Do nothing with the result./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/emitter.go Emitter reads from a cursor into rows. NewEmitter returns a new instance of Emitter that pulls from itrs. Close closes the underlying iterators. Emit returns the next row from the iterators. Continually read from the cursor until it is exhausted. Scan the next row. If there are no rows left, return the current row. If there's no row yet then create one. If the name and tags match the existing row, append to that row if the number of values doesn't exceed the chunk size. Otherwise return existing row and add values to next emitted row. createRow creates a new row attached to the emitter./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/execution_context.goiql ExecutionContext contains state that the query is currently executing with. The statement ID of the executing query. Output channel where results and errors should be sent. StatisticsGatherer gathers metrics about the execution of a query. Options used to start this query. Send sends a Result to the Results channel and will exit if the query has been interrupted or aborted./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/executor.gostatisticsdefaultDBnewStmtstmtDurstmtStartstmtStatsgathererstatusLabelgithub.com/opentracing/opentracing-go/log"github.com/opentracing/opentracing-go/log"invalid query"invalid query"not executed"not executed"query interrupted"query interrupted"INFLUXDB_PANIC_CRASH"INFLUXDB_PANIC_CRASH"database not found: %s"database not found: %s"max-select-point limit exceeed: (%d/%d)"max-select-point limit exceeed: (%d/%d)"max-concurrent-queries limit exceeded(%d, %d)"max-concurrent-queries limit exceeded(%d, %d)"LOOPHasDefaultDatabaseIsSystemNamethe appropriate meta command"the appropriate meta command"_fieldKeys"_fieldKeys"SHOW FIELD KEYS"SHOW FIELD KEYS"_measurements"_measurements"SHOW MEASUREMENTS"SHOW MEASUREMENTS"SHOW SERIES"SHOW SERIES"_tagKeys"_tagKeys"SHOW TAG KEYS"SHOW TAG KEYS"_tags"_tags"SHOW TAG VALUES"SHOW TAG VALUES"unable to use system source '%s': use %s instead"unable to use system source '%s': use %s instead"Executing query"Executing query"normalized_query"normalized_query"%s [panic:%s] %s"%s [panic:%s] %s"%s [panic:%s]"%s [panic:%s]"

=====
All goroutines now follow:"\n\n=====\nAll goroutines now follow:" ErrInvalidQuery is returned when executing an unknown query type. ErrNotExecuted is returned when a statement is not executed in a query. This can occur when a previous statement in the same query has errored. ErrQueryInterrupted is an error returned when the query is interrupted. PanicCrashEnv is the environment variable that, when set, will prevent the handler from recovering any panics. ErrDatabaseNotFound returns a database not found error for the given database name. ErrMaxSelectPointsLimitExceeded is an error when a query hits the maximum number of points. ErrMaxConcurrentQueriesLimitExceeded is an error when a query cannot be run because the maximum number of queries has been reached. Authorizer determines if certain operations are authorized. AuthorizeDatabase indicates whether the given Privilege is authorized on the database with the given name. AuthorizeQuery returns an error if the query cannot be executed AuthorizeSeriesRead determines if a series is authorized for reading AuthorizeSeriesWrite determines if a series is authorized for writing OpenAuthorizer is the Authorizer used when authorization is disabled. It allows all operations. OpenAuthorizer can be shared by all goroutines. AuthorizeDatabase returns true to allow any operation on a database. AuthorizeSeriesRead allows access to any series. AuthorizeSeriesWrite allows access to any series. AuthorizeSeriesRead allows any query to execute. AuthorizerIsOpen returns true if the provided Authorizer is guaranteed to authorize anything. A nil Authorizer returns true for this function, and this function should be preferred over directly checking if an Authorizer is nil or not. ExecutionOptions contains the options for executing a query. OrgID is the organization for which this query is being executed. The database the query is running against. The retention policy the query is running against. How to determine whether the query is allowed to execute, what resources can be returned in SHOW queries, etc. The requested maximum number of points to return in each result. If this query is being executed in a read-only context. Node to execute on. Quiet suppresses non-essential output from the query executor. NewContextWithIterators returns a new context.Context with the *Iterators slice added. The query planner will add instances of AuxIterator to the Iterators slice. StatementExecutor executes a statement within the Executor. ExecuteStatement executes a statement. Results should be sent to the results channel in the ExecutionContext. StatementNormalizer normalizes a statement before it is executed. NormalizeStatement adds a default database and policy to the measurements in the statement. Executor executes every statement in an Query. Used for executing a statement in the query. NewExecutor returns a new instance of Executor. Close kills all running queries and prevents new queries from being attached. ExecuteQuery executes each statement within a query. Setup the execution context that will be used when executing statements. If a default database wasn't passed in by the caller, check the statement. Do not let queries manually use the system measurements. If we find one, return an error. This prevents a person from using the measurement incorrectly and causing a panic. Rewrite statements, if necessary. This can occur on meta read statements which convert to SELECT statements. Log each normalized statement. Send any other statements to the underlying statement executor. Send an error for this result if it failed for some reason. Stop after the first error. Check if the query was interrupted during an uninterruptible statement. Send error results for any statements which were not executed. Determines if the Executor will recover any panics or let them crash/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/explain.gocostEXPRESSION: %s
"EXPRESSION: %s\n"AUXILIARY FIELDS: %s
"AUXILIARY FIELDS: %s\n"NUMBER OF SHARDS: %d
"NUMBER OF SHARDS: %d\n"NUMBER OF SERIES: %d
"NUMBER OF SERIES: %d\n"CACHED VALUES: %d
"CACHED VALUES: %d\n"NUMBER OF FILES: %d
"NUMBER OF FILES: %d\n"NUMBER OF BLOCKS: %d
"NUMBER OF BLOCKS: %d\n"SIZE OF BLOCKS: %d
"SIZE OF BLOCKS: %d\n" Determine the cost of all iterators created as part of this plan./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/functions.gen.gorndptsNewSource Generated by tmpl https://github.com/benbjohnson/tmpl DO NOT EDIT! Source: functions.gen.go.tmpl FloatPointAggregator aggregates points to produce a single point. FloatBulkPointAggregator aggregates multiple points at a time. AggregateFloatPoints feeds a slice of FloatPoint into an aggregator. If the aggregator is a FloatBulkPointAggregator, it will use the AggregateBulk method. FloatPointEmitter produces a single point from an aggregate. FloatReduceFunc is the function called by a FloatPoint reducer. FloatFuncReducer is a reducer that reduces the passed in points to a single point using a reduce function. NewFloatFuncReducer creates a new FloatFuncFloatReducer. AggregateFloat takes a FloatPoint and invokes the reduce function with the current and new point to modify the current point. Emit emits the point that was generated when reducing the points fed in with AggregateFloat. FloatReduceSliceFunc is the function called by a FloatPoint reducer. FloatSliceFuncReducer is a reducer that aggregates the passed in points and then invokes the function to reduce the points when they are emitted. NewFloatSliceFuncReducer creates a new FloatSliceFuncReducer. AggregateFloat copies the FloatPoint into the internal slice to be passed to the reduce function when Emit is called. AggregateFloatBulk performs a bulk copy of FloatPoints into the internal slice. This is a more efficient version of calling AggregateFloat on each point. Emit invokes the reduce function on the aggregated points to generate the aggregated points. This method does not clear the points from the internal slice. FloatReduceIntegerFunc is the function called by a FloatPoint reducer. FloatFuncIntegerReducer is a reducer that reduces NewFloatFuncIntegerReducer creates a new FloatFuncIntegerReducer. FloatReduceIntegerSliceFunc is the function called by a FloatPoint reducer. FloatSliceFuncIntegerReducer is a reducer that aggregates NewFloatSliceFuncIntegerReducer creates a new FloatSliceFuncIntegerReducer. FloatReduceUnsignedFunc is the function called by a FloatPoint reducer. FloatFuncUnsignedReducer is a reducer that reduces NewFloatFuncUnsignedReducer creates a new FloatFuncUnsignedReducer. FloatReduceUnsignedSliceFunc is the function called by a FloatPoint reducer. FloatSliceFuncUnsignedReducer is a reducer that aggregates NewFloatSliceFuncUnsignedReducer creates a new FloatSliceFuncUnsignedReducer. FloatReduceStringFunc is the function called by a FloatPoint reducer. FloatFuncStringReducer is a reducer that reduces NewFloatFuncStringReducer creates a new FloatFuncStringReducer. FloatReduceStringSliceFunc is the function called by a FloatPoint reducer. FloatSliceFuncStringReducer is a reducer that aggregates NewFloatSliceFuncStringReducer creates a new FloatSliceFuncStringReducer. FloatReduceBooleanFunc is the function called by a FloatPoint reducer. FloatFuncBooleanReducer is a reducer that reduces NewFloatFuncBooleanReducer creates a new FloatFuncBooleanReducer. FloatReduceBooleanSliceFunc is the function called by a FloatPoint reducer. FloatSliceFuncBooleanReducer is a reducer that aggregates NewFloatSliceFuncBooleanReducer creates a new FloatSliceFuncBooleanReducer. FloatDistinctReducer returns the distinct points in a series. NewFloatDistinctReducer creates a new FloatDistinctReducer. AggregateFloat aggregates a point into the reducer. Emit emits the distinct points that have been aggregated into the reducer. FloatElapsedReducer calculates the elapsed of the aggregated points. NewFloatElapsedReducer creates a new FloatElapsedReducer. AggregateFloat aggregates a point into the reducer and updates the current window. Emit emits the elapsed of the reducer at the current point. FloatSampleReducer implements a reservoir sampling to calculate a random subset of points how many points we've iterated over random number generator for each reducer the reservoir NewFloatSampleReducer creates a new FloatSampleReducer seed with current time as suggested by https://golang.org/pkg/math/rand/ Fill the reservoir with the first n points Generate a random integer between 1 and the count and if that number is less than the length of the slice replace the point at that index rnd with p. Emit emits the reservoir sample as many points. IntegerPointAggregator aggregates points to produce a single point. IntegerBulkPointAggregator aggregates multiple points at a time. AggregateIntegerPoints feeds a slice of IntegerPoint into an aggregator. If the aggregator is a IntegerBulkPointAggregator, it will IntegerPointEmitter produces a single point from an aggregate. IntegerReduceFloatFunc is the function called by a IntegerPoint reducer. IntegerFuncFloatReducer is a reducer that reduces NewIntegerFuncFloatReducer creates a new IntegerFuncFloatReducer. AggregateInteger takes a IntegerPoint and invokes the reduce function with the Emit emits the point that was generated when reducing the points fed in with AggregateInteger. IntegerReduceFloatSliceFunc is the function called by a IntegerPoint reducer. IntegerSliceFuncFloatReducer is a reducer that aggregates NewIntegerSliceFuncFloatReducer creates a new IntegerSliceFuncFloatReducer. AggregateInteger copies the IntegerPoint into the internal slice to be passed AggregateIntegerBulk performs a bulk copy of IntegerPoints into the internal slice. This is a more efficient version of calling AggregateInteger on each point. IntegerReduceFunc is the function called by a IntegerPoint reducer. IntegerFuncReducer is a reducer that reduces NewIntegerFuncReducer creates a new IntegerFuncIntegerReducer. IntegerReduceSliceFunc is the function called by a IntegerPoint reducer. IntegerSliceFuncReducer is a reducer that aggregates NewIntegerSliceFuncReducer creates a new IntegerSliceFuncReducer. IntegerReduceUnsignedFunc is the function called by a IntegerPoint reducer. IntegerFuncUnsignedReducer is a reducer that reduces NewIntegerFuncUnsignedReducer creates a new IntegerFuncUnsignedReducer. IntegerReduceUnsignedSliceFunc is the function called by a IntegerPoint reducer. IntegerSliceFuncUnsignedReducer is a reducer that aggregates NewIntegerSliceFuncUnsignedReducer creates a new IntegerSliceFuncUnsignedReducer. IntegerReduceStringFunc is the function called by a IntegerPoint reducer. IntegerFuncStringReducer is a reducer that reduces NewIntegerFuncStringReducer creates a new IntegerFuncStringReducer. IntegerReduceStringSliceFunc is the function called by a IntegerPoint reducer. IntegerSliceFuncStringReducer is a reducer that aggregates NewIntegerSliceFuncStringReducer creates a new IntegerSliceFuncStringReducer. IntegerReduceBooleanFunc is the function called by a IntegerPoint reducer. IntegerFuncBooleanReducer is a reducer that reduces NewIntegerFuncBooleanReducer creates a new IntegerFuncBooleanReducer. IntegerReduceBooleanSliceFunc is the function called by a IntegerPoint reducer. IntegerSliceFuncBooleanReducer is a reducer that aggregates NewIntegerSliceFuncBooleanReducer creates a new IntegerSliceFuncBooleanReducer. IntegerDistinctReducer returns the distinct points in a series. NewIntegerDistinctReducer creates a new IntegerDistinctReducer. AggregateInteger aggregates a point into the reducer. IntegerElapsedReducer calculates the elapsed of the aggregated points. NewIntegerElapsedReducer creates a new IntegerElapsedReducer. AggregateInteger aggregates a point into the reducer and updates the current window. IntegerSampleReducer implements a reservoir sampling to calculate a random subset of points NewIntegerSampleReducer creates a new IntegerSampleReducer UnsignedPointAggregator aggregates points to produce a single point. UnsignedBulkPointAggregator aggregates multiple points at a time. AggregateUnsignedPoints feeds a slice of UnsignedPoint into an aggregator. If the aggregator is a UnsignedBulkPointAggregator, it will UnsignedPointEmitter produces a single point from an aggregate. UnsignedReduceFloatFunc is the function called by a UnsignedPoint reducer. UnsignedFuncFloatReducer is a reducer that reduces NewUnsignedFuncFloatReducer creates a new UnsignedFuncFloatReducer. AggregateUnsigned takes a UnsignedPoint and invokes the reduce function with the Emit emits the point that was generated when reducing the points fed in with AggregateUnsigned. UnsignedReduceFloatSliceFunc is the function called by a UnsignedPoint reducer. UnsignedSliceFuncFloatReducer is a reducer that aggregates NewUnsignedSliceFuncFloatReducer creates a new UnsignedSliceFuncFloatReducer. AggregateUnsigned copies the UnsignedPoint into the internal slice to be passed AggregateUnsignedBulk performs a bulk copy of UnsignedPoints into the internal slice. This is a more efficient version of calling AggregateUnsigned on each point. UnsignedReduceIntegerFunc is the function called by a UnsignedPoint reducer. UnsignedFuncIntegerReducer is a reducer that reduces NewUnsignedFuncIntegerReducer creates a new UnsignedFuncIntegerReducer. UnsignedReduceIntegerSliceFunc is the function called by a UnsignedPoint reducer. UnsignedSliceFuncIntegerReducer is a reducer that aggregates NewUnsignedSliceFuncIntegerReducer creates a new UnsignedSliceFuncIntegerReducer. UnsignedReduceFunc is the function called by a UnsignedPoint reducer. UnsignedFuncReducer is a reducer that reduces NewUnsignedFuncReducer creates a new UnsignedFuncUnsignedReducer. UnsignedReduceSliceFunc is the function called by a UnsignedPoint reducer. UnsignedSliceFuncReducer is a reducer that aggregates NewUnsignedSliceFuncReducer creates a new UnsignedSliceFuncReducer. UnsignedReduceStringFunc is the function called by a UnsignedPoint reducer. UnsignedFuncStringReducer is a reducer that reduces NewUnsignedFuncStringReducer creates a new UnsignedFuncStringReducer. UnsignedReduceStringSliceFunc is the function called by a UnsignedPoint reducer. UnsignedSliceFuncStringReducer is a reducer that aggregates NewUnsignedSliceFuncStringReducer creates a new UnsignedSliceFuncStringReducer. UnsignedReduceBooleanFunc is the function called by a UnsignedPoint reducer. UnsignedFuncBooleanReducer is a reducer that reduces NewUnsignedFuncBooleanReducer creates a new UnsignedFuncBooleanReducer. UnsignedReduceBooleanSliceFunc is the function called by a UnsignedPoint reducer. UnsignedSliceFuncBooleanReducer is a reducer that aggregates NewUnsignedSliceFuncBooleanReducer creates a new UnsignedSliceFuncBooleanReducer. UnsignedDistinctReducer returns the distinct points in a series. NewUnsignedDistinctReducer creates a new UnsignedDistinctReducer. AggregateUnsigned aggregates a point into the reducer. UnsignedElapsedReducer calculates the elapsed of the aggregated points. NewUnsignedElapsedReducer creates a new UnsignedElapsedReducer. AggregateUnsigned aggregates a point into the reducer and updates the current window. UnsignedSampleReducer implements a reservoir sampling to calculate a random subset of points NewUnsignedSampleReducer creates a new UnsignedSampleReducer StringPointAggregator aggregates points to produce a single point. StringBulkPointAggregator aggregates multiple points at a time. AggregateStringPoints feeds a slice of StringPoint into an aggregator. If the aggregator is a StringBulkPointAggregator, it will StringPointEmitter produces a single point from an aggregate. StringReduceFloatFunc is the function called by a StringPoint reducer. StringFuncFloatReducer is a reducer that reduces NewStringFuncFloatReducer creates a new StringFuncFloatReducer. AggregateString takes a StringPoint and invokes the reduce function with the Emit emits the point that was generated when reducing the points fed in with AggregateString. StringReduceFloatSliceFunc is the function called by a StringPoint reducer. StringSliceFuncFloatReducer is a reducer that aggregates NewStringSliceFuncFloatReducer creates a new StringSliceFuncFloatReducer. AggregateString copies the StringPoint into the internal slice to be passed AggregateStringBulk performs a bulk copy of StringPoints into the internal slice. This is a more efficient version of calling AggregateString on each point. StringReduceIntegerFunc is the function called by a StringPoint reducer. StringFuncIntegerReducer is a reducer that reduces NewStringFuncIntegerReducer creates a new StringFuncIntegerReducer. StringReduceIntegerSliceFunc is the function called by a StringPoint reducer. StringSliceFuncIntegerReducer is a reducer that aggregates NewStringSliceFuncIntegerReducer creates a new StringSliceFuncIntegerReducer. StringReduceUnsignedFunc is the function called by a StringPoint reducer. StringFuncUnsignedReducer is a reducer that reduces NewStringFuncUnsignedReducer creates a new StringFuncUnsignedReducer. StringReduceUnsignedSliceFunc is the function called by a StringPoint reducer. StringSliceFuncUnsignedReducer is a reducer that aggregates NewStringSliceFuncUnsignedReducer creates a new StringSliceFuncUnsignedReducer. StringReduceFunc is the function called by a StringPoint reducer. StringFuncReducer is a reducer that reduces NewStringFuncReducer creates a new StringFuncStringReducer. StringReduceSliceFunc is the function called by a StringPoint reducer. StringSliceFuncReducer is a reducer that aggregates NewStringSliceFuncReducer creates a new StringSliceFuncReducer. StringReduceBooleanFunc is the function called by a StringPoint reducer. StringFuncBooleanReducer is a reducer that reduces NewStringFuncBooleanReducer creates a new StringFuncBooleanReducer. StringReduceBooleanSliceFunc is the function called by a StringPoint reducer. StringSliceFuncBooleanReducer is a reducer that aggregates NewStringSliceFuncBooleanReducer creates a new StringSliceFuncBooleanReducer. StringDistinctReducer returns the distinct points in a series. NewStringDistinctReducer creates a new StringDistinctReducer. AggregateString aggregates a point into the reducer. StringElapsedReducer calculates the elapsed of the aggregated points. NewStringElapsedReducer creates a new StringElapsedReducer. AggregateString aggregates a point into the reducer and updates the current window. StringSampleReducer implements a reservoir sampling to calculate a random subset of points NewStringSampleReducer creates a new StringSampleReducer BooleanPointAggregator aggregates points to produce a single point. BooleanBulkPointAggregator aggregates multiple points at a time. AggregateBooleanPoints feeds a slice of BooleanPoint into an aggregator. If the aggregator is a BooleanBulkPointAggregator, it will BooleanPointEmitter produces a single point from an aggregate. BooleanReduceFloatFunc is the function called by a BooleanPoint reducer. BooleanFuncFloatReducer is a reducer that reduces NewBooleanFuncFloatReducer creates a new BooleanFuncFloatReducer. AggregateBoolean takes a BooleanPoint and invokes the reduce function with the Emit emits the point that was generated when reducing the points fed in with AggregateBoolean. BooleanReduceFloatSliceFunc is the function called by a BooleanPoint reducer. BooleanSliceFuncFloatReducer is a reducer that aggregates NewBooleanSliceFuncFloatReducer creates a new BooleanSliceFuncFloatReducer. AggregateBoolean copies the BooleanPoint into the internal slice to be passed AggregateBooleanBulk performs a bulk copy of BooleanPoints into the internal slice. This is a more efficient version of calling AggregateBoolean on each point. BooleanReduceIntegerFunc is the function called by a BooleanPoint reducer. BooleanFuncIntegerReducer is a reducer that reduces NewBooleanFuncIntegerReducer creates a new BooleanFuncIntegerReducer. BooleanReduceIntegerSliceFunc is the function called by a BooleanPoint reducer. BooleanSliceFuncIntegerReducer is a reducer that aggregates NewBooleanSliceFuncIntegerReducer creates a new BooleanSliceFuncIntegerReducer. BooleanReduceUnsignedFunc is the function called by a BooleanPoint reducer. BooleanFuncUnsignedReducer is a reducer that reduces NewBooleanFuncUnsignedReducer creates a new BooleanFuncUnsignedReducer. BooleanReduceUnsignedSliceFunc is the function called by a BooleanPoint reducer. BooleanSliceFuncUnsignedReducer is a reducer that aggregates NewBooleanSliceFuncUnsignedReducer creates a new BooleanSliceFuncUnsignedReducer. BooleanReduceStringFunc is the function called by a BooleanPoint reducer. BooleanFuncStringReducer is a reducer that reduces NewBooleanFuncStringReducer creates a new BooleanFuncStringReducer. BooleanReduceStringSliceFunc is the function called by a BooleanPoint reducer. BooleanSliceFuncStringReducer is a reducer that aggregates NewBooleanSliceFuncStringReducer creates a new BooleanSliceFuncStringReducer. BooleanReduceFunc is the function called by a BooleanPoint reducer. BooleanFuncReducer is a reducer that reduces NewBooleanFuncReducer creates a new BooleanFuncBooleanReducer. BooleanReduceSliceFunc is the function called by a BooleanPoint reducer. BooleanSliceFuncReducer is a reducer that aggregates NewBooleanSliceFuncReducer creates a new BooleanSliceFuncReducer. BooleanDistinctReducer returns the distinct points in a series. NewBooleanDistinctReducer creates a new BooleanDistinctReducer. AggregateBoolean aggregates a point into the reducer. BooleanElapsedReducer calculates the elapsed of the aggregated points. NewBooleanElapsedReducer creates a new BooleanElapsedReducer. AggregateBoolean aggregates a point into the reducer and updates the current window. BooleanSampleReducer implements a reservoir sampling to calculate a random subset of points NewBooleanSampleReducer creates a new BooleanSampleReducer/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/functions.goremainderroundedphigammabetab0bestParamsforecastedl0minSSEparametersbTbTplTlTpphiHsTsTmsTmhyTyThhmseasonalsstmstmhptneldermeadcontainer/heap"container/heap"github.com/influxdata/influxdb/v2/influxql/query/neldermead"github.com/influxdata/influxdb/v2/influxql/query/neldermead"MaxInt64MinInt64MaxUint6418446744073709551615NewEMANewDEMANewTEMANewRSINewTRIXIsInfNewKERNewKAMACMOcmoPointsumUpsumDownNewCMOCMOScmosNewCMOS1.0e-40.000100000000000000004791/100000.30.29999999999999998893/100.40.40000000000000002222/57378697629483821/737869762948382064645404319552844595/180143985094819843602879701896397/9007199254740992Fix queryFieldMapper is a FieldMapper that wraps another FieldMapper and exposes the functions implemented by the query engine. Use the default FunctionTypeMapper for the query engine. CallTypeMapper returns the types for call iterator functions. Call iterator functions are commonly implemented within the storage engine so this mapper is limited to only the return values of those functions. If the function is not implemented by the embedded field mapper, then see if we implement the function and return the type here. TODO(jsternberg): Verify the input type. FunctionTypeMapper handles the type mapping for all functions implemented by the query engine. Handle functions implemented by the query engine. TODO(jsternberg): Do not use default for this. FloatMeanReducer calculates the mean of the aggregated points. NewFloatMeanReducer creates a new FloatMeanReducer. Emit emits the mean of the aggregated points as a single point. IntegerMeanReducer calculates the mean of the aggregated points. NewIntegerMeanReducer creates a new IntegerMeanReducer. UnsignedMeanReducer calculates the mean of the aggregated points. NewUnsignedMeanReducer creates a new UnsignedMeanReducer. FloatDerivativeReducer calculates the derivative of the aggregated points. NewFloatDerivativeReducer creates a new FloatDerivativeReducer. Skip past a point when it does not advance the stream. A joined series may have multiple points at the same time so we will discard anything except the first point we encounter. Emit emits the derivative of the reducer at the current point. Calculate the derivative of successive points by dividing the difference of each value by the elapsed time normalized to the interval. Mark this point as read by changing the previous point to nil. Drop negative values for non-negative derivatives. IntegerDerivativeReducer calculates the derivative of the aggregated points. NewIntegerDerivativeReducer creates a new IntegerDerivativeReducer. UnsignedDerivativeReducer calculates the derivative of the aggregated points. NewUnsignedDerivativeReducer creates a new UnsignedDerivativeReducer. FloatDifferenceReducer calculates the derivative of the aggregated points. NewFloatDifferenceReducer creates a new FloatDifferenceReducer. Emit emits the difference of the reducer at the current point. Calculate the difference of successive points. If it is non_negative_difference discard any negative value. Since prev is still marked as unread. The correctness can be ensured. IntegerDifferenceReducer calculates the derivative of the aggregated points. NewIntegerDifferenceReducer creates a new IntegerDifferenceReducer. UnsignedDifferenceReducer calculates the derivative of the aggregated points. NewUnsignedDifferenceReducer creates a new UnsignedDifferenceReducer. FloatMovingAverageReducer calculates the moving average of the aggregated points. NewFloatMovingAverageReducer creates a new FloatMovingAverageReducer. Emit emits the moving average of the current window. Emit should be called after every call to AggregateFloat and it will produce one point if there is enough data to fill a window, otherwise it will produce zero points. IntegerMovingAverageReducer calculates the moving average of the aggregated points. NewIntegerMovingAverageReducer creates a new IntegerMovingAverageReducer. after every call to AggregateInteger and it will produce one point if there UnsignedMovingAverageReducer calculates the moving average of the aggregated points. NewUnsignedMovingAverageReducer creates a new UnsignedMovingAverageReducer. after every call to AggregateUnsigned and it will produce one point if there FloatCumulativeSumReducer cumulates the values from each point. NewFloatCumulativeSumReducer creates a new FloatCumulativeSumReducer. IntegerCumulativeSumReducer cumulates the values from each point. NewIntegerCumulativeSumReducer creates a new IntegerCumulativeSumReducer. UnsignedCumulativeSumReducer cumulates the values from each point. NewUnsignedCumulativeSumReducer creates a new UnsignedCumulativeSumReducer. FloatHoltWintersReducer forecasts a series into the future. This is done using the Holt-Winters damped method.    1. Using the series the initial values are calculated using a SSE.    2. The series is forecasted into the future using the iterative relations. Season period Horizon Interval between points interval / 2 -- used to perform rounding Whether to include all data or only future values NelderMead optimizer Small difference bound for the optimizer Arbitrary weight for initializing some intial guesses. This should be in the  range [0,1] Epsilon value for the minimization process Define a grid of initial guesses for the parameters: alpha, beta, gamma, and phi. Keep in mind that this grid is N^4 so we should keep N small The starting lower guess  The upper bound on the grid The step between guesses NewFloatHoltWintersReducer creates a new FloatHoltWintersReducer. Overflow safe round function Round up Round down Emit returns the points generated by the HoltWinters algorithm. First fill in r.y with values and NaNs for missing values Drop values that occur for the same time bucket Add any missing values before the next point Add in a NaN so we can skip it later. Seasonality Starting guesses NOTE: Since these values are guesses in the cases where we were missing data, we can just skip the value and call it good. Determine best fit for the various parameters Forecast Clear data set Using the recursive relations compute the next values Forecast the data h points into the future. Constrain parameters seasonals is a ring buffer of past sT values Season index offset alpha beta gamma Compute sum squared error for the given parameters. Skip missing values since we cannot use them to compute an error. Compute error Penalize forecasted NaNs Constrain alpha, beta, gamma, phi in the range [0, 1] phi FloatIntegralReducer calculates the time-integral of the aggregated points. NewFloatIntegralReducer creates a new FloatIntegralReducer. If this is the first point, just save it Record the end of the time interval. We do not care for whether the last number is inclusive or exclusive because we treat both the same for the involved math. If this point has the same timestamp as the previous one, skip the point. Points sent into this reducer are expected to be fed in order. If our previous time is not equal to the window, we need to interpolate the area at the end of this interval. Emit the current point through the channel and then clear it. Normal operation: update the sum using the trapezium rule Emit emits the time-integral of the aggregated points as a single point. InfluxQL convention dictates that outside a group-by-time clause we return a timestamp of zero.  Within a group-by-time, we can set the time to ZeroTime and a higher level will change it to the start of the time group. Close flushes any in progress points to ensure any remaining points are emitted. If our last point is at the start time, then discard this point since there is no area within this bucket. Otherwise, send off what we currently have as the final point. IntegerIntegralReducer calculates the time-integral of the aggregated points. NewIntegerIntegralReducer creates a new IntegerIntegralReducer. If we see the minimum allowable time, set the time to zero so we don't break the default returned time for aggregate queries without times. Emit emits the time-integral of the aggregated points as a single FLOAT point NewUnsignedIntegralReducer creates a new UnsignedIntegralReducer. Compare the minimum point and the aggregated point. If our value is larger, replace the current min value. Ensure the points are sorted with the maximum value last. While the first point may be the minimum value, the rest is not guaranteed to be in any particular order while it is a heap./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/internal/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/internal/gota/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/internal/gota/cmo.goParseWarmupTypeWarmEMAWarmSMAidxOldestoldestoutV CMO - Chande Momentum Oscillator (https://www.fidelity.com/learning-center/trading-investing/technical-analysis/technical-indicator-guide/cmo) index of newest point NewCMO constructs a new CMO. WarmCount returns the number of samples that must be provided for the algorithm to be fully "warmed". Add adds a new sample value to the algorithm and returns the computed value.NOTE: because we're just adding and subtracting the difference, and not recalculating sumUp/sumDown using cmo.points[].price, it's possible for imprecision to creep in over time. Not sure how significant this is going to be, but if we want to fix it, we could recalculate it from scratch every N points. Warmed indicates whether the algorithm has enough data to generate accurate results. CMOS is a smoothed version of the Chande Momentum Oscillator. This is the version of CMO utilized by ta-lib. NewCMOS constructs a new CMOS. Last returns the last output value./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/internal/gota/ema.gowtlastAvgavgavg1avg2avg3invalid warmup type '%s'"invalid warmup type '%s'" Exponential Moving Average Simple Moving Average EMA - Exponential Moving Average (http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:moving_averages#exponential_moving_average_calculation) NewEMA constructs a new EMA. When warmed with WarmSMA the first inTimePeriod samples will result in a simple average, switching to exponential moving average after warmup is complete. When warmed with WarmEMA the algorithm immediately starts using an exponential moving average for the output values. During the warmup period the alpha value is scaled to prevent unbalanced weighting on initial values. ema.warmType == WarmEMA scale the alpha so that we don't excessively weight the result towards the first value don't just keep incrementing to prevent potential overflow DEMA - Double Exponential Moving Average (https://en.wikipedia.org/wiki/Double_exponential_moving_average) NewDEMA constructs a new DEMA. TEMA - Triple Exponential Moving Average (https://en.wikipedia.org/wiki/Triple_exponential_moving_average) NewTEMA constructs a new TEMA./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/internal/gota/kama.goer2.00.666666666666666629662/330.00.0645161290322580627252/310.6021505376344086224456/935423689873822533/90071992547409921162219258676257/18014398509481984 KER - Kaufman's Efficiency Ratio (http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:kaufman_s_adaptive_moving_average#efficiency_ratio_er) NewKER constructs a new KER.TODO this does not return a sensible value if not warmed. KAMA - Kaufman's Adaptive Moving Average (http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:kaufman_s_adaptive_moving_average) NewKAMA constructs a new KAMA.
			// initialize with a simple moving average
			kama.last = 0
			for _, v := range kama.ker.points[:kama.ker.count] {
				kama.last += v
			}
			kama.last /= float64(kama.ker.count + 1)
		 initialize with the last value/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/internal/gota/rsi.go RSI - Relative Strength Index (http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:relative_strength_index_rsi) NewRSI constructs a new RSI./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/internal/gota/trix.gorate Trix - TRIple Exponential average (http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:trix) NewTRIX constructs a new TRIX./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/internal/internal.pb.goItemsGetItemsGoGoProtoPackageIsVersion2protobuf:"bytes,1,req,name=Name" json:"Name,omitempty"`protobuf:"bytes,1,req,name=Name" json:"Name,omitempty"`protobuf:"bytes,2,req,name=Tags" json:"Tags,omitempty"`protobuf:"bytes,2,req,name=Tags" json:"Tags,omitempty"`protobuf:"varint,3,req,name=Time" json:"Time,omitempty"`protobuf:"varint,3,req,name=Time" json:"Time,omitempty"`protobuf:"varint,4,req,name=Nil" json:"Nil,omitempty"`protobuf:"varint,4,req,name=Nil" json:"Nil,omitempty"`protobuf:"bytes,5,rep,name=Aux" json:"Aux,omitempty"`protobuf:"bytes,5,rep,name=Aux" json:"Aux,omitempty"`protobuf:"varint,6,opt,name=Aggregated" json:"Aggregated,omitempty"`protobuf:"varint,6,opt,name=Aggregated" json:"Aggregated,omitempty"`protobuf:"fixed64,7,opt,name=FloatValue" json:"FloatValue,omitempty"`protobuf:"fixed64,7,opt,name=FloatValue" json:"FloatValue,omitempty"`protobuf:"varint,8,opt,name=IntegerValue" json:"IntegerValue,omitempty"`protobuf:"varint,8,opt,name=IntegerValue" json:"IntegerValue,omitempty"`protobuf:"bytes,9,opt,name=StringValue" json:"StringValue,omitempty"`protobuf:"bytes,9,opt,name=StringValue" json:"StringValue,omitempty"`protobuf:"varint,10,opt,name=BooleanValue" json:"BooleanValue,omitempty"`protobuf:"varint,10,opt,name=BooleanValue" json:"BooleanValue,omitempty"`protobuf:"varint,12,opt,name=UnsignedValue" json:"UnsignedValue,omitempty"`protobuf:"varint,12,opt,name=UnsignedValue" json:"UnsignedValue,omitempty"`protobuf:"bytes,11,opt,name=Stats" json:"Stats,omitempty"`protobuf:"bytes,11,opt,name=Stats" json:"Stats,omitempty"`protobuf:"bytes,13,opt,name=Trace" json:"Trace,omitempty"`protobuf:"bytes,13,opt,name=Trace" json:"Trace,omitempty"`protobuf:"varint,1,req,name=DataType" json:"DataType,omitempty"`protobuf:"varint,1,req,name=DataType" json:"DataType,omitempty"`protobuf:"fixed64,2,opt,name=FloatValue" json:"FloatValue,omitempty"`protobuf:"fixed64,2,opt,name=FloatValue" json:"FloatValue,omitempty"`protobuf:"varint,3,opt,name=IntegerValue" json:"IntegerValue,omitempty"`protobuf:"varint,3,opt,name=IntegerValue" json:"IntegerValue,omitempty"`protobuf:"bytes,4,opt,name=StringValue" json:"StringValue,omitempty"`protobuf:"bytes,4,opt,name=StringValue" json:"StringValue,omitempty"`protobuf:"varint,5,opt,name=BooleanValue" json:"BooleanValue,omitempty"`protobuf:"varint,5,opt,name=BooleanValue" json:"BooleanValue,omitempty"`protobuf:"varint,6,opt,name=UnsignedValue" json:"UnsignedValue,omitempty"`protobuf:"varint,6,opt,name=UnsignedValue" json:"UnsignedValue,omitempty"`protobuf:"bytes,1,opt,name=Expr" json:"Expr,omitempty"`protobuf:"bytes,1,opt,name=Expr" json:"Expr,omitempty"`protobuf:"bytes,2,rep,name=Aux" json:"Aux,omitempty"`protobuf:"bytes,2,rep,name=Aux" json:"Aux,omitempty"`protobuf:"bytes,17,rep,name=Fields" json:"Fields,omitempty"`protobuf:"bytes,17,rep,name=Fields" json:"Fields,omitempty"`protobuf:"bytes,3,rep,name=Sources" json:"Sources,omitempty"`protobuf:"bytes,3,rep,name=Sources" json:"Sources,omitempty"`protobuf:"bytes,4,opt,name=Interval" json:"Interval,omitempty"`protobuf:"bytes,4,opt,name=Interval" json:"Interval,omitempty"`protobuf:"bytes,5,rep,name=Dimensions" json:"Dimensions,omitempty"`protobuf:"bytes,5,rep,name=Dimensions" json:"Dimensions,omitempty"`protobuf:"bytes,19,rep,name=GroupBy" json:"GroupBy,omitempty"`protobuf:"bytes,19,rep,name=GroupBy" json:"GroupBy,omitempty"`protobuf:"varint,6,opt,name=Fill" json:"Fill,omitempty"`protobuf:"varint,6,opt,name=Fill" json:"Fill,omitempty"`protobuf:"fixed64,7,opt,name=FillValue" json:"FillValue,omitempty"`protobuf:"fixed64,7,opt,name=FillValue" json:"FillValue,omitempty"`protobuf:"bytes,8,opt,name=Condition" json:"Condition,omitempty"`protobuf:"bytes,8,opt,name=Condition" json:"Condition,omitempty"`protobuf:"varint,9,opt,name=StartTime" json:"StartTime,omitempty"`protobuf:"varint,9,opt,name=StartTime" json:"StartTime,omitempty"`protobuf:"varint,10,opt,name=EndTime" json:"EndTime,omitempty"`protobuf:"varint,10,opt,name=EndTime" json:"EndTime,omitempty"`protobuf:"bytes,21,opt,name=Location" json:"Location,omitempty"`protobuf:"bytes,21,opt,name=Location" json:"Location,omitempty"`protobuf:"varint,11,opt,name=Ascending" json:"Ascending,omitempty"`protobuf:"varint,11,opt,name=Ascending" json:"Ascending,omitempty"`protobuf:"varint,12,opt,name=Limit" json:"Limit,omitempty"`protobuf:"varint,12,opt,name=Limit" json:"Limit,omitempty"`protobuf:"varint,13,opt,name=Offset" json:"Offset,omitempty"`protobuf:"varint,13,opt,name=Offset" json:"Offset,omitempty"`protobuf:"varint,14,opt,name=SLimit" json:"SLimit,omitempty"`protobuf:"varint,14,opt,name=SLimit" json:"SLimit,omitempty"`protobuf:"varint,15,opt,name=SOffset" json:"SOffset,omitempty"`protobuf:"varint,15,opt,name=SOffset" json:"SOffset,omitempty"`protobuf:"varint,22,opt,name=StripName" json:"StripName,omitempty"`protobuf:"varint,22,opt,name=StripName" json:"StripName,omitempty"`protobuf:"varint,16,opt,name=Dedupe" json:"Dedupe,omitempty"`protobuf:"varint,16,opt,name=Dedupe" json:"Dedupe,omitempty"`protobuf:"varint,18,opt,name=MaxSeriesN" json:"MaxSeriesN,omitempty"`protobuf:"varint,18,opt,name=MaxSeriesN" json:"MaxSeriesN,omitempty"`protobuf:"varint,20,opt,name=Ordered" json:"Ordered,omitempty"`protobuf:"varint,20,opt,name=Ordered" json:"Ordered,omitempty"`protobuf:"bytes,1,rep,name=Items" json:"Items,omitempty"`protobuf:"bytes,1,rep,name=Items" json:"Items,omitempty"`protobuf:"bytes,1,opt,name=Database" json:"Database,omitempty"`protobuf:"bytes,1,opt,name=Database" json:"Database,omitempty"`protobuf:"bytes,2,opt,name=RetentionPolicy" json:"RetentionPolicy,omitempty"`protobuf:"bytes,2,opt,name=RetentionPolicy" json:"RetentionPolicy,omitempty"`protobuf:"bytes,3,opt,name=Name" json:"Name,omitempty"`protobuf:"bytes,3,opt,name=Name" json:"Name,omitempty"`protobuf:"bytes,4,opt,name=Regex" json:"Regex,omitempty"`protobuf:"bytes,4,opt,name=Regex" json:"Regex,omitempty"`protobuf:"varint,5,opt,name=IsTarget" json:"IsTarget,omitempty"`protobuf:"varint,5,opt,name=IsTarget" json:"IsTarget,omitempty"`protobuf:"bytes,6,opt,name=SystemIterator" json:"SystemIterator,omitempty"`protobuf:"bytes,6,opt,name=SystemIterator" json:"SystemIterator,omitempty"`protobuf:"varint,1,opt,name=Duration" json:"Duration,omitempty"`protobuf:"varint,1,opt,name=Duration" json:"Duration,omitempty"`protobuf:"varint,2,opt,name=Offset" json:"Offset,omitempty"`protobuf:"varint,2,opt,name=Offset" json:"Offset,omitempty"`protobuf:"varint,1,opt,name=SeriesN" json:"SeriesN,omitempty"`protobuf:"varint,1,opt,name=SeriesN" json:"SeriesN,omitempty"`protobuf:"varint,2,opt,name=PointN" json:"PointN,omitempty"`protobuf:"varint,2,opt,name=PointN" json:"PointN,omitempty"`protobuf:"bytes,1,req,name=Val" json:"Val,omitempty"`protobuf:"bytes,1,req,name=Val" json:"Val,omitempty"`protobuf:"varint,2,opt,name=Type" json:"Type,omitempty"`protobuf:"varint,2,opt,name=Type" json:"Type,omitempty"`query.Point"query.Point"query.Aux"query.Aux"query.IteratorOptions"query.IteratorOptions"query.Measurements"query.Measurements"query.Measurement"query.Measurement"query.Interval"query.Interval"query.IteratorStats"query.IteratorStats"query.VarRef"query.VarRef"internal/internal.proto"internal/internal.proto" source: internal/internal.proto
Package query is a generated protocol buffer package.

It is generated from these files:
	internal/internal.proto

It has these top-level messages:
	Point
	Aux
	IteratorOptions
	Measurements
	Measurement
	Interval
	IteratorStats
	VarRef
 796 bytes of a gzipped FileDescriptorProto/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/iterator.gen.goitrsbufInputinWindowxTagsyTagsxtythasDuplessok1ok2aggregatoremittersortedByTimepencCONSTRUCT0xFFStable "\x00"RewriteFuncCloneExpr Source: iterator.gen.go.tmpllint:file-ignore U1000 this is generated code DefaultStatsInterval is the default value for IteratorEncoder.StatsInterval. FloatIterator represents a stream of float points. newFloatIterators converts a slice of Iterator to a slice of FloatIterator. Drop and closes any iterator in itrs that is not a FloatIterator and cannot be cast to a FloatIterator. bufFloatIterator represents a buffered FloatIterator. newBufFloatIterator returns a buffered FloatIterator. Stats returns statistics from the input iterator. Close closes the underlying iterator. peek returns the next point without removing it from the iterator. peekTime returns the time of the next point. Returns zero time if no more points available. Next returns the current buffer, if exists, or calls the underlying iterator. NextInWindow returns the next value if it is between [startTime, endTime). If the next value is outside the range then it is moved to the buffer. unread sets v to the buffer. It is read on the next call to Next(). floatMergeIterator represents an iterator that combines multiple float iterators. Current iterator and window. newFloatMergeIterator returns a new instance of floatMergeIterator. Initialize heap items. Wrap in buffer, ignore any inputs without anymore points. Append to the heap. Stats returns an aggregation of stats from the underlying iterators. Next returns the next point from the iterator. Initialize the heap. This needs to be done lazily on the first call to this iterator so that iterator initialization done through the Select() call returns quickly. Queries can only be interrupted after the Select() call completes so any operations done during iterator creation cannot be interrupted, which is why we do it here instead so an interrupt can happen while initializing the heap. Retrieve the next iterator if we don't have one. Read point and set current window. Read the next point from the current iterator. If there are no more points then remove iterator from heap and find next. Check if the point is inside of our current window. If it's outside our window then push iterator back on the heap and find new iterator. floatMergeHeap represents a heap of floatMergeHeapItems. Items are sorted by their next window and then by name/tags. floatSortedMergeIterator is an iterator that sorts and merges multiple iterators into one. newFloatSortedMergeIterator returns an instance of floatSortedMergeIterator. Next returns the next points from the iterator. pop returns the next point from the heap. Reads the next point from item's cursor and puts it back on the heap. Initialize the heap. See the MergeIterator to see why this has to be done lazily. Read the next item from the heap. Copy the point for return. Read the next item from the cursor. Push back to heap if one exists. floatSortedMergeHeap represents a heap of floatSortedMergeHeapItems. Items are sorted with the following priority:     - By their measurement name;     - By their tag keys/values;     - By time; or     - By their Aux field values. if each input comes from a unique single time series, we can make a shortcut. detection of the shortcut introduces some overhead but it gets significant performance improvement in cases like SELECT * FROM m GROUP BY * TT ret | == -1 | h.opt.Ascending | result  1  | false |  false          | true -1  | true  |  false          | false  1  | false |  true           | false -1  | true  |  true           | true Unsupported types used in Aux fields. Maybe they need to be added here? Times and/or Aux fields are equal. index for fast shortcut floatIteratorScanner scans the results of a FloatIterator into a map. newFloatIteratorScanner creates a new IteratorScanner. Insert the fill value if one was specified. floatParallelIterator represents an iterator that pulls data in a separate goroutine. newFloatParallelIterator returns a new instance of floatParallelIterator. Stats returns stats from the underlying iterator. monitor runs in a separate goroutine and actively pulls the next point. Read next point. floatLimitIterator represents an iterator that limits points per group. newFloatLimitIterator returns a new instance of floatLimitIterator. Reset window and counter if a new window is encountered. Increment counter. Read next point if not beyond the offset. Read next point if we're beyond the limit. Check if the next point is outside of our window or is nil. If we are inside of an interval, unread the point and continue below to constructing a new point. We are *not* in a current interval. If there is no next point, we are at the end of all intervals. Set the new interval. Check if the point is our next expected point. Advance the expected time. Do not advance to a new window here as there may be lingering points with the same timestamp in the previous window. Check to see if we have passed over an offset change and adjust the time to account for this new offset. floatIntervalIterator represents a float implementation of IntervalIterator. floatInterruptIterator represents a float implementation of InterruptIterator. Only check if the channel is closed every N points. This intentionally checks on both 0 and N so that if the iterator has been interrupted before the first point is emitted it will not emit any points. Reset iterator count to zero and fall through to emit the next point. Increment the counter for every point read. floatCloseInterruptIterator represents a float implementation of CloseInterruptIterator. Check if the iterator was closed. floatReduceFloatIterator executes a reducer for every interval and buffers the result. Stats returns stats from the input iterator. Close closes the iterator and all child iterators. Next returns the minimum value for the next available interval. Calculate next window if we have no more points. Pop next point off the stack. floatReduceFloatPoint stores the reduced data for a name/tag combination. reduce executes fn once for every point in the next window. The previous value for the dimension is passed to fn. Calculate next window. Unread the point so it can be processed. Create points by tags. Ensure this point is within the same final window. Retrieve the tags on this point for this level of the query. This may be different than the bucket dimensions. Retrieve the aggregator for this name/tag combination or create one. Reverse sort points by name & tag. This ensures a consistent order of output. Assume the points are already sorted until proven otherwise. Emit the points for each name & tag combination. Set the points time to the interval time if the reducer didn't provide one. Points may be out of order. Perform a stable sort by time if requested. floatStreamFloatIterator streams inputs into the iterator and emits points gradually. newFloatStreamFloatIterator returns a new instance of floatStreamFloatIterator. Next returns the next value for the stream iterator. reduce creates and manages aggregators for every point from the input. After aggregating a point, it always tries to emit a value using the emitter. We have already read all of the input points. Close all of the aggregators to flush any remaining points to emit. Eliminate the aggregators and emitters. Attempt to emit points from the aggregator. floatReduceIntegerIterator executes a reducer for every interval and buffers the result. floatReduceIntegerPoint stores the reduced data for a name/tag combination. floatStreamIntegerIterator streams inputs into the iterator and emits points gradually. newFloatStreamIntegerIterator returns a new instance of floatStreamIntegerIterator. floatReduceUnsignedIterator executes a reducer for every interval and buffers the result. floatReduceUnsignedPoint stores the reduced data for a name/tag combination. floatStreamUnsignedIterator streams inputs into the iterator and emits points gradually. newFloatStreamUnsignedIterator returns a new instance of floatStreamUnsignedIterator. floatReduceStringIterator executes a reducer for every interval and buffers the result. floatReduceStringPoint stores the reduced data for a name/tag combination. floatStreamStringIterator streams inputs into the iterator and emits points gradually. newFloatStreamStringIterator returns a new instance of floatStreamStringIterator. floatReduceBooleanIterator executes a reducer for every interval and buffers the result. floatReduceBooleanPoint stores the reduced data for a name/tag combination. floatStreamBooleanIterator streams inputs into the iterator and emits points gradually. newFloatStreamBooleanIterator returns a new instance of floatStreamBooleanIterator. floatDedupeIterator only outputs unique points. This differs from the DistinctIterator in that it compares all aux fields too. This iterator is relatively inefficient and should only be used on small datasets such as meta query results. lookup of points already sent which iterator to use for the primary value, can be nil which iterator to use for an aux field Strip out time conditions from the WHERE clause. TODO(jsternberg): This should really be done for us when creating the IteratorOptions struct. newFloatDedupeIterator returns a new instance of floatDedupeIterator. Next returns the next unique point from the input iterator. Serialize to bytes to store in lookup. If the point has already been output then move to the next point. Otherwise mark it as emitted and return point. floatReaderIterator represents an iterator that streams from a reader. newFloatReaderIterator returns a new instance of floatReaderIterator. Stats returns stats about points processed. Close closes the underlying reader, if applicable. OPTIMIZE(benbjohnson): Reuse point on iterator. Unmarshal next point. IntegerIterator represents a stream of integer points. newIntegerIterators converts a slice of Iterator to a slice of IntegerIterator. Drop and closes any iterator in itrs that is not a IntegerIterator and cannot be cast to a IntegerIterator. bufIntegerIterator represents a buffered IntegerIterator. newBufIntegerIterator returns a buffered IntegerIterator. integerMergeIterator represents an iterator that combines multiple integer iterators. newIntegerMergeIterator returns a new instance of integerMergeIterator. integerMergeHeap represents a heap of integerMergeHeapItems. integerSortedMergeIterator is an iterator that sorts and merges multiple iterators into one. newIntegerSortedMergeIterator returns an instance of integerSortedMergeIterator. integerSortedMergeHeap represents a heap of integerSortedMergeHeapItems. integerIteratorScanner scans the results of a IntegerIterator into a map. newIntegerIteratorScanner creates a new IteratorScanner. integerParallelIterator represents an iterator that pulls data in a separate goroutine. newIntegerParallelIterator returns a new instance of integerParallelIterator. integerLimitIterator represents an iterator that limits points per group. newIntegerLimitIterator returns a new instance of integerLimitIterator. integerIntervalIterator represents a integer implementation of IntervalIterator. integerInterruptIterator represents a integer implementation of InterruptIterator. integerCloseInterruptIterator represents a integer implementation of CloseInterruptIterator. integerReduceFloatIterator executes a reducer for every interval and buffers the result. integerReduceFloatPoint stores the reduced data for a name/tag combination. integerStreamFloatIterator streams inputs into the iterator and emits points gradually. newIntegerStreamFloatIterator returns a new instance of integerStreamFloatIterator. integerReduceIntegerIterator executes a reducer for every interval and buffers the result. integerReduceIntegerPoint stores the reduced data for a name/tag combination. integerStreamIntegerIterator streams inputs into the iterator and emits points gradually. newIntegerStreamIntegerIterator returns a new instance of integerStreamIntegerIterator. integerReduceUnsignedIterator executes a reducer for every interval and buffers the result. integerReduceUnsignedPoint stores the reduced data for a name/tag combination. integerStreamUnsignedIterator streams inputs into the iterator and emits points gradually. newIntegerStreamUnsignedIterator returns a new instance of integerStreamUnsignedIterator. integerReduceStringIterator executes a reducer for every interval and buffers the result. integerReduceStringPoint stores the reduced data for a name/tag combination. integerStreamStringIterator streams inputs into the iterator and emits points gradually. newIntegerStreamStringIterator returns a new instance of integerStreamStringIterator. integerReduceBooleanIterator executes a reducer for every interval and buffers the result. integerReduceBooleanPoint stores the reduced data for a name/tag combination. integerStreamBooleanIterator streams inputs into the iterator and emits points gradually. newIntegerStreamBooleanIterator returns a new instance of integerStreamBooleanIterator. integerDedupeIterator only outputs unique points. newIntegerDedupeIterator returns a new instance of integerDedupeIterator. integerReaderIterator represents an iterator that streams from a reader. newIntegerReaderIterator returns a new instance of integerReaderIterator. UnsignedIterator represents a stream of unsigned points. newUnsignedIterators converts a slice of Iterator to a slice of UnsignedIterator. Drop and closes any iterator in itrs that is not a UnsignedIterator and cannot be cast to a UnsignedIterator. bufUnsignedIterator represents a buffered UnsignedIterator. newBufUnsignedIterator returns a buffered UnsignedIterator. unsignedMergeIterator represents an iterator that combines multiple unsigned iterators. newUnsignedMergeIterator returns a new instance of unsignedMergeIterator. unsignedMergeHeap represents a heap of unsignedMergeHeapItems. unsignedSortedMergeIterator is an iterator that sorts and merges multiple iterators into one. newUnsignedSortedMergeIterator returns an instance of unsignedSortedMergeIterator. unsignedSortedMergeHeap represents a heap of unsignedSortedMergeHeapItems. unsignedIteratorScanner scans the results of a UnsignedIterator into a map. newUnsignedIteratorScanner creates a new IteratorScanner. unsignedParallelIterator represents an iterator that pulls data in a separate goroutine. newUnsignedParallelIterator returns a new instance of unsignedParallelIterator. unsignedLimitIterator represents an iterator that limits points per group. newUnsignedLimitIterator returns a new instance of unsignedLimitIterator. unsignedIntervalIterator represents a unsigned implementation of IntervalIterator. unsignedInterruptIterator represents a unsigned implementation of InterruptIterator. unsignedCloseInterruptIterator represents a unsigned implementation of CloseInterruptIterator. unsignedReduceFloatIterator executes a reducer for every interval and buffers the result. unsignedReduceFloatPoint stores the reduced data for a name/tag combination. unsignedStreamFloatIterator streams inputs into the iterator and emits points gradually. newUnsignedStreamFloatIterator returns a new instance of unsignedStreamFloatIterator. unsignedReduceIntegerIterator executes a reducer for every interval and buffers the result. unsignedReduceIntegerPoint stores the reduced data for a name/tag combination. unsignedStreamIntegerIterator streams inputs into the iterator and emits points gradually. newUnsignedStreamIntegerIterator returns a new instance of unsignedStreamIntegerIterator. unsignedReduceUnsignedIterator executes a reducer for every interval and buffers the result. unsignedReduceUnsignedPoint stores the reduced data for a name/tag combination. unsignedStreamUnsignedIterator streams inputs into the iterator and emits points gradually. newUnsignedStreamUnsignedIterator returns a new instance of unsignedStreamUnsignedIterator. unsignedReduceStringIterator executes a reducer for every interval and buffers the result. unsignedReduceStringPoint stores the reduced data for a name/tag combination. unsignedStreamStringIterator streams inputs into the iterator and emits points gradually. newUnsignedStreamStringIterator returns a new instance of unsignedStreamStringIterator. unsignedReduceBooleanIterator executes a reducer for every interval and buffers the result. unsignedReduceBooleanPoint stores the reduced data for a name/tag combination. unsignedStreamBooleanIterator streams inputs into the iterator and emits points gradually. newUnsignedStreamBooleanIterator returns a new instance of unsignedStreamBooleanIterator. unsignedDedupeIterator only outputs unique points. newUnsignedDedupeIterator returns a new instance of unsignedDedupeIterator. unsignedReaderIterator represents an iterator that streams from a reader. newUnsignedReaderIterator returns a new instance of unsignedReaderIterator. StringIterator represents a stream of string points. newStringIterators converts a slice of Iterator to a slice of StringIterator. Drop and closes any iterator in itrs that is not a StringIterator and cannot be cast to a StringIterator. bufStringIterator represents a buffered StringIterator. newBufStringIterator returns a buffered StringIterator. stringMergeIterator represents an iterator that combines multiple string iterators. newStringMergeIterator returns a new instance of stringMergeIterator. stringMergeHeap represents a heap of stringMergeHeapItems. stringSortedMergeIterator is an iterator that sorts and merges multiple iterators into one. newStringSortedMergeIterator returns an instance of stringSortedMergeIterator. stringSortedMergeHeap represents a heap of stringSortedMergeHeapItems. stringIteratorScanner scans the results of a StringIterator into a map. newStringIteratorScanner creates a new IteratorScanner. stringParallelIterator represents an iterator that pulls data in a separate goroutine. newStringParallelIterator returns a new instance of stringParallelIterator. stringLimitIterator represents an iterator that limits points per group. newStringLimitIterator returns a new instance of stringLimitIterator. stringIntervalIterator represents a string implementation of IntervalIterator. stringInterruptIterator represents a string implementation of InterruptIterator. stringCloseInterruptIterator represents a string implementation of CloseInterruptIterator. stringReduceFloatIterator executes a reducer for every interval and buffers the result. stringReduceFloatPoint stores the reduced data for a name/tag combination. stringStreamFloatIterator streams inputs into the iterator and emits points gradually. newStringStreamFloatIterator returns a new instance of stringStreamFloatIterator. stringReduceIntegerIterator executes a reducer for every interval and buffers the result. stringReduceIntegerPoint stores the reduced data for a name/tag combination. stringStreamIntegerIterator streams inputs into the iterator and emits points gradually. newStringStreamIntegerIterator returns a new instance of stringStreamIntegerIterator. stringReduceUnsignedIterator executes a reducer for every interval and buffers the result. stringReduceUnsignedPoint stores the reduced data for a name/tag combination. stringStreamUnsignedIterator streams inputs into the iterator and emits points gradually. newStringStreamUnsignedIterator returns a new instance of stringStreamUnsignedIterator. stringReduceStringIterator executes a reducer for every interval and buffers the result. stringReduceStringPoint stores the reduced data for a name/tag combination. stringStreamStringIterator streams inputs into the iterator and emits points gradually. newStringStreamStringIterator returns a new instance of stringStreamStringIterator. stringReduceBooleanIterator executes a reducer for every interval and buffers the result. stringReduceBooleanPoint stores the reduced data for a name/tag combination. stringStreamBooleanIterator streams inputs into the iterator and emits points gradually. newStringStreamBooleanIterator returns a new instance of stringStreamBooleanIterator. stringDedupeIterator only outputs unique points. newStringDedupeIterator returns a new instance of stringDedupeIterator. stringReaderIterator represents an iterator that streams from a reader. newStringReaderIterator returns a new instance of stringReaderIterator. BooleanIterator represents a stream of boolean points. newBooleanIterators converts a slice of Iterator to a slice of BooleanIterator. Drop and closes any iterator in itrs that is not a BooleanIterator and cannot be cast to a BooleanIterator. bufBooleanIterator represents a buffered BooleanIterator. newBufBooleanIterator returns a buffered BooleanIterator. booleanMergeIterator represents an iterator that combines multiple boolean iterators. newBooleanMergeIterator returns a new instance of booleanMergeIterator. booleanMergeHeap represents a heap of booleanMergeHeapItems. booleanSortedMergeIterator is an iterator that sorts and merges multiple iterators into one. newBooleanSortedMergeIterator returns an instance of booleanSortedMergeIterator. booleanSortedMergeHeap represents a heap of booleanSortedMergeHeapItems. booleanIteratorScanner scans the results of a BooleanIterator into a map. newBooleanIteratorScanner creates a new IteratorScanner. booleanParallelIterator represents an iterator that pulls data in a separate goroutine. newBooleanParallelIterator returns a new instance of booleanParallelIterator. booleanLimitIterator represents an iterator that limits points per group. newBooleanLimitIterator returns a new instance of booleanLimitIterator. booleanIntervalIterator represents a boolean implementation of IntervalIterator. booleanInterruptIterator represents a boolean implementation of InterruptIterator. booleanCloseInterruptIterator represents a boolean implementation of CloseInterruptIterator. booleanReduceFloatIterator executes a reducer for every interval and buffers the result. booleanReduceFloatPoint stores the reduced data for a name/tag combination. booleanStreamFloatIterator streams inputs into the iterator and emits points gradually. newBooleanStreamFloatIterator returns a new instance of booleanStreamFloatIterator. booleanReduceIntegerIterator executes a reducer for every interval and buffers the result. booleanReduceIntegerPoint stores the reduced data for a name/tag combination. booleanStreamIntegerIterator streams inputs into the iterator and emits points gradually. newBooleanStreamIntegerIterator returns a new instance of booleanStreamIntegerIterator. booleanReduceUnsignedIterator executes a reducer for every interval and buffers the result. booleanReduceUnsignedPoint stores the reduced data for a name/tag combination. booleanStreamUnsignedIterator streams inputs into the iterator and emits points gradually. newBooleanStreamUnsignedIterator returns a new instance of booleanStreamUnsignedIterator. booleanReduceStringIterator executes a reducer for every interval and buffers the result. booleanReduceStringPoint stores the reduced data for a name/tag combination. booleanStreamStringIterator streams inputs into the iterator and emits points gradually. newBooleanStreamStringIterator returns a new instance of booleanStreamStringIterator. booleanReduceBooleanIterator executes a reducer for every interval and buffers the result. booleanReduceBooleanPoint stores the reduced data for a name/tag combination. booleanStreamBooleanIterator streams inputs into the iterator and emits points gradually. newBooleanStreamBooleanIterator returns a new instance of booleanStreamBooleanIterator. booleanDedupeIterator only outputs unique points. newBooleanDedupeIterator returns a new instance of booleanDedupeIterator. booleanReaderIterator represents an iterator that streams from a reader. newBooleanReaderIterator returns a new instance of booleanReaderIterator. encodeFloatIterator encodes all points from itr to the underlying writer. Emit initial stats. Continually stream points from the iterator into the encoder. Emit stats periodically. Retrieve the next point from the iterator. Write the point to the point encoder. Emit final stats. encodeIntegerIterator encodes all points from itr to the underlying writer. encodeUnsignedIterator encodes all points from itr to the underlying writer. encodeStringIterator encodes all points from itr to the underlying writer. encodeBooleanIterator encodes all points from itr to the underlying writer./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/iterator.gosliceoutputsparallelismhasDatasubOptstartOffsetendOffsetmmsigngithub.com/influxdata/influxdb/v2/influxql/query/internal"github.com/influxdata/influxdb/v2/influxql/query/internal"unknown call"unknown call"unsupported merge iterator type: %T"unsupported merge iterator type: %T"unsupported sorted merge iterator type: %T"unsupported sorted merge iterator type: %T"unsupported parallel iterator type: %T"unsupported parallel iterator type: %T"unsupported limit iterator type: %T"unsupported limit iterator type: %T"unsupported filter iterator type: %T"unsupported filter iterator type: %T"unsupported tag subset iterator type: %T"unsupported tag subset iterator type: %T"unsupported dedupe iterator type: %T"unsupported dedupe iterator type: %T"unsupported fill iterator type: %T"unsupported fill iterator type: %T"unsupported interval iterator type: %T"unsupported interval iterator type: %T"unsupported interrupt iterator type: %T"unsupported interrupt iterator type: %T"unsupported type for iterator scanner: %T"unsupported type for iterator scanner: %T"unsupported iterator type for draining: %T"unsupported iterator type for draining: %T"LoadLocationinvalid binary measurement regex: value=%q, err=%s"invalid binary measurement regex: value=%q, err=%s"unsupported iterator for encoder: %T"unsupported iterator for encoder: %T"ByteOrder ErrUnknownCall is returned when operating on an unknown function call. secToNs is the number of nanoseconds in a second. Iterator represents a generic interface for all Iterators. Most iterator operations are done on the typed sub-interfaces. Iterators represents a list of iterators. Stats returns the aggregation of all iterator stats. Close closes all iterators. filterNonNil returns a slice of iterators that removes all nil iterators. dataType determines what slice type this set of iterators should be. An iterator type is chosen by looking at the first element in the slice and then returning the data type for that iterator. coerce forces an array of iterators to be a single type. Iterators that are not of the same type as the first element in the slice will be closed and dropped. Merge combines all iterators into a single iterator. A sorted merge iterator or a merge iterator can be used based on opt. Check if this is a call expression. Merge into a single iterator. We do not need an ordered output so use a merge iterator. This is not a call expression so do not use a call iterator. When merging the count() function, use sum() to sum the counted points. NewMergeIterator returns an iterator to merge itrs into one. Inputs must either be merge iterators or only contain a single name/tag in sorted order. The iterator will output all points by window, name/tag, then time. This iterator is useful when you need all of the points for an interval. Aggregate functions can use a more relaxed sorting so that points within a window are grouped. This is much more efficient. NewParallelMergeIterator returns an iterator that breaks input iterators into groups and processes them in parallel. Limit parallelism to the number of inputs. Determine the number of inputs per output iterator. Group iterators together. Merge all groups together. NewSortedMergeIterator returns an iterator to merge itrs into one. Inputs must either be sorted merge iterators or only contain a single name/tag in sorted order. The iterator will output all points by name/tag, then time. This iterator is useful when you need all points for a name/tag to be in order. newParallelIterator returns an iterator that runs in a separate goroutine. NewLimitIterator returns an iterator that limits the number of points per grouping. NewFilterIterator returns an iterator that filters the points based on the condition. This iterator is not nearly as efficient as filtering points within the query engine and is only used when filtering subqueries. NewTagSubsetIterator will strip each of the points to a subset of the tag key values for each point it processes. NewDedupeIterator returns an iterator that only outputs unique points. This iterator maintains a serialized copy of each row so it is inefficient to use on large datasets. It is intended for small datasets such as meta queries. NewFillIterator returns an iterator that fills in missing points in an aggregate. NewIntervalIterator returns an iterator that sets the time on each point to the interval. NewInterruptIterator returns an iterator that will stop producing output when the passed-in channel is closed. IteratorScanner is used to scan the results of an iterator into a map. Peek retrieves information about the next point. It returns a timestamp, the name, and the tags. ScanAt will take a time, name, and tags and scan the point that matches those into the map. Stats returns the IteratorStats from the Iterator. Err returns an error that was encountered while scanning. SkipDefault is a sentinel value to tell the IteratorScanner to skip setting the default value if none was present. This causes the map to use the previous value if it was previously set. NewIteratorScanner produces an IteratorScanner for the Iterator. DrainIterator reads and discards all points from itr. DrainIterators reads and discards all points from itrs. Exit once all iterators return a nil point. NewReaderIterator returns an iterator that streams from a reader. IteratorCreator is an interface to create Iterators. Creates a simple iterator for use in an InfluxQL query. Determines the potential cost for creating an iterator. IteratorOptions is an object passed to CreateIterator to specify creation options. Expression to iterate for. This can be VarRef or a Call. Auxiliary tags or values to also retrieve for the point. Data sources from which to receive data. This is only used for encoding measurements over RPC and is no longer used in the open source version. Group by interval and tags. The final dimensions of the query (stays the same even in subqueries). Dimensions to group points by in intermediate iterators. Fill options. Condition to filter by. Time range for the iterator. Sorted in time ascending order if true. Limits the number of points per series. Limits the number of series. Removes the measurement name. Useful for meta queries. Removes duplicate rows from raw queries. Determines if this is a query for raw data or an aggregate/selector. Limits on the creation of iterators. If this channel is set and is closed, the iterator should try to exit and close as soon as possible. Authorizer can limit access to data newIteratorOptionsStmt creates the iterator options from stmt. Determine time range from the condition. Determine group by interval. Set duration to zero if a negative interval has been used. Always request an ordered output for the top level iterators. The emitter will always emit points as ordered. Determine dimensions. Set the fill option to none if a target has been given. Null values will get ignored when being written to the target so fill(null) wouldn't write any null values to begin with. Propagate the dimensions to the inner subquery. Extract the time range and condition from the condition. If the time range is more constrained, use it instead. A less constrained time range should be ignored. Propagate the SLIMIT and SOFFSET from the outer query. Propagate the ordering from the parent query. If the inner query uses a null fill option and is not a raw query, switch it to none so we don't hit an unnecessary penalty from the fill iterator. Null values will end up getting stripped by an outer query anyway so there's no point in having them here. We still need all other types of fill iterators because they can affect the result of the outer query. We also do not do this for raw queries because there is no fill iterator for them and fill(none) doesn't work with raw queries. Inherit the ordering method from the outer query. If there is no interval for this subquery, but the outer query has an interval, inherit the parent interval. MergeSorted returns true if the options require a sorted merge. SeekTime returns the time the iterator should start from. For ascending iterators this is the start time, for descending iterators it's the end time. StopTime returns the time the iterator should end at. For ascending iterators this is the end time, for descending iterators it's the start time. Window returns the time window [start,end) that t falls within. Subtract the offset to the time so we calculate the correct base interval. Retrieve the zone offset for the start time. Truncate time by duration. Negative modulo rounds up instead of down, so offset with the duration. Find the start time. Look for the start offset again because the first time may have been after the offset switch. Now that we are at midnight in UTC, we can lookup the zone offset again to get the real starting offset. Do not adjust the offset if the offset change is greater than or equal to the duration. Find the end time. Retrieve the zone offset for the end time. Adjust the end time if the offset is different from the start offset. Only apply the offset if it is smaller than the duration. This prevents going back in time and creating time windows that don't make any sense. If the offset is greater than 0, that means we are adding time. Added time goes into the previous interval because the clocks move backwards. If the offset is less than 0, then we are skipping time. Skipped time comes after the switch so if we have a time interval that lands on the switch, it comes from the next interval and not the current one. For this reason, we need to know when the actual switch happens by seeing if the time switch is within the current interval. We calculate the zone offset with the offset and see if the value is the same. If it is, we apply the offset. DerivativeInterval returns the time interval for the derivative function. Use the interval on the derivative() call, if specified. Otherwise use the group by interval, if specified. ElapsedInterval returns the time interval for the elapsed function. Use the interval on the elapsed() call, if specified. IntegralInterval returns the time interval for the integral function. Use the interval on the integral() call, if specified. GetDimensions retrieves the dimensions for this query. Zone returns the zone information for the given time. The offset is in nanoseconds. MarshalBinary encodes opt into a binary format. UnmarshalBinary decodes from a binary format in to opt. Set expression, if set. Set the location, if set. Convert and encode aux fields as variable references. Encode group by dimensions from a map. Convert and encode sources to measurements. Fill value can only be a number. Set it if available. Set condition, if set. Convert and decode variable references. Convert and decode sources to measurements. Convert group by dimensions to a map. Set the fill value, if set. Interval represents a repeating interval for a query. IsZero returns true if the interval has no duration. IteratorStats represents statistics about an iterator. Some statistics are available immediately upon iterator creation while some are derived as the iterator processes data. series represented points returned Add aggregates fields from s and other together. Overwrites s. IteratorCost contains statistics retrieved for explaining what potential cost may be incurred by instantiating an iterator. The total number of shards that are touched by this query. The total number of non-unique series that are accessed by this query. This number matches the number of cursors created by the query since one cursor is created for every series. CachedValues returns the number of cached values that may be read by this query. The total number of non-unique files that may be accessed by this query. This will count the number of files accessed by each series so files will likely be double counted. The number of blocks that had the potential to be accessed. The amount of data that can be potentially read. Combine combines the results of two IteratorCost structures into one. floatFastDedupeIterator outputs unique points where the point has a single aux field. newFloatFastDedupeIterator returns a new instance of floatFastDedupeIterator. Skip if there are not any aux fields. IteratorEncoder is an encoder for encoding an iterator's points to w. Frequency with which stats are emitted. NewIteratorEncoder encodes an iterator's points to w. EncodeIterator encodes and writes all of itr's points to the underlying writer. encode a stats object in the point stream./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/iterator_mapper.gounable to create iterator mapper with driver expression type: %T"unable to create iterator mapper with driver expression type: %T" If the value is a null float, then convert it back to NaN so it is treated as a float for eval. The driver doesn't appear to to have a valid driver type. We should close the cursor and return a blank iterator. We close the cursor because we own it and have a responsibility to close it once it is passed into this function./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/linear.gonextValuepreviousTimepreviousValuewindowTime linearFloat computes the the slope of the line between the points (previousTime, previousValue) and (nextTime, nextValue) and returns the value of the point on the line with time windowTime y = mx + b the slope of the line how far into the interval we are linearInteger computes the the slope of the line between the points (previousTime, previousValue) and (nextTime, nextValue)/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/math.go"abs"sin"sin"cos"cos"tan"tan"asin"asin"acos"acos"atan"atan""ln"log2"log2"log10"log10""sqrt"floor"floor"ceil"ceil""round"invalid argument type for the first argument in %s(): %s"invalid argument type for the first argument in %s(): %s"invalid argument type for the second argument in %s(): %s"invalid argument type for the second argument in %s(): %s"CallValuerSinCosTanCeilAsinAcosAtanLog2Log10Atan2TruncCopysign Pass through to verify the second argument./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/mocks/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/mocks/ShardGroup.goMockShardGroupMockShardGroupMockRecorderMockShardMapperMockShardMapperMockRecorderMockStatementExecutorMockStatementExecutorMockRecorderNewMockShardGroupNewMockShardMapperNewMockStatementExecutor"Close""CreateIterator""FieldDimensions""IteratorCost""MapType" Source: github.com/influxdata/idpe/influxql/query (interfaces: ShardGroup) MockShardGroup is a mock of ShardGroup interface MockShardGroupMockRecorder is the mock recorder for MockShardGroup NewMockShardGroup creates a new mock instance Close mocks base method Close indicates an expected call of Close CreateIterator mocks base method CreateIterator indicates an expected call of CreateIterator FieldDimensions mocks base method FieldDimensions indicates an expected call of FieldDimensions IteratorCost mocks base method IteratorCost indicates an expected call of IteratorCost MapType mocks base method MapType indicates an expected call of MapType/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/mocks/ShardMapper.go"MapShards" Source: github.com/influxdata/idpe/influxql/query (interfaces: ShardMapper) MockShardMapper is a mock of ShardMapper interface MockShardMapperMockRecorder is the mock recorder for MockShardMapper NewMockShardMapper creates a new mock instance MapShards mocks base method MapShards indicates an expected call of MapShards/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/mocks/StatementExecutor.go"ExecuteStatement" Source: github.com/influxdata/idpe/influxql/query (interfaces: StatementExecutor) MockStatementExecutor is a mock of StatementExecutor interface MockStatementExecutorMockRecorder is the mock recorder for MockStatementExecutor NewMockStatementExecutor creates a new mock instance ExecuteStatement mocks base method ExecuteStatement indicates an expected call of ExecuteStatement/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/neldermead/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/neldermead/neldermead.godefaultAlphadefaultBetadefaultGammadefaultMaxIterationscentfavgfsumvgvhobjfuncpnqnvcvevmvr Package neldermead is an implementation of the Nelder-Mead optimization method. Based on work by Michael F. Hutt: http://www.mikehutt.com/neldermead.html reflection coefficient contraction coefficient expansion coefficient Optimizer represents the parameters to the Nelder-Mead simplex method. Maximum number of iterations. Reflection coefficient. Contraction coefficient. Expansion coefficient. New returns a new instance of Optimizer with all values set to the defaults. Optimize applies the Nelder-Mead simplex method with the Optimizer's settings.holds vertices of simplexvalue of function at each vertexreflection - coordinatesexpansion - coordinatescontraction - coordinatescentroid - coordinates create the initial simplex assume one of the vertices is 0,0 find the initial function values begin the main loop of the minimization find the indexes of the largest and smallest values find the index of the second largest value calculate the centroid reflect vg to new vertex vr value of function at reflection point investigate a step further in this direction value of function at expansion point by making fe < fr as opposed to fe < f[vs], Rosenbrocks function takes 63 iterations as opposed to 64 when using double variables. check to see if a contraction is necessary perform outside contraction perform inside contraction value of function at contraction point at this point the contraction is not successful, we must halve the distance from vs to all the vertices of the simplex and then continue. test for convergence find the index of the smallest value/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/point.gen.go Source: point.gen.go.tmpllint:file-ignore U1000 Ignore all unused code, it's generated FloatPoint represents a point with a float64 value. DO NOT ADD ADDITIONAL FIELDS TO THIS STRUCT. See TestPoint_Fields in influxql/point_test.go for more details. Total number of points that were combined into this point from an aggregate. If this is zero, the point is not the result of an aggregate function. Clone returns a copy of v. CopyTo makes a deep copy into the point. floatPoints represents a slice of points sortable by value. floatPointsByValue represents a slice of points sortable by value. floatPointsByTime represents a slice of points sortable by value. floatPointByFunc represents a slice of points sortable by a function. FloatPointEncoder encodes FloatPoint points to a writer. NewFloatPointEncoder returns a new instance of FloatPointEncoder that writes to w. EncodeFloatPoint marshals and writes p to the underlying writer. Marshal to bytes. Write the length. Write the encoded point. FloatPointDecoder decodes FloatPoint points from a reader. NewFloatPointDecoder returns a new instance of FloatPointDecoder that reads from r. Stats returns iterator stats embedded within the stream. DecodeFloatPoint reads from the underlying reader and unmarshals into p. Read length. Read point data. Unmarshal into point. If the point contains stats then read stats and retry. Decode into point object. IntegerPoint represents a point with a int64 value. integerPoints represents a slice of points sortable by value. integerPointsByValue represents a slice of points sortable by value. integerPointsByTime represents a slice of points sortable by value. integerPointByFunc represents a slice of points sortable by a function. IntegerPointEncoder encodes IntegerPoint points to a writer. NewIntegerPointEncoder returns a new instance of IntegerPointEncoder that writes to w. EncodeIntegerPoint marshals and writes p to the underlying writer. IntegerPointDecoder decodes IntegerPoint points from a reader. NewIntegerPointDecoder returns a new instance of IntegerPointDecoder that reads from r. DecodeIntegerPoint reads from the underlying reader and unmarshals into p. UnsignedPoint represents a point with a uint64 value. unsignedPoints represents a slice of points sortable by value. unsignedPointsByValue represents a slice of points sortable by value. unsignedPointsByTime represents a slice of points sortable by value. unsignedPointByFunc represents a slice of points sortable by a function. UnsignedPointEncoder encodes UnsignedPoint points to a writer. NewUnsignedPointEncoder returns a new instance of UnsignedPointEncoder that writes to w. EncodeUnsignedPoint marshals and writes p to the underlying writer. UnsignedPointDecoder decodes UnsignedPoint points from a reader. NewUnsignedPointDecoder returns a new instance of UnsignedPointDecoder that reads from r. DecodeUnsignedPoint reads from the underlying reader and unmarshals into p. StringPoint represents a point with a string value. stringPoints represents a slice of points sortable by value. stringPointsByValue represents a slice of points sortable by value. stringPointsByTime represents a slice of points sortable by value. stringPointByFunc represents a slice of points sortable by a function. StringPointEncoder encodes StringPoint points to a writer. NewStringPointEncoder returns a new instance of StringPointEncoder that writes to w. EncodeStringPoint marshals and writes p to the underlying writer. StringPointDecoder decodes StringPoint points from a reader. NewStringPointDecoder returns a new instance of StringPointDecoder that reads from r. DecodeStringPoint reads from the underlying reader and unmarshals into p. BooleanPoint represents a point with a bool value. booleanPoints represents a slice of points sortable by value. booleanPointsByValue represents a slice of points sortable by value. booleanPointsByTime represents a slice of points sortable by value. booleanPointByFunc represents a slice of points sortable by a function. BooleanPointEncoder encodes BooleanPoint points to a writer. NewBooleanPointEncoder returns a new instance of BooleanPointEncoder that writes to w. EncodeBooleanPoint marshals and writes p to the underlying writer. BooleanPointDecoder decodes BooleanPoint points from a reader. NewBooleanPointDecoder returns a new instance of BooleanPointDecoder that reads from r. DecodeBooleanPoint reads from the underlying reader and unmarshals into p./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/point.godestunable to clone point: %T"unable to clone point: %T"'\x00' ZeroTime is the Unix nanosecondÂ timestamp for no time. This time is not used by the query engine or the storage engine as a valid time. Point represents a value in a series that occurred at a given time. Name and tags uniquely identify the series the value belongs to. The time that the value occurred at. The value at the given time. Auxiliary values passed along with the value. Points represents a list of points. Clone returns a deep copy of a. Tags represent a map of keys and values. It memorizes its key so it can be used efficiently during query execution. NewTags returns a new instance of Tags. newTagsID returns a new instance of Tags by parsing the given tag ID. Equal compares if the Tags are equal to each other. ID returns the string identifier for the tags. KeyValues returns the underlying map for the tags. Keys returns a sorted list of all keys on the tag. Values returns a sorted list of all values on the tag. Value returns the value for a given key. Subset returns a new tags object with a subset of the keys. If keys match existing keys, simply return this tagset. Otherwise create new tag set. Equals returns true if t equals other. keysMatch returns true if m has exactly the same keys as listed in keys. encodeTags converts a map of strings to an identifier. Empty maps marshal to empty bytes. Extract keys and determine final size. separators Generate marshaled bytes. decodeTags parses an identifier into a map of tags. There must be an even number of segments. Return nil if there are no segments. Decode key/value tags. PointDecoder decodes generic points from a reader. NewPointDecoder returns a new instance of PointDecoder that reads from r. DecodePoint reads from the underlying reader and unmarshals into p./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/proxy_executor.golastSeriesrowsMergedQuery Service"Query Service"executing new query"executing new query"failed to parse query"failed to parse query" Ignore nil results. if requested, convert result timestamps to epoch GatherResults consumes the results from the given channel and organizes them correctly. Results for various statements need to be combined together. It's not chunked so buffer results in memory. Results for statements need to be combined together. We need to check if this new result is for the same statement as the last result, or for the next statement. Next row is for a different series than last. Values are for the same series, so append them. Append remaining rows as new rows. convertToEpoch converts result timestamps from time.Time to the specified epoch./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/query.go import "github.com/influxdata/influxdb/v2/influxql/query"go:generate tmpl -data=@tmpldata iterator.gen.go.tmplgo:generate tmpl -data=@tmpldata point.gen.go.tmplgo:generate tmpl -data=@tmpldata functions.gen.go.tmplgo:generate protoc --gogo_out=. internal/internal.proto/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/response.gojson:"results,omitempty"`json:"results,omitempty"` Response represents a list of statement results. MarshalJSON encodes a Response struct into JSON. Define a struct that outputs "error" as a string. Copy fields to output struct. UnmarshalJSON decodes the data into the Response struct. Error returns the first error from any statement. Returns nil if no errors occurred on any statements./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/response_writer.gohashKeysuppressHeaderspreviousHeaderscolumnNamecolumnNamesseparatormsgpgithub.com/tinylib/msgp/msgp"github.com/tinylib/msgp/msgp""false"application/x-msgpack"application/x-msgpack""results"statement_id"statement_id"messages"messages""level""series""columns""values"partial"partial"%s: %s.
"%s: %s.\n"name: %s"name: %s"tags: %s"tags: %s"uintptrlint:file-ignore SA1019 Ignore for now ResponseWriter is an interface for writing a response. WriteResponse writes a response. NewResponseWriter creates a new ResponseWriter based on the Accept header in the request that wraps the ResponseWriter. TODO(sgc): Add EncodingFormatJSONPretty If there are no series in the result, skip past this result. Set the statement id and print out a newline if this is not the first statement. Flush the csv writer and write a newline. Print out the column headers from the first series. The columns have changed. Print a newline and reprint the header. Create a tabbed writer for each result as they won't always line up Print out all messages first Check to see if the headers are the same as the previous row.  If so, suppress them in the output If we are suppressing headers, don't output the extra line return. If we aren't suppressing headers, then we put out line returns between results (not before the first result, and not after the last result). gather tags Output a line separator if we have more than one set or results and format is column If we are column format, we break out the name/tag to separate lines if format is column, write dashes under each column Default for floats via `fmt.Sprintf("%v", t)` is to represent them in scientific notation. We want to represent them as they are, with the least digits as possible (prec: -1)./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/result.goslimitdeprecated use of '%s' in a read only context, please use a POST request instead"deprecated use of '%s' in a read only context, please use a POST request instead"json:"statement_id"`json:"statement_id"`json:"series,omitempty"`json:"series,omitempty"`json:"messages,omitempty"`json:"messages,omitempty"`json:"partial,omitempty"`json:"partial,omitempty"` WarningLevel is the message level for a warning. TagSet is a fundamental concept within the query system. It represents a composite series, composed of multiple individual series that share a set of tag attributes. AddFilter adds a series-level filter to the Tagset. Reverse reverses the order of series keys and filters in the TagSet. LimitTagSets returns a tag set list with SLIMIT and SOFFSET applied. Ignore if no limit or offset is specified. If offset is beyond the number of tag sets then return nil. Clamp limit to the max number of tag sets. Message represents a user-facing message to be included with the result. ReadOnlyWarning generates a warning message that tells the user the command they are using is being used for writing in a read only context. This is a temporary method while to be used while transitioning to read only operations for issue #6290. Result represents a resultset returned from a single statement. Rows represents a list of rows that can be sorted consistently by name/tag. StatementID is just the statement's position in the query. It's used to combine statement results if they're being buffered in memory. MarshalJSON encodes the result into JSON. UnmarshalJSON decodes the data into the Result struct/Users/austinjaybecker/projects/abeck-go-testing/influxql/query/select.gocallOptnfauxKeyssymbolNamesymbolMultiTypeMapperinvalid expression type: %T"invalid expression type: %T"invalid series aggregate function: %s"invalid series aggregate function: %s"top() requires 2 or more arguments, got %d"top() requires 2 or more arguments, got %d"bottom() requires 2 or more arguments, got %d"bottom() requires 2 or more arguments, got %d"unsupported call: %s"unsupported call: %s"VarRefsIsSelectorRewriteExprval%d"val%d"TypeValuerEvalevalVarRefExprTypeevalCallExprTypeevalBinaryExprType SelectOptions are options that customize the select call. Node to exclusively read from. If zero, all nodes are used. Maximum number of concurrent series. Maximum number of points to read from the query. This requires the passed in context to have a Monitor that is created using WithMonitor. Maximum number of buckets for a statement. StatisticsGatherer gathers metrics about the execution of the query. ShardMapper retrieves and maps shards into an IteratorCreator that can later be used for executing queries. TypeMapper maps a data type to the measurement and field. FieldMapper returns the data type for the field inside of the measurement. contextFieldMapper adapts a FieldMapper to an influxql.FieldMapper as FieldMapper requires a context.Context and orgID ShardGroup represents a shard or a collection of shards that can be accessed for creating iterators. When creating iterators, the resource used for reading the iterators should be separate from the resource used to map the shards. When the ShardGroup is closed, it should not close any resources associated with the created Iterator. Those resources belong to the Iterator and will be closed when the Iterator itself is closed. The query engine operates under this assumption and will close the shard group after creating the iterators, but before the iterators are actually read. Select is a prepared statement that is ready to be executed. Select creates the Iterators that will be used to read the query. Explain outputs the explain plan for this statement. Close closes the resources associated with this prepared statement. This must be called as the mapped shards may hold open resources such as network connections. Prepare will compile the statement with the default compile options and then prepare the query. Select compiles, prepares, and then initiates execution of the query using the default compile options. Must be deferred so it runs after Select. TODO(jsternberg): Remove this hacky method of propagating now. Each level of the query should use a time range discovered during compilation, but that requires too large of a refactor at the moment. buildExprIterator creates an iterator for an expression. Variable references in this section will always go into some call iterator. Combine it with a merge iterator. TODO(jsternberg): Refactor this. This section needs to die in a fire. Eliminate limits and offsets if they were previously set. These are handled by the caller. Redefine interval to be unbounded to capture all aggregate results Create a max iterator using the groupings in the arguments. There are no arguments so do not organize the points by tags. OPTIMIZE(benbjohnson): convert to map/reduce Identify the name of the field we are using. Wrap the result in a call iterator. Add a field with the variable "time" if we have not omitted time. Iterate through each of the fields to add them to the value mapper. If the field is a top() or bottom() call, we need to also add the extra variables if we are not writing into a target. Set the aliases on each of the columns to what the final name should be. Retrieve the refs to retrieve the auxiliary fields. If there are no calls, then produce an auxiliary cursor. If all of the auxiliary keys are of an unknown type, do not construct the iterator and return a null cursor. Create a slice with an empty first element. Check to see if this is a selector statement. It is a selector if it is the only selector call and the call itself is a selector. Produce an iterator for every single call and create an iterator scanner associated with it. The primary driver of this call is of unknown type, so skip this. Close all scanners if any iterator fails. Merge iterators to read auxiliary fields. Filter out duplicate rows, if required. If there is no group by and it is a float iterator, see if we can use a fast dedupe. Apply limit & offset. An index that maps a node's string output to its symbol so that all nodes with the same signature are mapped the same. An index that maps a specific expression to a symbol. This ensures that only expressions that were mapped get symbolized. A collection of all of the calls in the table. This symbol has not been assigned yet. If this is a call or expression, mark the node as stored in the symbol table. Determine the symbol name and the symbol type. Assign this symbol to the symbol table if it is not presently there and increment the value index number. Store the symbol for this expression so we can later rewrite the query correctly. hasValidType returns true if there is at least one non-unknown type in the slice./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/statement_rewriter.gotagExprnewMdefaultDatabasenewSourcessystemIteratorscond.+`.+`"fieldType"HasTimeExprSHOW FIELD KEY CARDINALITY doesn't support time in WHERE clause"SHOW FIELD KEY CARDINALITY doesn't support time in WHERE clause"_fieldKey"_fieldKey"SHOW MEASUREMENTS doesn't support time in WHERE clause"SHOW MEASUREMENTS doesn't support time in WHERE clause"SHOW MEASUREMENT EXACT CARDINALITY doesn't support time in WHERE clause"SHOW MEASUREMENT EXACT CARDINALITY doesn't support time in WHERE clause"_name"_name"_seriesKey"_seriesKey"SHOW SERIES EXACT CARDINALITY doesn't support time in WHERE clause"SHOW SERIES EXACT CARDINALITY doesn't support time in WHERE clause"_tagKey"_tagKey"_tagValue"_tagValue"SHOW TAG KEY EXACT CARDINALITY doesn't support time in WHERE clause"SHOW TAG KEY EXACT CARDINALITY doesn't support time in WHERE clause"EQREGEX RewriteStatement rewrites stmt into a new statement, if applicable. Check for time in WHERE clause (not supported). Use all field keys, if zero. Currently time based SHOW MEASUREMENT queries can't be supported because it's not possible to appropriate set operations such as a negated regex using the query engine. rewrite condition to push a source measurement into a "_name" tag. TODO(edd): currently we only support cardinality estimation for certain types of query. As the estimation coverage is expanded, this condition will become less strict. Use all measurements, if zero. Check if we can exclusively use the index. The query is bounded by time then it will have to query TSM data rather than utilising the index via system iterators. Set condition or "AND" together. rewriteSources rewrites sources to include the provided system iterator. rewriteSources also sets the default database where necessary. rewriteSourcesCondition rewrites sources into `name` expressions. Merges with cond and returns a new condition. Generate an OR'd set of filters on source name. Generate a filtering expression on the measurement name. This is the case where the original query has a WHERE on a tag, and also is requesting from a specific source. This is the case where the original query has a WHERE on a tag but is not requesting from a specific source./Users/austinjaybecker/projects/abeck-go-testing/influxql/query/subquery.go buildAuxIterator constructs an auxiliary Iterator from a subquery. Map the desired auxiliary fields from the substatement. Filter the cursor by a condition if one was given. Construct the iterators for the subquery. If this field doesn't map to anything, use the NullMap so it shows up as null. Cast the result of the field into the desired type. We may match one of the arguments in "top" or "bottom". Increment the offset so we have the correct index for later fields. Unable to find this in the list of fields. Look within the dimensions and create a field if we find it. Unable to find any matches. Look for the field or tag that is driving this query. Exit immediately if there is no driver. If there is no driver, there are no results. Period. Map the auxiliary fields to their index in the subquery./Users/austinjaybecker/projects/abeck-go-testing/influxql/query_request.goapplication/csv"application/csv"text/plain"text/plain"json:"authorization,omitempty"`json:"authorization,omitempty"`json:"epoch"`json:"epoch"`json:"encoding_format"`json:"encoding_format"`json:"content_type"`json:"content_type"`json:"chunked"`json:"chunked"`json:"chunk_size"`json:"chunk_size"`json:"params,omitempty"`json:"params,omitempty"`organization_id is not valid"organization_id is not valid" Returns closed encoding format from the specified mime type. The default is JSON if no exact match is found. Content type is the desired response format. Chunked indicates responses should be chunked using ChunkSize ChunkSize is the number of points to be encoded per batch. 0 indicates no chunking. Query contains the InfluxQL. Source represents the ultimate source of the request. The HTTP query requests represented the body expected by the QueryHandler/Users/austinjaybecker/projects/abeck-go-testing/influxql/service.go"queue"unexpected %s type: %s"unexpected %s type: %s"proxy-mode"proxy-mode"request-mode"request-mode" ProxyQueryService performs InfluxQL queries and encodes the result into a writer. The results are opaque to a ProxyQueryService. ProxyMode enumerates the possible ProxyQueryService operating modes used by a downstream client. ProxyModeHTTP specifies a ProxyQueryService that forwards InfluxQL requests via HTTP to influxqld. ProxyModeQueue specifies a ProxyQueryService that pushes InfluxQL requests to a queue and influxqld issues a callback request to the initiating service. RequestMode is enumerates the possible influxqld operating modes for receiving InfluxQL requests. RequestModeHTTP specifies the HTTP listener should be active. RequestModeQueue specifies the queue dispatcher should be active. RequestModeAll specifies both the HTTP listener and queue dispatcher should be active./Users/austinjaybecker/projects/abeck-go-testing/influxql/statistics.gocolljson:"plan_duration"`json:"plan_duration"`json:"execute_duration"`json:"execute_duration"`json:"statement_count"`json:"statement_count"`json:"scanned_values"`json:"scanned_values"`json:"scanned_bytes"`json:"scanned_bytes"`stats_plan_duration_seconds"stats_plan_duration_seconds"stats_execute_duration_seconds"stats_execute_duration_seconds"stats_statement_count"stats_statement_count"stats_scanned_values"stats_scanned_values"stats_scanned_bytes"stats_scanned_bytes" Statistics is a collection of statistics about the processing of a query. PlanDuration is the duration spent planning the query. ExecuteDuration is the duration spent executing the query. StatementCount is the number of InfluxQL statements executed ScannedValues is the number of values scanned from storage ScannedBytes is the number of bytes scanned from storage Adding returns the sum of s and other. Add adds other to s. TotalDuration returns the sum of all durations for s./Users/austinjaybecker/projects/abeck-go-testing/inmem/Users/austinjaybecker/projects/abeck-go-testing/inmem/dbrp_mapping_service.goDefaultSourceIDDefaultSourceOrganizationIDcursorBatchSizeencodeDBRPMappingKeyerrDBRPMappingNotFoundauthorizationKVorganizationKVbucketKVuserKVdashboardKVviewKVvariableKVdbrpMappingKVuserResourceMappingKVlabelKVlabelMappingKVscraperTargetKVtelegrafConfigKVonboardingKVbasicAuthKVsessionKVsourceKVloadDBRPMappingforEachDBRPMappingfilterDBRPMappingsPutDBRPMappinginitializeSourcesdbrp mapping not found"dbrp mapping not found"type %T is not a dbrp mapping"type %T is not a dbrp mapping"no filter parameters provided"no filter parameters provided"dbrp mapping already exists"dbrp mapping already exists" FindBy returns a single dbrp mapping by cluster, db and rp. Find returns the first dbrp mapping that matches filter. filter by dbrpMapping id FindMany returns a list of dbrpMappings that match filter and the total count of matching dbrp mappings. Create creates a new dbrp mapping. PutDBRPMapping sets dbrpMapping with the current ID. Delete removes a dbrp mappingpairs/Users/austinjaybecker/projects/abeck-go-testing/inmem/kv.goskipFirst"github.com/google/btree"error item is type %T not *item"error item is type %T not *item"NewStaticCursor ensure *KVStore implement kv.SchemaStore interface cursorBatchSize is the size of a batch sent by a forward cursors tree iterator KVStore is an in memory btree backed kv.Store. NewKVStore creates an instance of a KVStore. View opens up a transaction with a read lock. Update opens up a transaction with a write lock. CreateBucket creates a bucket with the provided name if one does not exist. DeleteBucket creates a bucket with the provided name if one Flush removes all data from the buckets.  Used for testing. Buckets returns the names of all buckets within inmem.KVStore. Tx is an in memory transaction. TODO: make transactions actually transactional Bucket retrieves the bucket at the provided key. Bucket is a btree that implements kv.Bucket. Put wraps the put method of a kv bucket and ensures that the bucket is writable. Delete wraps the delete method of a kv bucket and ensures that the Less is used to implement btree.Item. Get retrieves a batch of values for the provided keys. leave value as nil slice Put sets the key value pair provided. Delete removes the key provided. Cursor creates a static cursor from all entries in the database. TODO we should do this by using the Ascend/Descend methods that  the btree provides. ForwardCursor returns a directional cursor which starts at the provided seeked key if signalled to stop then exit iteration if skip first enforce limit batch flushed successfully so we can begin a new batch we've been signalled to stop send if any left in batch ForwardCursor is a kv.ForwardCursor which iterates over an in-memory btree error found during iteration Err returns a non-nil error when an error occurred during cursor iteration. Close releases the producing goroutines for the forward cursor. It blocks until the producing goroutine exits. Next returns the next key/value pair in the cursor/Users/austinjaybecker/projects/abeck-go-testing/inmem/service.goinmem/"inmem/" OpPrefix is the op prefix. Service implements various top level services. NewService creates an instance of a Service. Flush removes all data from the in-memory store/Users/austinjaybecker/projects/abeck-go-testing/inmem/session_store.goexpireAtexistingTimersession has expired"session has expired" key is already expired. no problem/Users/austinjaybecker/projects/abeck-go-testing/inmem/source.go50616e67652c206c"50616e67652c206c"failed to decode default source id: %v"failed to decode default source id: %v"failed to decode default source organization id: %v"failed to decode default source organization id: %v"no default source found"no default source found"type %T is not a source"type %T is not a source" DefaultSource is the default source. DefaultSourceID it the default source identifier DefaultSourceOrganizationID is the default source's organization identifier DefaultSource retrieves the default source. TODO(desa): make this faster by putting the default source in an index. FindSourceByID retrieves a source by id. FindSources retrieves all sources that match an arbitrary source filter. Filters using ID, or OrganizationID and source Name should be efficient. Other filters will do a linear scan across all sources searching for a match. PutSource will put a source without setting an ID./Users/austinjaybecker/projects/abeck-go-testing/internal/Users/austinjaybecker/projects/abeck-go-testing/internal/array_cursors.goArrayCursorMockAuthorizerMockBooleanArrayCursorMockFloatArrayCursorMockIntegerArrayCursorMockMetaClientMockNewArrayCursorMockNewBooleanArrayCursorMockNewFloatArrayCursorMockNewIntegerArrayCursorMockNewStringArrayCursorMockNewUnsignedArrayCursorMockStringArrayCursorMockTSDBStoreMockUnsignedArrayCursorMockCloseFnErrFnStatsFnNextFnIntegerArrayCursorFloatArrayCursorUnsignedArrayCursorStringArrayCursorBooleanArrayCursorNewIntegerArrayLenNewFloatArrayLenNewUnsignedArrayLenNewStringArrayLenNewBooleanArrayLen ArrayCursorMock provides a mock base implementation for batch cursors. NewArrayCursorMock returns an initialised ArrayCursorMock, which returns the zero value for all methods. Close closes the cursor. Err returns the latest error, if any. IntegerArrayCursorMock provides a mock implementation of an IntegerArrayCursorMock. NewIntegerArrayCursorMock returns an initialised IntegerArrayCursorMock, which Next returns the next set of keys and values. FloatArrayCursorMock provides a mock implementation of a FloatArrayCursor. NewFloatArrayCursorMock returns an initialised FloatArrayCursorMock, which UnsignedArrayCursorMock provides a mock implementation of an UnsignedArrayCursorMock. NewUnsignedArrayCursorMock returns an initialised UnsignedArrayCursorMock, which StringArrayCursorMock provides a mock implementation of a StringArrayCursor. NewStringArrayCursorMock returns an initialised StringArrayCursorMock, which BooleanArrayCursorMock provides a mock implementation of a BooleanArrayCursor. NewBooleanArrayCursorMock returns an initialised BooleanArrayCursorMock, whichCreateContinuousQueryFnCreateDatabaseFnCreateDatabaseWithRetentionPolicyFnCreateRetentionPolicyFnCreateShardGroupFnCreateSubscriptionFnCreateUserFnDatabaseFnDatabasesFnDataFnDeleteShardGroupFnDropContinuousQueryFnDropDatabaseFnDropRetentionPolicyFnDropSubscriptionFnDropShardFnDropUserFnOpenFnPrecreateShardGroupsFnPruneShardGroupsFnRetentionPolicyFnAuthenticateFnAdminUserExistsFnSetAdminPrivilegeFnSetDataFnSetPrivilegeFnShardGroupsByTimeRangeFnShardOwnerFnTruncateShardGroupsFnUpdateRetentionPolicyFnUpdateUserFnUserPrivilegeFnUserPrivilegesFnUserFnUsersFnBackupShardFnBackupSeriesFileFnExportShardFnCreateShardFnCreateShardSnapshotFnDeleteDatabaseFnDeleteMeasurementFnDeleteRetentionPolicyFnDeleteSeriesFnDeleteShardFnDiskSizeFnExpandSourcesFnImportShardFnMeasurementSeriesCountsFnMeasurementsCardinalityFnMeasurementNamesFnPathFnRestoreShardFnSeriesCardinalityFnSetShardEnabledFnShardFnShardGroupFnShardIDsFnShardNFnShardRelativePathFnShardsFnStatisticsFnTagKeysFnTagValuesFnWithLoggerFnWriteToShardFnBackupSeriesFileAuthorizeDatabaseFnAuthorizeQueryFnAuthorizeSeriesReadFnAuthorizeSeriesWriteFn/Users/austinjaybecker/projects/abeck-go-testing/internal/authorizer.go AuthorizerMock is a mockable implementation of a query.Authorizer. AuthorizeDatabase determines if the provided privilege is sufficient to authorise access to the database. AuthorizeQuery determines if the query can be executed against the provided database. AuthorizeSeriesRead determines if the series comprising measurement and tags can be read on the provided database. AuthorizeSeriesWrite determines if the series comprising measurement and tags can be written to, on the provided database./Users/austinjaybecker/projects/abeck-go-testing/internal/cmd/Users/austinjaybecker/projects/abeck-go-testing/internal/cmd/kvmigrate/Users/austinjaybecker/projects/abeck-go-testing/internal/cmd/kvmigrate/main.gousageMsgUsage: kvmigrate create <migration name / description>"Usage: kvmigrate create <migration name / description>"unrecognized command %q
"unrecognized command %q\n"CreateNewMigration/Users/austinjaybecker/projects/abeck-go-testing/internal/fs/Users/austinjaybecker/projects/abeck-go-testing/internal/fs/influx_dir.goBoltFile"credentials".influxdbv2".influxdbv2"bolt file found"bolt file found".bolt".bolt"bolt file not found"bolt file not found" DefaultTokenFile is deprecated, and will be only used for migration. DefaultConfigsFile stores cli credentials and hosts. InfluxDir retrieves the influxdbv2 directory. BoltFile returns the path to the bolt file for influxdb/Users/austinjaybecker/projects/abeck-go-testing/internal/meta_client.gomakeDefaultdestinationsfrom MetaClientMock is a mockable implementation of meta.MetaClient./Users/austinjaybecker/projects/abeck-go-testing/internal/resource/Users/austinjaybecker/projects/abeck-go-testing/internal/resource/resolve.gounsupported resource type %s"unsupported resource type %s" Resolver is a type which combines multiple resource services in order to resolve the resources associated org ID. Ideally you do not need to use this type, it is mostly a stop-gap while we migrate responsibilities off of *kv.Service. Consider it deprecated. FindResourceOrganizationID is used to find the organization that a resource belongs to five the id of a resource and a resource type. FindResourceName is used to find the name of the resource associated with the provided type and id. keeping this consistent with the original kv implementation default behaviour (in-line with original implementation) is to just return an empty name/Users/austinjaybecker/projects/abeck-go-testing/internal/testutil/Users/austinjaybecker/projects/abeck-go-testing/internal/testutil/strings.goMakeSentenceUnzipwordstestutillorem"lorem"ipsum"ipsum"dolor"dolor"sit"sit"amet"amet"consectetuer"consectetuer"adipiscing"adipiscing"elit"elit""in""mi"mauris"mauris"ornare"ornare"sagittis"sagittis"suspendisse"suspendisse"potenti"potenti"dapibus"dapibus"dignissim"dignissim"nam"nam"sapien"sapien"tellus"tellus"tempus"tempus"et"et""ac"tincidunt"tincidunt"arcu"arcu"duis"duis"dictum"dictum"proin"proin"magna"magna"nulla"nulla"pellentesque"pellentesque"non"non"commodo"commodo"iaculis"iaculis"condimentum"condimentum"massa"massa""ut"metus"metus"donec"donec"viverra"viverra"mattis"mattis"rutrum"rutrum"tristique"tristique"lacus"lacus"eros"eros"semper"semper"molestie"molestie"nisi"nisi"eu"eu"vestibulum"vestibulum"ante"ante"primis"primis"faucibus"faucibus"orci"orci"luctus"luctus"ultrices"ultrices"posuere"posuere"cubilia"cubilia"curae"curae"fusce"fusce"erat"erat"tortor"tortor"mollis"mollis"accumsan"accumsan"lacinia"lacinia"gravida"gravida"libero"libero"curabitur"curabitur"felis"felis"feugiat"feugiat"convallis"convallis"porta"porta"vel"vel"neque"neque"ligula"ligula"ultricies"ultricies"tempor"tempor"quisque"quisque"malesuada"malesuada"velit"velit"sed"sed"purus"purus"imperdiet"imperdiet"eleifend"eleifend"quam"quam"nullam"nullam"egestas"egestas"interdum"interdum"nec"nec"nunc"nunc"suscipit"suscipit"pharetra"pharetra"sollicitudin"sollicitudin"turpis"turpis"facilisis"facilisis"vitae"vitae"urna"urna"congue"congue"praesent"praesent"est"est"class"class"aptent"aptent"taciti"taciti"sociosqu"sociosqu"ad"ad"litora"litora"torquent"torquent"per"per"conubia"conubia"nostra"nostra"inceptos"inceptos"hymenaeos"hymenaeos"cras"cras"nibh"nibh""sem"phasellus"phasellus"enim"enim"justo"justo"ullamcorper"ullamcorper"quis"quis"volutpat"volutpat"nonummy"nonummy"vulputate"vulputate"aliquet"aliquet"risus"risus""at"aliquam"aliquam"vivamus"vivamus"consequat"consequat"pulvinar"pulvinar"eget"eget"scelerisque"scelerisque"morbi"morbi"habitant"habitant"senectus"senectus"netus"netus"fames"fames"hac"hac"habitasse"habitasse"platea"platea"dictumst"dictumst"nisl"nisl"blandit"blandit"bibendum"bibendum"augue"augue"leo"leo"cursus"cursus"fermentum"fermentum"pretium"pretium"aenean"aenean"cum"cum"sociis"sociis"natoque"natoque"penatibus"penatibus"magnis"magnis"dis"dis"parturient"parturient"montes"montes"nascetur"nascetur"ridiculus"ridiculus"mus"mus"varius"varius"porttitor"porttitor"pede"pede"lobortis"lobortis"diam"diam"vehicula"vehicula"laoreet"laoreet"sodales"sodales"hendrerit"hendrerit"dui"dui"venenatis"venenatis"placerat"placerat"etiam"etiam"lectus"lectus"auctor"auctor"rhoncus"rhoncus"elementum"elementum"odio"odio" MakeSentence returns a string made up of n words. MakeSentence uses rand.Int31n and therefore calling rand.Seed will produce deterministic results./Users/austinjaybecker/projects/abeck-go-testing/internal/testutil/unzip.gofpathoutFileziparchive/zip"archive/zip"NonUTF8CreatorVersionReaderVersionModifiedTimeModifiedDateCompressedSizeUncompressedSizeCompressedSize64UncompressedSize64ExternalAttrsSetModTimeSetModeisZip64hasDataDescriptorziprheaderOffsetzip64DataOffsetOpenRawfindBodyOffsetfileListEntryisDirisDupstatdecompressorsbaseOffsetfileListOncefileListRegisterDecompressorinitFileListopenLookupopenReadDirOpenReaderPathSeparator%s: illegal file path"%s: illegal file path"O_TRUNC1537 Unzip will extract a zip archive into dest Store filename/path for returning and using later on Check for ZipSlip. More Info: http://bit.ly/2MsjAWE Make FolderfileInfoDirEntry/Users/austinjaybecker/projects/abeck-go-testing/internal/tsdb_store.goExportEndExportStart TSDBStoreMock is a mockable implementation of tsdb.Store./Users/austinjaybecker/projects/abeck-go-testing/jsonweb/Users/austinjaybecker/projects/abeck-go-testing/jsonweb/token.goverr"jwt"key not found"key not found"missing kid in token claims"missing kid in token claims"token is unexpected type"token is unexpected type"ValidationErrorInnerValidationErrorMalformedjson:"uid,omitempty"`json:"uid,omitempty"` ErrKeyNotFound should be returned by a KeyStore when a key cannot be located for the provided key ID EmptyKeyStore is a KeyStore implementation which contains no keys KeyStore is a type which holds a set of keys accessed via an id KeyStoreFunc is a function which can be used as a KeyStore Key delegates to the receiver KeyStoreFunc TokenParser is a type which can parse and validate tokens NewTokenParser returns a configured token parser used to parse Token types from strings Parse takes a string then parses and validates it as a jwt based on the key described within the token fetch key for "kid" from key store IsMalformedError returns true if the error returned represents a jwt malformed token error Token is a structure which is serialized as a json web token It contains the necessary claims required to authorize KeyID is the identifier of the key used to sign the token Permissions is the set of authorized permissions for the token UserID for the token PermissionSet returns the set of permissions associated with the token. Identifier returns the identifier for this Token as found in the standard claims GetUserID returns an invalid id as tokens are generated with permissions rather than for or by a particular user Kind returns the string "jwt" which is used for auditing EphemeralAuth creates a influxdb Auth form a jwt token/Users/austinjaybecker/projects/abeck-go-testing/keyvalue_log.go KeyValueLog is a generic type logs key-value pairs. This interface is intended to be used to construct other higher-level log-like resources such as an oplog or audit log. The idea is to create a log who values can be accessed at the key k: k -> [(v0,t0) (v1,t1) ... (vn,tn)] Logs may be retrieved in ascending or descending time order and support limits and offsets. AddLogEntry adds an entry (v,t) to the log defined for the key k. ForEachLogEntry iterates through all the log entries at key k and applies the function fn for each record. FirstLogEntry is used to retrieve the first entry in the log at key k. LastLogEntry is used to retrieve the last entry in the log at key k./Users/austinjaybecker/projects/abeck-go-testing/kit/Users/austinjaybecker/projects/abeck-go-testing/kit/check/Users/austinjaybecker/projects/abeck-go-testing/kit/check/check.goCheckerFuncDefaultCheckNameErrCheckNamedCheckerNamedFuncNewCheckPasswriteResponsedisableenablehealthChecksreadyCheckshealthOverridereadyOverridepassthroughHandlerAddHealthCheckAddReadyCheckCheckHealthCheckReadySetPassthroughoverrideResponseoverridingpathHealthpathReadyqueryForce"fail""internal""Health"manual-override"manual-override"health manually overridden"health manually overridden"Ready"Ready"ready manually overridden"ready manually overridden"healthy"healthy"StatusServiceUnavailable503{"message": "error marshaling response", "status": "fail"}`{"message": "error marshaling response", "status": "fail"}` Package check standardizes /health and /ready endpoints. This allows you to easily know when your server is ready and healthy. Status string to indicate the overall status of the check. StatusFail indicates a specific check has failed. StatusPass indicates a specific check has passed. DefaultCheckName is the name of the default checker. Check wraps a map of service names to status checkers. Checker indicates a service whose health can be checked. NewCheck returns a Health with a default checker. AddHealthCheck adds the check to the list of ready checks. If c is a NamedChecker, the name will be added. AddReadyCheck adds the check to the list of ready checks. CheckHealth evaluates c's set of health checks and returns a populated Response. CheckReady evaluates c's set of ready checks and returns a populated Response. SetPassthrough allows you to set a handler to use if the request is not a ready or health check. This can be useful if you intend to use this as a middleware. ServeHTTP serves /ready and /health requests with the respective checks. Allow requests not intended for checks to pass through. We can't handle this request. writeResponse writes a Response to the wire as JSON. The HTTP status code accompanying the payload is the primary means for signaling the status of the checks. The possible status codes are: - 200 OK: All checks pass. - 503 Service Unavailable: Some checks are failing. - 500 Internal Server Error: There was a problem serializing the Response. override is a manual override for an entire group of checks. get returns the Status of an override as well as whether or not an override is currently active. disable disables the override. enable turns on the override and establishes a specific Status for which to./Users/austinjaybecker/projects/abeck-go-testing/kit/check/helpers.go NamedChecker is a superset of Checker that also indicates the name of the service. Prefer to implement NamedChecker if your service has a fixed name, as opposed to calling *Health.AddNamed. CheckerFunc is an adapter of a plain func() error to the Checker interface. Check implements Checker. Named returns a Checker that will attach a name to the Response from the check. This way, it is possible to augment a Response with a human-readable name, but not have to encode that logic in the actual check itself. NamedFunc is the same as Named except it takes a CheckerFunc. ErrCheck will create a health checker that executes a function. If the function returns an error, it will return an unhealthy response. Otherwise, it will be as if the Ok function was called. Note: it is better to use CheckFunc, because with Check, the context is ignored. Pass is a utility function to generate a passing status response with the default parameters. Info is a utility function to generate a healthy status with a printf message. Error is a utility function for creating a response from an error message./Users/austinjaybecker/projects/abeck-go-testing/kit/check/response.gojson:"checks,omitempty"`json:"checks,omitempty"` Response is a result of a collection of health checks. HasCheck verifies whether the receiving Response has a check with the given name or not. Responses is a sortable collection of Response objects. Less defines the order in which responses are sorted. Failing responses are always sorted before passing responses. Responses with the same status are then sorted according to the name of the check./Users/austinjaybecker/projects/abeck-go-testing/kit/cli/Users/austinjaybecker/projects/abeck-go-testing/kit/cli/doc.goIDVarPNewOptOrgBucketidValueinitializeConfigmustBindPFlagnewIDValue Package cli creates simple CLI options with ENV overrides using viper. This is a small simplification over viper to move most of the boilerplate into one place. In this example the flags can be set with MYPROGRAM_MONITOR_HOST and MYPROGRAM_NUMBER or with the flags --monitor-host and --number var flags struct { 	monitorHost string 	number int func main() { 	cmd := cli.NewCommand(&cli.Program{ 		Run:  run, 		Name: "myprogram", 		Opts: []cli.Opt{ 			{ 				DestP:   &flags.monitorHost, 				Flag:    "monitor-host", 				Default: "http://localhost:8086", 				Desc:    "host to send influxdb metrics", 			}, 			 	DestP:   &flags.number, 				Flag:    "number", 				Default: 2, 				Desc:    "number of times to loop", 		}, 	}) 	if err := cmd.Execute(); err != nil { 		fmt.Fprintln(os.Stderr, err) 		os.Exit(1) 	} func run() error { 	for i := 0; i < number; i++ { 		fmt.Printf("%d\n", i) 		feturn nilAddFlagsOrgBucketID/Users/austinjaybecker/projects/abeck-go-testing/kit/cli/idflag.goshorthandpflag"github.com/spf13/pflag"organization id"organization id"bucket id"bucket id"TODO: Fix"TODO: Fix" Wrapper for influxdb.ID IDVar defines an influxdb.ID flag with specified name, default value, and usage string. The argument p points to an influxdb.ID variable in which to store the value of the flag. IDVarP is like IDVar, but accepts a shorthand letter that can be used after a single dash. TODO: FIX THIS/Users/austinjaybecker/projects/abeck-go-testing/kit/cli/viper.godestPdfltflagsetCONFIG_PATH"CONFIG_PATH".toml".toml""config"invalid config file caused panic: "invalid config file caused panic: "ConfigFileNotFoundErrorlocationsfnfeunknown destination type %t"unknown destination type %t" Opt is a single command-line option pointer to the destination using rune b/c it guarantees correctness. a short must always be a string of length 1 NewOpt creates a new command line option. Program parses CLI options Run is invoked by cobra on execute. Name is the name of the program in help usage and the env var prefix. Opts are the command line/env var options to the program NewCommand creates a new cobra command to be executed that respects env vars. Uses the upper-case version of the program's name as a prefix to all environment variables. This is to simplify the viper/cobra boilerplate. This normalizes "-" to an underscore in env names. defaults to looking in same directory as program running for a file with base `config` and extensions .json|.toml|.yaml|.yml done before we bind flags to viper keys. order of precedence (1 highest -> 3 lowest):	1. flags  2. env vars	3. config file BindOptions adds opts to the specified command and automatically registers those options with viper. if you get a panic here, sorry about that! anyway, go ahead and make a PR and add another type. so weirdness with the flagset her, the flag must be set before marking it hidden. This is in contrast to the MarkRequired, which can be set before.../Users/austinjaybecker/projects/abeck-go-testing/kit/errors/Users/austinjaybecker/projects/abeck-go-testing/kit/errors/errors.goBadRequestErrorForbiddenForbiddenfInternalErrorInternalErrorfInvalidDataInvalidDatafMalformedDataMalformedDatafrefCodejson:"referenceCode"`json:"referenceCode"`json:"statusCode"`json:"statusCode"`json:"err"`json:"err"`%s: %s"%s: %s" TODO: move to base directory InternalError indicates an unexpected error condition. MalformedData indicates malformed input, such as unparsable JSON. InvalidData indicates that data is well-formed, but invalid. Forbidden indicates a forbidden operation. NotFound indicates a resource was not found. Error indicates an error with a reference code and an HTTP status code. Error implements the error interface. Errorf constructs an Error with the given reference code and format. New creates a new error with a message and error code. InternalErrorf constructs an InternalError with the given format. MalformedDataf constructs a MalformedData error with the given format. InvalidDataf constructs an InvalidData error with the given format. Forbiddenf constructs a Forbidden error with the given format./Users/austinjaybecker/projects/abeck-go-testing/kit/errors/list.gosb List represents a list of errors. cached error Append adds err to the errors list. AppendString adds a new error that formats as the given text. Clear removes all the previously appended errors from the list. Err returns an error composed of the list of errors, separated by a new line, or nil if no errors were appended./Users/austinjaybecker/projects/abeck-go-testing/kit/feature/Users/austinjaybecker/projects/abeck-go-testing/kit/feature/doc.goAppMetricsBackendExampleBandPlotTypeCommunityTemplatesExposedFlagsFromContextFlagsFromContextFloatFlagFrontendExampleGroupWindowAggregateTransposeHTTPProxyInjectLatestSuccessTimeIntFlagMakeBaseMakeBoolFlagMakeFlagMakeFloatFlagMakeIntFlagMakeStringFlagMemoryOptimizedFillMemoryOptimizedSchemaMutationMosaicGraphTypeNewHTTPProxyNewLabelPackageNotebooksPermanentProxyEnablerQueryTracingSimpleTaskOptionsExtractionStringFlagTemporaryTimeFilterFlagsappMetricsbackendExamplebandPlotTypecommunityTemplatesdefaultFlaggerenforceOrgDashboardLimitsfeatureContextKeyfrontendExamplegroupWindowAggregateTransposeheaderProxyFlaginjectLatestSuccessTimememoryOptimizedFillmemoryOptimizedSchemaMutationmosaicGraphTypenewLabelsnewReverseProxynotebooksqueryTracingsimpleTaskOptionsExtractiontimeFilterFlags Package feature provides feature flagging capabilities for InfluxDB servers. This document describes this package and how it is used to control experimental features in `influxd`. Flags are configured in `flags.yml` at the top of this repository. Running `make flags` generates Go code based on this configuration to programmatically test flag values in a given request context. Boolean flags are the most common case, but integers, floats and strings are supported for more complicated experiments. The `Flagger` interface is the crux of this package. It computes a map of feature flag values for a given request context. The default implementation always returns the flag default configured in `flags.yml`. The override implementation allows an operator to override feature flag defaults at startup. Changing these overrides requires a restart. In `influxd`, a `Flagger` instance is provided to a `Handler` middleware configured to intercept all API requests and annotate their request context with a map of feature flags. A flag can opt in to be exposed externally in `flags.yml`. If exposed, this flag will be included in the response from the `/api/v2/flags` endpoint. This allows the UI and other API clients to control their behavior according to the flag in addition to the server itself. A concrete example to illustrate the above: I have a feature called "My Feature" that will involve turning on new code in both the UI and the server. First, I add an entry to `flags.yml`. ```yaml - name: My Feature   description: My feature is awesome   key: myFeature   default: false   expose: true   contact: My Name My flag type is inferred to be boolean by my default of `false` when I run `make flags` and the `feature` package now includes `func MyFeature() BoolFlag`. I use this to control my backend code with ```go if feature.MyFeature.Enabled(ctx) {   // new code... } else { and the `/api/v2/flags` response provides the same information to the frontend. ```json {   "myFeature": false While `false` by default, I can turn on my experimental feature by starting my server with a flag override. env INFLUXD_FEATURE_FLAGS="{\"flag1\":\value1\",\"key2\":\"value2\"}" influxd influxd --feature-flags flag1=value1,flag2=value2defaultStringenablerdefaultIntdefaultFloat/Users/austinjaybecker/projects/abeck-go-testing/kit/feature/feature.gocomputedinflux/feature/v1"influx/feature/v1"permanent"permanent" Flagger returns flag values. Flags returns a map of flag keys to flag values. If an authorization is present on the context, it may be used to compute flag values according to the affiliated user ID and its organization and other mappings. Otherwise, they should be computed generally or return a default. One or more flags may be provided to restrict the results. Otherwise, all flags should be computed. Annotate the context with a map computed of computed flags. FlagsFromContext returns the map of flags attached to the context by Annotate, or nil if none is found. ExposedFlagsFromContext returns the filtered map of exposed  flags attached to the context by Annotate, or nil if none is found. Lifetime represents the intended lifetime of the feature flag. The zero value is Temporary, the most common case, but Permanent is included to mark special cases where a flag is not intended to be removed, e.g. enabling debug tracing for an organization. TODO(gavincabbage): This may become a stale date, which can then 		be used to trigger a notification to the contact when the flag		has become stale, to encourage flag cleanup. Temporary indicates a flag is intended to be removed after a feature is no longer in development. Permanent indicates a flag is not intended to be removed. UnmarshalYAML implements yaml.Unmarshaler and interprets a case-insensitive text representation as a lifetime constant. DefaultFlagger returns a flagger that always returns default values. Flags returns a map of default values. It never returns an error. Flags returns all feature flags. ByKey returns the Flag corresponding to the given key./Users/austinjaybecker/projects/abeck-go-testing/kit/feature/flag.gogo:generate go run ./_codegen/main.go --in ../../flags.yml --out ./list.go Flag represents a generic feature flag with a key and a default. Key returns the programmatic backend identifier for the flag. Default returns the type-agnostic zero value for the flag. Type-specific flag implementations may expose a typed default (e.g. BoolFlag includes a boolean Default field). Expose the flag. MakeFlag constructs a Flag. The concrete implementation is inferred from the provided default. flag base type. name of the flag. key is the programmatic backend identifier for the flag. defaultValue for the flag. owner is an individual or team responsible for the flag. lifetime of the feature flag. expose the flag. MakeBase constructs a flag flag. StringFlag implements Flag for string values. MakeStringFlag returns a string flag with the given Base and default. String value of the flag on the request context. FloatFlag implements Flag for float values. MakeFloatFlag returns a string flag with the given Base and default. Float value of the flag on the request context. IntFlag implements Flag for integer values. MakeIntFlag returns a string flag with the given Base and default. Int value of the flag on the request context. BoolFlag implements Flag for boolean values. MakeBoolFlag returns a string flag with the given Base and default. Enabled indicates whether flag is true or false on the request context./Users/austinjaybecker/projects/abeck-go-testing/kit/feature/http_proxy.godefaultDirectorenablerKeyX-Platform-Proxy-Flag"X-Platform-Proxy-Flag"NewSingleHostReverseProxy HTTPProxy is an HTTP proxy that's guided by a feature flag. If the feature flag presented to it is enabled, it will perform the proxying behavior. Otherwise it will be a no-op. NewHTTPProxy returns a new Proxy. Do performs the proxying. It returns whether or not the request was proxied. headerProxyFlag is the HTTP header for enriching the request and response with the feature flag key that precipitated the proxying behavior. newReverseProxy creates a new single-host reverse proxy. Override r.Host to prevent us sending this request back to ourselves. A bug in the stdlib causes this value to be preferred over the r.URL.Host (which is set in the default Director) if r.Host isn't empty (which it isn't). https://github.com/golang/go/issues/28168 ProxyEnabler is a boolean feature flag./Users/austinjaybecker/projects/abeck-go-testing/kit/feature/list.goApp Metrics"App Metrics""appMetrics"Bucky, Monitoring Team"Bucky, Monitoring Team"Backend Example"Backend Example""backendExample"Gavin Cabbage"Gavin Cabbage"Community Templates"Community Templates""communityTemplates"Bucky"Bucky"Frontend Example"Frontend Example""frontendExample"Group Window Aggregate Transpose"Group Window Aggregate Transpose""groupWindowAggregateTranspose"Query Team"Query Team"New Label Package"New Label Package""newLabels"Alirie Gray"Alirie Gray"Memory Optimized Fill"Memory Optimized Fill""memoryOptimizedFill"Memory Optimized Schema Mutation"Memory Optimized Schema Mutation""memoryOptimizedSchemaMutation"Query Tracing"Query Tracing""queryTracing"Simple Task Options Extraction"Simple Task Options Extraction""simpleTaskOptionsExtraction"Brett Buddin"Brett Buddin"Band Plot Type"Band Plot Type""bandPlotType"Monitoring Team"Monitoring Team"Mosaic Graph Type"Mosaic Graph Type""mosaicGraphType""Notebooks""notebooks"Inject Latest Success Time"Inject Latest Success Time""injectLatestSuccessTime"Compute Team"Compute Team"Enforce Organization Dashboard Limits"Enforce Organization Dashboard Limits""enforceOrgDashboardLimits"Time Filter Flags"Time Filter Flags""timeFilterFlags" Code generated by the feature package; DO NOT EDIT. AppMetrics - Send UI Telementry to Tools cluster - should always be false in OSS BackendExample - A permanent backend example boolean flag CommunityTemplates - Replace current template uploading functionality with community driven templates FrontendExample - A temporary frontend example integer flag GroupWindowAggregateTranspose - Enables the GroupWindowAggregateTransposeRule for all enabled window aggregates NewLabelPackage - Enables the refactored labels api MemoryOptimizedFill - Enable the memory optimized fill() MemoryOptimizedSchemaMutation - Enable the memory optimized schema mutation functions QueryTracing - Turn on query tracing for queries that are sampled SimpleTaskOptionsExtraction - Simplified task options extraction to avoid undefined functions when saving tasks BandPlotType - Enables the creation of a band plot in Dashboards MosaicGraphType - Enables the creation of a mosaic graph in Dashboards Notebooks - Determine if the notebook feature's route and navbar icon are visible to the user InjectLatestSuccessTime - Inject the latest successful task run timestamp into a Task query extern when executing. EnforceOrganizationDashboardLimits - Enforces the default limit params for the dashboards api when orgs are set TimeFilterFlags - Filter task run list based on before and after flags/Users/austinjaybecker/projects/abeck-go-testing/kit/feature/middleware.goUnable to annotate context with feature flags"Unable to annotate context with feature flags" Handler is a middleware that annotates the context with a map of computed feature flags. To accurately compute identity-scoped flags, this middleware should be executed after any authorization middleware has annotated the request context with an authorizer. NewHandler returns a configured feature flag middleware that will annotate request context with a computed map of the given flags using the provided Flagger. ServeHTTP annotates the request context with a map of computed feature flags before continuing to serve the request. HTTPErrorHandler is an influxdb.HTTPErrorHandler. It's defined here instead of referencing the other interface type, because we want to try our best to avoid cyclical dependencies when feature package is used throughout the codebase. NewFlagsHandler returns a handler that returns the map of computed feature flags on the request context./Users/austinjaybecker/projects/abeck-go-testing/kit/feature/override/Users/austinjaybecker/projects/abeck-go-testing/kit/feature/override/override.gomissingifaceoverriddenconfigured overrides for non-existent flags: %s"configured overrides for non-existent flags: %s"coercing string %q based on flag type %T: %v"coercing string %q based on flag type %T: %v" Flagger can override default flag values. Make a Flagger that returns defaults with any overrides parsed from the string. Check all provided override keys correspond to an existing Flag. Flags returns a map of default values with overrides applied. It never returns an error./Users/austinjaybecker/projects/abeck-go-testing/kit/io/Users/austinjaybecker/projects/abeck-go-testing/kit/io/limited_read_closer.goread limit exceeded"read limit exceeded" LimitedReadCloser wraps an io.ReadCloser in limiting behavior using io.LimitedReader. It allows us to obtain the limit error at the time of close instead of just when writing. underlying reader max bytes remaining NewLimitedReadCloser returns a new LimitedReadCloser. Close returns an ErrReadLimitExceeded when the wrapped reader exceeds the set limit for number of bytes.  This is safe to call more than once but not concurrently. Close has already been called. Prevent l.closer.Close from being called again./Users/austinjaybecker/projects/abeck-go-testing/kit/metric/Users/austinjaybecker/projects/abeck-go-testing/kit/metric/client.goRecordAdditionalWithVecvecvecOptscall_total"call_total"Number of calls"Number of calls"error_total"error_total"Number of errors encountered"Number of errors encountered"Duration of calls"Duration of calls" REDClient is a metrics client for collection RED metrics. New creates a new REDClient. RecordAdditional provides an extension to the base method, err data provided to the metrics. Record returns a record fn that is called on any given return err. If an error is encountered it will register the err metric. The err is never altered./Users/austinjaybecker/projects/abeck-go-testing/kit/metric/metrics_options.gosuffix%s_%s"%s_%s" CollectFnOpts provides arguments to the collect operation of a metric. VecOpts expands on the ClientOptFn is an option used by a metric middleware. WithVec sets a new counter vector to be collected. WithSuffix returns a metric option that applies a suffix to the service name of the metric./Users/austinjaybecker/projects/abeck-go-testing/kit/prom/Users/austinjaybecker/projects/abeck-go-testing/kit/prom/promtest/Users/austinjaybecker/projects/abeck-go-testing/kit/prom/promtest/promtest.goFindMetricFromHTTPResponseMustFindMetricMustGatherfindMetricmfsfampromtestResponseFormatmetric family with name %q not found"metric family with name %q not found"available names:"available names:"	%s"\t%s"found metric family with name %q, but metric with labels %v not found"found metric family with name %q, but metric with labels %v not found"available labels on metric family %q:"available labels on metric family %q:"%q: %q"%q: %q"error while gathering metrics: %v"error while gathering metrics: %v" Package promtest provides helpers for parsing and extracting prometheus metrics. These functions are only intended to be called from test files, as there is a dependency on the standard library testing package. FromHTTPResponse parses the prometheus metrics from the given *http.Response. It relies on properly set response headers to correctly parse. It will unconditionally close the response body. This is particularly helpful when testing the output of the /metrics endpoint of a service. However, for comprehensive testing of metrics, it usually makes more sense to add collectors to a registry and call Registry.Gather to get the metrics without involving HTTP. FindMetric iterates through mfs to find the first metric family matching name. If a metric family matches, then the metrics inside the family are searched, and the first metric whose labels match the given labels are returned. If no matches are found, FindMetric returns nil. FindMetric assumes that the labels on the metric family are well formed, i.e. there are no duplicate label names, and the label values are not empty strings. MustFindMetric returns the matching metric, or if no matching metric could be found, it calls tb.Log with helpful output of what was actually available, before calling tb.FailNow. Need an explicit return here for test. findMetric is a helper that returns the matching family and the matching metric. The exported FindMetric function specifically only finds the metric, not the family, but for test it is more helpful to identify whether the family was matched. No family matching the name. All labels matched. Didn't find a metric whose labels all matched. MustGather calls g.Gather and calls tb.Fatal if there was an error./Users/austinjaybecker/projects/abeck-go-testing/kit/prom/registry.gopromLoggerplpromhttpgithub.com/prometheus/client_golang/prometheus/promhttp"github.com/prometheus/client_golang/prometheus/promhttp"HandlerOptsHandlerErrorHandlingMaxRequestsInFlightEnableOpenMetricsHandlerFor Package prom provides a wrapper around a prometheus metrics registry so that all services are unified in how they expose prometheus metrics. PrometheusCollector is the interface for a type to expose prometheus metrics. This interface is provided as a convention, so that you can optionally check if a type implements it and then pass its collectors to (*Registry).MustRegister. PrometheusCollectors returns a slice of prometheus collectors containing metrics for the underlying instance. Registry embeds a prometheus registry and adds a couple convenience methods. NewRegistry returns a new registry. HTTPHandler returns an http.Handler for the registry, so that the /metrics HTTP handler is uniformly configured across all apps in the platform. TODO(mr): decide if we want to set MaxRequestsInFlight or Timeout. promLogger satisfies the promhttp.logger interface with the registry. Because normal usage is that WithLogger is called after HTTPHandler, we refer to the Registry rather than its logger. Println implements promhttp.logger./Users/austinjaybecker/projects/abeck-go-testing/kit/signals/Users/austinjaybecker/projects/abeck-go-testing/kit/signals/context.goWithSignalssigChsigs WithSignals returns a context that is canceled with any signal in sigs. WithStandardSignals cancels the context on os.Interrupt, syscall.SIGTERM./Users/austinjaybecker/projects/abeck-go-testing/kit/tracing/Users/austinjaybecker/projects/abeck-go-testing/kit/tracing/testing/Users/austinjaybecker/projects/abeck-go-testing/kit/tracing/testing/testing.goSetupInMemoryTracing"github.com/uber/jaeger-client-go"GlobalTracerTracerOptionNewConstSamplerInMemoryReporterspansSpansSubmittedGetSpansNewInMemoryReporter SetupInMemoryTracing sets the global tracer to an in memory Jaeger instance for testing. The returned function should be deferred by the caller to tear down this setup after testing is complete./Users/austinjaybecker/projects/abeck-go-testing/kit/tracing/tracing.goStartSpanFromContextWithPromMetricsannotateSpanpcshandlerNamespanContextsctxlastSlashsampledgithub.com/opentracing/opentracing-go/ext"github.com/opentracing/opentracing-go/ext"Callersruntime.Callers failed"runtime.Callers failed"FuncForPCBuiltinFormatHTTPHeadersHTTPHeadersCarrierForeachKey"request"ChildOfRPCServerOptionContextWithSpanMatchedRouteFromContext"route"RouteContextStartSpanFromContext called with nil context"StartSpanFromContext called with nil context"LastIndexByte'/'StartSpanFromContextWithOperationName called with nil context"StartSpanFromContextWithOperationName called with nil context" LogError adds a span log for an error. Returns unchanged error, so useful to wrap as in:  return 0, tracing.LogError(err) Get caller frame. InjectToHTTPRequest adds tracing headers to an HTTP request. Easier than adding this boilerplate everywhere. ExtractFromHTTPRequest gets a child span of the parent referenced in HTTP request headers. Returns the request with updated tracing context. span is a simple wrapper around opentracing.Span in order to get access to the duration of the span for metrics reporting. StartSpanFromContext is an improved opentracing.StartSpanFromContext. Uses the calling function as the operation name, and logs the filename and line number. Passing nil context induces panic. Context without parent span reference triggers root span construction. This function never returns nil values. Performance This function incurs a small performance penalty, roughly 1000 ns/op, 376 B/op, 6 allocs/op. Jaeger timestamp and duration precision is only Âµs, so this is pretty negligible. Alternatives If this performance penalty is too much, try these, which are also demonstrated in benchmark tests:  // Create a root span  span := opentracing.StartSpan("operation name")  ctx := opentracing.ContextWithSpan(context.Background(), span)  // Create a child span  span := opentracing.StartSpan("operation name", opentracing.ChildOf(sc))  // Sugar to create a child span  span, ctx := opentracing.StartSpanFromContext(ctx, "operation name") Create a child span. Create a root span. New context references this span, not the parent (if there was one). StartSpanFromContextWithOperationName is like StartSpanFromContext, but the caller determines the operation name. InfoFromSpan returns the traceID and if it was sampled from the span, given it is a jaeger span. It returns whether a span associated to the context has been found. InfoFromContext returns the traceID and if it was sampled from the Jaeger span found in the given context. It returns whether a span associated to the context has been found./Users/austinjaybecker/projects/abeck-go-testing/kit/transport/Users/austinjaybecker/projects/abeck-go-testing/kit/transport/http/Users/austinjaybecker/projects/abeck-go-testing/kit/transport/http/api.goCtxOrgKeyEnablerErrBodyErrorCodeToStatusCodeFeatureHandlerNewFeatureHandlerOrgContextOrgIDFromContextWithEncodeGZIPWithErrFnWithOKErrFnWithPrettyJSONWithUnmarshalErrFnhttpStatusCodeToInfluxDBErrorinfluxDBErrorToStatusCodenoopClosernormalizePathokerwcebgobencoding/gob"encoding/gob""X-Platform-Error-Code"failed to unmarshal %s: %s"failed to unmarshal %s: %s"an internal error has occurred"an internal error has occurred""gob"decBufferDroptypeIdgobTypewireTypearrayTypeCommonTypesetIdsafeStringsliceTypestructTypegobEncoderTypeArrayTSliceTStructTMapTGobEncoderTBinaryMarshalerTTextMarshalerTdecEnginedecInstrdecOpdecoderStatefieldnumgetLengthovflinstrnumInstrdecoderCacheignorerCachefreeListcountBufnewDecoderStatefreeDecoderStatedecodeSingledecodeStructignoreStructignoreSingledecodeArrayHelperdecodeArrayignoreArrayHelperignoreArraydecodeSliceignoreSlicedecodeInterfaceignoreInterfacedecodeGobDecoderignoreGobDecoderdecOpFordecIgnoreOpForgobDecodeOpForcompatibleTypecompileSinglecompileIgnoreSinglecompileDecgetDecEnginePtrgetIgnoreEnginePtrdecodeIgnoredValuerecvTyperecvMessagereadMessagenextIntnextUintdecodeTypeSequenceDecodeValuefailed to write to response writer"failed to write to response writer"failed to close response writer"failed to close response writer"api error encountered"api error encountered"failed to write err to response writer"failed to write err to response writer"an unexpected error occurred"an unexpected error occurred" PlatformErrorCodeHeader shows the error code of platform error. API provides a consolidated means for handling API interface concerns. Concerns such as decoding/encoding request and response bodies as well as adding headers for content type and content encoding. APIOptFn is a functional option for setting fields on the API type. WithLog sets the logger. WithErrFn sets the err handling func for issues when writing to the response body. WithOKErrFn is an error handler for failing validation for request bodies. WithPrettyJSON sets the json encoder to marshal indent or not. WithEncodeGZIP sets the encoder to gzip contents. WithUnmarshalErrFn sets the error handler for errors that occur when unmarshalling the request body. NewAPI creates a new API type. DecodeJSON decodes reader with json. DecodeGob decodes reader with gob. Respond writes to the response writer, handling all errors in writing. we'll double close to make sure its always closed evenon issues before the write this marshal block is to catch failures before they hit the http writer. default behavior for http.ResponseWriter is when body is written and no status is set, it writes a 200. Or if a status is set before encoding and an error occurs, there is no means to write a proper status code (i.e. 500) when that is to occur. This brings that step out before and then writes the data and sets the status code after marshaling succeeds. Write allows the user to write raw bytes to the response writer. This operation does not have a fail case, all failures here will be logged. Err is used for writing an error to the response. ErrBody is an err response body.oldHandlernewHandleruserTypeInfoindirexternalEncexternalDecencIndirdecIndirdecHelper/Users/austinjaybecker/projects/abeck-go-testing/kit/transport/http/error_handler.goerrorCodeAn internal error has occurred"An internal error has occurred"DeadlineExceeded499StatusNotImplemented501StatusTooManyRequests429StatusMethodNotAllowed405StatusRequestEntityTooLarge413 ErrorHandler is the error handler in http package. HandleHTTPError encodes err with the appropriate status code and format, sets the X-Platform-Error-Code headers on the response. We're no longer using X-Influx-Error and X-Influx-Reference. and sets the response status to the corresponding status code. StatusCodeToErrorCode maps a http status code integer to an influxdb error code string. ErrorCodeToStatusCode maps an influxdb error code string to a http status code integer. If the client disconnects early or times out then return a different error than the passed in error code. Client timeouts return a 408 while disconnections return a non-standard Nginx HTTP 499 code. https://httpstatuses.com/499 Otherwise map internal error codes to HTTP status codes. influxDBErrorToStatusCode is a mapping of ErrorCode to http status code./Users/austinjaybecker/projects/abeck-go-testing/kit/transport/http/feature_controller.go Enabler allows the switching between two HTTP Handlers FeatureHandler is used to switch requests between an existing and a feature flagged HTTP Handler on a per-request basis/Users/austinjaybecker/projects/abeck-go-testing/kit/transport/http/handler.go ResourceHandler is an HTTP handler for a resource. The prefix describes the url path prefix that relates to the handler endpoints./Users/austinjaybecker/projects/abeck-go-testing/kit/transport/http/middleware.gostatusWdurMetricreqMetricpiecelookupOrgByResourceID"Origin"Access-Control-Allow-Origin"Access-Control-Allow-Origin"MethodOptionsAccess-Control-Allow-Methods"Access-Control-Allow-Methods"POST, GET, OPTIONS, PUT, DELETE, PATCH"POST, GET, OPTIONS, PUT, DELETE, PATCH"Access-Control-Allow-Headers"Access-Control-Allow-Headers"Accept, Content-Type, Content-Length, Accept-Encoding, Authorization, User-Agent"Accept, Content-Type, Content-Length, Accept-Encoding, Authorization, User-Agent":id":id"404 page not found"404 page not found" Middleware constructor. Access-Control-Allow-Origin must be present in every response allow and stop processing in pre-flight requests Preflight CORS requests from the browser will send an options request, so we need to make sure we satisfy them If header has multiple values, only the first value will be logged on the trace. ValidResource make sure a resource exists when a sub system needs to be mounted to an api if this function returns an error we will squash the error message and replace it with a not found error embed OrgID into context OrgIDFromContext ..../Users/austinjaybecker/projects/abeck-go-testing/kit/transport/http/status_response_writer.go1XX"1XX"2XX"2XX"3XX"3XX"4XX"4XX"5XX"5XX" WriteHeader writes the header and captures the status code. When statusCode is 0 then WriteHeader was never called and we can assume that the ResponseWriter wrote an http.StatusOK./Users/austinjaybecker/projects/abeck-go-testing/kv/Users/austinjaybecker/projects/abeck-go-testing/kv/backup.goCorruptScraperErrorDecIndexIDDecodeOrgNameKeyEncBytesEncStringCaseInsensitiveEncUniqKeyErrInvalidScraperIDErrInvalidScrapersBucketIDErrInvalidScrapersOrgIDErrScraperNotFoundErrUnprocessableScraperExtractTaskOptionsIndexMigrationIndexMigrationOptionInitialMigrationInternalScraperServiceErrorNewIndexMigrationUnexpectedScrapersBucketErrorWalkCursorWithCursorHintKeyStartWithCursorHintPrefixWithCursorHintsWithCursorLimitWithCursorSkipFirstItemWithIndexMigationBatchSizeWithIndexMigrationCleanupconsumeBucketdecodeLogEntryKeydecodeVariableOrgsIndexKeydefaultIndexMigrationOpBatchSizedocumentContentBucketdocumentMetaBucketencodeKeyValueIndexKeyencodeLogEntryKeyencodeVariableOrgsIndexfilterVariablesFnindexKeyPartsindexMappingindexReadAllindexVerifyindexWalkkvSlicekvTaskkvToInfluxTaskkvlogBucketkvlogIndexmarshalScrapernewKeyValueLogBoundsnewTaskMatchFnnewVariableStoreputAsJsonsameKeysstaticCursortaskBuckettaskIndexBuckettaskKeytaskLatestCompletedKeytaskManualRunKeytaskMatchFntaskOrgKeytaskRunBuckettaskRunKeyunmarshalScrapervariableBucketvariableIndexBucketvariableOrgsIndexnextFnisNextgetValueAtIndexoperationBatchSizeremoveDanglingForeignKeysPopulatePutDocument/Users/austinjaybecker/projects/abeck-go-testing/kv/cursor.go staticCursor implements the Cursor interface for a slice of static key value pairs. Pair is a struct for key value pairs. NewStaticCursor returns an instance of a StaticCursor. It destructively sorts the provided pairs to be in key ascending order. Seek searches the slice for the first key with the provided prefix. TODO: do binary search for prefix since pairs are ordered. First retrieves the first element in the cursor. Last retrieves the last element in the cursor. Next retrieves the next entry in the cursor. Prev retrieves the previous entry in the cursor./Users/austinjaybecker/projects/abeck-go-testing/kv/doc.go package kv The KV package is a set of services and abstractions built around key value storage. There exist in-memory and persisted implementations of the core `Store` family of interfaces outside of this package (see `inmem` and `bolt` packages). The `Store` interface exposes transactional access to a backing kv persistence layer. It allows for read-only (View) and read-write (Update) transactions to be opened. These methods take a function which is passed an implementation of the transaction interface (Tx). This interface exposes a way to manipulate namespaced keys and values (Buckets). All keys and values are namespaced (grouped) using buckets. Buckets can only be created on implementations of the `SchemaStore` interface. This is a superset of the `Store` interface, which has the additional bucket creation and deletion methods. Bucket creation and deletion should be facilitated via a migration (see `kv/migration`)./Users/austinjaybecker/projects/abeck-go-testing/kv/document.gometab/documents/content"/documents/content"/documents/meta"/documents/meta" DocumentStore implements influxdb.DocumentStore. CreateDocumentStore creates an instance of a document store by instantiating the buckets for the store. TODO(desa): keep track of which namespaces exist. FindDocumentStore finds the buckets associated with the namespace provided. CreateDocument creates an instance of a document and sets the ID. After which it applies each of the options provided. TODO(desa): index document meta FindDocument retrieves the specified document with all its content and labels. FindDocuments retrieves all documents returned by the document find options. TODO(desa): might be a better way to do get all./Users/austinjaybecker/projects/abeck-go-testing/kv/encode.gopartno ID was provided"no ID was provided" EncodeFn returns an encoding when called. Closures are your friend here. Encode concatenates a list of encodings together. EncString encodes a string. EncStringCaseInsensitive encodes a string and makes it case insensitive by lower casing everything. EncID encodes an influx ID. EncBytes is a basic pass through for providing raw bytes./Users/austinjaybecker/projects/abeck-go-testing/kv/errors.gounexpected error retrieving index; Err: %v"unexpected error retrieving index; Err: %v"kv/index"kv/index"name already exists"name already exists" UnexpectedIndexError is used when the error comes from an internal system. NotUniqueError is used when attempting to create a resource that already exists./Users/austinjaybecker/projects/abeck-go-testing/kv/index.goforeignKeynewKeyprimaryKeyfkpkvisitFnikcontindexCursorcorruptfkmincludeMissingSourcepkssourceKVsmalformed index key"malformed index key" Index is used to define and manage an index for a source bucket. When using the index you must provide it with an IndexMapping. The IndexMapping provides the index with the contract it needs to populate the entire index and traverse a populated index correctly. The IndexMapping provides a way to retrieve the key on which to index with when provided with the value from the source. It also provides the way to access the source bucket. The following is an illustration of its use:  byUserID := func(v []byte) ([]byte, error) {      auth := &influxdb.Authorization{}      if err := json.Unmarshal(v, auth); err != nil {          return err      }      return auth.UserID.Encode()  }  // configure a write only index  indexByUser := NewIndex(NewSource([]byte(`authorizationsbyuserv1/), byUserID))  indexByUser.Insert(tx, someUserID, someAuthID)  indexByUser.Delete(tx, someUserID, someAuthID)  indexByUser.Walk(tx, someUserID, func(k, v []byte) error {      // do something with auth      return nil  })  // verify the current index against the source and return the differences  // found in each  diff, err := indexByUser.Verify(ctx, tx) canRead configures whether or not Walk accesses the index at all or skips the index altogether and returns nothing. This is used when you want to integrate only the write path before releasing the read path. IndexOption is a function which configures an index WithIndexReadPathEnabled enables the read paths of the index (Walk) This should be enabled once the index has been fully populated and the Insert and Delete paths are correctly integrated. IndexMapping is a type which configures and Index to map items from a source bucket to an index bucket via a mapping known as IndexSourceOn. This function is called on the values in the source to derive the foreign key on which to index each item. IndexSourceOnFunc is a function which can be used to derive the foreign key of a value in a source bucket. NewIndexMapping creates an implementation of IndexMapping for the provided source bucket to a destination index bucket. NewIndex configures and returns a new *Index for a given index mapping. By default the read path (Walk) is disabled. This is because the index needs to be fully populated before depending upon the read path. The read path can be enabled using WithIndexReadPathEnabled option. this function is called with items missing in index parts are fk/pk Insert creates a single index entry for the provided primary key on the foreign key. Delete removes the foreignKey and primaryKey mapping from the underlying index. Walk walks the source bucket using keys found in the index using the provided foreign key given the index has been fully populated. skip walking if configured to do so as the index is currently being used purely to write the index indexWalk consumes the indexKey and primaryKey pairs in the index bucket and looks up their associated primaryKey's value in the provided source bucket. When an item is located in the source, the provided visit function is called with primary key and associated value. IndexDiff contains a set of items present in the source not in index, along with a set of things in the index which are not in the source. PresentInIndex is a map of foreign key to primary keys present in the index. MissingFromIndex is a map of foreign key to associated primary keys missing from the index given the source bucket. These items could be due to the fact an index populate migration has not yet occurred, the index populate code is incorrect or the write path for your resource type does not yet insert into the index as well (Create actions). MissingFromSource is a map of foreign key to associated primary keys missing from the source but accounted for in the index. This happens when index items are not properly removed from the index when an item is removed from the source (Delete actions). Corrupt returns a list of foreign keys which have corrupted indexes (partial) These are foreign keys which map to a subset of the primary keys which they should be associated with. Verify returns the difference between a source and its index The difference contains items in the source that are not in the index and vice-versa. pks is a map of primary keys in source look for items missing from index this is only useful for missing source look for items missing from source indexReadAll returns the entire current state of the index consumeBucket consumes the entire k/v space for the provided bucket function applied to the provided store/Users/austinjaybecker/projects/abeck-go-testing/kv/index_migration.goadd index %q"add index %q"migration (up) %s: %w"migration (up) %s: %w"migration (down) %s: %w"migration (down) %s: %w"looking up missing indexes: %w"looking up missing indexes: %w"updating index: %w"updating index: %w"removing dangling foreign keys: %w"removing dangling foreign keys: %w" defaultIndexMigrationOpBatchSize configures the size of batch operations done by the index migration when populating or removing items from an entire index IndexMigration is a migration for adding and removing an index. These are constructed via the Index.Migration function. IndexMigrationOption is a functional option for the IndexMigration type WithIndexMigationBatchSize configures the size of the batches when committing changes to entire index during migration (e.g. size of put batch on index populate). WithIndexMigrationCleanup removes index entries which point to missing items in the source bucket. NewIndexMigration construct a migration for creating and populating an index Name returns a readable name for the index migration. Up initializes the index bucket and populates the index. Down deletes all entries from the index. Populate does a full population of the index using the IndexSourceOn IndexMapping function. Once completed it marks the index as ready for use. It return a nil error on success and the count of inserted items. verify the index to derive missing index we can skip missing source lookup as we're only interested in populating the missing index insert missing item into index delete dangling foreign key/Users/austinjaybecker/projects/abeck-go-testing/kv/initial_migration.goinitial migration"initial migration"bucketindexv1"bucketindexv1"labelsv1"labelsv1"labelmappingsv1"labelmappingsv1"labelindexv1"labelindexv1"onboardingv1"onboardingv1"organizationindexv1"organizationindexv1"userspasswordv1"userspasswordv1"secretsv1"secretsv1"userresourcemappingsv1"userresourcemappingsv1"notificationRulev1"notificationRulev1"userindexv1"userindexv1"templates/documents/content"templates/documents/content"templates/documents/meta"templates/documents/meta"notificationEndpointv1"notificationEndpointv1"notificationEndpointIndexv1"notificationEndpointIndexv1"sessionsv1"sessionsv1" MigrationName returns the string initial migration which allows this store to be used as a migration Up initializes all the owned buckets of the underlying store please do not initialize anymore buckets here add them as a new migration to the list of migrations defined in NewInitialMigration. these are the "document" (aka templates) key prefixes store base backed services deprecated: removed in later migration seed initial sources (default source) Down is a no operation required for service to be used as a migration/Users/austinjaybecker/projects/abeck-go-testing/kv/kvlog.goboundsstartKeystopKeysha1crypto/sha1"crypto/sha1"keyvaluelogv1"keyvaluelogv1"keyvaluelogindexv1"keyvaluelogindexv1"oplog not found"oplog not found"log entry not found"log entry not found" ErrKeyValueLogBoundsNotFound is returned when oplog entries cannot be located for the provided bounds StartTime retrieves the start value of a bounds as a time.Time StopTime retrieves the stop value of a bounds as a time.Time Bounds returns the key boundaries for the keyvaluelog for a resourceType/resourceID pair. This needs to be big-endian so that the iteration order is preserved when scanning keys keys produced must be fixed length to ensure that we can iterate through the keyspace without any error. retrieve the keyValue log boundaries if the bounds don't exist yet, create them update the bounds to if needed ForEachLogEntry retrieves the keyValue log for a resource type ID combination. KeyValues may be returned in ascending and descending order. Skip offset many items if we've reached the stop key, there are no keys log entries left in the keyspace. AddLogEntry logs an keyValue for a particular resource type ID pairing. FirstLogEntry retrieves the first log entry for a key value log. LastLogEntry retrieves the first log entry for a key value log./Users/austinjaybecker/projects/abeck-go-testing/kv/migration/Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/0001_initial_migration.goMigration0001_InitialMigrationMigration0002_AddURMByUserIndexMigration0003_TaskOwnerIDUpMigrationMigration0004_AddDbrpBucketsMigration0005_AddPkgerBucketsMigration0006_DeleteBucketSessionsv1Migration0007_CreateMetaDataBucketMigration0008_LegacyAuthBucketsMigration0009_LegacyAuthPasswordBucketsMigration0010_AddIndexTelegrafByOrgMigration0011_PopulateDashboardsOwnerIdMigrationFuncUpOnlyMigrationdbrpBucketdbrpDefaultBucketdbrpIndexBucketnoopMigrationpkgerStackIndexBucketpkgerStacksBucket Migration0001_InitialMigration contains all the buckets created before the time of migrations in kv/Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/0002_urm_by_user_index.gogithub.com/influxdata/influxdb/v2/tenant/index"github.com/influxdata/influxdb/v2/tenant/index"URMByUserIndexMapping Migration0002_AddURMByUserIndex creates the URM by user index and populates missing entries based on the source./Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/0003_task_owners.goauthTypetaskBytesownerlessTaskstasksv1"tasksv1"migrate task owner id"migrate task owner id"AuthorizationIDjson:"authorizationID"`json:"authorizationID"`could not populate owner ID for task"could not populate owner ID for task" Migration0003_TaskOwnerIDUpMigration adds missing owner IDs to some legacy tasks loop through the tasks and collect a set of tasks that are missing the owner id. loop through tasks open transaction try populating the owner from auth try populating owner from urm if population fails return error save task/Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/0004_add_dbrp_buckets.goCreateBucketscreate DBRP buckets"create DBRP buckets" Migration0004_AddDbrpBuckets creates the buckets necessary for the DBRP Service to operate./Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/0005_add_pkger_buckets.gov1_pkger_stacks"v1_pkger_stacks"v1_pkger_stacks_index"v1_pkger_stacks_index"create pkger stacks buckets"create pkger stacks buckets" Migration0005_AddPkgerBuckets creates the buckets necessary for the pkger service to operate./Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/0006_delete-bucket-sessionsv1.goDeleteBucketsdelete sessionsv1 bucket"delete sessionsv1 bucket" Migration0006_DeleteBucketSessionsv1 removes the sessionsv1 bucket from the backing kv store./Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/0007_CreateMetaDataBucket.goCreate TSM metadata buckets"Create TSM metadata buckets"/Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/0008_LegacyAuthBuckets.goCreate Legacy authorization buckets"Create Legacy authorization buckets"legacy/authorizationsv1"legacy/authorizationsv1"legacy/authorizationindexv1"legacy/authorizationindexv1"/Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/0009_LegacyAuthPasswordBuckets.goCreate legacy auth password bucket"Create legacy auth password bucket"legacy/authorizationPasswordv1"legacy/authorizationPasswordv1"/Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/0010_add-index-telegraf-by-org.gogithub.com/influxdata/influxdb/v2/telegraf"github.com/influxdata/influxdb/v2/telegraf"ByOrganizationIndexMapping Migration0010_AddIndexTelegrafByOrg adds the index telegraf configs by organization ID/Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/0011_populate-dashboards-owner-id.goupdateddashboardsBucketurmBucketuserResourceMappingpopulate dashboards owner id"populate dashboards owner id"json:"userType"`json:"userType"`json:"mappingType"`json:"mappingType"`json:"resourceType"`json:"resourceType"`json:"resourceID"`json:"resourceID"`dashboard %q already has owner %q"dashboard %q already has owner %q" Migration0011_PopulateDashboardsOwnerId backfills owner IDs on dashboards based on the presence of user resource mappings collect all dashboard mappings we're interesting in dashboard owners dashboard represents all visual and query data for a dashboard. update bucket owner to owner dashboard urm mapping user target update bucket entry/Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/all.go Migrations contains all the migrations required for the entire of the kv store backing influxdb's metadata. initial migrations add index user resource mappings by user id add index for tasks with missing owner IDs add dbrp buckets add pkger buckets delete bucket sessionsv1 CreateMetaDataBucket LegacyAuthBuckets LegacyAuthPasswordBuckets add index telegraf by org populate dashboards owner id {{ do_not_edit . }}/Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/doc.go package all This package is the canonical location for all migrations being made against the single shared kv.Store implementation used by InfluxDB (while it remains a single store). The array all.Migrations contains the list of migration specifications which drives the serial set of migration operations required to correctly configure the backing metadata store for InfluxDB. This package is arranged like so:  doc.go - this piece of documentation.  all.go - definition of Migration array referencing each of the name migrations in number migration files (below).  migration.go - an implementation of migration.Spec for convenience.  000X_migration_name.go (example) - N files contains the specific implementations of each migration enumerated in `all.go`.  ... Managing this list of files and all.go can be fiddly. There is a buildable cli utility called `kvmigrate` in the `internal/cmd/kvmigrate` package. This has a command `create` which automatically creates a new migration in the expected location and appends it appropriately into the all.go Migration array./Users/austinjaybecker/projects/abeck-go-testing/kv/migration/all/migration.go Up is a convenience methods which creates a migrator for all migrations and calls Up on it. MigrationFunc is a function which can be used as either an up or down operation. Migration is a type which implements the migration packages Spec interface It can be used to conveniently create migration specs for the all package UpOnlyMigration is a migration with an up function and a noop down function MigrationName returns the underlying name of the migation Up delegates to the underlying anonymous up migration function Down delegates to the underlying anonymous down migration function/Users/austinjaybecker/projects/abeck-go-testing/kv/migration/buckets.goBucketsMigrationDownMigrationStateErrMigrationSpecNotFoundUpMigrationStatebucketMigrationTypecreateBucketMigrationdeleteBucketMigrationmigrationBucketnewMigrationFmtextraBucketsunrecognized buckets migration type"unrecognized buckets migration type" BucketsMigration is a migration Spec which creates the provided list of buckets on a store when Up is called and deletes them on Down. CreateBuckets returns a new BucketsMigration Spec. DeleteBuckets returns a new BucketsMigration Spec. MigrationName returns the name of the migration. Up creates the buckets on the store. Down delets the buckets on the store./Users/austinjaybecker/projects/abeck-go-testing/kv/migration/create.gocamelNamenewMigrationFilenewMigrationNumbernewMigrationVariabletmplDatago/format"go/format"package all

var %s = &Migration{}
`package all

var %s = &Migration{}
`Migration%04d_%s"Migration%04d_%s"./kv/migration/all/%04d_%s.go"./kv/migration/all/%04d_%s.go"Creating new migration:"Creating new migration:"Inserting migration into ./kv/migration/all/all.go"Inserting migration into ./kv/migration/all/all.go"./kv/migration/all/all.go"./kv/migration/all/all.go"Mustdo_not_edit"do_not_edit"%s
%s,
// {{ do_not_edit . }}"%s\n%s,\n// {{ do_not_edit . }}" CreateNewMigration persists a new migration file in the appropriate location and updates the appropriate all.go list of migrations/Users/austinjaybecker/projects/abeck-go-testing/kv/migration/doc.go package migration This package contains utility types for composing and running schema and data migrations in a strictly serial and ordered nature; against a backing kv.SchemaStore implementation. The goal is provide a mechanism to ensure an ordered set of changes are applied once and only once to a persisted kv store. To ensure we can make guarantees from one migration to the next, based on the mutations of the previous migrations. The package offers the `Migrator` type which takes a slice of `Spec` implementations. A spec is a single migration definition, which exposes a name, up and down operations expressed as an Up and Down function on the Spec implementation. The `Migrator` on a call to `Up(ctx)` applies these defined list of migrations respective `Up(...)` functions on a `kv.SchemaStore` in order and persists their invocation on the store in a reserved Bucket `migrationsv1`. This is to ensure the only once invocation of the migration takes place and allows to the resuming or introduction of new migrations at a later date. This means the defined list needs to remain static from the point of application. Otherwise an error will be raised. This package also offer utilities types for quickly defining common changes as specifications. For example creating buckets, when can be quickly constructed via `migration.CreateBuckets("create buckets ...", []byte("foo"), []byte{"bar"})`. As of today all migrations be found in a single defintion in the sub-package to this one named `all` (see `kv/migration/all/all.go`). The `migration.CreateNewMigration()` method can be used to manipulate this `all.go` file in the package and quickly add a new migration file to be populated. This is accessible on the command line via the `internal/cmd/kvmigrate` buildable go tool. Try `go run internal/cmd/kvmigrate/main.go`./Users/austinjaybecker/projects/abeck-go-testing/kv/migration/migration.gomigrationsLenmiglastMigrationeventmigrationsv1"migrationsv1"migration specification not found"migration specification not found""down""up"json:"started_at"`json:"started_at"`json:"finished_at,omitempty"`json:"finished_at,omitempty"`up: %w"up: %w"started"started"completed"completed"down: %w"down: %w"Migration %q %s (%s)"Migration %q %s (%s)"decoding migration id: %w"decoding migration id: %w"migration %q: %w"migration %q: %w"expected migration %q, found %q"expected migration %q, found %q"reading migrations: %w"reading migrations: %w" ErrMigrationSpecNotFound is returned when a migration specification is missing for an already applied migration. MigrationState is a type for describing the state of a migration. DownMigrationState is for a migration not yet applied. UpMigration State is for a migration which has been applied. String returns a string representation for a migration state. Migration is a record of a particular migration. Spec is a specification for a particular migration. It describes the name of the migration and up and down operations needed to fulfill the migration. Migrator is a type which manages migrations. It takes a list of migration specifications and undo (down) all or apply (up) outstanding migrations. It records the state of the world in store under the migrations bucket. NewMigrator constructs and configures a new Migrator. create migration bucket if it does not exist AddMigrations appends the provided migration specs onto the Migrator. List returns a list of migrations and their states within the provided store. Up applies each outstanding migration in order. Migrations are applied in order from the lowest indexed migration in a down state. For example, given: 0001 add bucket foo         | (up) 0002 add bucket bar         | (down) 0003 add index "foo on baz" | (down) Up would apply migration 0002 and then 0003. we're interested in the last up migration Down applies the down operation of each currently applied migration. Migrations are applied in reverse order from the highest indexed migration in a down state. 0002 add bucket bar         | (up) Down would call down() on 0002 and then on 0001./Users/austinjaybecker/projects/abeck-go-testing/kv/scrapers.goscraper target is not found"scraper target is not found"unexpected error retrieving scrapers bucket"unexpected error retrieving scrapers bucket"kv/scraper"kv/scraper"Unknown internal scraper data error; Err: %v"Unknown internal scraper data error; Err: %v"unable to convert scraper target into JSON; Err %v"unable to convert scraper target into JSON; Err %v" ErrScraperNotFound is used when the scraper configuration is not found. ErrInvalidScraperID is used when the service was provided an invalid ID format. ErrInvalidScrapersBucketID is used when the service was provided ErrInvalidScrapersOrgID is used when the service was provided UnexpectedScrapersBucketError is used when the error comes from an internal system. CorruptScraperError is used when the config cannot be unmarshalled from the bytes stored in the kv. ErrUnprocessableScraper is used when a scraper is not able to be converted to JSON. InternalScraperServiceError is used when the error comes from an internal system. ListTargets will list all scrape targets. AddTarget add a new scraper target into storage. RemoveTarget removes a scraper target from the bucket. UpdateTarget updates a scraper target. If the bucket or org are invalid, just use the ids from the original. GetTargetByID retrieves a scraper target by id. PutTarget will put a scraper target without setting an ID. unmarshalScraper turns the stored byte slice in the kv into a *influxdb.ScraperTarget./Users/austinjaybecker/projects/abeck-go-testing/kv/service.go"github.com/benbjohnson/clock"github.com/influxdata/influxdb/v2/resource"github.com/influxdata/influxdb/v2/resource"github.com/influxdata/influxdb/v2/resource/noop"github.com/influxdata/influxdb/v2/resource/noop"kv/"kv/"ResourceLogger OpPrefix is the prefix for kv errors. Service is the struct that influxdb services are implemented on. FluxLanguageService is used for parsing flux. If this is unset, operations that require parsing flux TODO(desa:ariel): this should not be embedded NewService returns an instance of a Service. ServiceConfig allows us to configure Services WithResourceLogger sets the resource audit logger for the service. WithStore sets kv store for the service. Should only be used in tests for mocking./Users/austinjaybecker/projects/abeck-go-testing/kv/source.gosrsourcesv1"sourcesv1"cannot delete autogen source"cannot delete autogen source" CreateSource creates a influxdb source and sets s.ID. Generating an organization id if it missing or invalid forEachSource will iterate through all sources while fn returns true. UpdateSource updates a source according the parameters set on upd. DeleteSource deletes a source and prunes it from the index./Users/austinjaybecker/projects/abeck-go-testing/kv/store.gohintshinttransaction is not writable"transaction is not writable"seek missing prefix bytes"seek missing prefix bytes" ErrKeyNotFound is the error returned when the key requested is not found. ErrBucketNotFound is the error returned when the bucket cannot be found. ErrTxNotWritable is the error returned when an mutable operation is called during a non-writable transaction. ErrSeekMissingPrefix is returned when seek bytes is missing the prefix defined via WithCursorPrefix IsNotFound returns a boolean indicating whether the error is known to report that a key or was not found. SchemaStore is a superset of Store along with store schema change functionality like bucket creation and deletion. This type is made available via the `kv/migration` package. It should be consumed via this package to create and delete buckets using a migration. Checkout the internal tool `cmd/internal/kvmigrate` for building a new migration Go file into the correct location (in kv/migration/all.go). Configuring your bucket here will ensure it is created properly on initialization of InfluxDB. CreateBucket creates a bucket on the underlying store if it does not exist DeleteBucket deletes a bucket on the underlying store if it exists Store is an interface for a generic key value store. It is modeled after the boltdb database struct. Backup copies all K:Vs to a writer, file format determined by implementation. Restore replaces the underlying data file with the data from r. Tx is a transaction in the store. Bucket possibly creates and returns bucket, b. Context returns the context associated with this Tx. WithContext associates a context with this Tx. CursorHint configures CursorHints WithCursorHintPrefix is a hint to the store that the caller is only interested keys with the specified prefix. WithCursorHintKeyStart is a hint to the store that the caller is interested in reading keys from start. WithCursorHintPredicate is a hint to the store to return only key / values which return true for the f. The primary concern of the predicate is to improve performance. Therefore, it should perform tests on the data at minimal cost. If the predicate has no meaningful impact on reducing memory or CPU usage, there is no benefit to using it. Bucket is the abstraction used to perform get/put/delete/get-many operations in a key value store. TODO context? Get returns a key within this bucket. Errors if key does not exist. GetBatch returns a corresponding set of values for the provided set of keys. If a value cannot be found for any provided key its value will be nil at the same index for the provided key. Cursor returns a cursor at the beginning of this bucket optionally using the provided hints to improve performance. Put should error if the transaction it was called in is not writable. Delete should error if the transaction it was called in is not writable. ForwardCursor returns a forward cursor from the seek position provided. Other options can be supplied to provide direction and hints. Cursor is an abstraction for iterating/ranging through data. A concrete implementation of a cursor can be found in cursor.go. Seek moves the cursor forward until reaching prefix in the key name. First moves the cursor to the first key in the bucket. Last moves the cursor to the last key in the bucket. Next moves the cursor to the next key in the bucket. Prev moves the cursor to the prev key in the bucket. ForwardCursor is an abstraction for interacting/ranging through data in one direction. Err returns non-nil if an error occurred during cursor iteration. This should always be checked after Next returns a nil key/value. Close is reponsible for freeing any resources created by the cursor. CursorDirection is an integer used to define the direction a request cursor operates in. CursorAscending directs a cursor to range in ascending order CursorAscending directs a cursor to range in descending order CursorConfig is a type used to configure a new forward cursor. It includes a direction and a set of hints NewCursorConfig constructs and configures a CursorConfig used to configure a forward cursor. CursorOption is a functional option for configuring a forward cursor WithCursorDirection sets the cursor direction on a provided cursor config WithCursorHints configs the provided hints on the cursor config WithCursorPrefix configures the forward cursor to retrieve keys with a particular prefix. This implies the cursor will start and end at a specific location based on the prefix [prefix, prefix + 1). The value of the seek bytes must be prefixed with the provided prefix, otherwise an error will be returned. WithCursorSkipFirstItem skips returning the first item found within the seek. WithCursorLimit restricts the number of key values return by the cursor to the provided limit count. VisitFunc is called for each k, v byte slice pair from the underlying source bucket which are found in the index bucket for a provided foreign key. WalkCursor consumers the forward cursor call visit for each k/v pair found/Users/austinjaybecker/projects/abeck-go-testing/kv/store_base.gokeyRepeatnameEnccaseSensitivedecToEntFnencBodyFnencKeyFndeleteFnfindOptsiErrencodedvRawno ID provided"no ID provided"Unique Key"Unique Key"no unique key provided"no unique key provided"entity body"entity body"%s is not unique"%s is not unique"unexpected error retrieving bucket %q; Err %v"unexpected error retrieving bucket %q; Err %v"failed to retrieve cursor"failed to retrieve cursor"%s does exist for key: %q"%s does exist for key: %q"%s not found for key %q"%s not found for key %q"failed to decode %s body"failed to decode %s body"no key was provided for %s"no key was provided for %s"provided %s %s is an invalid format"provided %s %s is an invalid format""Resource"unexpected value decoded"unexpected value decoded" EncodeEntFn encodes the entity. This is used both for the key and vals in the store base. EncIDKey encodes an entity into a key that represents the encoded ID provided. EncUniqKey encodes the unique key. EncBodyJSON JSON encodes the entity body and returns the raw bytes and indicates that it uses the entity body. DecodeBucketValFn decodes the raw []byte. DecIndexID decodes the bucket val into an influxdb.ID. ConvertValToEntFn converts a key and decoded bucket value to an entity. DecodeOrgNameKey decodes a raw bucket key into the organization id and name used to create it. NewOrgNameKeyStore creates a store for an entity's unique index on organization id and name. This is used throughout the kv pkg here to provide an entity uniquness by name within an org. StoreBase is the base behavior for accessing buckets in kv. It provides mechanisms that can be used in composing stores together (i.e. IndexStore). NewStoreBase creates a new store base. EntKey returns the key for the entity provided. This is a shortcut for grabbing the EntKey without having to juggle the encoding funcs. DeleteOpts provides indicators to the store.Delete call for deleting a given entity. The FilterFn indicates the current value should be deleted when returning true. DeleteRelationsFn is a hook that a store that composes other stores can use to delete an entity and any relations it may share. An example would be deleting an an entity and its associated index. Delete deletes entities by the provided options. DeleteEnt deletes an entity. FindOpts provided a means to search through the bucket. When a filter func is provided, that will run against the entity and if the filter responds true, will count it towards the number of entries seen and the capture func will be run with it provided to it. FindCaptureFn is the mechanism for closing over the key and decoded value pair for adding results to the call sites collection. This generic implementation allows it to be reused. The returned decodedVal should always satisfy whatever decoding of the bucket value was set on the storeo that calls Find. FilterFn will provide an indicator to the Find or Delete calls that the entity that was seen is one that is valid and should be either captured or deleted (depending on the caller of the filter func). Find provides a mechanism for looking through the bucket via the set options. When a prefix is provided, the prefix is used to seek the bucket. FindEnt returns the decoded entity body via the provided entity. An example entity should not include a Body, but rather the ID, Name, or OrgID. TODO: fix this error up PutOptionFn provides a hint to the store to make some guarantees about the put action. I.e. If it is new, then will validate there is no existing entity by the given PK. PutNew will create an entity that is not does not already exist. Guarantees uniqueness by the store's uniqueness guarantees. PutUpdate will update an entity that must already exist. Put will persist the entity. ignore key here increase counter here since the entity is a valid ent and counts towards the total the user is looking for 	i.e. limit = 5 => 5 valid ents	i.e. offset = 5 => return valid ents after seeing 5 valid ents/Users/austinjaybecker/projects/abeck-go-testing/kv/store_index.godeleteIndexedRelationFndecodedEntidxErridxEncodedIDindexEntexistingEntexistingValidxKeyidxValkey1key2pk1pk2ierrorsno key was provided for "no key was provided for "%s is not unique for key %s"%s is not unique for key %s"failed to encode PK"failed to encode PK"failed to convert value"failed to convert value"%s does not exist for key %s"%s does not exist for key %s"%s entity update conflicts with an existing entity for key %s"%s entity update conflicts with an existing entity for key %s"keys differ"keys differ" IndexStore provides a entity store that uses an index lookup. The index store manages deleting and creating indexes for the caller. The index is automatically used if the FindEnt entity entity does not have the primary key. Delete deletes entities and associated indexes. DeleteEnt deletes an entity and associated index. the set options. When a prefix is provided, it will be used within the entity store. If you would like to search the index store, then you can by calling the index store directly. FindEnt returns the decoded entity body via teh provided entity. Name, or OrgID. If no ID is provided, then the algorithm assumes you are looking up the entity by the index. Put will persist the entity into both the entity store and the index store. first check to make sure the existing entity exists in the ent store we need to cleanup the unique key entry when this is deemed a valid update/Users/austinjaybecker/projects/abeck-go-testing/kv/task.goorgFilteruserAuthmatchFnorgKeyupdatedAtTrunctlculclastCompletedKeyrunBucketrtnparsedFilterAfterTimeparsedFilterBeforeTimerunBytesrunKeyrunsBytesrunAtmRunBytesmRunsmRunsBytesrunsKeylatestFailurelatestSuccessscheduledencodedOrgIDencodedRunIDtaskRunsv1"taskRunsv1"taskIndexsv1"taskIndexsv1"json:"latestSuccess,omitempty"`json:"latestSuccess,omitempty"`json:"latestFailure,omitempty"`json:"latestFailure,omitempty"`finding tasks by organization ID: %w"finding tasks by organization ID: %w"organization required"organization required""failed""canceled""manualRuns""latestCompleted"/latestCompleted"/latestCompleted"/manualRuns"/manualRuns"FromScriptASTFromScript Task Storage Schema taskBucket:   <taskID>: task data storage taskRunBucket:   <taskID>/<runID>: run data storage   <taskID>/manualRuns: list of runs to run manually   <taskID>/latestCompleted: run data for the latest completed run of a task taskIndexBucket   <orgID>/<taskID>: index for tasks by org We may want to add a <taskName>/<taskID> index to allow us to look up tasks by task name. findTaskByID is an internal method used to do any action with tasks internally that do not require authorization. complain about limits if no user or organization is passed, assume contexts auth is the user we are looking for. it is possible for a  internal system to call this with no auth so we shouldnt fail if no auth is found. filter by user id. findTasksByUser is a subset of the find tasks function. Used for cleanliness findTasksByOrg is a subset of the find tasks function. Used for cleanliness we can filter by orgID free cursor resources we might have some crufty index's If the new task doesn't belong to the org we have looped outside the org filter Check if we are over running the limit newTaskMatchFn returns a function for validating a task matches the filter. Will return nil if the filter should match all tasks. findAllTasks is a subset of the find tasks function. Used for cleanliness. This function should only be executed internally because it doesn't force organization or user filtering. Enforcing filters should be done in a validation layer. The owner of the task is inferred from the authorizer associated with ctx. TODO: Uncomment this once the checks/notifications no longer create tasks in kv confirm the owner is a real user. if _, err = s.findUserByID(ctx, tx, tc.OwnerID); err != nil { 	return nil, influxdb.ErrInvalidOwnerID write the task write the org index retrieve the task update the flux script task is transitioning from inactive to active, ensure scheduled and completed are updated make sure we only update latest completed one way make sure we only update latest scheduled one way make sure we only update latest success one way make sure we only update latest failure one way save the updated task remove the orgs index remove latest completed remove the runs remove the task manual runs append currently running FindRunByID returns a single run. CancelRun cancels a currently running run. get the run set status to canceled save find the run add a clean copy of the run to the manual runs save manual runs ForceRun forces a run to occur with unix timestamp scheduledFor, to be executed as soon as possible. The value of scheduledFor may or may not align with the task's schedule. create a run check to see if this run is already queued CreateRun creates a run with a scheduledFor time as now. if the run no longer belongs to the task we are done add mRun to the list of currently running FinishRun removes runID from the list of running tasks and if its `now` is later then last completed update it. tell task to update latest completed prefer the second to last log message as the error message per https://github.com/influxdata/influxdb/issues/15153#issuecomment-547706005 remove run UpdateRunState sets the run state at the respective time. find run update state save run AddRunLog adds a log line to the run. update log ExtractTaskOptions is a feature-flag driven switch between normal options parsing and a more simplified variant. The simplified variant extracts the options assignment and passes only that content through the parser. This allows us to allow scenarios like [1] to pass through options validation. One clear drawback of this is that it requires constant values for the parameter assignments. However, most people are doing that anyway. [1]: https://github.com/influxdata/influxdb/issues/17666/Users/austinjaybecker/projects/abeck-go-testing/kv/variable.goentitydecodeVarEntFnputOptsmIDvariablesv1"variablesv1"variablesindexv1"variablesindexv1"variableorgsv1"variableorgsv1"malformed variable orgs index key (please report this error)"malformed variable orgs index key (please report this error)"bad variable id"bad variable id"bad organization id"bad organization id" TODO: eradicate this with migration strategy TODO(jsteenb2): investigate why we don't implement the find options for vars? FindVariables returns all variables in the store FindVariableByID finds a single variable in the store by its ID CreateVariable creates a new variable and assigns it an ID ReplaceVariable replaces a variable that exists in the store or creates it if it does not UpdateVariable updates a single variable in the store with a changeset TODO: should be moved to service layer DeleteVariable removes a single variable from the store by its ID/Users/austinjaybecker/projects/abeck-go-testing/label/Users/austinjaybecker/projects/abeck-go-testing/label/error.godecodeLabelMappingKeyfilterLabelsFnforEachLabellabelAlreadyExistsErrorlabelBucketlabelIndexlabelIndexKeylabelMappingBucketlabelMappingKeyuniqueLabelNamelabel not found"label not found" NotUniqueIDError occurs when attempting to create a Label with an ID that already belongs to another one ErrLabelNotFound occurs when a label cannot be found by its ID/Users/austinjaybecker/projects/abeck-go-testing/label/http_client.go ******* Label Mappings ******* // CreateLabelMapping will create a label mapping/Users/austinjaybecker/projects/abeck-go-testing/label/http_handler.goembeddedID/{labelID}"/{labelID}" NewHTTPEmbeddedHandler create a label handler for embedding in other service apis handlePostLabelMapping create a new label mapping for the host service api handleFindResourceLabels list labels that reference the host api handleDeleteLabelMapping delete a mapping for this host and label combination/Users/austinjaybecker/projects/abeck-go-testing/label/http_server.go/Users/austinjaybecker/projects/abeck-go-testing/label/middleware_auth.go NewAuthedLabelService constructs an instance of an authorizing label serivce. first fetch all labels for this resource then filter the labels we got to return only the ones the user is authorized to read/Users/austinjaybecker/projects/abeck-go-testing/label/middleware_logging.gofailed to create label"failed to create label"label create"label create"failed to find label with ID %v"failed to find label with ID %v"label find by ID"label find by ID"failed to find labels matching the given filter"failed to find labels matching the given filter"labels find"labels find"failed to find resource labels matching the given filter"failed to find resource labels matching the given filter"labels for resource find"labels for resource find"failed to update label"failed to update label"label update"label update"failed to delete label"failed to delete label"label delete"label delete"failed to create label mapping"failed to create label mapping"label mapping create"label mapping create"failed to delete label mapping"failed to delete label mapping"label mapping delete"label mapping delete"/Users/austinjaybecker/projects/abeck-go-testing/label/middleware_metrics.gocreate_label"create_label"find_label_by_id"find_label_by_id"find_labels"find_labels"find_labels_for_resource"find_labels_for_resource"update_label"update_label"delete_label"delete_label"create_label_mapping"create_label_mapping"delete_label_mapping"delete_label_mapping"/Users/austinjaybecker/projects/abeck-go-testing/label/service.go FindLabelByID finds a label by its ID FindLabels returns a list of labels that match a filter. UpdateLabel updates a label. DeleteLabel deletes a label.******* Label Mappings *******// CreateLabelMapping creates a new mapping between a resource and a label. DeleteLabelMapping deletes a label mapping.******* helper functions *******// if not found then this is  _unique_. labels are unique by `organization:label_name`/Users/austinjaybecker/projects/abeck-go-testing/label/storage.go/Users/austinjaybecker/projects/abeck-go-testing/label/storage_label.goidErrfilter requires a valid resource id"filter requires a valid resource id"label with name %s already exists"label with name %s already exists"malformed label mapping key (please report this error)"malformed label mapping key (please report this error)"bad resource id"bad resource id"bad label id"bad label id"********* Label Mappings *********// TODO(jm): return error instead of continuing once orphaned mappings are fixed (see https://github.com/influxdata/influxdb/issues/11278)********* helper functions *********// len(rid) + len(lid) labelAlreadyExistsError is used when creating a new label with a name that has already been used. Label names must be unique./Users/austinjaybecker/projects/abeck-go-testing/label.go"FindLabels""FindLabelByID"FindLabelMapping"FindLabelMapping""CreateLabel""CreateLabelMapping""UpdateLabel""DeleteLabel""DeleteLabelMapping"label name is empty"label name is empty"Cannot add label, label already exists on resource"Cannot add label, label already exists on resource"label name is required"label name is required"orgID is required"orgID is required"json:"labelID"`json:"labelID"`json:"resourceID,omitempty"`json:"resourceID,omitempty"`label id is required"label id is required"resource id is required"resource id is required" ErrLabelNotFound is the error for a missing Label. errors on label ErrLabelNameisEmpty is error when org name is empty ErrLabelExistsOnResource is used when attempting to add a label to a resource when that label already exists on the resource LabelService represents a service for managing resource labels FindLabelByID a single label by ID. FindLabels returns a list of labels that match a filter FindResourceLabels returns a list of labels that belong to a resource CreateLabel creates a new label CreateLabelMapping maps a resource to an existing label UpdateLabel updates a label with a changeset. DeleteLabel deletes a label DeleteLabelMapping deletes a label mapping Label is a tag set on a resource, typically used for filtering on a UI. Validate returns an error if the label is invalid. LabelMapping is used to map resource to its labels. It should not be shared directly over the HTTP API. Validate returns an error if the mapping is invalid. LabelUpdate represents a changeset for a label. Only the properties specified are updated. LabelFilter represents a set of filters that restrict the returned results. LabelMappingFilter represents a set of filters that restrict the returned results./Users/austinjaybecker/projects/abeck-go-testing/logger/Users/austinjaybecker/projects/abeck-go-testing/logger/config.goDBInstanceKeyDBRetentionKeyDBShardGroupKeyDBShardIDKeyIsTerminalLoggerFromContextNewContextWithLoggerNewOperationOperationElapsedOperationElapsedKeyOperationEventEndOperationEventKeyOperationEventStartOperationNameKeyTraceIDKeyTraceSampledKeyeventEndeventStartloggerKeynewEncodernewEncoderConfigyeartoml:"format"`toml:"format"`toml:"level"`toml:"level"`toml:"suppress-logo"`toml:"suppress-logo"` NewConfig returns a new instance of Config with defaults./Users/austinjaybecker/projects/abeck-go-testing/logger/context.go NewContextWithLogger returns a new context with log added. LoggerFromContext returns the zap.Logger associated with ctx or nil if no logger has been assigned./Users/austinjaybecker/projects/abeck-go-testing/logger/fields.gogithub.com/influxdata/influxdb/v2/pkg/snowflake"github.com/influxdata/influxdb/v2/pkg/snowflake"op_name"op_name"op_event"op_event"op_elapsed"op_elapsed"db_instance"db_instance"db_rp"db_rp"db_shard_group"db_shard_group"db_shard_id"db_shard_id"ot_trace_id"ot_trace_id"ot_trace_sampled"ot_trace_sampled""end"trace_id"trace_id" (start)" (start)" (end)" (end)" OperationNameKey is the logging context key used for identifying name of an operation. OperationEventKey is the logging context key used for identifying a notable event during the course of an operation. OperationElapsedKey is the logging context key used for identifying time elapsed to finish an operation. DBInstanceKey is the logging context key used for identifying name of the relevant database. DBRetentionKey is the logging context key used for identifying name of the relevant retention policy. DBShardGroupKey is the logging context key used for identifying relevant shard group. DBShardIDKey is the logging context key used for identifying name of the relevant shard number. TraceIDKey is the logging context key used for identifying the current trace. TraceSampledKey is the logging context key used for determining whether the current trace will be sampled. OperationName returns a field for tracking the name of an operation. OperationElapsed returns a field for tracking the duration of an operation. OperationEventStart returns a field for tracking the start of an operation. OperationEventFinish returns a field for tracking the end of an operation. Database returns a field for tracking the name of a database. ShardGroup returns a field for tracking the shard group identifier. Shard returns a field for tracking the shard identifier. TraceFields returns a fields "ot_trace_id" and "ot_trace_sampled", values pulled from the (Jaeger) trace ID found in the given context. Returns nil if the context doesn't have a trace ID. TraceID returns a field "trace_id", value pulled from the (Jaeger) trace ID found in the given context. Returns zap.Skip() if the context doesn't have a trace ID. NewOperation uses the exiting log to create a new logger with context containing a trace id and the operation. Prior to returning, a standardized message is logged indicating the operation has started. The returned function should be called when the operation concludes in order to log a corresponding message which includes an elapsed time and that the operation has ended./Users/austinjaybecker/projects/abeck-go-testing/logger/logger.godefaultOutputisattyzaplogfmt"github.com/jsternberg/zap-logfmt""github.com/mattn/go-isatty"2006-01-02T15:04:05.000000Z07:00"2006-01-02T15:04:05.000000Z07:00"console"console"unknown logging format: %s"unknown logging format: %s"logfmt"logfmt"NewCoreAddSynclog_id"log_id"NewJSONEncoderNewConsoleEncoderNewProductionEncoderConfig%.3fms"%.3fms""lvl"%d%s"%d%s" Disallow the console logger if the output is not a terminal. If the format is empty or auto, then set the format depending on whether or not a terminal is present. IsTerminal checks if w is a file and whether it is an interactive terminal session./Users/austinjaybecker/projects/abeck-go-testing/lookup.go LookupService provides field lookup for the resource and ID. FindResourceName returns the name for the resource and ID./Users/austinjaybecker/projects/abeck-go-testing/measurement.gomeasurement %v has invalid length (%d)"measurement %v has invalid length (%d)"org %v has invalid length (%d)"org %v has invalid length (%d)"bucket %v has invalid length (%d)"bucket %v has invalid length (%d)" Length of components of measurement names. ReadMeasurement reads the provided measurement name and returns an Org ID and bucket ID. It returns an error if the provided name has an invalid length. ReadMeasurement does not allocate, and instead returns sub-slices of name, so callers should be careful about subsequent mutations to the provided name slice. CreateMeasurement returns 16 bytes that represent a measurement. If either org or bucket are short then an error is returned, otherwise the first 8 bytes of each are combined and returned./Users/austinjaybecker/projects/abeck-go-testing/mock/Users/austinjaybecker/projects/abeck-go-testing/mock/auth_service.goFirstMockIDGeneratorOptionFnGeneratorResultSetMockIDGeneratorMockPasswordsServiceMockPasswordsServiceMockRecorderNatsPublisherNatsServerNatsSubscriberNewBucketOperationLogServiceNewDBRPMappingServiceNewDashboardOperationLogServiceNewDeleteServiceNewDocumentStoreNewIncrementingIDGeneratorNewLabelServiceNewLookupServiceNewMockIDGeneratorNewMockPasswordsServiceNewNatsNewOnboardingServiceNewOrganizationOperationLogServiceNewOrganizationServiceNewPasswordsServiceNewResultSetFromSeriesGeneratorNewRetentionServiceNewSessionServiceNewStaticIDGeneratorNewTelegrafConfigStoreNewUserOperationLogServiceNewUserResourceMappingServiceSafeCountSetIDForFuncWithGeneratorMaxValuesbooleanTimeValuesGeneratorCursorcopyTagsfloatTimeValuesGeneratorCursorintegerTimeValuesGeneratorCursornatsMessagenatsSubscriptionstringTimeValuesGeneratorCursortimeValuesGeneratorCursorunsignedTimeValuesGeneratorCursorFindAuthorizationByIDFnFindAuthorizationByTokenFnFindAuthorizationsFnCreateAuthorizationFnDeleteAuthorizationFnUpdateAuthorizationFn AuthorizationService is a mock implementation of a retention.AuthorizationService, which also makes it a suitable mock to use wherever an platform.AuthorizationService is required. Methods for a retention.AuthorizationService Methods for an platform.AuthorizationService NewAuthorizationService returns a mock AuthorizationService where its methods will return zero values. FindAuthorizationByID returns a single authorization by ID. DeleteAuthorization removes a authorization by ID.FindOrganizationsFCreateOrganizationFUpdateOrganizationFDeleteOrganizationFFindResourceOrganizationIDFAuthorizeFnLoadSecretFnGetSecretKeysFnPutSecretFnPutSecretsFnPatchSecretsFnDeleteSecretFnSetPasswordFnComparePasswordFnCompareAndSetPasswordFnIncrFnFindTaskByIDFnFindTaskByIDCallsFindTasksFnFindTasksCallsCreateTaskFnCreateTaskCallsUpdateTaskFnUpdateTaskCallsDeleteTaskFnDeleteTaskCallsFindLogsFnFindLogsCallsFindRunsFnFindRunsCallsFindRunByIDFnFindRunByIDCallsCancelRunFnCancelRunCallsRetryRunFnRetryRunCallsForceRunFnForceRunCallsPrometheusCollectorsFnCreateDocumentStoreFnFindDocumentStoreFnFindUsersFnDeleteUserFnFindPermissionForUserFnFindMappingsFnCreateMappingFnDeleteMappingFnFindNotificationRuleByIDFFindNotificationRuleByIDCallsFindNotificationRulesFFindNotificationRulesCallsCreateNotificationRuleFCreateNotificationRuleCallsUpdateNotificationRuleFUpdateNotificationRuleCallsPatchNotificationRuleFPatchNotificationRuleCallsDeleteNotificationRuleFDeleteNotificationRuleCallsGetDashboardOperationLogFnReadFilterFnReadGroupFnReadTagKeysFnReadTagValuesFnReadWindowAggregateFnCreateLabelFnCreateLabelCallsDeleteLabelFnDeleteLabelCallsFindLabelByIDFnFindLabelByIDCallsFindLabelsFnFindLabelsCallsFindResourceLabelsFnFindResourceLabelsCallsUpdateLabelFnUpdateLabelCallsCreateLabelMappingFnCreateLabelMappingCallsDeleteLabelMappingFnDeleteLabelMappingCallsDefaultSourceFnFindSourceByIDFnFindSourcesFnCreateSourceFnUpdateSourceFnDeleteSourceFnGetOrganizationOperationLogFnGetBucketOperationLogFnLookupNameFindByFnFindFnFindManyFnCreateFnDeleteFnGetFnGetBatchFnCursorFnPutFnForwardCursorFnTimeValuesSequencecheckCountGetUserOperationLogFnSeriesGeneratorTimeValuesGeneratorFindBucketByIDCallsFindBucketByNameFnFindBucketByNameCallsFindBucketFnFindBucketCallsFindBucketsFnFindBucketsCallsCreateBucketFnCreateBucketCallsUpdateBucketFnUpdateBucketCallsDeleteBucketFnDeleteBucketCallsNameFnCreateDashboardFCreateDashboardCallsFindDashboardByIDFFindDashboardByIDCallsFindDashboardsFFindDashboardsCallsUpdateDashboardFUpdateDashboardCallsDeleteDashboardFDeleteDashboardCallsAddDashboardCellFAddDashboardCellCallsRemoveDashboardCellFRemoveDashboardCellCallsGetDashboardCellViewFGetDashboardCellViewCallsUpdateDashboardCellViewFUpdateDashboardCellViewCallsUpdateDashboardCellFUpdateDashboardCellCallsCopyDashboardCellFCopyDashboardCellCallsReplaceDashboardCellsFReplaceDashboardCellsCallsCopyDashboardCellinitSubjectSeekFnFirstFnLastFnPrevFnFindCheckByIDFnFindCheckByIDCallsFindCheckFnFindCheckCallsFindChecksFnFindChecksCallsCreateCheckFnCreateCheckCallsUpdateCheckFnUpdateCheckCallsPatchCheckFnPatchCheckCallsDeleteCheckFnDeleteCheckCallsFindNotificationEndpointByIDFFindNotificationEndpointByIDCallsFindNotificationEndpointsFFindNotificationEndpointsCallsCreateNotificationEndpointFCreateNotificationEndpointCallsUpdateNotificationEndpointFUpdateNotificationEndpointCallsPatchNotificationEndpointFPatchNotificationEndpointCallsDeleteNotificationEndpointFDeleteNotificationEndpointCallsBucketFnContextFnWithContextFnDeleteBucketRangePredicateFFindSessionFnExpireSessionFnCreateSessionFnRenewSessionFnViewFnUpdateFnBackupFnRestoreFnCreateDocumentFnFindDocumentFnFindDocumentsFnTokenFnIsOnboardingFnOnboardInitialUserFnOnboardUserFnListTargetsFAddTargetFGetTargetByIDFRemoveTargetFUpdateTargetFCreateVariableFCreateVariableCallsDeleteVariableFDeleteVariableCallsFindVariableByIDFFindVariableByIDCallsFindVariablesFFindVariablesCallsReplaceVariableFReplaceVariableCallsUpdateVariableFUpdateVariableCallsFindByIDFnFindTelegrafConfigByIDFFindTelegrafConfigByIDCallsFindTelegrafConfigsFFindTelegrafConfigsCallsCreateTelegrafConfigFCreateTelegrafConfigCallsUpdateTelegrafConfigFUpdateTelegrafConfigCallsDeleteTelegrafConfigFDeleteTelegrafConfigCallsCreateRunFnCurrentlyRunningFnManualRunsFnStartManualRunFnFinishRunFnUpdateRunStateFnAddRunLogFntimesWriteCalledWritePointsFnForceErrorWritePointsCalledWriteToF/Users/austinjaybecker/projects/abeck-go-testing/mock/authorization.goallowAll"mock" ensure Authorizer implements influxdb.Authorizer Authorizer is an Authorizer for testing that can allow everything or use specific permissions/Users/austinjaybecker/projects/abeck-go-testing/mock/authorizer_v1.go/Users/austinjaybecker/projects/abeck-go-testing/mock/bucket_service.goSystem bucket for task logs"System bucket for task logs" BucketService is a mock implementation of a retention.BucketService, which also makes it a suitable mock to use wherever an platform.BucketService is required. Methods for a retention.BucketService Methods for an platform.BucketService NewBucketService returns a mock BucketService where its methods will return Open opens the BucketService. Close closes the BucketService. FindBucketByName returns a single bucket by name./Users/austinjaybecker/projects/abeck-go-testing/mock/check_service.go CheckService is a mock implementation of a retention.CheckService, which also makes it a suitable mock to use wherever an influxdb.CheckService is required. Methods for an influxdb.CheckService NewCheckService returns a mock CheckService where its methods will return UpdateCheck updates everything except id orgID. PatchCheck updates a single check with changeset. DeleteCheck removes a check by ID./Users/austinjaybecker/projects/abeck-go-testing/mock/dashboard_service.go NewDashboardService returns a mock of DashboardService where its methods will return zero values./Users/austinjaybecker/projects/abeck-go-testing/mock/dbrp_mapping.go/Users/austinjaybecker/projects/abeck-go-testing/mock/delete.go DeleteService is a mock delete server. NewDeleteService returns a mock DeleteService where its methods will returnDeleteBucketRangePredicate calls DeleteBucketRangePredicateF./Users/austinjaybecker/projects/abeck-go-testing/mock/dependencies.gomy-bucket"my-bucket"my-org"my-org" BucketLookup implements the BucketLookup interface needed by flux "from" and "to". OrganizationLookup implements the OrganizationLookup interface needed by flux "from" and "to"./Users/austinjaybecker/projects/abeck-go-testing/mock/document_service.go DocumentService is mocked document service. CreateDocumentStore calls the mocked CreateDocumentStoreFn. FindDocumentStore calls the mocked FindDocumentStoreFn. NewDocumentService returns a mock of DocumentService where its methods will return zero values. DocumentStore is the mocked document store. NewDocumentStore returns a mock of DocumentStore where its methods will return zero values. CreateDocument will call the mocked CreateDocumentFn. FindDocument will call the mocked FindDocumentFn. FindDocuments will call the mocked FindDocumentsFn./Users/austinjaybecker/projects/abeck-go-testing/mock/flagger.go Flagger is a mock. NewFlagger returns a mock Flagger. Flags returns a map of flag keys to flag values according to its configured flag map. It never returns an error./Users/austinjaybecker/projects/abeck-go-testing/mock/generators.go65536 IDGenerator is mock implementation of influxdb.IDGenerator. ID generates a new influxdb.ID from a mock function. NewIDGenerator is a simple way to create immutable id generator NewStaticIDGenerator returns an IDGenerator which produces the ID provided to this function on a call to ID(). NewIncrementingIDGenerator returns an ID generator which starts at the provided ID and increments on each call to ID(). SetIDForFunc replaces the id generator at the end of the pointer with one which returns the provided id. It then invokes the provided function before restoring the original value at the end of the pointer. NewTokenGenerator is a simple way to create immutable token generator. TokenGenerator is mock implementation of influxdb.TokenGenerator. Token generates a new influxdb.Token from a mock function. TimeGenerator stores a fake value of time. Now will return the FakeValue stored in the struct./Users/austinjaybecker/projects/abeck-go-testing/mock/kv.go Store is a mock kv.Store Tx is mock of a kv.Tx. in a key value store GetBatch returns a set of keys values within this bucket. Cursor returns a cursor at the beginning of this bucket. ForwardCursor returns a cursor from the seek points in the configured direction./Users/austinjaybecker/projects/abeck-go-testing/mock/label_service.go LabelService is a mock implementation of platform.LabelService NewLabelService returns a mock of LabelService where its methods will return zero values. FindLabelByID finds mappings by their ID FindLabels finds mappings that match a given filter. FindResourceLabels finds mappings that match a given filter. CreateLabel creates a new Label. CreateLabelMapping creates a new Label mapping. DeleteLabel removes a Label. DeleteLabelMapping removes a Label mapping./Users/austinjaybecker/projects/abeck-go-testing/mock/lookup_service.go NewLookupService returns a mock of LookupService where its methods will return zero values./Users/austinjaybecker/projects/abeck-go-testing/mock/nats.go NatsServer is the mocked nats server based buffered channel. create an empty channel for a subject NewNats returns a mocked version of publisher, subscriber NatsPublisher is a mocked nats publisher. Publish add subject and msg to server. NatsSubscriber is mocked nats subscriber. Subscribe implements nats.Subscriber interface./Users/austinjaybecker/projects/abeck-go-testing/mock/notification_endpoint_service.go NotificationEndpointService represents a service for managing notification rule data. FindNotificationEndpointByID returns a single telegraf config by ID. FindNotificationEndpoints returns a list of notification rules that match filter and the total count of matching notification rules. CreateNotificationEndpoint creates a new notification rule and sets ID with the new identifier. UpdateNotificationEndpoint updates a single notification rule. PatchNotificationEndpoint updates a single  notification rule with changeset. DeleteNotificationEndpoint removes a notification rule by ID./Users/austinjaybecker/projects/abeck-go-testing/mock/notification_rule_store.go NotificationRuleStore represents a service for managing notification rule data. NewNotificationRuleStore creats a fake notification rules tore. FindNotificationRuleByID returns a single telegraf config by ID. CreateNotificationRule creates a new notification rule and sets ID with the new identifier./Users/austinjaybecker/projects/abeck-go-testing/mock/onboarding_service.go OnboardingService is a mock implementation of platform.OnboardingService. NewOnboardingService returns a mock of OnboardingService where its methods will return zero values. IsOnboarding determine if onboarding request is allowed. OnboardInitialUser OnboardingResults. OnboardUser OnboardingResults./Users/austinjaybecker/projects/abeck-go-testing/mock/operation_log_service.go NewBucketOperationLogService returns a mock of BucketOperationLogService. NewDashboardOperationLogService returns a mock of DashboardOperationLogService. NewOrganizationOperationLogService returns a mock of OrganizationOperationLogService. NewUserOperationLogService returns a mock of UserOperationLogService. BucketOperationLogService is a mock implementation of platform.BucketOperationLogService. DashboardOperationLogService is a mock implementation of platform.DashboardOperationLogService. OrganizationOperationLogService is a mock implementation of platform.OrganizationOperationLogService. UserOperationLogService is a mock implementation of platform.UserOperationLogService. GetBucketOperationLog retrieves the operation log for the bucket with the provided id. GetDashboardOperationLog retrieves the operation log for the dashboard with the provided id. GetOrganizationOperationLog retrieves the operation log for the org with the provided id. GetUserOperationLog retrieves the operation log for the user with the provided id./Users/austinjaybecker/projects/abeck-go-testing/mock/org_service.go OrganizationService is a mock organization server. NewOrganizationService returns a mock OrganizationService where its methods will returnFindOrganizations calls FindOrganizationsF. CreateOrganization calls CreateOrganizationF. UpdateOrganization calls UpdateOrganizationF. DeleteOrganization calls DeleteOrganizationF. FindResourceOrganizationID calls FindResourceOrganizationIDF./Users/austinjaybecker/projects/abeck-go-testing/mock/paging.go/Users/austinjaybecker/projects/abeck-go-testing/mock/passwords.gomock error"mock error" PasswordsService is a mock implementation of a retention.PasswordsService, which also makes it a suitable mock to use wherever an platform.PasswordsService is required. NewPasswordsService returns a mock PasswordsService where its methods will return SetPassword sets the users current password to be the provided password. ComparePassword password compares the provided password. CompareAndSetPassword compares the provided password and sets it to the new password./Users/austinjaybecker/projects/abeck-go-testing/mock/passwords_service.go"CompareAndSetPassword""ComparePassword""SetPassword" Source: github.com/influxdata/influxdb/v2 (interfaces: PasswordsService) Package mock is a generated GoMock package. MockPasswordsService is a mock of PasswordsService interface MockPasswordsServiceMockRecorder is the mock recorder for MockPasswordsService NewMockPasswordsService creates a new mock instance CompareAndSetPassword mocks base method CompareAndSetPassword indicates an expected call of CompareAndSetPassword ComparePassword mocks base method ComparePassword indicates an expected call of ComparePassword SetPassword mocks base method SetPassword indicates an expected call of SetPassword/Users/austinjaybecker/projects/abeck-go-testing/mock/points_writer.go PointsWriter is a mock structure for writing points. ForceError is for error testing, if WritePoints is called after ForceError, it will return that error. WritePoints writes points to the PointsWriter that will be exposed in the Values. Next returns the next (oldest) batch of values./Users/austinjaybecker/projects/abeck-go-testing/mock/reader.gogithub.com/influxdata/flux/memory"github.com/influxdata/flux/memory" Only invoke the close function if it is set. We want this to be a no-op and work without explicitly setting up a close function./Users/austinjaybecker/projects/abeck-go-testing/mock/reads_resultset.gocursorsgithub.com/influxdata/influxdb/v2/pkg/data/gen"github.com/influxdata/influxdb/v2/pkg/data/gen"github.com/influxdata/influxdb/v2/storage/reads"github.com/influxdata/influxdb/v2/storage/reads"github.com/influxdata/influxdb/v2/tsdb/cursors"github.com/influxdata/influxdb/v2/tsdb/cursors"unreachable"unreachable"MeasurementTagKeyBytesFieldKeyTagKeyBytesFloatValuesIntegerValuesUnsignedValuesStringValuesBooleanValues WithGeneratorMaxValues limits the number of values produced by GeneratorResultSet to n. NewResultSetFromSeriesGenerator transforms a SeriesGenerator into a ResultSet, and therefore may be used anywhere a ResultSet is required. cursors/Users/austinjaybecker/projects/abeck-go-testing/mock/retention_service.go/Users/austinjaybecker/projects/abeck-go-testing/mock/safe_count.go SafeCount provides a safe counter, useful for call counts to maintain thread safety. Removes burden of having to introduce serialization when concurrency is brought in. IncrFn increments the safe counter by 1. Count returns the current count. Reset will reset the count to 0./Users/austinjaybecker/projects/abeck-go-testing/mock/scraper_service.go ScraperTargetStoreService is a mock implementation of a platform.ScraperTargetStoreService. ListTargets lists all the scraper targets. AddTarget adds a scraper target. RemoveTarget deletes a scraper target./Users/austinjaybecker/projects/abeck-go-testing/mock/secret_service.goksnot implmemented"not implmemented" SecretService is a mock implementation of a retention.SecretService, which also makes it a suitable mock to use wherever an platform.SecretService is required. NewSecretService returns a mock SecretService where its methods will return LoadSecret retrieves the secret value v found at key k for organization orgID. GetSecretKeys retrieves all secret keys that are stored for the organization orgID. PutSecret stores the secret pair (k,v) for the organization orgID. PutSecrets puts all provided secrets and overwrites any previous values. PatchSecrets patches all provided secrets and updates any previous values. DeleteSecret removes a single secret from the secret store./Users/austinjaybecker/projects/abeck-go-testing/mock/session_service.gonewExpirationexpiredAtmock session"mock session" SessionService is a mock implementation of a retention.SessionService, which also makes it a suitable mock to use wherever an platform.SessionService is required. NewSessionService returns a mock SessionService where its methods will return FindSession returns the session found at the provided key. CreateSession creates a session for a user with the users maximal privileges. ExpireSession expires the session provided at key. RenewSession extends the expire time to newExpiration./Users/austinjaybecker/projects/abeck-go-testing/mock/source_service.go SourceService is a mock implementation of platform.SourceService. NewSourceService returns a mock of SourceService where its methods will return zero values. FindSourceByID retrieves a source by its ID. FindSources returns a list of all sources. CreateSource sets the sources ID and stores it. DeleteSource removes the source. UpdateSource updates the source./Users/austinjaybecker/projects/abeck-go-testing/mock/task_service.gotaskCreateid2backend/Users/austinjaybecker/projects/abeck-go-testing/mock/telegraf_service.go TelegrafConfigStore represents a service for managing telegraf config data. NewTelegrafConfigStore constructs a new fake TelegrafConfigStore./Users/austinjaybecker/projects/abeck-go-testing/mock/user_resource_mapping_service.go UserResourceMappingService is a mock implementation of platform.UserResourceMappingService NewUserResourceMappingService returns a mock of UserResourceMappingService FindUserResourceMappings finds mappings that match a given filter. CreateUserResourceMapping creates a new UserResourceMapping. DeleteUserResourceMapping removes a UserResourceMapping./Users/austinjaybecker/projects/abeck-go-testing/mock/user_service.go UserService is a mock implementation of a retention.UserService, which also makes it a suitable mock to use wherever an platform.UserService is required. Methods for a platform.UserService NewUserService returns a mock of UserService where its methods will return zero values. CreateUser creates a new User and sets b.ID with the new identifier. DeleteUser removes a User by ID. FindUser finds the first user that matches a filter UpdateUser updates a user/Users/austinjaybecker/projects/abeck-go-testing/mock/variable_service.go NewVariableService returns a mock of VariableService where its methods will return zero values./Users/austinjaybecker/projects/abeck-go-testing/mock/write_service.go WriteService writes data read from the reader. Write calls the mocked WriteF function with arguments./Users/austinjaybecker/projects/abeck-go-testing/models/Users/austinjaybecker/projects/abeck-go-testing/models/consistency.goAppendMakeKeyCheckTimeCompareTagsConsistencyLevelAllConsistencyLevelAnyConsistencyLevelOneConsistencyLevelQuorumCopyTagsDeepCopyTagsErrInvalidConsistencyLevèÿ    \d¾ä    