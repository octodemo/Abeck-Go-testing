ErrInvalidConsistencyLevelErrInvalidKevValuePairsErrInvalidNumberErrInvalidPointErrPointMustHaveAFieldErrTimeOutOfRangeEscapeMeasurementEscapeStringFieldFieldKeyTagKeyGetPrecisionMultiplierInlineFNV64aMakeKeyMaxKeyLengthMaxNanoTimeMeasurementTagKeyMustNewPointNewInlineFNV64aNewPointFromBytesNewStatisticNewTagsKeyValuesNewTagsKeyValuesStringsParseConsistencyLevelParseKeyParsePointsParsePointsStringParseTagsParseTagsWithTagsSafeCalcTimeTagKeysSetValidKeyTokensValidTagTokensValidToken_FieldType_index_FieldType_nameappendFieldescapeSetescapeStringFieldReplacerescapeTagfieldIteratorfieldsStateinsertionSortisNumericmaxFloat64DigitsmaxInt64DigitsmaxNanoTimemaxUint64DigitsmeasurementEscapeCodesminFloat64DigitsminInt64DigitsminNanoTimeoffset64parseBoolBytesparseFloatBytesparseIntBytesparsePointparseTagsparseUintBytespointKeyprime64reservedFieldTagKeyreservedMeasurementTagKeyreservedTagKeysreservedTimeTagKeysafeSignedMultscanBooleanscanFieldValuescanFieldsscanKeyscanLinescanMeasurementscanTagValuescanTagsscanTagsKeyscanTagsValuescanTimescanToscanToSpaceOrseriesKeySizeskipWhitespacetagEscapeCodestagKeyStatetagValueStateunescapeMeasurementunescapeStringFieldunescapeTagwalkFieldswalkTagsinvalid consistency level"invalid consistency level"any"any"one"one"quorum"quorum" ConsistencyLevel represent a required replication criteria before a write can be returned as successful. The consistency level is handled in open-source InfluxDB but only applicable to clusters. ConsistencyLevelAny allows for hinted handoff, potentially no write happened yet. ConsistencyLevelOne requires at least one data node acknowledged a write. ConsistencyLevelQuorum requires a quorum of data nodes to acknowledge a write. ConsistencyLevelAll requires all data nodes to acknowledge a write. ErrInvalidConsistencyLevel is returned when parsing the string version of a consistency level. ParseConsistencyLevel converts a consistency level string to the corresponding ConsistencyLevel const.Sum64KeysBytesIsSupersetKeysIsSupersetBytesUnionKeysUnionByteskeybufcachedNameSetPrecisionunmarshalBinary/Users/austinjaybecker/projects/abeck-go-testing/models/fieldtype_string.goIntegerFloatBooleanStringEmptyUnsigned"IntegerFloatBooleanStringEmptyUnsigned"FieldType("FieldType(")")" Code generated by "stringer -type=FieldType"; DO NOT EDIT./Users/austinjaybecker/projects/abeck-go-testing/models/gen.gogo:generate stringer -type=FieldType/Users/austinjaybecker/projects/abeck-go-testing/models/inline_fnv.go109951162821114695981039346656037 import "github.com/influxdata/influxdb/models" from stdlib hash/fnv/fnv.go InlineFNV64a is an alloc-free port of the standard library's fnv64a. See https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function. NewInlineFNV64a returns a new instance of InlineFNV64a. Write adds data to the running hash. Sum64 returns the uint64 of the current resulting hash./Users/austinjaybecker/projects/abeck-go-testing/models/inline_strconv_parse.gobitSize parseIntBytes is a zero-alloc wrapper around strconv.ParseInt. parseUintBytes is a zero-alloc wrapper around strconv.ParseUint. parseFloatBytes is a zero-alloc wrapper around strconv.ParseFloat. parseBoolBytes is a zero-alloc wrapper around strconv.ParseBool. unsafeBytesToString converts a []byte to a string without a heap allocation./Users/austinjaybecker/projects/abeck-go-testing/models/points.godigitsdefaultTimemaxKeyErrreservedcommasnewIndicsquoteddecimalisIntisUnsignednumericDigitsscientifichasFieldhasEscapeothersmergedndgithub.com/influxdata/influxdb/v2/pkg/escape"github.com/influxdata/influxdb/v2/pkg/escape"ï¿½"\xff"_field"_field"_measurement"_measurement"'\\''='point without fields is unsupported"point without fields is unsupported"invalid number"invalid number"point is invalid"point is invalid"key/value pairs is an odd length"key/value pairs is an odd length"65535'#'unable to parse '%s': %v"unable to parse '%s': %v"missing measurement"missing measurement"max key length exceeded: %v > %v"max key length exceeded: %v > %v"missing fields"missing fields"cannot use reserved tag key %q"cannot use reserved tag key %q"duplicate tags"duplicate tags"missing tag key"missing tag key"missing tag value"missing tag value"invalid tag format"invalid tag format"'"'missing field key"missing field key"missing field value"missing field value"'N'unbalanced quotes"unbalanced quotes"invalid field format"invalid field format"bad timestamp"bad timestamp"'E''+'unable to parse integer %s: %s"unable to parse integer %s: %s"unable to parse unsigned %s: %s"unable to parse unsigned %s: %s"invalid float"invalid float"'T'F'F'invalid boolean"invalid boolean"TRUE"TRUE""True"FALSE"FALSE"False"False"+/-Inf is an unsupported value for field %s"+/-Inf is an unsupported value for field %s"NaN is an unsupported value for field %s"NaN is an unsupported value for field %s"all fields must have non-empty names"all fields must have non-empty names"unable to unmarshal field %s: %s"unable to unmarshal field %s: %s"Unescapeinvalid value: field-key=%s"invalid value: field-key=%s"ErrShortBuffer%s %s"%s %s"%s %s %d"%s %s %d"'{''}'']'IsEscapedAppendUnescaped0123456789-.nNiIu`0123456789-.nNiIu`unable to parse integer value %q: %v"unable to parse integer value %q: %v"unable to parse unsigned value %q: %v"unable to parse unsigned value %q: %v"unable to parse bool value %q: %v"unable to parse bool value %q: %v"unable to parse floating point value %q: %v"unable to parse floating point value %q: %v"ReplacementChar Package models implements basic objects used throughout the TICK stack. Values used to store the field key and measurement name as special internal tags. reserved tag keys which when present cause the point to be discarded and an error returned Predefined byte representations of special tag keys. set of reserved tag keys which cannot be present when a point is being parsed. ErrPointMustHaveAField is returned when operating on a point that does not have any fields. ErrInvalidNumber is returned when a number is expected but not provided. ErrInvalidPoint is returned when a point cannot be parsed correctly. ErrInvalidKevValuePairs is returned when the number of key, value pairs is odd, indicating a missing value. MaxKeyLength is the largest allowed size of the combined measurement and tag keys. Point defines the values that will be written to the database. Name return the measurement name for the point. SetName updates the measurement name for the point. Tags returns the tag set for the point. ForEachTag iterates over each tag invoking fn.  If fn return false, iteration stops. AddTag adds or replaces a tag value for a point. SetTags replaces the tags for the point. HasTag returns true if the tag exists for the point. Fields returns the fields for the point. Time return the timestamp for the point. SetTime updates the timestamp for the point. UnixNano returns the timestamp of the point as nanoseconds since Unix epoch. HashID returns a non-cryptographic checksum of the point's key. Key returns the key (measurement joined with tags) of the point. String returns a string representation of the point. If there is a timestamp associated with the point then it will be specified with the default precision of nanoseconds. MarshalBinary returns a binary representation of the point. PrecisionString returns a string representation of the point. If there is a timestamp associated with the point then it will be specified in the given unit. RoundedString returns a string representation of the point. If there is a timestamp associated with the point, then it will be rounded to the given duration. Split will attempt to return multiple points with the same timestamp whose string representations are no longer than size. Points with a single field or a point without a timestamp may exceed the requested size. Round will round the timestamp of the point to the given duration. StringSize returns the length of the string that would be returned by String(). AppendString appends the result of String() to the provided buffer and returns the result, potentially reducing string allocations. FieldIterator returns a FieldIterator that can be used to traverse the fields of a point without constructing the in-memory map. FieldType represents the type of a field. Integer indicates the field's type is integer. Float indicates the field's type is float. Boolean indicates the field's type is boolean. String indicates the field's type is string. Empty is used to indicate that there is no field. Unsigned indicates the field's type is an unsigned integer. FieldIterator provides a low-allocation interface to iterate through a point's fields. Next indicates whether there any fields remaining. FieldKey returns the key of the current field. Type returns the FieldType of the current field. StringValue returns the string value of the current field. IntegerValue returns the integer value of the current field. UnsignedValue returns the unsigned value of the current field. BooleanValue returns the boolean value of the current field. FloatValue returns the float value of the current field. Reset resets the iterator to its initial state. Points represents a sortable list of points by timestamp. Len implements sort.Interface. Less implements sort.Interface. Swap implements sort.Interface. point is the default implementation of Point. text encoding of measurement and tags key must always be stored sorted by tags, if the original line was not sorted, we need to resort it text encoding of field data text encoding of timestamp cached version of parsed fields from data cached version of parsed name from key cached version of parsed tags type assertions the number of characters for the largest possible int64 (9223372036854775807) the number of characters for the smallest possible int64 (-9223372036854775808) the number of characters for the largest possible uint64 (18446744073709551615) the number of characters required for the largest float64 before a range check would occur during parsing the number of characters required for smallest float64 before a range check occur ParsePoints returns a slice of Points from a text representation of a point with each point separated by newlines.  If any points fail to parse, a non-nil error will be returned in addition to the points that parsed successfully. ParsePointsString is identical to ParsePoints but accepts a string. ParseKey returns the measurement name and tags from a point. NOTE: to minimize heap allocations, the returned Tags will refer to subslices of buf. This can have the unintended effect preventing buf from being garbage collected. Ignore the error because scanMeasurement returns "missing fields" which we ignore when just parsing a key scanMeasurement returns the location of the comma if there are tags, strip that off ValidPrecision checks if the precision is known. ParsePointsWithPrecision is similar to ParsePoints, but allows the caller to provide a precision for time. NOTE: to minimize heap allocations, the returned Points will refer to subslices of buf. If line is all whitespace, just skip it lines which start with '#' are comments strip the newline if one is present scan the first block which is measurement[,tag1=value1,tag2=value2...] measurement name is required scan the second block is which is field1=value1[,field2=value2,...] at least one field is required scan the last block which is an optional integer timestamp Determine if there are illegal non-whitespace characters after the timestamp block. GetPrecisionMultiplier will return a multiplier for the precision specified. scanKey scans buf starting at i for the measurement and tag portion of the point. It returns the ending position and the byte slice of key within buf.  If there are tags, they will be sorted if they are not already. Determines whether the tags are sort, assume they are indices holds the indexes within buf of the start of each tag.  For example, a buf of 'cpu,host=a,region=b,zone=c' would have indices slice of [4,11,20] which indicates that the first tag starts at buf[4], seconds at buf[11], and last at buf[20] tracks how many commas we've seen so we know how many values are indices. Since indices is an arbitrarily large slice, we need to know how many values in the buffer are in use. First scan the Point's measurement. Optionally scan tags if needed. Iterate over tags keys ensure that we do not encounter any of the reserved tag keys such as _measurement or _field. Now we know where the key region is within buf, and the location of tags, we need to determine if duplicate tags exist and if the tags are sorted. This iterates over the list comparing each tag in the sequence with each other. get the left and right tags If left is greater than right, the tags are not sorted. We do not have to continue because the short path no longer works. If the tags are equal, then there are duplicate tags, and we should abort. If the tags are not sorted, this pass may not find duplicate tags and we need to do a more exhaustive search later. If the tags are not sorted, then sort them.  This sort is inline and uses the tag indices we created earlier.  The actual buffer is not sorted, the indices are using the buffer for value comparison.  After the indices are sorted, the buffer is reconstructed from the sorted indices. Get the measurement name for later Sort the indices Create a new key using the measurement and sorted indices Check again for duplicate tags now that the tags are sorted. The following constants allow us to specify which state to move to next, when scanning sections of a Point. scanMeasurement examines the measurement part of a Point, returning the next state to move to, and the current location in the buffer. Check first byte of measurement, anything except a comma is fine. It can't be a space, since whitespace is stripped prior to this function call. cpu Skip character (it's escaped). Unescaped comma; move onto scanning the tags. Unescaped space; move onto scanning the fields. cpu value=1.0 scanTags examines all the tags in a Point, keeping track of and returning the updated indices slice, number of commas and location in buf where to start examining the Point fields. Grow our indices slice if we have too many tags. tag value always follows a tag key scanTagsKey scans each character in a tag key. First character of the key. cpu,{'', ' ', ',', '='} Examine each character in the tag key until we hit an unescaped equals (the tag value), or we hit an error (i.e., unescaped space or comma). Either we reached the end of the buffer or we hit an unescaped comma or space. cpu,tag{'', ' ', ','} cpu,tag= scanTagsValue scans each character in a tag value. Tag value cannot be empty. cpu,tag={',', ' '} Examine each character in the tag value until we hit an unescaped comma (move onto next tag key), an unescaped space (move onto fields), or we error out. cpu,tag=value An unescaped equals sign is an invalid tag value. cpu,tag={'=', 'fo=o'} cpu,tag=foo, cpu,tag=foo value=1.0 cpu, tag=foo\= value=1.0 This grabs the tag names for i & j, it ignores the values scanFields scans buf, starting at i for the fields section of a point.  It returns the ending position and the byte slice of the fields within buf. tracks how many '=' we've seen tracks how many commas we've seen reached the end of buf? escaped characters? If the value is quoted, scan until we get to the end quote Only quote values in the field value since quotes are not significant in the field key If we see an =, ensure that there is at least on char before and after it check for "... =123" but allow "a\ =123" check for "...a=123,=456" but allow "a=123,a\,=456" check for "... value=" check for "... value=,value2=..." If next byte is not a double-quote, the value must be a boolean reached end of block? check that all field sections had key and values (e.g. prevent "a=1,b" scanTime scans buf, starting at i for the time section of a point. It returns the ending position and the byte slice of the timestamp within buf and and error if the timestamp is not in the correct numeric format. Reached end of block or trailing whitespace? Handle negative timestamps Timestamps should be integers, make sure they are so we don't need to actually  parse the timestamp until needed. scanNumber returns the end position within buf, start at i after scanning over buf for an integer, or float.  It returns an error if a invalid number is scanned. Is negative number? There must be more characters now, as just '-' is illegal. how many decimal points we've see indicates the number is float in scientific notation Can't have more than 1 decimal (e.g. 1.1.1 should fail) `e` is valid for floats but not as the first char + and - are only valid at this point if they follow an e (scientific notation) NaN is an unsupported value It's more common that numbers will be within min/max range for their type but we need to prevent out or range numbers from being parsed successfully.  This uses some simple heuristics to decide if we should parse the number to the actual type.  It does not do it all the time because it incurs extra allocations and we end up converting the type again when writing points to disk. Make sure the last char is an 'i' for integers (e.g. 9i10 is not valid) Parse the int to check bounds the number of digits could be larger than the max range We subtract 1 from the index to remove the `i` from our tests Make sure the last char is a 'u' for unsigned Make sure the first char is not a '-' for unsigned Parse the uint to check bounds the number of digits could be larger than the max range We subtract 1 from the index to remove the `u` from our tests Parse the float to check bounds if it's scientific or the number of digits could be larger than the max range scanBoolean returns the end position within buf, start at i after scanning over buf for boolean. Valid values for a boolean are t, T, true, TRUE, f, F, false, FALSE.  It returns an error if a invalid boolean is scanned. Single char bool (t, T, f, F) is ok length must be 4 for true or TRUE length must be 5 for false or FALSE Otherwise skipWhitespace returns the end position within buf, starting at i after scanning over spaces in tags. scanLine returns the end position in buf and the next line found within buf. tracks how many '=' and commas we've seen this duplicates some of the functionality in scanFields skip past escaped characters If we see a double quote, makes sure it is not escaped scanTo returns the end position in buf and the next consecutive block of bytes, starting from i and ending with stop byte, where stop byte has not been escaped. If there are leading spaces, they are skipped. Reached unescaped stop value? of bytes, starting from i and ending with stop byte.  If there are leading spaces, they are skipped. Only escape char for a field value is a double-quote and backslash Quoted value? (e.g. string) escapeStringFieldReplacer replaces double quotes and backslashes with the same character preceded by a backslash. As of Go 1.7 this benchmarked better in allocations and CPU time compared to iterating through a string byte-by-byte and appending to a new byte slice, calling strings.Replace twice, and better than (*Regex).ReplaceAllString. EscapeStringField returns a copy of in with any double quotes or backslashes with escaped values. unescapeStringField returns a copy of in with any escaped double-quotes or backslashes unescaped. unescape backslashes unescape double-quotes NewPoint returns a new point with the given measurement name, tags, fields and timestamp.  If an unsupported field value (NaN, or +/-Inf) or out of range time is passed, this function returns an error. pointKey checks some basic requirements for valid points, and returns the key, along with an possible error. Ensure the caller validates and handles invalid field values 4 is the length of the tsm1.fieldKeySeparator constant.  It's inlined here to avoid a circular dependency. NewPointFromBytes returns a new Point from a marshalled Point. This does some basic validation to ensure there are fields and they can be unmarshalled as well. Skip since this won't return an error MustNewPoint returns a new point with the given measurement name, tags, fields and timestamp.  If an unsupported field value (NaN) is passed, this function panics. it's an empty key, so there are no tags walkFields walks each field key and value via fn.  If fn returns false, the iteration is stopped.  The values are the raw byte slices and not the converted types. slice off comma parseTags parses buf into the provided destination tags, returning destination Tags, which may have a different length and capacity. Ensure existing behaviour when point has no tags and nil slice passed in. Series keys can contain escaped commas, therefore the number of commas in a series key only gives an estimation of the upper bound on the number of tags. MakeKey creates a key for a set of tags. AppendMakeKey appends the key derived from name and tags to dst and returns the extended buffer. unescape the name and then re-escape it to avoid double escaping. The key should always be stored in escaped form. SetPrecision will round a time to the specified precision. String returns the string representation of the point. AppendString appends the string representation of the point to buf. even "0" has one digit account for negative sign, then negate already accounted for one digit digits and a space UnmarshalBinary decodes a binary representation of the point into a point struct. Read key length. Read key. Read fields length. Read fields. Read timestamp. key string, timestamp string, spaces Tag represents a single key/value tag pair. NewTag returns a new Tag. Size returns the size of the key and value. Clone returns a shallow copy of Tag. Tags associated with a Point created by ParsePointsWithPrecision will hold references to the byte slice that was parsed. Use Clone to create a Tag with new byte slices that do not refer to the argument to ParsePointsWithPrecision. String returns the string reprsentation of the tag. Tags represents a sorted list of tags. NewTags returns a new Tags from a map. NewTagsKeyValues returns a new Tags from a list of key, value pairs, ensuring the returned result is correctly sorted. Duplicate keys are removed, however, it which duplicate that remains is undefined. NewTagsKeyValues will return ErrInvalidKevValuePairs if len(kvs) is not even. If the input is guaranteed to be even, the error can be safely ignored. If a has enough capacity, it will be reused. only copy if j has deviated from i, indicating duplicates NewTagsKeyValuesStrings is equivalent to NewTagsKeyValues, except that it will allocate new byte slices for each key, value pair. Keys returns the list of keys for a tag set. Values returns the list of values for a tag set. String returns the string representation of the tags. Size returns the number of bytes needed to store all tags. Note, this is the number of bytes needed to store all keys and values and does not account for data structures or delimiters for example. Clone returns a copy of the slice where the elements are a result of calling `Clone` on the original elements Use Clone to create Tags with new byte slices that do not refer to the argument to ParsePointsWithPrecision. KeyValues returns the Tags as a list of key, value pairs, maintaining the original order of a. v will be used if it has capacity. sorted returns true if a is sorted and is an optimization to avoid an allocation when calling sort.IsSorted, improving performance as much as 50%. Equal returns true if a equals other. CompareTags returns -1 if a < b, 1 if a > b, and 0 if a == b. Compare each key & value until a mismatch. If all tags are equal up to this point then return shorter tagset. All tags are equal. Get returns the value for a key. OPTIMIZE: Use sort.Search if tagset is large. GetString returns the string value for a string key. Set sets the value for a key. SetString sets the string value for a string key. Delete removes a tag by key. Map returns a map representation of the tags. Merge merges the tags combining the two. If both define a tag with the same key, the merged value overwrites the old value. A new map is returned. HashKey hashes all of a tag's keys. AppendHashKey appends the result of hashing all of a tag's keys and values to dst and returns the extended buffer. Type invariant: Tags are sorted CopyTags returns a shallow copy of tags. DeepCopyTags returns a deep copy of tags. Calculate size of keys/values in bytes. Build single allocation for all key/values. Copy tags to new set. Fields represents a mapping between a Point's field names and their values. to keep the same behavior that currently exists, default to boolean MarshalBinary encodes all the fields to their proper type and returns the binary representation NOTE: uint64 is specifically not supported due to potential overflow when we decode again later to an int64 NOTE2: uint is accepted, and may be 64 bits, and is for some reason accepted... Only sort if we have multiple fields to sort. This length check removes an allocation incurred by the sort. check popular types first TODO: 'uint' should be converted to writing as an unsigned integer, but we cannot since that would break backwards compatibility. skip Can't determine the type, so convert to string ValidToken returns true if the provided token is a valid unicode string, and only contains printable, non-replacement characters. ValidTagTokens returns true if all the provided tag key and values are valid. ValidTagTokens does not validate the special tag keys used to represent the measurement name and field key, but it does validate the associated values. Validate all external tag keys. Validate all tag values (this will also validate the field key, which is a tag value for the special field key tag key). ValidKeyTokens returns true if the measurement name and all tags are valid./Users/austinjaybecker/projects/abeck-go-testing/models/rows.gojson:"columns,omitempty"`json:"columns,omitempty"`json:"values,omitempty"`json:"values,omitempty"` Row represents a single row returned from the execution of a statement. SameSeries returns true if r contains values for the same series as o. tagsHash returns a hash of tag key/value pairs. tagKeys returns a sorted list of tag keys. Rows represents a collection of rows. Rows implements sort.Interface. Sort by name first. Sort by tag set hash. Tags don't have a meaningful sort order so we just compute a hash and sort by that instead. This allows the tests to receive rows in a predictable order every time./Users/austinjaybecker/projects/abeck-go-testing/models/statistic.go Statistic is the representation of a statistic used by the monitoring service. NewStatistic returns an initialized Statistic. StatisticTags is a map that can be merged with others without causing mutations to either map. Merge creates a new map containing the merged contents of tags and t. If both tags and the receiver map contain the same key, the value in tags is used in the resulting map. Merge always returns a usable map. Add everything in tags to the result. Only add values from t that don't appear in tags./Users/austinjaybecker/projects/abeck-go-testing/models/tagkeysset.gokeya TagKeysSet provides set operations for combining Tags. Clear removes all the elements of TagKeysSet and ensures all internal buffers are reset. KeysBytes returns the merged keys in lexicographical order. The slice is valid until the next call to UnionKeys, UnionBytes or Reset. Keys returns a copy of the merged keys in lexicographical order. IsSupersetKeys returns true if the TagKeysSet is a superset of all the keys contained in other. IsSupersetBytes returns true if the TagKeysSet is a superset of all the keys in other. Other must be lexicographically sorted or the results are undefined. UnionKeys updates the set so that it is the union of itself and all the keys contained in other. UnionBytes updates the set so that it is the union of itself and all the/Users/austinjaybecker/projects/abeck-go-testing/models/time.gotmemulttime outside range %d - %d"time outside range %d - %d" Helper time methods since parsing time can easily overflow and we only support a specific time range. MinNanoTime is the minimum time that can be represented. 1677-09-21 00:12:43.145224194 +0000 UTC The two lowest minimum integers are used as sentinel values.  The minimum value needs to be used as a value lower than any other value for comparisons and another separate value is needed to act as a sentinel default value that is unusable by the user, but usable internally. Because these two values need to be used for a special purpose, we do not allow users to write points at these two times. MaxNanoTime is the maximum time that can be represented. 2262-04-11 23:47:16.854775806 +0000 UTC The highest time represented by a nanosecond needs to be used for an exclusive range in the shard group, so the maximum time needs to be one less than the possible maximum number of nanoseconds representable by an int64 so that we don't lose a point at that one time. ErrTimeOutOfRange gets returned when time is out of the representable range using int64 nanoseconds since the epoch. SafeCalcTime safely calculates the time given. Will return error if the time is outside the supported range. CheckTime checks that a time is within the safe range. Perform the multiplication and check to make sure it didn't overflow./Users/austinjaybecker/projects/abeck-go-testing/nats/Users/austinjaybecker/projects/abeck-go-testing/nats/handler.goErrNoNatsConnectionLogHandlermessageHandler Process does something with a received subscription message, then acks it./Users/austinjaybecker/projects/abeck-go-testing/nats/message.go"github.com/nats-io/go-nats-streaming"/Users/austinjaybecker/projects/abeck-go-testing/nats/publisher.goguidahConnectionLostHandlerConnectTimeoutAckTimeoutMaxPubAcksInflightPingItervalConnectionLostCB Publish a new message to channel Open creates and maintains a connection to NATS server/Users/austinjaybecker/projects/abeck-go-testing/nats/server.gosservergithub.com/nats-io/gnatsd/server"github.com/nats-io/gnatsd/server"github.com/nats-io/nats-streaming-server/server"github.com/nats-io/nats-streaming-server/server"github.com/nats-io/nats-streaming-server/stores"github.com/nats-io/nats-streaming-server/stores"RANDOM_PORTnats connection has not been established. Call Open() first"nats connection has not been established. Call Open() first"GetDefaultOptionsTypeMemoryMEMORYRunServerWithOptsDefaultNatsServerOptions RandomPort is the value for port that, when supplied, will cause the server to listen on a randomly-chosen available port. The resolved port will be reassigned to the Port field of server.Options. Server wraps a connection to a NATS streaming server Open starts a NATS streaming server Streaming options Close stops the embedded NATS server. NewDefaultServerOptions returns the default NATS server options, allowing the caller to  override specific fields. NewServer creates a new streaming server with the provided server options./Users/austinjaybecker/projects/abeck-go-testing/nats/subscriber.goSetManualAckMode Subscribe listens to a channel, handling messages with Handler/Users/austinjaybecker/projects/abeck-go-testing/nats/subscription.go Pending returns the number of queued messages and queued bytes for this subscription. Delivered returns the number of delivered messages for this subscription. Close removes this subscriber/Users/austinjaybecker/projects/abeck-go-testing/notification/Users/austinjaybecker/projects/abeck-go-testing/notification/check/Users/austinjaybecker/projects/abeck-go-testing/notification/check/check.goCustomDeadmanGreaterLesserThresholdThresholdConfigaddCreateEmptyFalseToAggregateWindowassignPipelineToDatacustomAliasdeadmanAliasgetFieldsmultiErrorpropertyHasValueremoveAggregateWindowremoveStopFromRangereplaceDurationsWithEverythresholdAliasthresholdConfigDecodethresholdDecodetypeToCheckTimeDurationgenerateFluxASTMessageFunctiongenerateTaskOptiongenerateFluxASTCheckDefinitioncheckTypetagPropsconvertedconvertedFuncnotificationgithub.com/influxdata/influxdb/v2/notification"github.com/influxdata/influxdb/v2/notification"github.com/influxdata/influxdb/v2/notification/flux"github.com/influxdata/influxdb/v2/notification/flux"json:"taskID,omitempty"`json:"taskID,omitempty"`Check ID is invalid"Check ID is invalid"Check OwnerID is invalid"Check OwnerID is invalid"Check OrgID is invalid"Check OrgID is invalid"Check Every must exist"Check Every must exist"Check Every can't be empty"Check Every can't be empty"Check Offset can't be empty"Check Offset can't be empty"Offset should not be equal or greater than the interval"Offset should not be equal or greater than the interval"FunctionExpressionPropertyKeyFunctionParamsVariableAssignmentassignmentDefineVariablemessageFn"messageFn""every"OptionStatementAssignmentObjectExpressionDefineTaskOption_check_id"_check_id"_check_name"_check_name""_type"deadman"deadman"GenerateFluxASTgenerateFluxASTBodygenerateLevelFngenerateFluxASTChecksFunctiongenerateFluxASTChecksCallgenerateFluxASTThresholdFunctiongenerateFluxASTThresholdFunctionscustom"custom"sanitizeFluxhasRequiredTaskOptionshasRequiredCheckParametersunable to detect the check type from json"unable to detect the check type from json"invalid check type %s"invalid check type %s" Base will embed inside a check. Care should be taken to prevent TaskID from being exposed publicly. } todo: separate these NonCustomCheckBase will embed inside non-custom checks. type NonCustomCheckBase struct { Offset represents a delay before execution. It gets marshalled from a string duration, i.e.: "10s" is 10 seconds Valid returns err if the check is invalid. TODO(desa): eventually tags will be flattened out into the data struct GetID implements influxdb.Getter interface. GetOrgID implements influxdb.Getter interface. GetOwnerID gets the ownerID associated with a Base. GetTaskID retrieves the task ID for a check. GetCRUDLog implements influxdb.Getter interface. GetName implements influxdb.Getter interface. GetDescription implements influxdb.Getter interface. SetID will set the primary key. SetOrgID will set the org key. ClearPrivateData remove any data that we don't want to be exposed publicly. SetTaskID sets the taskID for a check. SetOwnerID sets the taskID for a check. SetName implements influxdb.Updator interface. SetDescription implements influxdb.Updator interface. UnmarshalJSON will convertInit_typeMutateInit_typeInit_tdKeyTypeMutateKeyTypeMutateValueTypePropertiesLengthCallExpressionCalleeParamsLengthBodyTypeMutateBodyTypeAssignmentTypeMutateAssignmentTypeCalleeTypeMutateCalleeType/Users/austinjaybecker/projects/abeck-go-testing/notification/check/custom.goidPropobjectExpvariableAssignstringLithasEveryhasOffsethasOptionTasknameMatchesCheckcheckNameMatchescheckTypeIsCustomhasCheckObjectGetErrorsCustom flux missing task option statement"Custom flux missing task option statement"Custom flux missing name parameter from task option statement"Custom flux missing name parameter from task option statement"Name parameter from task option statement must match check name"Name parameter from task option statement must match check name"Custom flux missing every parameter from task option statement"Custom flux missing every parameter from task option statement"Custom flux missing offset parameter from task option statement"Custom flux missing offset parameter from task option statement"Custom flux must have an object called 'check'"Custom flux must have an object called 'check'"_check_name parameter on check object must match check name"_check_name parameter on check object must match check name"_type parameter on check object must be set to 'custom'"_type parameter on check object must be set to 'custom'" Custom is the custom check. flux example for threshold check for reference: package main import "influxdata/influxdb/monitor" import "influxdata/influxdb/v1" data = from(bucket: "_tasks") 	|> range(start: -1m) 	|> filter(fn: (r) => r._measurement == "runs") 	|> filter(fn: (r) => r._field == "finishedAt") 	|> aggregateWindow(every: 1m fn: mean, createEmpty: false) option task = { 	name: "Name this Check", 	every: 1m, 	offset: 0s check = { 	_check_id: "undefined", 	_check_name: "Name this Check", 	_type: "custom", 	tags: {a: "b",c: "d"} warn = (r) =>(r.finishedAt> 20) crit = (r) =>(r.finishedAt> 20) info = (r) =>(r.finishedAt> 20) messageFn = (r) =>("Check: ${ r._check_name } is: ${ r._level }") data 	|> v1.fieldsAsCols() 	|> monitor.check(data: check, messageFn:messageFn, warn:warn, crit:crit, info:info) GenerateFlux returns the check query text directly sanitizeFlux modifies the check query text to include correct _check_id param in check object Valid checks whether check flux is valid, returns error if invalid add or replace _check_id parameter on the check object MarshalJSON implement json.Marshaler interface. Type returns the type of the check. GetOwnerID gets the ownerID associated with a Check. SetCreatedAt sets the creation time for a check SetUpdatedAt sets the update time for a check SetID sets the primary key for a check SetOrgID is SetOrgID SetName implements influxdb.Updator interface SetDescription is SetDescription GetID is GetID GetCRUDLog gets crudLog GetOrgID gets the orgID associated with the Check GetDescription is GetDescription/Users/austinjaybecker/projects/abeck-go-testing/notification/check/deadman.gostatementsobjectPropsjson:"timeSince,omitempty"`json:"timeSince,omitempty"`json:"staleTime,omitempty"`json:"staleTime,omitempty"`expect a single file to be returned from query parsing got %d"expect a single file to be returned from query parsing got %d"influxdata/influxdb/monitor"influxdata/influxdb/monitor"experimental"experimental"influxdata/influxdb/v1"influxdata/influxdb/v1"MemberExpressiondead"dead"subDuration"subDuration""from"ExpressionStatementPipeExpressionfieldsAsCols"fieldsAsCols""monitor" Deadman is the deadman check. If only zero values reported since time, trigger alert. TODO(desa): Is this implemented in Flux? GenerateFlux returns a flux script for the Deadman provided. GenerateFluxAST returns a flux AST for the deadman provided. If there are any errors in the flux that the user provided the function will return an error for each error found when the script is parsed. TODO(desa): this is a hack that we had to do as a result of https://github.com/influxdata/flux/issues/1701 when it is fixed we should use a separate file and not manipulate the existing one. This assumes that the ThresholdConfigs we've been provided do not have duplicates.ArgumentTypeMutateArgumentTypeMutateObjectTypePropertyTypeMutatePropertyTypeExpressionTypeMutateExpressionType/Users/austinjaybecker/projects/abeck-go-testing/notification/check/threshold.gotdRawtdRawsfoundCreateEmptyobjnewEverysubPipethresholdStatementsfnBodylesserAliasgreaterAliasrangeAliasjson:"min"`json:"min"`json:"max"`json:"max"`lesser"lesser"greater"greater"range"range"invalid threshold type %s"invalid threshold type %s"expected a single field but got: %s"expected a single field but got: %s"aggregateWindow"aggregateWindow"createEmpty"createEmpty"UnaryExpressionOperatorKindBinaryExpressionexpected there to be a single statement in the flux script body, received %d"expected there to be a single statement in the flux script body, received %d"statement is not an *ast.Expression statement, received %T"statement is not an *ast.Expression statement, received %T"expression is not an *ast.PipeExpression statement, received %T"expression is not an *ast.PipeExpression statement, received %T"yield"yield"GreaterThanFloatLiteralLogicalExpressionLogicalOperatorKindjson:"allValues"`json:"allValues"`range threshold min can't be larger than max"range threshold min can't be larger than max" Threshold is the threshold check. Valid returns error if something is invalid. UnmarshalJSON implement json.Unmarshaler interface. GenerateFlux returns a flux script for the threshold provided. If there GenerateFluxAST returns a flux AST for the threshold provided. If there TODO(desa): we'll likely want something slightly more sophisitcated long term, but this should work for now. TODO(desa): we'll likely want to remove all other arguments to range that are provided, but for now this should work. When we decide to implement the full feature we'll have to do something more sophisticated. ThresholdConfig is the base of all threshold config. ThresholdConfigBase is the base of all threshold config. If true, only alert if all values meet threshold. GetLevel return the check level. Lesser threshold type. Type of the threshold config. Greater threshold type. Range threshold type. Valid overwrite the base threshold.MutateOperatorLeftTypeMutateLeftTypeRightTypeMutateRightTypeMutateValue/Users/austinjaybecker/projects/abeck-go-testing/notification/duration.goFromTimeDurationParseCheckLevelStatusRulecheckLevelMapscheckLevelsparseDurationmagnitudegithub.com/influxdata/flux/codes"github.com/influxdata/flux/codes"invalid rune in duration"invalid rune in duration"IsDigitDocURLWithDocURLInvalidinvalid duration %s"invalid duration %s"IsLetterduration is missing a unit: %s"duration is missing a unit: %s"Âµs"Âµs" Duration is a custom type used for generating flux compatible durations. TimeDuration convert notification.Duration to time.Duration. MarshalJSON turns a Duration into a JSON-ified string. UnmarshalJSON turns a flux duration literal into a Duration. FromTimeDuration converts a time.Duration to a notification.Duration type. TODO(jsternberg): This file copies over code from an internal package because we need them from an internal package and the only way they are exposed is through a package that depends on the core flux parser. We want to avoid a dependency on the core parser so we copy these implementations. In the future, we should consider exposing these functions from flux in a non-internal package outside of the parser package. parseDuration will convert a string into components of the duration./Users/austinjaybecker/projects/abeck-go-testing/notification/endpoint/Users/austinjaybecker/projects/abeck-go-testing/notification/endpoint/endpoint.goHTTPHTTPTypePagerDutyTypeSlackTypeTelegramTypegoodHTTPAuthMethodgoodHTTPMethodhttpPasswordSuffixhttpTokenSuffixhttpUsernameSuffixpagerdutyAliasroutingKeySuffixslackAliasslackTokenSuffixtelegramTokenSuffixtypeToEndpointslack"slack"pagerduty"pagerduty"telegram"telegram"ClientURLAuthMethodContentTemplateParseResponseunable to detect the notification endpoint type from json"unable to detect the notification endpoint type from json"invalid notification endpoint type %s"invalid notification endpoint type %s"Notification Endpoint ID is invalid"Notification Endpoint ID is invalid"Notification Endpoint Name can't be empty"Notification Endpoint Name can't be empty"invalid status"invalid status" types of endpoints. UnmarshalJSON will convert the bytes to notification endpoint. Base is the embed struct of every notification endpoint. GetStatus implements influxdb.Getter interface. SetStatus implements influxdb.Updator interface./Users/austinjaybecker/projects/abeck-go-testing/notification/endpoint/http.gohttpAliasarrjson:"headers,omitempty"`json:"headers,omitempty"`json:"token,omitempty"`json:"token,omitempty"`json:"authMethod"`json:"authMethod"`json:"method"`json:"method"`json:"contentTemplate"`json:"contentTemplate"`bearer"bearer"MethodPuthttp endpoint URL is empty"http endpoint URL is empty"http endpoint URL is invalid: %s"http endpoint URL is invalid: %s"invalid http http method"invalid http http method"invalid http auth method"invalid http auth method"invalid http username/password for basic auth"invalid http username/password for basic auth"invalid http token for bearer auth"invalid http token for bearer auth" HTTP is the notification endpoint config of http. Path is the API path of HTTP Token is the bearer token for authorization BackfillSecretKeys fill back fill the secret field key during the unmarshalling if value of that secret field is not nil. SecretFields return available secret fields. Valid returns error if some configuration is invalid Type returns the type. ParseResponse will parse the http response from http./Users/austinjaybecker/projects/abeck-go-testing/notification/endpoint/pagerduty.gojson:"clientURL"`json:"clientURL"`pagerduty routing key is invalid"pagerduty routing key is invalid" PagerDuty is the notification endpoint config of pagerduty. ClientURL is the url that is presented in the PagerDuty UI when this alert is triggered RoutingKey is a version 4 UUID expressed as a 32-digit hexadecimal number. This is the Integration Key for an integration on any given service./Users/austinjaybecker/projects/abeck-go-testing/notification/endpoint/service/Users/austinjaybecker/projects/abeck-go-testing/notification/endpoint/service/service.goErrNotificationEndpointNotFoundfilterEndpointsFnnewEndpointStorenotificationEndpointBucketnotificationEndpointIndexBucketupdatedEndpoint Service provides all the notification endpoint service behavior. New constructs a new Service./Users/austinjaybecker/projects/abeck-go-testing/notification/endpoint/service/store.gonotification endpoint not found"notification endpoint not found"notification endpoint"notification endpoint" ErrNotificationEndpointNotFound is used when the notification endpoint is not found. TODO(jsteenb2): every above here moves into service layer PutNotificationEndpoint put a notification endpoint to storage. TODO(jsteenb2): all the stuffs before the update should be moved up into the  service layer as well as all the id/time setting items FindNotificationEndpoints returns a list of notification endpoints that match isNext and the total count of matching notification endpoints. DeleteNotificationEndpoint removes a notification endpoint by ID./Users/austinjaybecker/projects/abeck-go-testing/notification/endpoint/service/testing/Users/austinjaybecker/projects/abeck-go-testing/notification/endpoint/service/testing/service.goNotificationEndpointFieldsfakeDatefakeGeneratorfiveIDfourIDinfluxErrsEqualnotificationEndpointCmpOptionsoneIDsixIDthreeIDtimeGen1timeGen2twoIDabwantedwbname3status3deletedEndpointassertgithub.com/stretchr/testify/assert"github.com/stretchr/testify/assert"github.com/stretchr/testify/require"github.com/stretchr/testify/require"2006July"Sort""CreateNotificationEndpoint""FindNotificationEndpointByID""FindNotificationEndpoints""UpdateNotificationEndpoint""PatchNotificationEndpoint""DeleteNotificationEndpoint"basic create notification endpoint"basic create notification endpoint"org1"org1"name2"name2"example-pagerduty.com"example-pagerduty.com"pagerduty secret2"pagerduty secret2"%s-routing-key"%s-routing-key"failed to retrieve notification endpoints: %v"failed to retrieve notification endpoints: %v"notificationEndpoints are different -got/+want
diff %s"notificationEndpoints are different -got/+want\ndiff %s"failed to retrieve secrets for endpoint: %v"failed to retrieve secrets for endpoint: %v"TestingTbad id"bad id"name1"name1"example-slack.com"example-slack.com"%s-token"%s-token"no key was provided for notification endpoint"no key was provided for notification endpoint"basic find telegraf config by id"basic find telegraf config by id"notification endpoint is different -got/+want
diff %s"notification endpoint is different -got/+want\ndiff %s"find nothing (empty set)"find nothing (empty set)"find all notification endpoints"find all notification endpoints"filter by organization id only"filter by organization id only"org4"org4"edp1"edp1"edp2"edp2"edp3"edp3"example-pagerduty2.com"example-pagerduty2.com"find options limit"find options limit"example-webhook.com"example-webhook.com"edp4"edp4"find options offset"find options offset"find by id"find by id"look for organization not bound to any notification endpoint"look for organization not bound to any notification endpoint"find nothing"find nothing"notification endpoints length is different got %d, want %d"notification endpoints length is different got %d, want %d"notification endpoints are different -got/+want
diff %s"notification endpoints are different -got/+want\ndiff %s"can't find the id"can't find the id"pager-duty-routing-key-2"pager-duty-routing-key-2"notification endpoint not found for key "0000000000000004"`notification endpoint not found for key "0000000000000004"`regular update"regular update""name3"secret value"secret value"update secret"update secret"pager-duty-value2"pager-duty-value2"Truefdid not get a pager duty endpoint; got: %#v"did not get a pager duty endpoint; got: %#v"NotZeroNoErrorsecretFldsnone existing endpoint"none existing endpoint"regular delete"regular delete"delete notification endpoint secret fields are different -got/+want
diff %s"delete notification endpoint secret fields are different -got/+want\ndiff %s"delete notification endpoint org id is different -got/+want
diff %s"delete notification endpoint org id is different -got/+want\ndiff %s"expected errors to be nil got '%v'"expected errors to be nil got '%v'"NotContainsexpected: %s got err: %s"expected: %s got err: %s" NotificationEndpointFields includes prepopulated data for mapping tests. NotificationEndpointService tests all the service functions. CreateNotificationEndpoint testing. FindNotificationEndpointByID testing. FindNotificationEndpoints testing UpdateNotificationEndpoint testing. zero out times PatchNotificationEndpoint testing.userID           influxdb.ID DeleteNotificationEndpoint testing./Users/austinjaybecker/projects/abeck-go-testing/notification/endpoint/slack.goslack endpoint URL must be provided"slack endpoint URL must be provided"slack endpoint URL is invalid: %s"slack endpoint URL is invalid: %s" Slack is the notification endpoint config of slack. URL is a valid slack webhook URL TODO(jm): validate this in unmarshaler example: https://slack.com/api/chat.postMessage/Users/austinjaybecker/projects/abeck-go-testing/notification/endpoint/telegram.gotelegramAliasempty telegram bot token"empty telegram bot token"empty telegram channel"empty telegram channel" Telegram is the notification endpoint config of telegram. Token is the telegram bot token, see https://core.telegram.org/bots#creating-a-new-bot Channel is an ID of the telegram channel, see https://core.telegram.org/bots/api#sendmessage BackfillSecretKeys fill back the secret field key during the unmarshalling/Users/austinjaybecker/projects/abeck-go-testing/notification/endpoint/testing/Users/austinjaybecker/projects/abeck-go-testing/notification/endpoint/testing/service.go/Users/austinjaybecker/projects/abeck-go-testing/notification/flux/Users/austinjaybecker/projects/abeck-go-testing/notification/flux/ast.goDictionaryFuncBlockIfObjectWithSubtractappendPipeimportsalternateconsequentteststmspkgsGreaterThanOperatorLessThanOperatorEqualOperatorSubtractionOperatorAdditionOperatorAndOperatorOrOperatorConditionalExpressionConsequentAlternatemust pipe forward to at least one *ast.CallExpression"must pipe forward to at least one *ast.CallExpression"BlockArrayExpressionElements File creates a new *ast.File. GreaterThan returns a greater than *ast.BinaryExpression. LessThan returns a less than *ast.BinaryExpression. Equal returns an equal to *ast.BinaryExpression. Subtract returns a subtraction *ast.BinaryExpression. Add returns a addition *ast.BinaryExpression. Member returns an *ast.MemberExpression where the key is p and the values is c. And returns an and *ast.LogicalExpression. Or returns an or *ast.LogicalExpression. If returns an *ast.ConditionalExpression Pipe returns a *ast.PipeExpression that is a piped sequence of call expressions starting at base. It requires at least one call expression and will panic otherwise. Call returns a *ast.CallExpression that is a function call of fn with args. ExpressionStatement returns an *ast.ExpressionStatement of e. Function returns an *ast.FunctionExpression with params with body b. FuncBlock takes a series of statements and produces a function. String returns an *ast.StringLiteral of s. Bool returns an *ast.BooleanLiteral of b. Duration returns an *ast.DurationLiteral for a single duration. Identifier returns an *ast.Identifier of i. Float returns an *ast.FloatLiteral of f. Integer returns an *ast.IntegerLiteral of i. Negative returns *ast.UnaryExpression for -(e). DefineVariable returns an *ast.VariableAssignment of id to the e. (e.g. id = <expression>) DefineTaskOption returns an *ast.OptionStatement with the object provided. (e.g. option task = {...}) Property returns an *ast.Property of key to e. (e.g. key: <expression>) Dictionary returns an *ast.Property of string key to value expression. Object returns an *ast.ObjectExpression with properties ps. ObjectWith adds many properties to an existing named identifier. Array returns *ast.ArrayExpression with elements es. FunctionParams returns a slice of *ast.Property for the parameters of a function. Imports returns a []*ast.ImportDeclaration for each package in pkgs. ImportDeclaration returns an *ast.ImportDeclaration for pkg.ElementsLengthTestTypeMutateTestTypeConsequentTypeMutateConsequentTypeAlternateTypeMutateAlternateTypeWrappedExpressionExprTypeMutateExprType/Users/austinjaybecker/projects/abeck-go-testing/notification/rule/Users/austinjaybecker/projects/abeck-go-testing/notification/rule/http.goactionFromLevelgenerateTimeincreaseDurpagerDutyAliasseverityFromLeveltypeToRulehttpEndpointSleepUntilRunbookLinkgenerateFluxASTNotificationDefinitiongenerateLevelChecksgenerateLevelCheckgenerateFluxASTStatusesgenerateHeadersgenerateFluxASTEndpointgenerateFluxASTNotifyPipegenerateBodypackagesendpointBodyendpointFnendpointPropsendpoint provided is a %s, not an HTTP endpoint"endpoint provided is a %s, not an HTTP endpoint""main"influxdata/influxdb/secrets"influxdata/influxdb/secrets""get"basicAuth"basicAuth""endpoint""encode"ReturnStatement"notification"mapFn"mapFn""notify"all_statuses"all_statuses"_version"_version" HTTP is the notification rule config of http. GenerateFlux generates a flux script for the http notification rule. GenerateFluxAST generates a flux AST for the http notification rule. {r with "_version": 1} Valid returns where the config is valid. Type returns the type of the rule config.generateFluxASTSecretsgenerateSilentgenerateSlackColors/Users/austinjaybecker/projects/abeck-go-testing/notification/rule/pagerduty.gopagerdutyEndpointjson:"messageTemplate"`json:"messageTemplate"`pagerduty invalid message template"pagerduty invalid message template"endpoint provided is a %s, not an PagerDuty endpoint"endpoint provided is a %s, not an PagerDuty endpoint"pagerduty_secret"pagerduty_secret"pagerduty_endpoint"pagerduty_endpoint""client"influxdata"influxdata"clientURL"clientURL"r._check_name"r._check_name"_source_measurement"_source_measurement"eventAction"eventAction"_notification_rule_name"_notification_rule_name""summary"_message"_message""severityFromLevel"_level"_level""actionFromLevel"_source_timestamp"_source_timestamp" PagerDuty is the rule config of pagerduty notification. GenerateFlux generates a flux script for the pagerduty notification rule. GenerateFluxAST generates a flux AST for the pagerduty notification rule. routing_key: required string A version 4 UUID expressed as a 32-digit hexadecimal number. This is the Integration Key for an integration on any given service. client: optional name of the client sending the alert. clientURL url of the client sending the alert. class: The class/type of the event, for example ping failure or cpu load group: Logical grouping of components of a service, for example app-stack severity: The perceived severity of the status the event is describing with respect to the affected system. This can be critical, error, warning or info. event_action: string trigger The type of event. Can be trigger, acknowledge or resolve. See Event Action. source: The unique location of the affected system, preferably a hostname or FQDN summary: A brief text summary of the event, used to generate the summaries/titles of any associated alerts. The maximum permitted length of this property is 1024 characters. timestamp: timestamp (rfc3339 milliseconds) The time at which the emitting tool detected or generated the event./Users/austinjaybecker/projects/abeck-go-testing/notification/rule/rule.gotagRuleruleIDruleNamestmtstablestimeFilterfromLeveltoLevelfilterTagisNRTagInFilterTagsNRtagTypunable to detect the notification type from json"unable to detect the notification type from json"invalid notification type %s"invalid notification type %s"json:"endpointID,omitempty"`json:"endpointID,omitempty"`json:"sleepUntil,omitempty"`json:"sleepUntil,omitempty"`json:"runbookLink"`json:"runbookLink"`json:"tagRules,omitempty"`json:"tagRules,omitempty"`json:"statusRules,omitempty"`json:"statusRules,omitempty"`Notification Rule ID is invalid"Notification Rule ID is invalid"Notification Rule Name can't be empty"Notification Rule Name can't be empty"Notification Rule OwnerID is invalid"Notification Rule OwnerID is invalid"Notification Rule OrgID is invalid"Notification Rule OrgID is invalid"Notification Rule EndpointID is invalid"Notification Rule EndpointID is invalid"if limit is set, limit and limitEvery must be larger than 0"if limit is set, limit and limitEvery must be larger than 0"_notification_rule_id"_notification_rule_id"_notification_endpoint_id"_notification_endpoint_id"_notification_endpoint_name"_notification_endpoint_name"GreaterThanEqualOperator_time"_time""fn"union"union""tables"statuses"statuses"stateChanges"stateChanges""fromLevel""toLevel"%s_to_%s"%s_to_%s" Base is the embed struct of every notification rule. SleepUntil is an optional sleeptime to start a task. increaseDur increases the duration of leading duration in a duration literal. It is used so that we will have overlapping windows. If the unit of the literal is `s`, we double the interval; otherwise we increase the value by 1. The reason for this is to that we query the minimal amount of time that is likely to have data in the time range. This is currently a hack around https://github.com/influxdata/flux/issues/1877 Make the windows overlap and filter records from previous queries. This is so that we wont miss the first points possible state change. GetEndpointID gets the endpointID for a base. GetTaskID gets the task ID for a base. SetTaskID sets the task ID for a base. ClearPrivateData clears the task ID from the base. MatchesTags returns true if the Rule matches all of the tags for each tag in NR if there exists a key value match with operator == equal in tags or a key match with a value mismatch with operator == notequal in tags then true GetOwnerID returns the owner id. GetLimit returns the limit pointer. SetOwnerID will set the owner id./Users/austinjaybecker/projects/abeck-go-testing/notification/rule/service/Users/austinjaybecker/projects/abeck-go-testing/notification/rule/service/service.goErrInvalidNotificationRuleIDErrNotificationRuleNotFoundInternalNotificationRuleStoreErrorUnavailableNotificationRuleStoreErrorfilterNotificationRulesFnepgithub.com/influxdata/influxdb/v2/pkg/pointer"github.com/influxdata/influxdb/v2/pkg/pointer"notification rule not found"notification rule not found"provided notification rule ID has invalid format"provided notification rule ID has invalid format"Unable to connect to notification rule store service. Please try again; Err: %v"Unable to connect to notification rule store service. Please try again; Err: %v"kv/notificationRule"kv/notificationRule"Unknown internal notificationRule data error; Err: %v"Unknown internal notificationRule data error; Err: %v"failed to remove task for invalid notification rule"failed to remove task for invalid notification rule" ErrNotificationRuleNotFound is used when the notification rule is not found. ErrInvalidNotificationRuleID is used when the service was provided RuleService is an implementation of the influxdb CheckService It is backed by the kv store abstraction. New constructs and configures a notification rule service UnavailableNotificationRuleStoreError is used if we aren't able to interact with the store, it means the store is not available at the moment (e.g. network). InternalNotificationRuleStoreError is used when the error comes from an CreateNotificationRule creates a new notification rule and sets b.ID with the new identifier. set notification rule ID set notification rule created / updated times create backing task and set ID (in inactive state initially) remove associated task set task to notification rule create status create task initially in inactive status PutNotificationRule put a notification rule to storage. FindNotificationRuleByID returns a single notification rule by ID. forEachNotificationRule will iterate through all notification rules while fn returns true./Users/austinjaybecker/projects/abeck-go-testing/notification/rule/slack.goslackEndpointendpoint provided is a %s, not an Slack endpoint"endpoint provided is a %s, not an Slack endpoint"slack_secret"slack_secret"slack_endpoint"slack_endpoint""channel"danger"danger"warn"warn""good"slack msg template is empty"slack msg template is empty" Slack is the notification rule config of slack. GenerateFlux generates a flux script for the slack notification rule. GenerateFluxAST generates a flux AST for the slack notification rule. TODO(desa): are these values correct?/Users/austinjaybecker/projects/abeck-go-testing/notification/rule/telegram.gotelegramEndpointendpoint provided is a %s, not a Telegram endpoint"endpoint provided is a %s, not a Telegram endpoint"contrib/sranka/telegram"contrib/sranka/telegram"telegram_secret"telegram_secret"parseMode"parseMode"disableWebPagePreview"disableWebPagePreview"telegram_endpoint"telegram_endpoint"silent"silent"Telegram MessageTemplate is invalid"Telegram MessageTemplate is invalid" Telegram is the notification rule config of telegram. GenerateFlux generates a flux script for the telegram notification rule. GenerateFluxAST generates a flux AST for the telegram notification rule./Users/austinjaybecker/projects/abeck-go-testing/notification/status.gojson:"currentLevel"`json:"currentLevel"`json:"previousLevel"`json:"previousLevel"`INFO"INFO"WARN"WARN"CRIT"CRIT"ANY"ANY" StatusRule includes parameters of status rules. CheckLevel is the enum value of status levels. consts of CheckStatusLevel MarshalJSON implements json.Marshaller. UnmarshalJSON implements json.Unmarshaller. String returns the string value, invalid CheckLevel will return Unknown. ParseCheckLevel will parse the string to checkLevel/Users/austinjaybecker/projects/abeck-go-testing/notification/tag.go TagRule is the struct of tag rule. Valid returns error for invalid operators. GenerateFluxAST generates the AST expression for a tag rule. TODO(desa): have this work for all operator types/Users/austinjaybecker/projects/abeck-go-testing/notification.gojson:"limitEvery,omitempty"`json:"limitEvery,omitempty"`Notification Rule Description can't be empty"Notification Rule Description can't be empty" NotificationRule is a *Query* of a *Status Bucket* that returns the *Status*. When warranted by the rules, sends a *Message* to a 3rd Party using the *Notification Endpoint* and stores a receipt in the *Notifications Bucket*. NotificationRuleStore represents a service for managing notification rule. UpdateNotificationRuleUpdateNotificationRule updates a single notification rule. Limit don't notify me more than <limit> times every <limitEvery> seconds. If set, limit cannot be empty. every seconds. NotificationRuleFilter represents a set of filter that restrict the returned notification rules. QueryParams Converts NotificationRuleFilter fields to url query params. NotificationRuleCreate is the struct providing data to create a Notification Rule. NotificationRuleUpdate is the set of upgrade fields for patch request. Valid will verify if the NotificationRuleUpdate is valid./Users/austinjaybecker/projects/abeck-go-testing/notification_endpoint.gounknown notification endpoint type"unknown notification endpoint type"FindNotificationEndpoint"FindNotificationEndpoint"Notification Endpoint Description can't be empty"Notification Endpoint Description can't be empty" ErrInvalidNotificationEndpointType denotes that the provided NotificationEndpoint is not a valid type NotificationEndpoint is the configuration describing how to call a 3rd party service. E.g. Slack, Pagerduty NotificationEndpointFilter represents a set of filter that restrict the returned notification endpoints. QueryParams Converts NotificationEndpointFilter fields to url query params. NotificationEndpointUpdate is the set of upgrade fields for patch request. Valid will verify if the NotificationEndpointUpdate is valid. NotificationEndpointService represents a service for managing notification endpoints./Users/austinjaybecker/projects/abeck-go-testing/onboarding.gojson:"bucket"`json:"bucket"`json:"retentionPeriodHrs,omitempty"`json:"retentionPeriodHrs,omitempty"`username is empty"username is empty"org name is empty"org name is empty"bucket name is empty"bucket name is empty" OnboardingService represents a service for the first run. OnboardUser creates a new user/org/buckets OnboardingResults is a group of elements required for first run. OnboardingRequest is the request to setup defaults./Users/austinjaybecker/projects/abeck-go-testing/operation_log.go OperationLogEntry is a record in an operation log. DashboardOperationLogService is an interface for retrieving the operation log for a dashboard. BucketOperationLogService is an interface for retrieving the operation log for a bucket. UserOperationLogService is an interface for retrieving the operation log for a user. OrganizationOperationLogService is an interface for retrieving the operation log for an org. DefaultOperationLogFindOptions are the default options for the operation log./Users/austinjaybecker/projects/abeck-go-testing/organization.goPutOrganization"PutOrganization"Please provide either orgID or org"Please provide either orgID or org"unexpected error in organizations; Err: %v"unexpected error in organizations; Err: %v" Organization is an organization. ð errors of org ErrOrgNameisEmpty is error when org name is empty ops for orgs error and orgs op logs. OrganizationService represents a service for managing organization data. Returns a single organization by ID. Returns the first organization that matches filter. Returns a list of organizations that match filter and the total count of matching organizations. Creates a new organization and sets b.ID with the new identifier. Updates a single organization with changeset. Returns the new organization state after update. Removes a organization by ID. OrganizationUpdate represents updates to a organization. ErrInvalidOrgFilter is the error indicate org filter is empty OrganizationFilter represents a set of filter that restrict the returned results./Users/austinjaybecker/projects/abeck-go-testing/paging.gonextOffsetprevOffsetoffset is invalid"offset is invalid"decoding after: %w"decoding after: %w"limit is invalid"limit is invalid""sortBy"descending is invalid"descending is invalid" PagingFilter represents a filter containing url query params. QueryParams returns a map containing url query params. PagingLinks represents paging links. FindOptions represents options passed to all find methods with multiple results. GetLimit returns the resolved limit between then limit boundaries. Given a limit <= 0 it returns the default limit. DecodeFindOptions returns a FindOptions decoded from http request. NewPagingLinks returns a PagingLinks. num is the number of returned results./Users/austinjaybecker/projects/abeck-go-testing/passwords.go PasswordsService is the service for managing basic auth passwords./Users/austinjaybecker/projects/abeck-go-testing/pkg/Users/austinjaybecker/projects/abeck-go-testing/pkg/binaryutil/Users/austinjaybecker/projects/abeck-go-testing/pkg/binaryutil/binaryutil.goUvarintSizeVarintSizeuxbinaryutil VarintSize returns the number of bytes to varint encode x. This code is copied from encoding/binary.PutVarint() with the buffer removed. UvarintSize returns the number of bytes to uvarint encode x. This code is copied from encoding/binary.PutUvarint() with the buffer removed./Users/austinjaybecker/projects/abeck-go-testing/pkg/bloom/Users/austinjaybecker/projects/abeck-go-testing/pkg/bloom/bloom.goEstimateNewFilterNewFilterBufferpow2xxhashbloom"github.com/cespare/xxhash"bloom.Filter: buffer bit count must a power of two: %d/%d"bloom.Filter: buffer bit count must a power of two: %d/%d"bloom.Filter.Merge(): m mismatch: %d <> %d"bloom.Filter.Merge(): m mismatch: %d <> %d"bloom.Filter.Merge(): k mismatch: %d <> %d"bloom.Filter.Merge(): k mismatch: %d <> %d"4611686018427387904 This package implements a limited bloom filter implementation based on Will Fitzgerald's bloom & bitset packages. It uses a zero-allocation xxhash implementation, rather than murmur3. It's implemented locally to support zero-copy memory-mapped slices. This also optimizes the filter by always using a bitset size with a power of 2. Filter represents a bloom filter. NewFilter returns a new instance of Filter using m bits and k hash functions. If m is not a power of two then it is rounded to the next highest power of 2. NewFilterBuffer returns a new instance of a filter using a backing buffer. The buffer length MUST be a power of 2. Len returns the number of bits used in the filter. K returns the number of hash functions used in the filter. Bytes returns the underlying backing slice. Clone returns a copy of f. Insert inserts data to the filter. Contains returns true if the filter possibly contains v. Returns false if the filter definitely does not contain v. Merge performs an in-place union of other into f. Returns an error if m or k of the filters differs. Ensure m & k fields match. Perform union of each byte. location returns the ith hashed location using two hash values. hash returns two 64-bit hashes based on the output of xxhash. We'll put the original byte back. Estimate returns an estimated bit count and hash count given the element count and false positive rate. pow2 returns the number that is the next highest power of 2. Returns v if it is a power of 2./Users/austinjaybecker/projects/abeck-go-testing/pkg/bytesutil/Users/austinjaybecker/projects/abeck-go-testing/pkg/bytesutil/bytesutil.goCloneSliceIsSortedPackSearchBytesSearchBytesFixedSortDedupbyteSlicesjStartbytesutilx is not a multiple of a: %d %d"x is not a multiple of a: %d %d" Sort sorts a slice of byte slices. SortDedup sorts the byte slice a and removes duplicates. The ret SearchBytes performs a binary search for x in the sorted slice a. Define f(i) => bytes.Compare(a[i], x) < 0 Define f(-1) == false and f(n) == true. Invariant: f(i-1) == false, f(j) == true. avoid overflow when computing h i â¤ h < j preserves f(i-1) == false preserves f(j) == true i == j, f(i-1) == false, and f(j) (= f(i)) == true  =>  answer is i. Contains returns true if x is an element of the sorted slice a. SearchBytesFixed searches a for x using a binary search.  The size of a must be a multiple of of x or else the function panics.  There returned value is the index within a where x should exist.  The caller should ensure that x does exist at this index. Union returns the union of a & b in sorted order. Intersect returns the intersection of a & b in sorted order. Clone returns a copy of b. CloneSlice returns a copy of a slice of byte slices. Pack converts a sparse array to a dense one.  It removes sections of a containing runs of val of length width.  The returned value is a subslice of a. Skip the first run that won't move Find the next gap to remove Find the next non-gap to keep Move the non-gap over the section to remove./Users/austinjaybecker/projects/abeck-go-testing/pkg/csv2lp/Users/austinjaybecker/projects/abeck-go-testing/pkg/csv2lp/csv2lp.goCreateRowColumnErrorCsvColumnErrorCsvLineErrorIsTypeSupportedNewLineReaderSizeannotationCommentappendConvertedappendProtocolValuebase64BinaryDataTypeboolDatatypecomputedReplacerconcatSetupTableconstantSetupTablecreateBoolParseFncreateColumnscreateConstantOrConcatColumncreateStrictLongParseFncreateStrictUnsignedLongParseFndataFormatNumberdateTimeDatatypedecodeNopdefaultBufSizedoubleDatatypedurationDatatypeescapeStringignoreLeadingCommentlabelFieldNamelabelFieldValuelabelMeasurementlabelStartlabelStoplabelTimelinePartFieldlinePartIgnoredlinePartMeasurementlinePartTaglinePartTimelongDatatypemultiClosernormalizeNumberStringparseTimeZonereplaceMeasurementreplaceQuotedreplaceTagskipFirstLinesstringDatatypesupportedAnnotationssupportedDataTypestoTypedValueuLongDatatypecolumnLabelline %d: %v"line %d: %v"ParseErrorErrFieldCountsep="sep=" Package csv2lp transforms CSV data to InfluxDB line protocol CsvLineError is returned for csv conversion errors 1 is the first line CreateRowColumnError wraps an existing error to add line and column coordinates CsvToLineReader represents state of transformation from csv data to lien protocol reader csv reading lineReader is used to report line number of the last read CSV line Table collects information about used columns LineNumber represents line number of csv.Reader, 1 is the first when true, log table data columns before reading data rows state variable that indicates whether any data row was read log CSV data errors to sterr and continue with CSV processing RowSkipped is called when a row is skipped because of data parsing error reader results LogTableColumns turns on/off logging of table data columns before reading data rows SkipRowOnError controls whether to fail on every CSV conversion error (false) or to log the error and continue (true) Comma returns a field delimiter used in an input CSV file Read implements io.Reader that returns protocol lines state1: finished state2: some data are in the buffer to copy we have remaining bytes to copy copy a part of the buffer copy the entire buffer state3: fill buffer with data to read from Read each record from csv every row can have different number of columns separator specified in the first line reuse line buffer CsvToLineProtocol transforms csv data into line protocol data start counting from 1setupColumnsetupTableisTableAnnotationskipLinesquotedStringmc/Users/austinjaybecker/projects/abeck-go-testing/pkg/csv2lp/csv_annotations.gocolumnValuecommentannotationNamedataTypeIndexplaceholderColumnplaceholderplaceholderstzintVal#constant"#constant"\$\{[^}]+\}`\$\{[^}]+\}`#concat"#concat"WARNING: column %s: column '%s' cannot be replaced, no such column available"WARNING: column %s: column '%s' cannot be replaced, no such column available"'%s' references an unknown column '%s', available columns are: %v"'%s' references an unknown column '%s', available columns are: %v"#group"#group"#datatype"#datatype"#default"#default"#timezone"#timezone"#timezone annotation: %v"#timezone annotation: %v"TrimLeft"local"[+-][0-9][0-9][0-9][0-9]"[+-][0-9][0-9][0-9][0-9]"timezone '%s' is not  +hhmm or -hhmm"timezone '%s' is not  +hhmm or -hhmm"3600FixedZone annotationComment describes CSV annotation prefix in a CSV row that recognizes this annotation flag is 0 to represent an annotation that is used for all data rows or a unique bit (>0) between supported annotation prefixes setupColumn setups metadata that drives the way of how column data are parsed, mandatory when flag > 0 setupTable setups metadata that drives the way of how the table data are parsed, mandatory when flag == 0 isTableAnnotation returns true for a table-wide annotation, false for column-based annotations matches tests whether an annotationComment can process the CSV comment row adds a virtual column with constant value to all data rows supported types of constant annotation rows are:  1. "#constant,datatype,label,defaultValue"  2. "#constant,measurement,value"  3. "#constant,dateTime,value"  4. "#constant datatype,label,defaultValue"  5. "#constant measurement,value"  6. "#constant dateTime,value" defaultValue is optional, additional columns are ignored this is a virtual column that never extracts data from data rows setup column data type type 1,2,3 type 4,5,6 setup label if available setup defaultValue if available support type 2,3,5,6 syntax for measurement and timestamp type 2,3,5,6 setup a label if no label is supplied for focused error messages add a virtual column to the table constantSetupTable setups the supplied CSV table from #constant annotation computedReplacer is used to replace value in computed columns concatSetupTable setups the supplied CSV table from #concat annotation ${columnLabel} add validator to report error when no placeholder column is available supportedAnnotations contains all supported CSV annotations comments standard flux query result annotation setup column's line part unless it is already set (#19452) setup timezone for parsing timestamps, UTC by default #timezone,Local ignoreLeadingComment returns a value without '#anyComment ' prefix parseTimeZone parses the supplied timezone from a string into a time.Location  parseTimeZone("")      // time.UTC  parseTimeZone("local") // time.Local  parseTimeZone("-0500") // time.FixedZone(-5*3600 + 0*60)  parseTimeZone("+0200") // time.FixedZone(2*3600 + 0*60)  parseTimeZone("EST")   // time.LoadLocation("EST")/Users/austinjaybecker/projects/abeck-go-testing/pkg/csv2lp/csv_table.gocolonIndexpipeIndexretValrowSizesupportedAnnotationdefaultIsFieldtimeValfieldAddedlineNumber_value"_value"_start"_start"_stop"_stop"|"|""ignore"dateTime"dateTime"long"strict"unsignedLongcolumn '%s': %v"column '%s': %v"CsvTable{ dataColumns: %d constantColumns: %d
"CsvTable{ dataColumns: %d constantColumns: %d\n" measurement: %+v
" measurement: %+v\n" tag:         %+v
" tag:         %+v\n" field:       %+v
" field:       %+v\n" time:        %+v
" time:        %+v\n"'|'WARNING: "WARNING: "WARNING: unsupported annotation: "WARNING: unsupported annotation: "WARNING: at most one dateTime column is expected, '%s' column is ignored
"WARNING: at most one dateTime column is expected, '%s' column is ignored\n"WARNING: ignoring duplicate tag '%s' at column index %d, using column at index %d
"WARNING: ignoring duplicate tag '%s' at column index %d, using column at index %d\n"data type '%s' is not supported"data type '%s' is not supported"no measurement column found"no measurement column found"no measurement supplied"no measurement supplied"no field data found"no field data found" column labels used in flux CSV result types of columns with respect to line protocol ignored in line protocol CsvTableColumn represents processing metadata about a csv column Label is a column label from the header row, such as "_start", "_stop", "_time" DataType such as "string", "long", "dateTime" ... DataFormat is a format of DataType, such as "RFC3339", "2006-01-02" LinePart is a line part of the column (0 means not determined yet), see linePart constants DefaultValue is used when column's value is an empty string. Index of this column when reading rows, -1 indicates a virtual column with DefaultValue data TimeZone of dateTime column, applied when parsing dateTime DataType ParseF is an optional function used to convert column's string value to interface{} ComputeValue is an optional function used to compute column value out of row data escapedLabel contains escaped label that can be directly used in line protocol LineLabel returns escaped name of the column so it can be then used as a tag name or field name in line protocol Value returns the value of the column for the supplied row setupDataType setups data type from the value supplied columnValue contains typeName and possibly additional column metadata, it can be  1. typeName  2. typeName:format  3. typeName|defaultValue  4. typeName:format|defaultValue  5. #anycomment (all options above) ignoreLeadingComment is required to specify datatype together with CSV annotation in annotations (such as #constant) | adds a default value to column setup column format setup column linePart depending dataType ignore or ignored dateTime field is used at most once in a protocol line this a generic field without a data type specified time is an alias for dateTime nothing to do since we don't know the linePart yet the line part is decided in recomputeLineProtocolColumns setup custom parsing CsvColumnError indicates conversion error in a specific column Error interface implementation CsvTable contains metadata about columns and a state of the CSV processing columns contains columns that extract values from data rows partBits is a bitmap that is used to remember that a particular column annotation (#group, #datatype and #default) was already processed for the table; it is used to detect start of a new table in CSV flux results, a repeated annotation is detected and a new CsvTable can be then created readTableData indicates that the table is ready to read table data, which is after reading annotation and header rows lpColumnsValid indicates whether line protocol columns are valid or must be re-calculated from columns extraColumns are added by table-wide annotations, such as #constant ignoreDataTypeInColumnName is true to skip parsing of data type as a part a column name timeZone of dateTime column(s), applied when parsing dateTime value without a time zone specified validators validate table structure right before processing data rows cached columns are initialized before reading the data rows using the computeLineProtocolColumns fn  cachedMeasurement is a required column that read (line protocol) measurement cachedTime is an optional column that reads timestamp of lp row cachedFieldName is an optional column that reads a field name to add to the protocol line cachedFieldValue is an optional column that reads a field value to add to the protocol line cachedFields are columns that read field values, a field name is taken from a column label cachedTags are columns that read tag values, a tag name is taken from a column label IgnoreDataTypeInColumnName sets a flag that can ignore dataType parsing in column names. When true, column names can then contain '|'. By default, column name can also contain datatype and a default value when named `name|datatype` or `name|datatype|default`, for example `ready|boolean|true` DataColumnsInfo returns a string representation of columns that are used to process CSV data censure that ached columns are initialized NextTable resets the table to a state in which it expects annotations and header rows no column annotations parsed yet createColumns create a slice of CsvTableColumn for the supplied rowSize AddRow updates the state of the CSV table with a new header, annotation or data row. Returns true if the row is a data row. detect data row or table header row expect a header row line protocol columns change create columns since no column annotations were processed assign column labels for the header row assign column data type if possible header row is read, now expect data rows process all supported annotations ignoring, not a supported annotation process table-level annotation invariant: !supportedAnnotation.isTableAnnotation() any column annotation stops reading of data rows create new columns upon new or repeated column annotation setup columns according to column annotation missing value warn about unsupported annotation unless a comment row computeLineProtocolColumns computes columns that are used to create line protocol rows when required to do so returns true if new columns were initialized or false if there was no change in line protocol columns recomputeLineProtocolColumns always computes the columns that are used to create line protocol rows reset results collect unique tag names (#19453) having a _field column indicates fields without a line type are ignored go over columns + extra columns ignored columns that are marked to be ignored or without a label line protocol requires sorted unique tags setup timezone for timestamp column line protocol columns are now fresh CreateLine produces a protocol line out of the supplied row or returns error AppendLine appends a protocol line to the supplied buffer using a CSV row and returns appended buffer or an error if any validate column data types assume dateTime data type (number or RFC3339) Column returns the first column of the supplied label or nil Columns returns available columns ColumnLabels returns available columns labels Measurement returns measurement column or nil Time returns time column or nil FieldName returns field name column or nil FieldValue returns field value column or nil Tags returns tags Fields returns fields/Users/austinjaybecker/projects/abeck-go-testing/pkg/csv2lp/data_conversion.gosupportedformatRunesfractionRuneignorednormalizedremoveFractiontruncateddataFormatv32typedValcolonfalsytruthyianaindexgolang.org/x/text/encoding/ianaindex"golang.org/x/text/encoding/ianaindex"double"double""long""unsignedLong"base64Binary"base64Binary""\\,""\\ ""\\=""\"""\\\"""\\""\\\\". 
	_". \n\t\r_"ForAllCharactersUnsupported boolean value '"Unsupported boolean value '"' , first character is expected to be 't','f','0','1','y','n'"' , first character is expected to be 't','f','0','1','y','n'"'Y''%s' truncated to '%s' to fit into long data type"'%s' truncated to '%s' to fit into long data type"WARNING: %v
"WARNING: %v\n"'%s' truncated to '%s' to fit into unsignedLong data type"'%s' truncated to '%s' to fit into unsignedLong data type"unsupported data type '%s'"unsupported data type '%s'"value is NaN"value is NaN"value is Infinite"value is Infinite"unsupported value type: %T"unsupported value type: %T"MIBtoMIBIANA%v, see https://www.iana.org/assignments/character-sets/character-sets.xhtml"%v, see https://www.iana.org/assignments/character-sets/character-sets.xhtml"unsupported encoding: %s"unsupported encoding: %s"unsupported boolean format: %s should be in 'true,yes,1:false,no,0' format, but no ':' is present"unsupported boolean format: %s should be in 'true,yes,1:false,no,0' format, but no ':' is present"unsupported boolean value: %s must one of %v or one of %v"unsupported boolean value: %s must one of %v or one of %v"'%s' cannot fit into long data type"'%s' cannot fit into long data type"'%s' cannot fit into unsignedLong data type"'%s' cannot fit into unsignedLong data type" see https://v2.docs.influxdata.com/v2.0/reference/syntax/annotated-csv/#valid-data-types predefined dateTime formatsthe same as long, but serialized without i suffix, used for timestamps IsTypeSupported returns true if the data type is supported normalizeNumberString normalizes the supplied value according to the supplied format. This normalization is intended to convert number strings of different locales to a strconv-parsable value. The format's first character is a fraction delimiter character.  Next characters in the format are simply removed, they are typically used to visually separate groups in large numbers. The removeFraction parameter controls whether the returned value can contain also the fraction part. An empty format means ". \n\t\r_" For example, to get a strconv-parsable float from a Spanish value '3.494.826.157,123', use format ",." . skip ignored characters number or time.RFC3339 keep the value as it is CreateDecoder creates a decoding reader from the supplied encoding to UTF-8, or returns an error createBoolParseFn returns a function that converts a string value to boolean according to format "true,yes,1:false,no,0" createStrictLongParseFn returns a function that converts a string value to long and fails also when a fraction digit is detected createStrictUnsignedLongParseFn returns a function that converts a string value to unsigned long and fails when a fraction digit is detected/Users/austinjaybecker/projects/abeck-go-testing/pkg/csv2lp/line_reader.go LineReader wraps an io.Reader to count lines that go though read function and returns at most one line during every invocation of read. It provides a workaround to golang's CSV reader that does not expose current line number at all (see https://github.com/golang/go/issues/26679) At most one line is returned by every read in order to ensure that golang's CSV reader buffers at most one single line into its nested bufio.Reader. LineNumber of the next read operation, 0 is the first line by default. It can be set to 1 start counting from 1. LastLineNumber is the number of the last read row. rs is a wrapped reader reader provided by the client buf contains last data read from rd readPos is a read position in the buffer bufSize is the length of data read from rd into buf err contains the last error during read NewLineReader returns a new LineReader. NewLineReaderSize returns a new Reader whose buffer has at least the specified size. Read reads data into p. It fills in data that either does not contain \n or ends with \n. It returns the number of bytes read into p. handle pathological case of reading into empty array read data into buf copy at most one line and don't overflow internal buffer or p read at most one line readErr returns the last error and resets err status/Users/austinjaybecker/projects/abeck-go-testing/pkg/csv2lp/multi_closer.go multicloser Close implements io.Closer to closes all nested closers and logs a warning on errorMultiCloser creates am io.Closer that silently closes supplied io.Closer instances/Users/austinjaybecker/projects/abeck-go-testing/pkg/csv2lp/skip_header_lines.goskipHeaderLines skipFirstLines is an io.Reader that skips first lines reader provides data skipLines contains the lines to skip line is a mutable variable that increases until skipLines is reached quotedString indicates whether a quoted CSV string is being read, a new line inside a quoted string does not start a new CSV line Read implements io.Reader a quoted string starts or stops modify the buffer and return continue with the next chunk copy all bytes after the newline SkipHeaderLinesReader wraps a reader to skip the first skipLines lines in CSV data input/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/arrays.gen.goBooleanValuesSequenceCompareSeriesCountableSequenceCounterByteSequenceFieldArraySourceFieldConstantValueFieldFloatRandomSourceFieldIntegerZipfSourceFieldSourceFieldValuesSpecFloatValuesSequenceIntegerValuesSequenceMeasurementSpecNewBooleanArrayValuesSequenceNewBooleanConstantValuesSequenceNewCountableSequenceFnNewCounterByteSequenceNewCounterByteSequenceCountNewFloatArrayValuesSequenceNewFloatConstantValuesSequenceNewFloatRandomValuesSequenceNewIntegerArrayValuesSequenceNewIntegerConstantValuesSequenceNewIntegerZipfValuesSequenceNewMergedSeriesGeneratorNewMergedSeriesGeneratorLimitNewSchemaFromPathNewSeriesGeneratorNewSeriesGeneratorFromSpecNewSeriesGeneratorLimitNewSpecFromPathNewSpecFromSchemaNewSpecFromTomlNewStringArraySequenceNewStringArrayValuesSequenceNewStringConstantSequenceNewStringConstantValuesSequenceNewTagsValuesSequenceCountsNewTagsValuesSequenceFnNewTagsValuesSequenceKeysValuesNewTagsValuesSequenceValuesNewTimeBooleanValuesSequenceNewTimeFloatValuesSequenceNewTimeIntegerValuesSequenceNewTimeStringValuesSequenceNewTimeUnsignedValuesSequenceNewTimeValuesSequenceFnNewTimestampSequenceFromSpecNewUnsignedArrayValuesSequenceNewUnsignedConstantValuesSequenceSchemaNodeSeriesLimitStringArraySequenceStringConstantSequenceStringValuesSequenceTagArraySourceTagFileSourceTagSequenceSourceTagSourceTagValuesLimitOptionTagValuesSampleOptionTagValuesSpecTagsSequenceTagsSpecTimeSequenceSpecTimestampSequenceUnsignedValuesSequenceVisitorFnWalkDownWalkUp_precision_index_precision_namebooleanArraybooleanArrayValuesSequencebooleanConstantValuesSequenceconstSeriesdecodeFieldArraySourcedecodeFieldSourcedecodeFloatRandomSourcedecodeIntegerZipfSourcedecodeTagArraySourcedecodeTagConstantSourcedecodeTagFileSourcedecodeTagSequenceSourcedecodeTagSourcefloatArrayfloatArrayValuesSequencefloatConstantValuesSequencefloatRandomValuesSequenceintegerArrayintegerArrayValuesSequenceintegerConstantValuesSequenceintegerRandomValuesSequencemergedSeriesGeneratornewBooleanArrayLennewFloatArrayLennewIntegerArrayLennewSeriesGeneratorFromMeasurementSpecnewSpecFromSchemanewStringArrayLennewTagsSequenceFromTagsSpecnewTimeValuesSequenceFromFieldValuesSpecnewUnsignedArrayLennilSeriesprecisionHourprecisionMicrosecondprecisionMillisecondprecisionMinuteprecisionNanosecondprecisionSecondprecisionToDurationschemaDirschemaToSpecschemaToSpecStateseriesGeneratorseriesGeneratorHeapseriesKeyFieldsortDedupStringsstateErrstateOkstringArraystringArrayValuesSequencestringConstantValuesSequencetagsValuesOptiontagsValuesSequencetimeBooleanValuesSequencetimeFloatValuesSequencetimeIntegerValuesSequencetimeStringValuesSequencetimeUnsignedValuesSequencetimestampSequencetoFloat64SliceEtoInt64SliceEunsignedArrayunsignedArrayValuesSequenceunsignedConstantValuesSequenceEncodeFloatArrayBlockEncodeIntegerArrayBlockEncodeUnsignedArrayBlockEncodeStringArrayBlockEncodeBooleanArrayBlock Source: arrays.gen.go.tmpltagsourcevifieldsourceSamplenfmtSIMAXnextSampleDeltaForTimeRangeToDurationTimePrecisionTimeIntervalvisitErrresolvePathZipfimaxoneminusQoneminusQinvhxmhx0minusHxmhinv/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/gen.gogo:generate tmpl -data=@types.tmpldata arrays.gen.go.tmpl values.gen.go.tmpl values_sequence.gen.go.tmplgo:generate stringer -type=precision -trimprefix=precision/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/merged_series_generator.go capture last key for duplicate checking duplicate key, get next/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/precision_string.goMillisecondNanosecondMicrosecondSecondMinuteHour"MillisecondNanosecondMicrosecondSecondMinuteHour"precision("precision(" Code generated by "stringer -type=precision -trimprefix=precision"; DO NOT EDIT. An "invalid array index" compiler error signifies that the constant values have changed. Re-run the stringer command to generate them again./Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/schema.gotoml:"series-limit"`toml:"series-limit"`array, source=%#v"array, source=%#v"sequence, prefix=%q, range=[%d,%d)"sequence, prefix=%q, range=[%d,%d)"file, path=%s"file, path=%s"toml:"time-precision"`toml:"time-precision"`toml:"time-interval"`toml:"time-interval"`TimeInterval and TimePrecision are nil"TimeInterval and TimePrecision are nil"constant, source=%#v"constant, source=%#v"rand<float>, seed=%d, min=%f, max=%f"rand<float>, seed=%d, min=%f, max=%f"rand<float>, seed=%d, s=%f, v=%f, imax=%d"rand<float>, seed=%d, s=%f, v=%f, imax=%d"schema.Walk: unexpected node type %T"schema.Walk: unexpected node type %T" TimePrecision determines the precision for generated timestamp values TimeInterval determines the duration between timestamp values WalkDown performs a pre-order, depth-first traversal of the graph, calling v for each node. Pre-order starts by calling the visitor for the root and each child as it traverses down the graph to the leaves. WalkUp performs a post-order, depth-first traversal of the graph, calling v for each node. Post-order starts by calling the visitor for the leaves then each parent as it traverses up the graph to the root. nothing to do/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/sequence.govalue%s"value%s"%%0%dd"%%0%dd"/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/series.go Key returns the series key. The returned value may be cached. Field returns the name of the field. The returned value may be modified by a subsequent call to Next. Compare returns an integer comparing two SeriesGenerator instances lexicographically. The result will be 0 if a==b, -1 if a < b, and +1 if a > b. A nil argument is equivalent to an empty SeriesGenerator./Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/series_generator.gointervals Next advances the series generator to the next series key. Name returns the name of the measurement. Tags returns the tag set. FieldType returns the data type for the field. TimeValuesGenerator returns a values sequence for the current series. Count specifies the maximum number of values to generate. Start specifies the starting time for the values. Delta specifies the interval between time stamps. Precision specifies the precision of timestamp intervals Truncate time range if the number of intervals in the specified time range exceeds the maximum count, move the start forward to limit the number of values count is too high for the range of time and precision/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/specs.gospecsftmgmsstagsSpecfpDecodeFilefield %q data-type conflict, found %s and %s"field %q data-type conflict, found %s and %s"missing measurement name"missing measurement name"invalid sample, must be 0 < sample â¤ 1.0"invalid sample, must be 0 < sample â¤ 1.0"duplicate tag keys %q"duplicate tag keys %q"duplicate field names %q"duplicate field names %q"unexpected type %T"unexpected type %T"error processing schema: %v"error processing schema: %v"measurement %q: %v"measurement %q: %v"tag %q: %v"tag %q: %v"field %q: %v"field %q: %v"error resolving path %q: %v"error resolving path %q: %v"path %q is not a file: resolved to %s"path %q is not a file: resolved to %s"path error: %v"path error: %v"ScanLinespath %q, invalid UTF-8 on line %d"path %q, invalid UTF-8 on line %d" NewTimeValuesSequenceFn returns a TimeValuesSequence that will generate a sequence of values based on the spec. flatten measurements validate field types are homogeneous for a single measurement default: sample 50% NOTE: sort each measurement name + field name to ensure series are produced  in correct order Tag keys must be sorted to produce a valid series key sequence combine fields skip empty lines/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/tags_sequence.gocounts20040409expect: 0.0 < n â¤ 1.0"expect: 0.0 < n â¤ 1.0"4611686018427387903%s%%0%dd"%s%%0%dd"9007199254740992.090071992547409921.1102230246251565404e-161/9007199254740992 models.Tags are ordered, so ensure vals are ordered with respect to keys max tag width/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/timestamp_sequence.go/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/toml.gocast"github.com/spf13/cast"series-limit: invalid value"series-limit: invalid value"series-limit: must be â¥ 0"series-limit: must be â¥ 0"sample: must be a float"sample: must be a float"sample: must be 0 < sample â¤ 1.0"sample: must be 0 < sample â¤ 1.0"invalid duration, expect a Go duration as a string: %T"invalid duration, expect a Go duration as a string: %T"invalid duration, must be > 0: %s"invalid duration, must be > 0: %s"0x7invalid precision, expect one of (ns, us, ms, s, m, h): %T"invalid precision, expect one of (ns, us, ms, s, m, h): %T"nanosecond"nanosecond"microsecond"microsecond"millisecond"millisecond""second""minute""hour"invalid precision, expect one of (ns, ms, s, m, h): %s"invalid precision, expect one of (ns, ms, s, m, h): %s"tag: missing or invalid value for name"tag: missing or invalid value for name"missing source for tag %q"missing source for tag %q"invalid source for tag %q: %T"invalid source for tag %q: %T"ToStringEinvalid constant tag source"invalid constant tag source"empty array source"empty array source"ToStringSliceEmissing type field"missing type field""sequence"invalid type field %q"invalid type field %q"file: missing path"file: missing path"ToInt64Etag.sequence: invalid start, %v"tag.sequence: invalid start, %v"tag.sequence: start must be â¥ 0"tag.sequence: start must be â¥ 0"tag.sequence: invalid count, %v"tag.sequence: invalid count, %v"tag.sequence: count must be > 0"tag.sequence: count must be > 0"tag.sequence: missing count"tag.sequence: missing count"field: missing or invalid value for name"field: missing or invalid value for name"field: missing value for count"field: missing value for count"field: invalid count, %v"field: invalid count, %v"field: count must be > 0"field: count must be > 0"time-precision"time-precision"time-interval"time-interval"missing source for field %q"missing source for field %q"empty array"empty array"ToBoolSliceEunsupported field source data type: %T"unsupported field source data type: %T"rand<float>"rand<float>"zipf<integer>"zipf<integer>"seed"seed"rand<float>: invalid seed, %v"rand<float>: invalid seed, %v"ToFloat64Erand<float>: invalid min, %v"rand<float>: invalid min, %v"rand<float>: invalid max, %v"rand<float>: invalid max, %v"rand<float>: min â¤ max"rand<float>: min â¤ max"zipf<integer>: invalid seed, %v"zipf<integer>: invalid seed, %v"zipf<integer>: invalid value for s (s > 1), %v"zipf<integer>: invalid value for s (s > 1), %v"zipf<integer>: missing value for s"zipf<integer>: missing value for s"zipf<integer>: invalid value for v (v â¥ 1), %v"zipf<integer>: invalid value for v (v â¥ 1), %v"zipf<integer>: missing value for v"zipf<integer>: missing value for v""imax"ToUint64Ezipf<integer>: invalid value for imax, %v"zipf<integer>: invalid value for imax, %v"zipf<integer>: missing value for imax"zipf<integer>: missing value for imax" default infer source TODO(sgc): validate format string unknown use first value to determine slice type/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/util.gounable to cast %#v of type %T to []int64"unable to cast %#v of type %T to []int64"ValueOfunable to cast %#v of type %T to []float64"unable to cast %#v of type %T to []float64" ToInt64SliceE casts an interface to a []int64 type. ToFloat64SliceE casts an interface to a []float64 type./Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/values.gen.go Source: values.gen.go.tmpl/Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/values.goNewZipf ax + b NewIntegerZipfValuesSequence produces int64 values using a Zipfian distribution described by s./Users/austinjaybecker/projects/abeck-go-testing/pkg/data/gen/values_sequence.gen.goDefaultMaxPointsPerBlock Source: values_sequence.gen.go.tmpl/Users/austinjaybecker/projects/abeck-go-testing/pkg/deep/Users/austinjaybecker/projects/abeck-go-testing/pkg/deep/equal.godeepValueEquala1a2addr1addr2f1f2depthharddeepcannot compare type: %s"cannot compare type: %s" Copyright 2009 The Go Authors. All rights reserved. Use of this source code is governed by a BSD-style License. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Package deep provides a deep equality check for use in tests. import "github.com/influxdata/influxdb/v2/pkg/deep" Equal is a copy of reflect.DeepEqual except that it treats NaN == NaN as true. Tests for deep equality using reflected types. The map argument tracks comparisons that have already been seen, which allows short circuiting on recursive types. if depth > 10 { panic("deepValueEqual") }	// for debugging Canonicalize order to reduce number of entries in visited. Short circuit if references are identical ... ... or already seen Remember for later. Can't do better than this: Special handling for floats so that NaN == NaN is true. During deepValueEqual, must keep track of checks that are in progress.  The comparison algorithm assumes that all checks in progress are true when it reencounters them. Visited comparisons are stored in a map indexed by visit./Users/austinjaybecker/projects/abeck-go-testing/pkg/encoding/Users/austinjaybecker/projects/abeck-go-testing/pkg/encoding/simple8b/Users/austinjaybecker/projects/abeck-go-testing/pkg/encoding/simple8b/encoding.goCountBytesCountBytesBetweenDecodeAllDecodeBytesBigEndianEncodeAllErrValueOutOfBoundsMaxValueS8B_BIT_SIZEcanPacknumBitspack1pack10pack12pack120pack15pack2pack20pack240pack3pack30pack4pack5pack6pack60pack7pack8packingunpack1unpack10unpack12unpack120unpack15unpack2unpack20unpack240unpack3unpack30unpack4unpack5unpack6unpack60unpack7unpack8SetValuesselmaxValueinVbitNintNmaxValremainingsimple8b11529215046068469761152921504606846975invalid selector value: %v"invalid selector value: %v"invalid slice len remaining: %v"invalid slice len remaining: %v"value out of bounds: %v"value out of bounds: %v"value out of bounds"value out of bounds"NEXTVALUECODESinvalid selector value: %b"invalid selector value: %b"0xfsrc length is not multiple of 8"src length is not multiple of 8"2305843009213693952345876451382054092857646075230342348806917529027641081856807045053224792883210376293541461622784115292150460684697601268213655067531673613835058055282163712149879795598890106881614090106449585766417293822569102704640102340953276710485751073741823 Package simple8b implements the 64bit integer encoding algorithm as published by Ann and Moffat in "Index compression using 64-bit words", Softw. Pract. Exper. 2010; 40:131â147 It is capable of encoding multiple integers with values betweeen 0 and to 1^60 -1, in a single word. Imported from github.com/jwilder/encoding Simple8b is 64bit word-sized encoder that packs multiple integers into a single word using a 4 bit selector values and up to 60 bits for the remaining values.  Integers are encoded using the following table: ââââââââââââââââ¬ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ â   Selector   â       0    1   2   3   4   5   6   7  8  9  0 11 12 13 14 15â ââââââââââââââââ¼ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¤ â     Bits     â       0    0   1   2   3   4   5   6  7  8 10 12 15 20 30 60â â      N       â     240  120  60  30  20  15  12  10  8  7  6  5  4  3  2  1â â   Wasted Bitsâ      60   60   0   0   0   0  12   0  4  4  0  0  0  0  0  0â ââââââââââââââââ´ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ For example, when the number of values can be encoded using 4 bits, selected 5 is encoded in the 4 most significant bits followed by 15 values encoded used 4 bits each in the remaining 60 bits. Encoder converts a stream of unsigned 64bit integers to a compressed byte slice. most recently written integers that have not been flushed index in buf of the head of the buf index in buf of the tail of the buf index into bytes of written bytes current bytes written and flushed NewEncoder returns an Encoder able to convert uint64s to compressed byte slices The buf is full but there is space at the front, just shift the values down for now. TODO: use ring buffer encode as many values into one as we can Move the head forward since we encoded those values If we encoded them all, reset the head/tail pointers to the beginning Decoder converts a compressed byte slice to a stream of unsigned 64bit integers. NewDecoder returns a Decoder from a byte slice Next returns true if there are remaining values to be read.  Successive calls to Next advance the current element pointer. Read returns the current value.  Successive calls to Read return the same value. Count returns the number of integers encoded in the byte slice Count returns the number of integers encoded within an uint64 If the max value that could be encoded by the uint64 is less than the min skip the whole thing. Encode packs as many values into a single uint64.  It returns the packed uint64, how many values from src were packed, or an error if the values exceed the maximum value range. { number of values, max bits per value } Encode returns a packed slice of the values from src.  If a value is over 1 << 60, an error is returned.  The input src is modified to avoid extra allocations.  If you need to re-use, use a copy. Re-use the input slice and write encoded values back in place try to pack run of 240 or 120 1s Invariant: len(a) is fixed to 120 or 240 values search for the longest sequence of 1s in a Postcondition: k equals the index of the last 1 or -1 240 1s at least 120 1s Decode writes the uncompressed values from src to dst.  It returns the number of values written or an error.go:nocheckptr nocheckptr while the underlying struct layout doesn't change DecodeBytesBigEndian writes the compressed, big-endian values from src to dst.  It returns the number canPack returns true if n elements from in can be stored using bits per element Selector 0,1 are special and use 0 bits to encode runs of 1's pack240 packs 240 ones from in using 1 bit each pack120 packs 120 ones from in using 1 bit each pack60 packs 60 values from in using 1 bit each eliminate multiple bounds checks pack30 packs 30 values from in using 2 bits each pack20 packs 20 values from in using 3 bits each pack15 packs 15 values from in using 3 bits each pack12 packs 12 values from in using 5 bits each pack10 packs 10 values from in using 6 bits each pack8 packs 8 values from in using 7 bits each pack7 packs 7 values from in using 8 bits each pack6 packs 6 values from in using 10 bits each pack5 packs 5 values from in using 12 bits each pack4 packs 4 values from in using 15 bits each pack3 packs 3 values from in using 20 bits each pack2 packs 2 values from in using 30 bits each pack1 packs 1 values from in using 60 bits each/Users/austinjaybecker/projects/abeck-go-testing/pkg/escape/Users/austinjaybecker/projects/abeck-go-testing/pkg/escape/bytes.goCodesUnescapeStringescapeCharsunescaperinLen," =`," =` Package escape contains utilities for escaping parts of InfluxQL and InfluxDB line protocol. import "github.com/influxdata/influxdb/v2/pkg/escape" Codes is a map of bytes to be escaped. Bytes escapes characters on the input slice, as defined by Codes. IsEscaped returns whether b has any escaped characters, i.e. whether b seems to have been processed by Bytes. AppendUnescaped appends the unescaped version of src to dst and returns the resulting slice. Unescape returns a new slice containing the unescaped version of in. The output size will be no more than inLen. Preallocating the capacity of the output is faster and uses less memory than letting append() do its own (over)allocation./Users/austinjaybecker/projects/abeck-go-testing/pkg/escape/strings.go UnescapeString returns unescaped version of in. String returns the escaped version of in./Users/austinjaybecker/projects/abeck-go-testing/pkg/estimator/Users/austinjaybecker/projects/abeck-go-testing/pkg/estimator/hll/Users/austinjaybecker/projects/abeck-go-testing/pkg/estimator/hll/compressed.goDefaultPrecisionNewDefaultPlusNewPlusPlusbextrbextr32compressedListiterablenewCompressedListuint64SlicevariableLengthListIternewVbdatahll42949671680xffffff80 Original author of this file is github.com/clarkduvall/hyperloglog Marshal the variableLengthList At least 4 bytes for the two fixed sized values plus the size of bdata. Marshal the count and last values. Number of items in the list. The last item in the list. Append the list Set the count. Set the last value. Set the list. 4 bytes for the size of the list, and a byte for each element in the list. Length of the list. We only need 32 bits because the size of the set couldn't exceed that on 32 bit architectures. Marshal each element in the list.tmpSetdenseListsparseListmergeSparsetoNormalencodeHashdecodeHashlinearCount/Users/austinjaybecker/projects/abeck-go-testing/pkg/estimator/hll/hll.goezzlrhosdatatsdatatsLastBytetsszdsznewhx2zerosnewSestimatormath/bits"math/bits"github.com/influxdata/influxdb/v2/pkg/estimator"github.com/influxdata/influxdb/v2/pkg/estimator"0.373318766437530590.3733187664375305914237331876643753059/100000000000000000-0.37331876643753059142-420319564354619/11258999068426241.417040774481229891.4170407744812298922141704077448122989/100000000000000000-1.4170407744812298922-3190892151961233/22517998136852480.407291847966125330.407291847966125331047337117658925131/180143985094819841.561520339065841641.56152033906584164497032462417148375/45035996273704960.992422335342861280.992422335342861283716202639595892883/6250000000000000-0.99242233534286128371-8938945719288535/90071992547409920.260646813994830920.260646813994830917062347697788764855/90071992547409920.030538113696828070.0305381136968280696153053811369682807/100000000000000000-0.030538113696828069615-8802011997800495/2882303761517117440.001557702101791050.00155770210179104998297183633003704741/4611686018427387904precision must be between 4 and 18"precision must be between 4 and 18"0.6730.67300000000000004263378865318652543/5629499534213120.6970.696999999999999952936278017880554471/90071992547409920.7090.708999999999999963586386104271611363/90071992547409920.72130.721300000000000052223248446411222339/45035996273704961.0791.07899999999999995914859383997932765/4503599627370496SizeofLeadingZeros64wrong type for merging: %T"wrong type for merging: %T"precisions must be equal"precisions must be equal"provided buffer %v too short for initializing HLL sketch"provided buffer %v too short for initializing HLL sketch"LeadingZeros32 Package hll contains a HyperLogLog++ with a LogLog-Beta bias correction implementation that is adapted (mostly copied) from an implementation provided by Clark DuVall github.com/clarkduvall/hyperloglog. The differences are that the implementation in this package:   * uses an AMD64 optimised xxhash algorithm instead of murmur;   * uses some AMD64 optimisations for things like clz;   * works with []byte rather than a Hash64 interface, to reduce allocations;   * implements encoding.BinaryMarshaler and encoding.BinaryUnmarshaler Based on some rough benchmarking, this implementation of HyperLogLog++ is around twice as fast as the github.com/clarkduvall/hyperloglog implementation. Current version of HLL implementation. DefaultPrecision is the default precision. Plus implements the Hyperloglog++ algorithm, described in the following paper: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/40671.pdf The HyperLogLog++ algorithm provides cardinality estimations. hash function used to hash values to add to the sketch. precision. p' (sparse) precision to be used when p â [4..pp] and pp < 64. Number of substream used for stochastic averaging of stream. m' (sparse) number of substreams. alpha is used for bias correction. Should we use a sparse sketch representation. The dense representation of the HLL. values that can be stored in the sparse representation. NewPlus returns a new Plus with precision p. p must be between 4 and 18. p' = 25 is used in the Google paper. Determine alpha. Bytes estimates the memory footprint of this Plus, in bytes. NewDefaultPlus creates a new Plus with the default precision. Clone returns a deep copy of h. Add adds a new value to the HLL. {x63,...,x64-p} {x63-p,...,x0} Count returns a cardinality estimate. Nothing to do. Use LogLog-Beta bias estimation Merge takes another HyperLogLogPlus and combines it with HyperLogLogPlus h. If HyperLogLogPlus h is using the sparse representation, it will be converted to the normal representation. Nothing to do MarshalBinary implements the encoding.BinaryMarshaler interface. Marshal a version marker. Marshal precision. It's using the sparse representation. Add the tmp_set Add the sparse representation It's using the dense representation. Add the dense sketch representation. UnmarshalBinary implements the encoding.BinaryUnmarshaler interface. Unmarshal version. We may need this in the future if we make non-compatible changes. Unmarshal precision. h is now initialised with the correct precision. We just need to fill the rest of the details out. Using the sparse representation. Unmarshal the tmp_set. We need to unmarshal tssz values in total, and each value requires us to read 4 bytes. Unmarshal the sparse representation. Using the dense representation. Convert from sparse representation to dense representation. Encode a hash to be used in the sparse representation. Decode a hash from the sparse representation. 4 bytes for the size of the set, and 4 bytes for each key. Length of the set. We only need 32 bits because the size of the set Marshal each element in the set. bextr performs a bitfield extract on v. start should be the LSB of the field you wish to extract, and length the number of bits to extract. For example: start=0 and length=4 for the following 64-bit word would result in 1111 being returned. <snip 56 bits>00011110 returns 1110/Users/austinjaybecker/projects/abeck-go-testing/pkg/estimator/sketch.go Sketch is the interface representing a sketch for estimating cardinality. Add adds a single value to the sketch. Count returns a cardinality estimate for the sketch. Merge merges another sketch into this one. Bytes estimates the memory footprint of the sketch, in bytes. Clone returns a deep copy of the sketch./Users/austinjaybecker/projects/abeck-go-testing/pkg/file/Users/austinjaybecker/projects/abeck-go-testing/pkg/file/file_unix.goSyncDirdirNamenewpatholdpathModeDir2147483648PathErrorErrnoEINVAL +build !windows fsync the dir to flush the rename While we're on unix, we may be running in a Docker container that is pointed at a Windows volume over samba. That doesn't support fsyncs on directories. This shows itself as an EINVAL, so we ignore that error. RenameFile will rename the source to target using os function./Users/austinjaybecker/projects/abeck-go-testing/pkg/fs/Users/austinjaybecker/projects/abeck-go-testing/pkg/fs/fs.goCreateFileWithReplacementFileExistsErrornewFileExistsErroroperation not allowed, file %q exists"operation not allowed, file %q exists" A FileExistsError is returned when an operation cannot be completed due to a file already existing. DiskStatus is returned by DiskUsage/Users/austinjaybecker/projects/abeck-go-testing/pkg/fs/fs_unix.godiskunixgolang.org/x/sys/unix"golang.org/x/sys/unix"Statfs_tFsidBsizeIosizeBfreeBavailFfreeFssubtypeFstypenameMntonnameMntfromnameReservedStatfs SyncDir flushes any file renames to the filesystem. RenameFileWithReplacement will replace any existing file at newpath with the contents of oldpath. If no file already exists at newpath, newpath will be created using the contents of oldpath. If this function returns successfully, the contents of newpath will be identical to oldpath, and oldpath will be removed. RenameFile renames oldpath to newpath, returning an error if newpath already exists. If this function returns successfully, the contents of newpath will CreateFileWithReplacement will create a new file at any path, removing the contents of the old file CreateFile creates a new file at newpath, returning an error if newpath already exists DiskUsage returns disk usage of disk of path/Users/austinjaybecker/projects/abeck-go-testing/pkg/httpc/Users/austinjaybecker/projects/abeck-go-testing/pkg/httpc/body_fns.goBodyEmptyBodyGobBodyJSONStatusInWithAuthWithRespFnWithSessionCookieWithWriterFnWithWriterGZIPdecodeReaderdefaultHTTPClientencodingReadersheaderContentEncodingheaderContentTypenopBufCloserwithDoerheaderValapplication/gob"application/gob"encoderStateencBuffersendZerocountStatenewEncoderStatefreeEncoderStateencodeSingleencodeStructencodeArrayencodeMapencodeInterfaceencodeGobEncoderpushWriterpopWriterwriteMessagesendActualTypesendTypesendTypeDescriptorsendTypeIdEncodeValue BodyFn provides a writer to which a value will be written to that will make it's way into the HTTP request. BodyEmpty returns an empty body. BodyGob gob encodes the value provided for the HTTP request. Sets the Content-Encoding to application/gob. BodyJSON JSON encodes the value provided for the HTTP request. Sets the Content-Type to application/json.encEngineencInstrencOpencHelper/Users/austinjaybecker/projects/abeck-go-testing/pkg/httpc/client.gobFnwwwriterFnbodyFexistingOptsmust provide a non empty host address"must provide a non empty host address"MethodDeleteMethodPatch WriteCloserFn is a write closer wrapper than indicates the type of writer by returning the header and header value associated with the writer closer.	i.e. GZIP writer returns header Content-Encoding with value gzip alongside	the writer. doer provides an abstraction around the actual http client behavior. The doer can be faked out in tests or another http client provided in its place. Client is a basic http client that can make cReqs with out having to juggle the token and so forth. It provides sane defaults for checking response statuses, sets auth token when provided, and sets the content type to application/json for each request. The token, response checker, and content type can be overridden on the Req as well. New creates a new httpc client. Delete generates a DELETE request. Get generates a GET request. Patch generates a PATCH request. PatchJSON generates a PATCH request. This is to be used with value or pointer to value type. Providing a stream/reader will result in disappointment. Post generates a POST request. PostJSON generates a POST request and json encodes the body. This is to be used with value or pointer to value type. Providing a stream/reader will result in disappointment. Put generates a PUT request. PutJSON generates a PUT request. This is to be used with value or pointer to value type. Req constructs a request. TODO(@jsteenb2): add a inspection for an OK() or Valid() method, then enforce  that across all consumers? Same for all bodyFns for that matter. w.Close here is necessary since we have to close any gzip writer or other writer that requires closing. Clone creates a new *Client type from an existing client. This may be useful if you want to have a shared base client, then take a specific client from that base and tack on some extra goodies like specific headers and whatever else that suits you. Note: a new net.http.Client type will not be created. It will share the existing http.Client from the parent httpc.Client. Same connection pool, different specifics./Users/austinjaybecker/projects/abeck-go-testing/pkg/httpc/options.go ClientOptFn are options to set different parameters on the Client. WithAddr sets the host address on the client. WithAuth provides a means to set a custom auth that doesn't match the provided auth types here. WithAuthToken provides token auth for requests. WithSessionCookie provides cookie auth for requests to mimic the browser. Typically, session is influxdb.Session.Key. WithContentType sets the content type that will be applied to the requests created by the Client. WithHeader sets a default header that will be applied to all requests created by the client. WithUserAgentHeader sets the user agent for the http client requests. WithHTTPClient sets the raw http client on the httpc Client. WithInsecureSkipVerify sets the insecure skip verify on the http client's htp transport. WithRespFn sets the default resp fn for the client that will be applied to all requests generated from it. WithStatusFn sets the default status fn for the client that will be applied to all requests WithWriterFn applies the provided writer behavior to all the request bodies' generated from the client. WithWriterGZIP gzips the request body generated from this client. DefaultTransportInsecure is identical to http.DefaultTransport, with/Users/austinjaybecker/projects/abeck-go-testing/pkg/httpc/req.gocontentEncoding"scheme"query_params"query_params"response_byte"response_byte"received unexpected status: %s %d"received unexpected status: %s %d" Req is a request type. Accept sets the Accept header to the provided content type on the request. Auth sets the authorization for a request. ContentType sets the Content-Type header to the provided content type on the request. Decode sets the decoding functionality for the request. All Decode calls are called after the status and response functions are called. Decoding will not happen if error encountered in the status check. DecodeGob sets the decoding functionality to decode gob for the request. DecodeJSON sets the decoding functionality to decode json for the request. Header adds the header to the http request. Headers adds all the headers to the http request. QueryParams adds the query params to the http request. RespFn provides a means to inspect the entire http response. This function runs first before the status and decode functions are called. StatusFn sets a status check function. This runs after the resp func but before the decode fn. Do makes the HTTP request. Any errors that had been encountered in the lifetime of the Req type will be returned here first, in place of the call. This makes it safe to call Do at anytime. TODO(@jsteenb2): wrap do with retry/backoff policy. drain body completely StatusIn validates the status code matches one of the provided statuses./Users/austinjaybecker/projects/abeck-go-testing/pkg/jsonnet/Users/austinjaybecker/projects/abeck-go-testing/pkg/jsonnet/decode.gojsonStr"github.com/google/go-jsonnet"VMvmExtMapvmExtisCodeNativeFunctionIdentifiersevalCallnativeErrorFormatterColorFormatterSetColorFormatterSetMaxStackTraceSizeMaxStacktlanativeFuncsimporterStringOutputExtVarExtCodeTLAVarTLACodeEvaluateEvaluateStreamEvaluateMultievaluateSnippetEvaluateSnippetEvaluateSnippetStreamEvaluateSnippetMultiMakeVM Decoder type can decoce a jsonnet stream into the given output. NewDecoder creates a new decoder. Decode decodes the stream into the provide value.namedParameterLocationRangeLinesWithCodeFreeVariablesSetContextSetFreeVariablesdefaultArgoptionalevalKindcallArgumentscachedThunkenvironmentselfBindingvalueObjectvalueBaseaValueobjectCacheKeygetTypeuncachedObjectinheritanceSizeassertionErroruncachedassertionsCheckedsetAssertionsCheckResultgetAssertionsCheckResultsuperDepthsuperbindingFrameupValuesaPotentialValuenamedCallArgumentpvnamedtailstrictinterpretercallStackcallFrametraceElementisCalltrimmablepopIfExiststailCallTrimStacknewCallnewLocalgetSelfBindinglookUpVarlookUpVarOrPanicgetCurrentEnvgetTopEnvcaptureimportCachepotentialValuefoundAtVerificationcodeCacheimportDataimportStringimportCodeextVarsbaseStdgetCurrentStackTraceevaluatemanifestJSONmanifestAndSerializeJSONmanifestStringmanifestAndSerializeMultimanifestAndSerializeYAMLStreamEvalInCleanEnvevaluatePVevaluateTailCalltypeErrorSpecifictypeErrorGeneralgetNumberevaluateNumbergetIntevaluateIntevaluateInt64getStringevaluateStringgetBooleanevaluateBooleangetArrayevaluateArraygetFunctionevaluateFunctiongetObjectevaluateObjectvalueBooleanvalueNumbertailCallStatusvalueFunctionevalCallablevalueArrayvalueStringtraceFrame/Users/austinjaybecker/projects/abeck-go-testing/pkg/jsonparser/Users/austinjaybecker/projects/abeck-go-testing/pkg/jsonparser/jsonparser.goNotExist GetID returns an influxdb.ID for the specified keys path or an error if the value cannot be decoded or does not exist. GetOptionalID returns an influxdb.ID for the specified keys path or an error if the value cannot be decoded. The value of exists will be false if the keys path/Users/austinjaybecker/projects/abeck-go-testing/pkg/lifecycle/Users/austinjaybecker/projects/abeck-go-testing/pkg/lifecycle/resource.goliveliveReferencesresourceClosedresourceDebugEnabledsummarizeStackstmuchmuOpenedAcquireClosinglifecycleuntracktrackleaked Resource keeps track of references and has some compile time debug hooks to help diagnose leaks. It keeps track of if it is open or not and allows blocking until all references are released. protects state transitions protects channel mutations signals references to close counts outstanding references Open marks the resource as open. Close waits for any outstanding references and marks the resource as closed so that Acquire returns an error. signal any references. stop future Acquires wait for any acquired references Opened returns true if the resource is currently open. It may be immediately false in the presence of concurrent Open and Close calls. Acquire returns a Reference used to keep alive some resource. Reference is an open reference for some resource. Closing returns a channel that will be closed when the associated resource begins closing. Release causes the Reference to be freed. It is safe to call multiple times. Close makes a Reference an io.Closer. It is safe to call multiple times. References is a helper to aggregate a group of references. Release releases all of the references. It is safe to call multiple times. Close makes References an io.Closer. It is safe to call multiple times./Users/austinjaybecker/projects/abeck-go-testing/pkg/lifecycle/resource_debug.goframesINFLUXDB_EXP_RESOURCE_DEBUG"INFLUXDB_EXP_RESOURCE_DEBUG"SIGUSR2====================================================="====================================================="=== Live reference with id"=== Live reference with id"created from"created from"resource closed"resource closed"resource closed:
%s"resource closed:\n%s"SetFinalizer=== Leaked a reference with no stack associated!? ==="=== Leaked a reference with no stack associated!? ==="=== Leaked a reference! Created from"=== Leaked a reference! Created from"FramescallersframeStoreCallersFrames    %s:%s:%d
"    %s:%s:%d\n" When in debug mode, we associate each reference an id and with that id the stack trace that created it. We can't directly refer to the reference here because we also associate a finalizer to print to stderr if a reference is leaked, including where it came from if possible. This goroutine will dump all live references and where they were created when SIGUSR2 is sent to the process. resourceClosed returns an error stating that some resource is closed with the stack trace of the caller embedded. liveReferences keeps track of the stack traces of all of the live references. finishId informs the liveReferences that the id is no longer in use. withFinalizer associates a finalizer with the Reference that will cause it to print a leak message if it is not closed before it is garbage collected. leaked prints a loud message on stderr that the Reference was leaked and what was responsible for calling it. summarizeStack prints a line for each stack entry in the pcs to the writer./Users/austinjaybecker/projects/abeck-go-testing/pkg/limiter/Users/austinjaybecker/projects/abeck-go-testing/pkg/limiter/fixed.goNewFixedNewRateNewWriterWithRate Package limiter provides concurrency limiters. Fixed is a simple channel-based concurrency limiter.  It uses a fixed size channel to limit callers from proceeding until there is a value available in the channel.  If all are in-use, the caller blocks until one is freed. Idle returns true if the limiter has all its capacity is available. Available returns the number of available tokens that may be taken. Capacity returns the number of tokens can be taken. TryTake attempts to take a token and return true if successful, otherwise returns false. Take attempts to take a token and blocks until one is available. Release releases a token back to the limiter./Users/austinjaybecker/projects/abeck-go-testing/pkg/limiter/writer.goburstLimitbytesPerSecwantToWriteNwroteNgolang.org/x/time/rate"golang.org/x/time/rate"NewLimiter spend initial burst NewWriter returns a writer that implements io.Writer with rate limiting. The limiter use a token bucket approach and limits the rate to bytesPerSec with a maximum burst of burstLimit. WithRate returns a Writer with the specified rate limiter. Write writes bytes from b./Users/austinjaybecker/projects/abeck-go-testing/pkg/metrics/Users/austinjaybecker/projects/abeck-go-testing/pkg/metrics/context.goDefaultGroupGIDGroupFromContextMustRegisterCounterMustRegisterGroupMustRegisterTimerNewContextWithGroupNewGroupWithGroupcounterMetricTypedefaultRegistrydescOptiongidShiftgroupDescgroupKeygroupRegistryidMaskmetricTypenewDesctimerMetricTypegidsetGIDdescriptorsmustRegisterCountermustRegisterTimernewGroupUpdateSincecountersGetTimer NewContextWithGroup returns a new context with the given Group added. GroupFromContext returns the Group associated with ctx or nil if no Group has been assigned.mustGetGroupRegistry/Users/austinjaybecker/projects/abeck-go-testing/pkg/metrics/counter.goLoadInt64 The Counter type represents a numeric counter that is safe to use from concurrent goroutines. Name identifies the name of the counter. Value atomically returns the current value of the counter. Add atomically adds d to the counter. String returns a string representation using the name and value of the counter./Users/austinjaybecker/projects/abeck-go-testing/pkg/metrics/default_registry.go MustRegisterGroup registers a new group using the specified name. If the group name is not unique, MustRegisterGroup will panic. MustRegisterGroup is not safe to call from multiple goroutines. MustRegisterCounter registers a new counter metric with the default registry using the provided descriptor. If the metric name is not unique, MustRegisterCounter will panic. MustRegisterCounter is not safe to call from multiple goroutines. MustRegisterTimer registers a new timer metric with the default registry If the metric name is not unique, MustRegisterTimer will panic. MustRegisterTimer is not safe to call from multiple goroutines. NewGroup returns a new measurement group from the default registry. NewGroup is safe to call from multiple goroutines./Users/austinjaybecker/projects/abeck-go-testing/pkg/metrics/descriptors.go42949672964294967295 WithGroup assigns the associated measurement to the group identified by gid originally returned from MustRegisterGroup./Users/austinjaybecker/projects/abeck-go-testing/pkg/metrics/doc.go
Package metrics provides various measurements that are safe for concurrent access.

Measurements are arranged into groups that are efficient to create and access.
/Users/austinjaybecker/projects/abeck-go-testing/pkg/metrics/group.go The Group type represents an instance of a set of measurements that are used for instrumenting a specific request. Name returns the name of the group. GetCounter returns the counter identified by the id that was returned by MustRegisterCounter for the same group. Using an id from a different group will result in undefined behavior. GetTimer returns the timer identified by the id that was returned by MustRegisterTimer for the same group. The Metric type defines a Name ForEach calls fn for all measurements of the group./Users/austinjaybecker/projects/abeck-go-testing/pkg/metrics/group_registry.gometric name '%s' already in use"metric name '%s' already in use" The groupRegistry type represents a set of metrics that are measured together. MustRegisterCounter registers a new counter metric using the provided descriptor. MustRegisterTimer registers a new timer metric using the provided descriptor. newCollector returns a Collector with a copy of all the registered counters. newCollector is safe to call from multiple goroutines./Users/austinjaybecker/projects/abeck-go-testing/pkg/metrics/registry.gogd"global"group name '%s' already in use"group name '%s' already in use"invalid group ID"invalid group ID" DefaultGroup is the identifier for the default group. NewRegistry creates a new Registry with a single group identified by DefaultGroup. MustRegisterGroup registers a new group and panics if a group already exists with the same name. MustRegisterGroup is not safe to call from concurrent goroutines. If the metric name is not unique within the group, MustRegisterCounter will panic. MustRegisterCounter is not safe to call from concurrent goroutines. If the metric name is not unique within the group, MustRegisterTimer will panic. MustRegisterTimer is not safe to call from concurrent goroutines./Users/austinjaybecker/projects/abeck-go-testing/pkg/metrics/timer.goStoreInt64 The timer type is used to store a duration. Name returns the name of the timer. Value atomically returns the value of the timer. Update sets the timer value to d. UpdateSince sets the timer value to the difference between since and the current time. String returns a string representation using the name and value of the timer. Time updates the timer to the duration it takes to call f./Users/austinjaybecker/projects/abeck-go-testing/pkg/mincore/Users/austinjaybecker/projects/abeck-go-testing/pkg/mincore/limiter.goDefaultUpdateIntervalMincoreunderlyingincoreUpdateIntervalWaitPointerWaitRangeIsInCoreisInCorecheckUpdatemincoreGetpagesize Limiter defaults. Limiter represents a token bucket rate limiter based on mmap reference in-core vector last incore update Frequency of updates of the in-core vector. Updates are performed lazily so this is the maximum frequency. OS mincore() function. NewLimiter returns a new instance of Limiter associated with an mmap. The underlying limiter can be shared to limit faults across the entire process. WaitPointer checks if ptr would cause a page fault and, if so, rate limits its access. Once a page access is limited, it's updated to be considered memory resident. Check if the page is in-memory under lock. However, we want to exclude the wait from the limiter lock. Update incore mapping if data is too stale. WaitRange checks all pages in b for page faults and, if so, rate limits their access. Empty byte slices will never access memory so skip them. Check every page for being in-memory under lock. Iterate over every page within the range. Check if page access requires page fault. If not, exit immediately. If so, mark the page as memory resident afterward. Otherwise mark page as resident in memory and rate limit. IsInCore returns true if the address is resident in memory or if the address is outside the range of the data the limiter is tracking. Update updates the vector of in-core pages. Automatically updated when calling Wait(). checkUpdate performs an update if one hasn't been done before or the interval has passed. index returns the position in the in-core vector that represents ptr./Users/austinjaybecker/projects/abeck-go-testing/pkg/mincore/mincore_unix.goSyscallSYS_MINCORE +build darwin dragonfly freebsd linux nacl netbsd openbsd Mincore is a wrapper function for mincore(2)./Users/austinjaybecker/projects/abeck-go-testing/pkg/mmap/Users/austinjaybecker/projects/abeck-go-testing/pkg/mmap/mmap_unix.goMmapPROT_READMAP_SHAREDMunmap Copyright 2015 The Go Authors. All rights reserved. license that can be found in the LICENSE file. Package mmap provides a way to memory-map a file. Map memory-maps a file. Use file size if map size is not passed in. Unmap closes the memory-map./Users/austinjaybecker/projects/abeck-go-testing/pkg/pointer/Users/austinjaybecker/projects/abeck-go-testing/pkg/pointer/pointer.go Package pointer provides utilities for pointer handling that aren't available in go. Feel free to add more pointerification functions for more types as you need them. Duration returns a pointer to its argument. Int returns a pointer to its argument. Int64 returns a pointer to its argument. String returns a pointer to its argument. Time returns a pointer to its argument./Users/austinjaybecker/projects/abeck-go-testing/pkg/pool/Users/austinjaybecker/projects/abeck-go-testing/pkg/pool/bytes.goLimitedBytesNewBytesNewGenericNewLimitedBytes Package pool provides pool structures to help reduce garbage collector pressure. Bytes is a pool of byte slices that can be re-used.  Slices in this pool will not be garbage collected when not in use. NewBytes returns a Bytes pool with capacity for max byte slices to be pool. Get returns a byte slice size with at least sz capacity. Items returned may not be in the zero state and should be reset by the caller. Put returns a slice back to the pool.  If the pool is full, the byte slice is discarded. LimitedBytes is a pool of byte slices that can be re-used.  Slices in this pool will not be garbage collected when not in use.  The pool will hold onto a fixed number of byte slices of a maximum size.  If the pool is empty and max pool size has not been allocated yet, it will return a new byte slice.  Byte slices added to the pool that are over the max size are dropped. If we have not allocated our capacity, return a new allocation, otherwise block until one frees up. slice is discarded.  If the byte slice is over the configured max size of any byte slice in the pool, it is discarded. Drop buffers that are larger than the max size/Users/austinjaybecker/projects/abeck-go-testing/pkg/pool/generic.go Generic is a pool of types that can be re-used.  Items in NewGeneric returns a Generic pool with capacity for max items Get returns a item from the pool or a new instance if the pool is empty.  Items returned may not be in the zero state and should be reset by the caller. Put returns an item back to the pool.  If the pool is full, the item is discarded./Users/austinjaybecker/projects/abeck-go-testing/pkg/radix/Users/austinjaybecker/projects/abeck-go-testing/pkg/radix/buffer.goNewFromMapSortUint64sdoSortedgeedgesleafNodelongestPrefixradixrecursiveWalkwalkFn bufferSize is the size of the buffer and the largest slice that can be contained in it. buffer is a type that amoritizes allocations into larger ones, handing out small subslices to make copies. Copy returns a copy of the passed in byte slice allocated using the byte slice in the buffer. if we can never have enough room, just return a copy if we don't have enough room, reallocate the buf first create a copy and hand out a sliceaddEdgereplaceEdgemergeChildDeletePrefixdeletePrefix/Users/austinjaybecker/projects/abeck-go-testing/pkg/radix/sort.gokeyMaskkeyOffset Portions of this file from github.com/shawnsmithdev/zermelo under the MIT license. The MIT License (MIT) Copyright (c) 2014 Shawn Smith Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. SortUint64s sorts a slice of uint64s. Each pass processes a byte offset, copying back and forth between slices Keep track of where groups start Current 'digit' to look at Keep track of the number of elements for each kind of byte Check for already sorted if elem is always >= prev it is already sorted fetch the byte at current 'digit' count of elems to put in this digit's bucket Detect sorted Short-circuit sorted Find target bucket offsets Rebucket while copying to other buffer Get the digit Copy the element to the digit's bucket One less space, move the offset On next pass copy data the other way/Users/austinjaybecker/projects/abeck-go-testing/pkg/radix/tree.gok1k2lk1lk2commonPrefixsubTreeSizereplacing missing edge"replacing missing edge"loop This is a fork of https://github.com/armon/go-radix that removes the ability to update nodes as well as uses fixed int value type. leafNode is used to represent a value true if key/val are valid edge is used to represent an edge node leaf is used to store possible leaf prefix is the common prefix we ignore Edges should be stored in-order for iteration. We avoid a fully materialized slice to save memory, since in most cases we expect to be sparse find the insertion point with bisection make room, copy the suffix, and insert. linear search for small slices binary search for larger Tree implements a radix tree. This can be treated as a Dictionary abstract data type. The main advantage over a standard hash map is prefix-based lookups and ordered iteration. The tree is safe for concurrent access. New returns an empty Tree NewFromMap returns a new tree containing the keys from an existing map Len is used to return the number of elements in the tree longestPrefix finds the length of the shared prefix of two strings for loops can't be inlined, but goto's can. we also use uint to help out the compiler to prove bounds checks aren't necessary on the index operations. Insert is used to add a newentry or update an existing entry. Returns if inserted. Handle key exhaution Look for the edge No edge, create one Determine longest prefix of the search key on match Split the node Restore the existing node Create a new leaf node If the new key is a subset, add to to this node Create a new edge for the node DeletePrefix is used to delete the subtree under a prefix Returns how many nodes were deleted Use this to delete large subtrees efficiently delete does a recursive deletion Check for key exhaustion Remove the leaf noderecursively walk from all edges of the node to be deleted deletes the entire subtree Check if we should merge the parent's other child Look for an edge Consume the search prefix Get is used to lookup a specific key, returning the value and if it was found Check for key exhaution walkFn is used when walking the tree. Takes a key and value, returning if iteration should be terminated. recursiveWalk is used to do a pre-order walk of a node recursively. Returns true if the walk should be aborted Visit the leaf values if any Recurse on the children Minimum is used to return the minimum value in the tree Maximum is used to return the maximum value in the tree/Users/austinjaybecker/projects/abeck-go-testing/pkg/rhh/Users/austinjaybecker/projects/abeck-go-testing/pkg/rhh/metrics.goDefaultOptionsDistHashUint64NewHashMapNewMetricsnewRHHTrackergetPutNamesrhhGaugeOptsNewGaugeVecload_percent"load_percent"Load factor of the hashmap."Load factor of the hashmap.""size"Number of items in the hashmap."Number of items in the hashmap."get_duration_ns"get_duration_ns"Times taken to retrieve elements in nanoseconds (sampled every 10% of retrievals)."Times taken to retrieve elements in nanoseconds (sampled every 10% of retrievals)."100.1.53/2get_duration_last_ns"get_duration_last_ns"Last retrieval duration in nanoseconds (sampled every 10% of retrievals)"Last retrieval duration in nanoseconds (sampled every 10% of retrievals)"put_duration_ns"put_duration_ns"Times taken to insert elements in nanoseconds (sampled every 10% of insertions)."Times taken to insert elements in nanoseconds (sampled every 10% of insertions)."put_duration_last_ns"put_duration_last_ns"Last insertion duration in nanoseconds (sampled every 10% of insertions)"Last insertion duration in nanoseconds (sampled every 10% of insertions)"grow_duration_s"grow_duration_s"Time in seconds to last grow the hashmap."Time in seconds to last grow the hashmap."mean_probes"mean_probes"Average probe count of all elements (sampled every 0.5% of insertions)."Average probe count of all elements (sampled every 0.5% of insertions)."get_total"get_total"Number of times elements retrieved."Number of times elements retrieved."put_total"put_total"Number of times elements inserted."Number of times elements inserted." Load factor of the hashmap. Number of items in hashmap. Sample of get times. Sample of most recent get time. Sample of insertion times. Sample of most recent insertion time. Most recent growth time. Average number of probes for each element. These metrics have an extra label status = {"hit", "miss"} Number of times item retrieved. Number of times item inserted. NewMetrics initialises prometheus metrics for tracking an RHH hashmap. 15 buckets spaced exponentially between 100 and ~30,000. PrometheusCollectors satisfies the prom.PrometheusCollector interface.MetricsEnabled/Users/austinjaybecker/projects/abeck-go-testing/pkg/rhh/rhh.goinstrumentoverwrittensamplePutelemDistcopiedsearchKeydefaultLabels0.10.100000000000000005553602879701896397/36028797018963968hit"hit"miss"miss" HashMap represents a hash map that implements Robin Hood Hashing. https://cs.uwaterloo.ca/research/tr/1986/CS-86-14.pdf NewHashMap initialises a new Hashmap with the provided options. Limited to 2^64. Reset clears the values in the map without deallocating the space. Get returns the value for a key from the Hashmap, or nil if no key exists. Grow the map if we've run out of slots. If the key was overwritten then decrement the size. Put stores the value at key in the Hashmap, overwriting an existing value if one exists. If the maximum load of the Hashmap is reached, the Hashmap will first resize itself. PutQuiet is equivalent to Put, but no instrumentation code is executed. It can be faster when many keys are being inserted into the Hashmap. Continue searching until we find an empty slot or lower probe distance. Empty slot found or matching key, insert and exit. If the existing elem has probed less than us, then swap places with existing elem, and keep going to find another slot for that elem. Swap with current position. Update current distance. Increment position, wrap around on overflow. alloc elems according to currently set capacity. Grow increases the capacity and reinserts all existing hashes & elements. Ensure new capacity is a power of two and greater than current capacity. Copy old elements and hashes. Increase capacity & reallocate. Copy old elements to new hash/elem list. index returns the position of key in the hash map. Elem returns the i-th key/value pair of the hash map. Len returns the number of key/values set in map. Cap returns the number of key/values set in map. AverageProbeCount returns the average number of probes for each element. Keys returns a list of sorted keys. PrometheusCollectors returns the metrics associated with this hashmap. Prevent allocations by initialising these static maps when creating a new tracker. Labels returns a copy of the default labels used by the tracker's metrics. The returned map is safe for modification. Create a copy of the provided labels. TODO(edd): currently no safe way to calculate this concurrently. reset clears the values in the element. setKey copies v to a key on e. Options represents initialization options that are passed to NewHashMap(). DefaultOptions represents a default set of options to pass to NewHashMap(). HashKey computes a hash of key. Hash is always non-zero. HashUint64 computes a hash of an int64. Hash is always non-zero. Dist returns the probe distance for a hash in a slot index. NOTE: Capacity must be a power of 2./Users/austinjaybecker/projects/abeck-go-testing/pkg/slices/Users/austinjaybecker/projects/abeck-go-testing/pkg/slices/bytes.goBytesToStringsCompareSliceCopyChunkedByteSlicesExistsExistsIgnoreCaseMergeSortedBytesMergeSortedFloatsMergeSortedIntsMergeSortedStringsMergeSortedUIntsStringsToByteschunkchunkByteSizechunkEndchunkBeginslices BytesToStrings converts a slice of []byte into a slice of strings. CopyChunkedByteSlices deep-copies a [][]byte to a new [][]byte that is backed by a small number of []byte "chunks". CompareSlice returns an integer comparing two slices of byte slices b is longer, so assume a is less a is longer, so assume b is less/Users/austinjaybecker/projects/abeck-go-testing/pkg/slices/merge.gen.goidxsvalue being merged out of order."value being merged out of order." Source: merge.gen.go.tmpl Merge uses a k-way merge to merge n collections of sorted byte slices. The resulting slice is returned in ascending order, with any duplicate values removed. Special case. Merge single slice with a nil slice, to remove any duplicates from the single slice. This will likely be too small but it's a start. Indexes we've processed. Index we currently think is minimum. Find the smallest minimum in all slices. We have completely drained all values in this slice. We haven't picked the minimum value yet. Pick this one. It this value key is lower than the candidate. Duplicate value. Throw it away. We could have drained all of the values and be done... First value to just append it and move on. Append the minimum value to results if it's not a duplicate of the existing one. Duplicate so drop it. Result of comparing most recent value./Users/austinjaybecker/projects/abeck-go-testing/pkg/slices/strings.goignoreCasesetAsetB Package slices contains functions to operate on slices treated as sets. import "github.com/influxdata/influxdb/v2/pkg/slices" Union combines two string sets. Exists checks if a string is in a set. ExistsIgnoreCase checks if a string is in a set but ignores its case. StringsToBytes converts a variable number of strings into a slice of []byte./Users/austinjaybecker/projects/abeck-go-testing/pkg/snowflake/Users/austinjaybecker/projects/abeck-go-testing/pkg/snowflake/gen.gosequenceBitssequenceMaskserverBitsserverMaxserverShifttimeBitstimeMasktimeShiftmachineIDcurrentSeqcurrentTime1491696000000-1024-4096-43980465111044398046511103invalid machine id; must be 0 â¤ id < %d"invalid machine id; must be 0 â¤ id < %d"LoadUint64CompareAndSwapUint64'2''4''5''7''8''A''B''C'D'D'G'G''H''I'J'J''K''L''M'O'O'P'P'Q'Q''R''S'U'U''V''W''X'Z'Z''a''g''j''k''l''q''x''z'~'~' we attempt 100 times to update the millisecond part of the state and increment the sequence atomically. each attempt is approx ~30ns so we spend around ~3Âµs total. this sequence of conditionals ensures a monotonically increasing state. if our time is in the future, use that with a zero sequence number. we now know that our time is at or before the current time. if we're at the maximum sequence, bump to the next millisecond otherwise, increment the sequence. since we failed 100 times, there's high contention. bail out of the loop to bound the time we'll spend in this method, and just add one to the counter. this can cause millisecond drift, but hopefully some CAS eventually succeeds and fixes the milliseconds. additionally, if the sequence is already at the maximum, adding 1 here can cause it to roll over into the machine id. giving the CAS 100 attempts helps to avoid these problems./Users/austinjaybecker/projects/abeck-go-testing/pkg/tar/Users/austinjaybecker/projects/abeck-go-testing/pkg/tar/stream.goSinceFilterTarFileStreamFileStreamRenameFileextractFileshardRelativePathsubDirrelativePathwriteFunctarHeaderFileNamedestPathhdrsectionstararchive/tar"archive/tar"fileWriterfileStatelogicalRemainingphysicalRemainingmayBemayOnlyBemustNotBeTypeflagLinknameUnameGnameAccessTimeChangeTimeDevmajorDevminorXattrsPAXRecordsallowedFormatstoV7toGNUtoSTARtoUSTARtoSparsegetFormatsetFormatcomputeChecksumwriteUSTARHeaderwritePAXHeaderwriteGNUHeadertemplateV7PluswriteRawFilewriteRawHeaderFileInfoHeaderToSlashCopyNfileReaderhandleRegularFilehandleSparseFilereadGNUSparsePAXHeadersreadOldGNUSparseMapFromSlashSeparatorinvalid archive path: %s"invalid archive path: %s"TypeDir4930755 Stream is a convenience function for creating a tar of a shard dir. It walks over the directory and subdirs, possibly writing each file to a tar writer stream.  By default StreamFile is used, which will result in all files being written.  A custom writeFunc can be passed so that each file may be written, modified+written, or skipped depending on the custom logic. Skip adding an entry for the root dir Figure out the the full relative path including any sub-dirs Generates a filtering function for Stream that checks an incoming file, and only writes the file to the stream if its mod time is later than since.  Example: to tar only files newer than a certain datetime, use tar.Stream(w, dir, relativePath, SinceFilterTarFile(datetime)) stream a single file to tw, extending the header name using the shardRelativePath/ Stream a single file to tw, using tarHeaderFileName instead of the actual filename e.g., when we want to write a *.tmp file using the original file's non-tmp name. Restore reads a tar archive from r and extracts all of its files into dir, using only the base name of each file. extractFile copies the next file from tr into dir, using the file's base name. Read next archive file. The hdr.Name is the relative path of the file from the root data dir. e.g (db/rp/1/xxxxx.tsm or db/rp/1/index/xxxxxx.tsi) If this is a directory entry (usually just `index` for tsi), create it an move on. Make sure the dir we need to write into exists.  It should, but just double check in case we get a slightly invalid tarball. Create new file on disk. Copy from archive to the file. Sync to disk & close.headerUSTARv7groupNamedevMajordevMinorheaderSTARaccessTimechangeTimestringFormatternumberFormattersparseDatassparseEntryheaderGNUrealSizeheaderV7chksumtypeFlaglinkNamesparseArrayisExtendedmaxEntriessparseElem/Users/austinjaybecker/projects/abeck-go-testing/pkg/testing/Users/austinjaybecker/projects/abeck-go-testing/pkg/testing/assert/Users/austinjaybecker/projects/abeck-go-testing/pkg/testing/assert/assertions.goPanicTestFuncPanicsWithValueValuesAreEqualValuesAreExactlyEqualdidPanicformatMsgAndArgsformatValuesthmsgAndArgsfuncDidPanicactualTypeexpectedValueNot Equal: got=%s, exp=%s"Not Equal: got=%s, exp=%s"Equal: should not be %s"Equal: should not be %s"unexpected error: %+v"unexpected error: %+v"func %#v should panic
	Panic value:	%v"func %#v should panic\n\r\tPanic value:\t%v"func %#v should panic with value:	%v
	Panic value:	%v"func %#v should panic with value:\t%v\n\r\tPanic value:\t%v" Equal asserts that the values are equal and returns true if the assertion was successful. NotEqual asserts that the values are not equal and returns NoError asserts that err is nil and returns PanicsWithValue asserts that fn panics, and that the recovered panic value equals the expected panic value. Returns true if the assertion was successful. ValuesAreEqual determines if the values are equal. ValuesAreExactlyEqual determines if the values are equal and their types are the same. Attempt comparison after type conversion PanicTestFunc defines a function that is called to determine whether a panic occurs./Users/austinjaybecker/projects/abeck-go-testing/pkg/testing/assert/doc.go
Package assert provides helper functions that can be used with the standard Go testing package.
/Users/austinjaybecker/projects/abeck-go-testing/pkg/testing/assert/helper.gofailureMsgpanicked%T(%#v)"%T(%#v)"%#v"%#v" didPanic returns true if fn panics when called./Users/austinjaybecker/projects/abeck-go-testing/pkg/testttp/Users/austinjaybecker/projects/abeck-go-testing/pkg/testttp/http.goRespmustEncodeJSONSetFormValueWithCtxWrapCtxResponseRecorderHeaderMapFlushedsnapHeaderRecExpectExpectStatusExpectBodyExpectHeadersExpectHeadertestttpNewRecorderunexpected status code: expected=%d got=%d"unexpected status code: expected=%d got=%d"body: %v"body: %v"did not find expected header: %q"did not find expected header: %q"did not find expected value for header %q; got: %v"did not find expected value for header %q; got: %v" Req is a request builder. HTTP runs creates a request for an http call. Delete creates a DELETE request. Get creates a GET request. Patch creates a PATCH request. PatchJSON creates a PATCH request with a json encoded body. Post creates a POST request. PostJSON returns a POST request with a json encoded body. Put creates a PUT request. PutJSON creates a PUT request with a json encoded body. Do runs the request against the provided handler. Headers allows the user to set headers on the http request. WithCtx sets the ctx on the request. WrapCtx provides means to wrap a request context. This is useful for stuffing in the auth stuffs that are required at times. Resp is a http recorder wrapper. Debug sets the debugger. If true, the debugger will print the body of the response when the expected status is not received. Expect allows the assertions against the raw Resp. ExpectStatus compares the expected status code against the recorded status code. ExpectBody provides an assertion against the recorder body. ExpectHeaders asserts that multiple headers with values exist in the recorder. ExpectHeader asserts that the header is in the recorder./Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/context.goNewContextWithSpanNewContextWithTraceNewTraceNewTraceFromSpanRawSpanTraceFromContextTreeNodefieldsToWirelabelsToWirenewTreeVisitorrandomID2seededIDGenseededIDLocksetOptionsspanContextKeytraceContextKeytreeSortVisitortreeVisitorwireToFieldsParentSpanIDaddRawSpanTreeFromtreeFromSetLabelsMergeLabelsSetFieldsMergeFields NewContextWithSpan returns a new context with the given Span added. SpanFromContext returns the Span associated with ctx or nil if no Span has been assigned. NewContextWithTrace returns a new context with the given Trace added. TraceFromContext returns the Trace associated with ctx or nil if no Trace has been assigned.applyStartField_FieldTypeisField_ValueGetFieldTypeGetNumericValGetStringValXXX_OneofFuncsMetaValueAddBranchAddMetaBranchAddMetaNodeAddNodeFindByMetaFindByValueFindLastNodeSetMetaValuedecodeVarintSlowDecodeVarintDecodeFixed64DecodeFixed32DecodeZigzag64DecodeZigzag32DecodeRawBytesDecodeStringBytesDecodeMessageDecodeGroupEncodeVarintEncodeFixed64EncodeFixed32EncodeZigzag64EncodeZigzag32EncodeRawBytesEncodeStringBytesEncodeMessageSetBufSetDeterministicDebugPrint/Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/doc.go
Package tracing provides a way for capturing hierarchical traces.

To start a new trace with a root span named select

    trace, span := tracing.NewTrace("select")

It is recommended that a span be forwarded to callees using the
context package. Firstly, create a new context with the span associated
as follows

	ctx = tracing.NewContextWithSpan(ctx, span)

followed by calling the API with the new context

	SomeAPI(ctx, ...)

Once the trace is complete, it may be converted to a graph with the Tree method.

	tree := t.Tree()

The tree is intended to be used with the Walk function in order to generate
different presentations. The default Tree#String method returns a tree.

/Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/fields/Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/fields/field.goboolTypedurationTypefloat64Typeint64TypestringTypeuint64TypeFloat64bitsFloat64frombits Field instances are constructed via Bool, String, and so on. "heavily influenced by" (i.e., partially stolen from) https://github.com/opentracing/opentracing-go/log String adds a string-valued key:value pair to a Span.LogFields() record Bool adds a bool-valued key:value pair to a Span.LogFields() record/ Int64 adds an int64-valued key:value pair to a Span.LogFields() record Uint64 adds a uint64-valued key:value pair to a Span.LogFields() record Float64 adds a float64-valued key:value pair to a Span.LogFields() record Key returns the field's key. Value returns the field's value as interface{}. String returns a string representation of the key and value./Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/fields/fields.go Merge merges other with the current set, replacing any matching keys from other. equal, then "other" replaces existing key New creates a new set of fields, sorted by Key. Duplicate keys are removed. deduplicate loop invariant: fields[:i] has no duplicates find all duplicate keys number of duplicate keys copy over duplicate keys in order to maintain loop invariant/Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/labels/Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/labels/labels.gouneven number of arguments to label.Labels"uneven number of arguments to label.Labels" The Labels type represents a set of labels, sorted by Key. New takes an even number of strings representing key-value pairs and creates a new slice of Labels. Duplicates are removed, however, there is no guarantee which will be removed loop invariant: labels[:i] has no duplicates/Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/rawspan.gogithub.com/influxdata/influxdb/v2/pkg/tracing/fields"github.com/influxdata/influxdb/v2/pkg/tracing/fields"github.com/influxdata/influxdb/v2/pkg/tracing/labels"github.com/influxdata/influxdb/v2/pkg/tracing/labels" RawSpan represents the data associated with a span. ParentSpanID identifies the parent of this span or 0 if this is the root span. Name is the operation name given to this span. Start identifies the start time of the span. Labels contains additional metadata about this span. Fields contains typed values associated with this span./Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/span.go The Span type denotes a specific operation for a Trace. A Span may have one or more children, identifying additional details about a trace. The StartTime start span option specifies the start time of the new span rather than using now. StartSpan creates a new child span using time.Now as the start time. Context returns a SpanContext that can be serialized and passed to a remote node to continue a trace. SetLabels replaces any existing labels for the Span with args. MergeLabels merges args with any existing labels defined for the Span. SetFields replaces any existing fields for the Span with args. MergeFields merges the provides args with any existing fields defined Finish marks the end of the span and records it to the associated Trace. If Finish is not called, the span will not appear in the trace./Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/spancontext.gowswiregithub.com/influxdata/influxdb/v2/pkg/tracing/wire"github.com/influxdata/influxdb/v2/pkg/tracing/wire"GetTraceIDGetSpanID A SpanContext represents the minimal information to identify a span in a trace. This is typically serialized to continue a trace on a remote node. TraceID is assigned a random number to this trace. SpanID is assigned a random number to identify this span./Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/trace.goltrn The Trace type functions as a container for capturing Spans used to trace the execution of a request. NewTrace starts a new trace and returns a root span identified by the provided name. Additional options may be specified to override the default behavior when creating the span. NewTraceFromSpan starts a new trace and returns the associated span, which is a child of the parent span context. Tree returns a graph of the current trace. Merge combines other with the current trace. This is typically necessary when traces are transferred from a remote. sort nodes/Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/trace_encoding.gospboolValwfsFieldTypeStringField_StringValFieldTypeBoolField_NumericValNumericValFieldTypeInt64FieldTypeUint64FieldTypeDurationFieldTypeFloat64GetParentSpanIDGetStartSpans/Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/tree.gocntreeprint"github.com/xlab/treeprint" A Visitor's Visit method is invoked for each node encountered by Walk. If the result of Visit is not nil, Walk visits each of the children. A TreeNode represents a single node in the graph. String returns the tree as a string. Walk traverses the graph in a depth-first order, calling v.Visit for each node until completion or v.Visit returns nil./Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/util.go/Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/wire/Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/wire/binary.goErrIntOverflowBinaryErrInvalidLengthBinaryField_FieldType_nameField_FieldType_value_Field_OneofMarshaler_Field_OneofSizer_Field_OneofUnmarshalerencodeFixed32BinaryencodeFixed64BinaryencodeVarintBinaryfileDescriptorBinaryskipBinarysovBinarysozBinary
Package wire is used to serialize a trace.

go:generate protoc -I$GOPATH/src -I. --gogofaster_out=Mgoogle/protobuf/timestamp.proto=github.com/gogo/protobuf/types:. binary.proto/Users/austinjaybecker/projects/abeck-go-testing/pkg/tracing/wire/binary.pb.godAtAn1n2nn3skippyfieldNumpreIndexiNdExmsglenpostIndexintStringLenstringLeninnerWireinnerWireTypegithub_com_gogo_protobuf_typesgithub.com/gogo/protobuf/gogoproto"github.com/gogo/protobuf/gogoproto"github.com/gogo/protobuf/types"github.com/gogo/protobuf/types"Kitchen3:04PMSTRING"STRING"BOOL"BOOL"INT_64"INT_64"UINT_64"UINT_64"DURATION"DURATION"FLOAT_64"FLOAT_64"EnumNameprotobuf:"varint,1,opt,name=trace_id,json=traceId,proto3" json:"trace_id,omitempty"`protobuf:"varint,1,opt,name=trace_id,json=traceId,proto3" json:"trace_id,omitempty"`protobuf:"varint,2,opt,name=span_id,json=spanId,proto3" json:"span_id,omitempty"`protobuf:"varint,2,opt,name=span_id,json=spanId,proto3" json:"span_id,omitempty"`protobuf:"bytes,1,opt,name=context" json:"context"`protobuf:"bytes,1,opt,name=context" json:"context"`protobuf:"varint,2,opt,name=parent_span_id,json=parentSpanId,proto3" json:"parent_span_id,omitempty"`protobuf:"varint,2,opt,name=parent_span_id,json=parentSpanId,proto3" json:"parent_span_id,omitempty"`protobuf:"bytes,3,opt,name=name,proto3" json:"name,omitempty"`protobuf:"bytes,3,opt,name=name,proto3" json:"name,omitempty"`protobuf:"bytes,4,opt,name=start_time,json=startTime,stdtime" json:"start_time"`protobuf:"bytes,4,opt,name=start_time,json=startTime,stdtime" json:"start_time"`protobuf:"bytes,5,rep,name=labels" json:"labels,omitempty"`protobuf:"bytes,5,rep,name=labels" json:"labels,omitempty"`protobuf:"bytes,6,rep,name=fields" json:"fields"`protobuf:"bytes,6,rep,name=fields" json:"fields"`protobuf:"bytes,1,rep,name=spans" json:"spans,omitempty"`protobuf:"bytes,1,rep,name=spans" json:"spans,omitempty"`protobuf:"bytes,1,opt,name=key,proto3" json:"key,omitempty"`protobuf:"bytes,1,opt,name=key,proto3" json:"key,omitempty"`protobuf:"varint,2,opt,name=field_type,json=fieldType,proto3,enum=wire.Field_FieldType" json:"field_type,omitempty"`protobuf:"varint,2,opt,name=field_type,json=fieldType,proto3,enum=wire.Field_FieldType" json:"field_type,omitempty"`protobuf_oneof:"value"`protobuf_oneof:"value"`protobuf:"fixed64,3,opt,name=numeric_val,json=numericVal,proto3,oneof"`protobuf:"fixed64,3,opt,name=numeric_val,json=numericVal,proto3,oneof"`protobuf:"bytes,4,opt,name=string_val,json=stringVal,proto3,oneof"`protobuf:"bytes,4,opt,name=string_val,json=stringVal,proto3,oneof"`WireFixed64WireBytesField.Value has unexpected type %T"Field.Value has unexpected type %T"ErrInternalBadWireTypeSizeVarintproto: unexpected type %T in oneof"proto: unexpected type %T in oneof"wire.SpanContext"wire.SpanContext"wire.Span"wire.Span"wire.Trace"wire.Trace"wire.Field"wire.Field"RegisterEnumwire.Field_FieldType"wire.Field_FieldType"0x80xaSizeOfStdTimeStdTimeMarshalToErrUnexpectedEOF0x7Fproto: SpanContext: wiretype end group for non-group"proto: SpanContext: wiretype end group for non-group"proto: SpanContext: illegal tag %d (wire type %d)"proto: SpanContext: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field TraceID"proto: wrong wireType = %d for field TraceID"proto: wrong wireType = %d for field SpanID"proto: wrong wireType = %d for field SpanID"proto: Span: wiretype end group for non-group"proto: Span: wiretype end group for non-group"proto: Span: illegal tag %d (wire type %d)"proto: Span: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Context"proto: wrong wireType = %d for field Context"proto: wrong wireType = %d for field ParentSpanID"proto: wrong wireType = %d for field ParentSpanID"proto: wrong wireType = %d for field Name"proto: wrong wireType = %d for field Name"proto: wrong wireType = %d for field Start"proto: wrong wireType = %d for field Start"StdTimeUnmarshalproto: wrong wireType = %d for field Labels"proto: wrong wireType = %d for field Labels"proto: wrong wireType = %d for field Fields"proto: wrong wireType = %d for field Fields"proto: Trace: wiretype end group for non-group"proto: Trace: wiretype end group for non-group"proto: Trace: illegal tag %d (wire type %d)"proto: Trace: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Spans"proto: wrong wireType = %d for field Spans"proto: Field: wiretype end group for non-group"proto: Field: wiretype end group for non-group"proto: Field: illegal tag %d (wire type %d)"proto: Field: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Key"proto: wrong wireType = %d for field Key"proto: wrong wireType = %d for field FieldType"proto: wrong wireType = %d for field FieldType"proto: wrong wireType = %d for field NumericVal"proto: wrong wireType = %d for field NumericVal"proto: wrong wireType = %d for field StringVal"proto: wrong wireType = %d for field StringVal"proto: illegal wireType %d"proto: illegal wireType %d"proto: negative length found during unmarshalling"proto: negative length found during unmarshalling"proto: integer overflow"proto: integer overflow"binary.proto"binary.proto" source: binary.proto
	Package wire is a generated protocol buffer package.

	It is generated from these files:
		binary.proto

	It has these top-level messages:
		SpanContext
		Span
		Trace
		Field
 Types that are valid to be assigned to Value:	*Field_NumericVal	*Field_StringVal XXX_OneofFuncs is for the internal use of the proto package. value value.numeric_val value.string_val 624 bytes of a gzipped FileDescriptorProto/Users/austinjaybecker/projects/abeck-go-testing/pkger/Users/austinjaybecker/projects/abeck-go-testing/pkger/clone_resource.goAPIVersion2ActionTypeSkipKindActionTypeSkipResourceBucketToObjectCheckToObjectEncodingUnknownErrInvalidEncodingFromStringIsExistingIsParseErrKindCheckDeadmanKindCheckThresholdKindNotificationEndpointHTTPKindNotificationEndpointPagerDutyKindNotificationEndpointSlackKindPackageLabelToObjectNewParseErrorNotificationEndpointToObjectNotificationRuleToObjectReqApplyReqCreateStackReqExportReqExportOrgIDOptReqRawActionReqRawTemplateReqTemplateRemoteReqUpdateStackReqUpdateStackResourceResourceTypeStackRespApplyRespExportRespListStacksRespStackRespStackEventRespStackResourceRespStackResourceAssocRespStackResourceLinksRoutePrefixStacksRoutePrefixTemplatesStackEventCreateStackEventUpdateStateStatusExistsStateStatusNewStateStatusRemoveTaskToObjectTelegrafToObjectValidWithoutResourcesWithIDGeneratorWithTimeGeneratoractionTypeapplyErrsapplyFailErrapplyMetricAdditionsapplyOptFromOptFnsassignNonZeroBoolsassignNonZeroFloatsassignNonZeroFluxDursassignNonZeroIntsassignNonZeroSecretsassignNonZeroStringsassignRangeThresholdastBoolFromIfaceastDurationFromIfaceastFloatFromIfaceastIntegerFromIfaceastNowastStringFromIfaceastTimeFromIfaceauthMWbucketNameMinLengthchartKindBandchartKindGaugechartKindHeatMapchartKindHistogramchartKindMarkdownchartKindMosaicchartKindScatterchartKindSingleStatchartKindSingleStatPlusLinechartKindTablechartKindUnknownchartKindXYcheckKindDeadmancheckKindThresholdcheckNameMinLengthcloneAssociationsFncolorTypeBackgroundcolorTypeMaxcolorTypeMincolorTypeScalecolorTypeTextcolorTypeThresholdconvertCellViewconvertChartToResourceconvertChartsToCellsconvertEntStackEventconvertParseErrconvertRefToRefSummaryconvertRespStackEventconvertRespStackResourcesconvertRespStackToStackconvertStackEntResourcesconvertStackEntToStackconvertStackEventconvertStackToEntconvertStackToRespStackconvertThresholddashboardNameMinLengthdecodeWithEncodingdns1123LabelErrMsgdns1123LabelFmtdns1123LabelMaxLengthdns1123LabelRegexpdoMutexdurToStrentStackentStackAssociationentStackEvententStackResourceerrMsgerrStreamexportKeyexportOptFromOptFnsexportVecfailedValidationErrfieldAPIVersionfieldArgTypeConstantfieldArgTypeMapfieldArgTypeQueryfieldAssociationsfieldAxisBasefieldAxisLabelfieldAxisScalefieldBucketRetentionRulesfieldChartAxesfieldChartBinCountfieldChartBinSizefieldChartColorsfieldChartDecimalPlacesfieldChartDomainfieldChartFieldOptionDisplayNamefieldChartFieldOptionFieldNamefieldChartFieldOptionVisiblefieldChartFieldOptionsfieldChartFillColumnsfieldChartGenerateXAxisTicksfieldChartGenerateYAxisTicksfieldChartGeomfieldChartHeightfieldChartHoverDimensionfieldChartLegendfieldChartLegendColorizeRowsfieldChartLegendOpacityfieldChartLegendOrientationThresholdfieldChartLowerColumnfieldChartMainColumnfieldChartNotefieldChartNoteOnEmptyfieldChartPositionfieldChartQueriesfieldChartShadefieldChartTableOptionFixFirstColumnfieldChartTableOptionSortByfieldChartTableOptionVerticalTimeAxisfieldChartTableOptionWrappingfieldChartTableOptionsfieldChartTickPrefixfieldChartTickSuffixfieldChartTimeFormatfieldChartUpperColumnfieldChartWidthfieldChartXColfieldChartXPosfieldChartXTickStartfieldChartXTickStepfieldChartXTotalTicksfieldChartYColfieldChartYPosfieldChartYSeriesColumnsfieldChartYTickStartfieldChartYTickStepfieldChartYTotalTicksfieldCheckAllValuesfieldCheckReportZerofieldCheckStaleTimefieldCheckStatusMessageTemplatefieldCheckTagsfieldCheckThresholdsfieldCheckTimeSincefieldColorHexfieldDashChartsfieldDefaultfieldDescriptionfieldEveryfieldKindfieldLabelColorfieldLanguagefieldLegendOrientationfieldLevelfieldMaxfieldMetadatafieldMinfieldNamefieldNotificationEndpointHTTPMethodfieldNotificationEndpointPasswordfieldNotificationEndpointRoutingKeyfieldNotificationEndpointTokenfieldNotificationEndpointURLfieldNotificationEndpointUsernamefieldNotificationRuleChannelfieldNotificationRuleCurrentLevelfieldNotificationRuleEndpointNamefieldNotificationRuleMessageTemplatefieldNotificationRulePreviousLevelfieldNotificationRuleStatusRulesfieldNotificationRuleTagRulesfieldOffsetfieldOperatorfieldParamsfieldPrefixfieldQueryfieldReferencesEnvfieldReferencesSecretfieldRetentionRulesEverySecondsfieldSpecfieldStatusfieldSuffixfieldTaskfieldTaskCronfieldTelegrafConfigfieldValuefieldValuesfieldVariableSelectedflt64PtrfluxRegexformatSourcesgeometryTypesgetLabelIDMapgetRequiredOrgIDFromQuerygithubHostgithubRawContentHostifaceToReferenceifaceToResourceifaceToStrimpactToRespApplyinfluxErrintPtrisDNS1123LabelisRestrictedTaskisSystemBucketisValidNamekindPrioritieskindslabelNameMinLengthlabelSlcToMaploggingMWmwMetricsnewDecodeErrnewErrStreamnewExportKeynewJSONEncnewObjectnewResourceExporternewRetentionRulenewRollbackCoordinatornewStateCoordinatornewSummaryFromStateTemplatenormStrnormalizeGithubURLToContentnormalizeRawGithubPathnormalizeRemoteSourcesnotificationHTTPAuthTypeBasicnotificationHTTPAuthTypeBearernotificationHTTPAuthTypeNonenotificationKindHTTPnotificationKindPagerDutynotificationKindSlackobjectValidationErrparseJSONparseJsonnetparseYAMLreCommunityTemplatesValidAddrregexErrorresourceActionsresourceExporterretentionRuleTypeExpiresortObjectsstackIDFromReqstackResLinksstateLabelMappingToInfluxLabelMappingstateLabelsToStackAssociationsstateToSummaryLabelsstoreListFilterFnstringsToColorssummarizeCommonReferencestaskFluxRegextaskFluxTranslationtemplateEncodingtemplateVecthresholdTypeGreaterthresholdTypeInsideRangethresholdTypeLesserthresholdTypeOutsideRangethresholdTypestoInfluxThresholdstoNotificationDurationtoSummaryLabelstoSummaryStatusRulestoSummaryTagRulestraceMWtraverseErrsuniqByNameResIDuniqMetaNameuniqResourcesToClonevalFromExprvalidEndpointHTTPMethodsvalidGeometryvalidPositionvalidTableOptionsvalidURLswithNameGenassociationsiKindiNamejKindjNamecloneAssFnmObjectsmPkgNamesmStackResourcesStackResourcesresourceCloneToKindresourceCloneAssociationsGenuniqNamemLabelIDsToMetaNameresourcesToCloneobjectsstackResourceuniqResIDchshasIDmappednotifEndpointallEndpointsallRulesendpointKeyendpointObjectNameruleEndpointasscFnmapResourceshouldSkiplabelObjectlabelIDsToMetaNamemLabelIDsmLabelNamesexistingNameskrrKeycTassignBaseallValuesassignLesserrangeThresholdthTypeiColorsiQueriesnoteOnEmptyfieldOptsetCommonsetLegendsetNoteFixestReszerofResfofieldOptsqqiAxesiqjcchartstRulesRessRulestatusRuleRestagResendpointPkgNameiRuleclrsnewColorsicheckwordplaygithub.com/influxdata/influxdb/v2/pkger/internal/wordplay"github.com/influxdata/influxdb/v2/pkger/internal/wordplay"json:"kind"`json:"kind"`must provide an ID or name"must provide an ID or name"CheckDeadmanNotificationEndpointHTTPNotificationEndpointPagerDutyNotificationEndpointSlackGetRandomNamefailed to clone resource: resource_id=%s resource_kind=%s err=%q"failed to clone resource: resource_id=%s resource_kind=%s err=%q"cloning resource"cloning resource"no buckets found"no buckets found"no checks found"no checks found"no notification endpoints found"no notification endpoints found"no notification rules found"no notification rules found"no tasks found"no tasks found"no telegraf configs found"no telegraf configs found"no variables found"no variables found"unsupported kind provided: "unsupported kind provided: "finding resource labels"finding resource labels"%s-%s"%s-%s"statusMessageTemplateoutside_rangeinside_rangesingle_stat_plus_linesingle_statheightySeriesColumnsupperColumnmainColumnlowerColumnfillColumnsgenerateXAxisTicksgenerateYAxisTicksverticalTimeAxisfixFirstColumnwrappingshadelegendColorizeRowsgeomxColyColtickPrefixtickSuffixtimeFormathoverDimensionxPosxTotalTicksyPosyTotalTicksbinCountbinSizelegendOrientationThresholdlegendOpacityxTickStartxTickStepyTickStartyTickStepoperatorcurrentLevelpreviousLevelmessageTemplateoption task = {(.|\n)*?}`option task = {(.|\n)*?}`languagesecretRef NameGenerator generates a random name. Includes an optional fuzz option to further randomize the name. ResourceToClone is a resource that will be cloned. note(jsteenb2): For time being we'll allow this internally, but not externally. A lot of issues to account for when exposing this to the outside world. Not something I'm keen to accommodate at this time. OK validates a resource clone is viable. sorting this in priority order guarantees that the dependencies/associations for a resource are handled prior to the resource being processed. 	i.e. if a bucket depends on a label, then labels need to be run first		to guarantee they are available before a bucket is exported. we only need an id when we have resources that are not unique by name via the metastore. resoureces that are unique by name will be provided a default stamp making looksup unique since each resource will be unique by name. overwrite the default metadata.name field with export generated one here check here verifies the label maps to an id of a valid label name BucketToObject converts a influxdb.Bucket into an Object. DashboardToObject converts an influxdb.Dashboard to an Object. LabelToObject converts an influxdb.Label to an Object. NotificationEndpointToObject converts an notification endpoint into a pkger Object. NotificationRuleToObject converts an notification rule into a pkger Object. regex used to rip out the hard coded task option stuffs TaskToObject converts an influxdb.Task into a pkger.Object. TelegrafToObject converts an influxdb.TelegrafConfig into a pkger.Object. VariableToObject converts an influxdb.Variable to a pkger.Object. this timestamp is added to make the resource unique. Should also indicate to the end user that this is machine readable and the spec.name field is the one they want to edit when a name change is desired.msgStreamtemplateCountsDateTimeLiteralskipKindsskipResourcesByLabelByResourceKindRawTemplatesRawTemplateSecretsRawActionsvalidActionssummaryLogFieldsStackstoErrorrawQuerytftseparateQueryImportsSkipKindsSkipResourcesSecsMutateSecsMutateNsecsMutateOffset/Users/austinjaybecker/projects/abeck-go-testing/pkger/doc.go
Package pkger implements a means to create and consume reusable
templates for what will eventually come to support all influxdb
resources.

The parser supports JSON, Jsonnet, and YAML encodings as well as a number
of different ways to read the file/reader/string as it may.

As an example, you can use the following to parse and validate a YAML
file and see a summary of its contents:

	newTemplate, err := Parse(EncodingYAML, FromFile(PATH_TO_FILE))
	if err != nil {
		panic(err) // handle error as you see fit
	}
	sum := newTemplate.Summary()
	fmt.Println(sum) // do something with the summary

The parser will validate all contents of the template and provide any
and all fields/entries that failed validation.

If you wish to use the Template type in your transport layer and let the
the transport layer manage the decoding, then you can run the following
to validate the template after the raw decoding is done:

	if err := template.Validate(); err != nil {
		panic(err) // handle error as you see fit
	}

If a validation error is encountered during the validation or parsing then
the error returned will be of type *parseErr. The parseErr provides a rich
set of validations failures. There can be numerous failures in a template
and we did our best to inform the caller about them all in a single run.

If you want to see the effects of a template before applying it to the
organization's influxdb platform, you have the flexibility to dry run the
template and see the outcome of what would happen after it were to be applied.
You may use the following to dry run a template within your organization:

	svc := NewService(serviceOpts...)
	summary, diff, err := svc.DryRun(ctx, orgID, userID, ApplyWithTemplate(template))
	if err != nil {
		panic(err) // handle error as you see fit
	}
	// explore the summary and diff

The diff provided here is a diff of the existing state of the platform for
your organization and the concluding the state after the application of a
template. All buckets, labels, and variables, when given a name that already
exists, will not create a new resource, but rather, will edit the existing
resource. If this is not a desired result, then rename your bucket to something
else to avoid the imposed changes applying this template would incur. The summary
provided is a summary of the template itself. If a resource exists all IDs will
be populated for them, if they do not, then they will be zero values. Any zero
value ID is safe to assume is not populated. All influxdb.ID's must be non zero
to be in existence.

If you would like to apply a template you may use the service to do so. The
following will apply the template in full to the provided organization.

	svc := NewService(serviceOpts...)
	summary, err := svc.Apply(ctx, orgID, userID, ApplyWithTemplate(template))
	if err != nil {
		panic(err) // handle error as you see fit
	}
	// explore the summary

The summary will be populated with valid IDs that were created during the
application of the template. If an error is encountered during the application
of a template, then all changes that had occurred will be rolled back. However, as
a warning for buckets, changes may have incurred destructive changes. The changes
are not applied inside a large transaction, for numerous reasons, but it is
something to be considered. If you have dry run the template before it is to be
applied, then the changes should have been made known to you. If not, then there is
potential loss of data if the changes to a bucket resulted in the retention period
being shortened in the template.

If you would like to export existing resources into the form of a template, then you
have the ability to do so using the following:

	resourcesToClone := []ResourceToClone{
		{
			Kind: KindBucket,
			ID:   Existing_BUCKET_ID,
			Name: "new bucket name"
		},
		{
			Kind: KindDashboard,
			ID:   Existing_Dashboard_ID,
		},
		{
			Kind: KindLabel,
			ID:   Existing_Label_ID,
		},
		{
			Kind: KindVarible,
			ID:   Existing_Var_ID,
		},
	}

	svc := NewService(serviceOpts...)
	newTemplate, err := svc.Export(ctx, ExportWithExistingResources(resourcesToClone...))
	if err != nil {
		panic(err) // handle error as you see fit
	}
	// explore newly created and validated template

Things to note about the behavior of exporting existing resources. All label
associations with existing resources will be included in the new template.
However, the variables that are used within a dashboard query will not be added
automatically to the template. Variables will need to be passed in alongside
the dashboard to be added to the template.
/Users/austinjaybecker/projects/abeck-go-testing/pkger/http_remote_service.goidentifiersrespBodyqueryParamsrawTemplaterespEveventsnewStackrespStackeventTyperesID/api/v2/stacks/uninstall"/uninstall""stackID"json:"byLabel"`json:"byLabel"`json:"byResourceKind"`json:"byResourceKind"`/api/v2/templates/export"/export"skipKind/apply"/apply" HTTPRemoteService provides an http client that is fluent in all things template. Export will produce a template from the parameters provided. DryRun provides a dry run of the template application. The template will be marked verified for later calls to Apply. This func will be run on an Apply if it has not been run already. Apply will apply all the resources identified in the provided template. The entire template will be applied in its entirety. If a failure happens midway then the entire template will be rolled back to the state from before the template was applied. valid response code when the template itself has parser errors. we short circuit on that and allow that response to pass through but consume the initial implementation if that does not hold. delete is included to maintain backwards compatibility/Users/austinjaybecker/projects/abeck-go-testing/pkger/http_server_stack.gosvrrawOrgIDasseslinkResourceorgIDRaw"/api/v2/stacks"/{stack_id}"/{stack_id}"json:"eventType"`json:"eventType"`json:"resources"`json:"resources"`json:"urls"`json:"urls"`json:"apiVersion"`json:"apiVersion"`json:"templateMetaName"`json:"templateMetaName"`json:"associations"`json:"associations"`json:"metaName"`json:"metaName"`json:"stacks"`json:"stacks"`organization id[%q] is invalid"organization id[%q] is invalid"failed to parse form from encoded url"failed to parse form from encoded url"stack ID[%q] provided is invalid"stack ID[%q] provided is invalid"provided org id[%q] is invalid"provided org id[%q] is invalid"provided url[%q] is invalid"provided url[%q] is invalid"json:"templateURLs"`json:"templateURLs"`json:"additionalResources"`json:"additionalResources"`stack resource id %q"stack resource id %q"stack_id"stack_id"the stack id provided in the path was invalid"the stack id provided in the path was invalid"the orgID query param is required"the orgID query param is required"the orgID query param was invalid"the orgID query param was invalid" HTTPServerStacks is a server that manages the stacks HTTP transport. NewHTTPServerStacks constructs a new http server. Prefix provides the prefix to this route tree. RespStack is the response body for a stack. maintain same interface for backward compatibility RespStackResource is the response for a stack resource. This type exists to decouple the internal service implementation from the deprecates usage of templates in the API. We could add a custom UnmarshalJSON method, but I would rather keep it obvious and explicit with a separate field. RespStackResourceAssoc is the response for a stack resource's associations. RespListStacks is the HTTP response for a stack list call. ReqCreateStack is a request body for a create stack call. OK validates the request body is valid. TODO: provide multiple errors back for failing validation ReqUpdateStack is the request body for updating a stack. Deprecating the urls field and replacing with templateURLs field. This is remaining here for backwards compatibility./Users/austinjaybecker/projects/abeck-go-testing/pkger/http_server_template.goexportAllowContentTypessetJSONContentTypesourceEncodingrawTmplasraskkindErrFnparsedTemplategithub.com/influxdata/influxdb/v2/pkg/jsonnet"github.com/influxdata/influxdb/v2/pkg/jsonnet""gopkg.in/yaml.v3""/api/v2/templates"AllowContentTypetext/yml"text/yml"application/x-yaml"application/x-yaml"json:"resourceFilters"`json:"resourceFilters"`json:"orgIDs"`json:"orgIDs"`at least 1 resource, 1 org id, or stack id must be provided"at least 1 resource, 1 org id, or stack id must be provided"provided org id is invalid: %q"provided org id is invalid: %q"invalid stack ID provided: %q"invalid stack ID provided: %q"yaml_emitter_tyaml_error_type_tyaml_write_handler_tyaml_encoding_tyaml_break_tyaml_emitter_state_tyaml_event_tyaml_event_type_tyaml_mark_tyaml_version_directive_tmajorminoryaml_tag_directive_tyaml_style_tstart_markend_markversion_directivetag_directiveshead_commentline_commentfoot_commenttail_commentanchorimplicitquoted_implicitstylescalar_stylesequence_stylemapping_styleyaml_scalar_style_tmultilineflow_plain_allowedblock_plain_allowedsingle_quoted_allowedblock_allowedserializedyaml_document_tyaml_node_tyaml_node_type_tyaml_node_item_tyaml_sequence_style_titems_datayaml_node_pair_tyaml_mapping_style_tpairs_datapairs_startpairs_endpairs_topscalartag_directives_datatag_directives_starttag_directives_endstart_implicitend_implicitproblemwrite_handleroutput_bufferoutput_writerbuffer_posraw_bufferraw_buffer_poscanonicalbest_indentbest_widthline_breakstatesevents_headindentsflow_levelroot_contextsequence_contextmapping_contextsimple_key_contextwhitespaceindentionopen_endedspace_abovefoot_indentanchor_datatag_datascalar_dataanchorslast_anchor_idflowdoneInitmustmarshalDocmapvfieldByIndexstructvmappingvslicevstringvboolvintvuintvtimevfloatvnilvemitScalarnodevjson:"url" yaml:"url"`json:"url" yaml:"url"`json:"contentType" yaml:"contentType"`json:"contentType" yaml:"contentType"`json:"sources" yaml:"sources"`json:"sources" yaml:"sources"`json:"contents" yaml:"contents"`json:"contents" yaml:"contents"`json:"dryRun" yaml:"dryRun"`json:"dryRun" yaml:"dryRun"`json:"orgID" yaml:"orgID"`json:"orgID" yaml:"orgID"`json:"stackID" yaml:"stackID"`json:"stackID" yaml:"stackID"`json:"remotes" yaml:"remotes"`json:"remotes" yaml:"remotes"`json:"templates" yaml:"templates"`json:"templates" yaml:"templates"`json:"template" yaml:"template"`json:"template" yaml:"template"`json:"envRefs"`json:"envRefs"`json:"secrets"`json:"secrets"`json:"actions"`json:"actions"`template from url[%s] had an issue: %s"template from url[%s] had an issue: %s"template[%d] from source(s) %q had an issue: %s"template[%d] from source(s) %q had an issue: %s""skipKind""skipResource"failed to unmarshal properties for actions[%d] %q"failed to unmarshal properties for actions[%d] %q"invalid kind for actions[%d] %q"invalid kind for actions[%d] %q"invalid action type %q provided for actions[%d] ; Must be one of [%s]"invalid action type %q provided for actions[%d] ; Must be one of [%s]"json:"diff" yaml:"diff"`json:"diff" yaml:"diff"`json:"summary" yaml:"summary"`json:"summary" yaml:"summary"`json:"errors,omitempty" yaml:"errors,omitempty"`json:"errors,omitempty" yaml:"errors,omitempty"`invalid organization ID provided: %q"invalid organization ID provided: %q"unable to marshal; Err: %v"unable to marshal; Err: %v"; "; "yaml_parser_tyaml_read_handler_tyaml_comment_tscan_marktoken_markfootyaml_token_tyaml_token_type_tyaml_simple_key_tpossibletoken_numbermarkyaml_parser_state_tyaml_alias_data_tproblem_offsetproblem_valueproblem_markcontext_markread_handlerinput_readerinput_posnewlinesstem_commentcommentscomments_headstream_start_producedstream_end_producedtokens_headtokens_parsedtoken_availablesimple_key_allowedsimple_keyssimple_keys_by_tokmarksStyleAnchorHeadCommentLineCommentFootCommentLongTagShortTagindicatedStringparseChildknownFieldsKnownFieldsapplication/x-jsonnet"application/x-jsonnet"unable to unmarshal %s"unable to unmarshal %s" HTTPServerTemplates is a server that manages the templates HTTP transport. NewHTTPServerTemplates constructs a new http server. ReqExportOrgIDOpt provides options to export resources by organization id. ReqExport is a request body for the export endpoint. OK validates a create request. RespExport is a response body for the create template endpoint. ReqTemplateRemote provides a package via a remote (i.e. a gist). If content type is not provided then the service will do its best to discern the content type of the contents. Encoding returns the encoding type that corresponds to the given content type. ReqRawAction is a raw action consumers can provide to change the behavior of the application of a template. ReqApply is the request body for a json or yaml body for the apply template endpoint. optional: non nil value signals stack should be used Templates returns all templates associated with the request. various ActionTypes the transport API speaks RespApply is the response body for the apply template endpoint. guarantee non nil slice/Users/austinjaybecker/projects/abeck-go-testing/pkger/internal/Users/austinjaybecker/projects/abeck-go-testing/pkger/internal/wordplay/Users/austinjaybecker/projects/abeck-go-testing/pkger/internal/wordplay/wordplay.goadmiring"admiring"adoring"adoring"adventuring"adventuring"affectionate"affectionate"agitated"agitated"agreeing"agreeing"alerting"alerting"amazing"amazing"amusing"amusing"angry"angry"annoying"annoying"awesome"awesome"beautiful"beautiful"bettering"bettering"blissful"blissful"bold"bold""boring"brave"brave"burfect"burfect"busy"busy"charming"charming"clever"clever"cool"cool"compassionate"compassionate"competent"competent"condescending"condescending"confident"confident"cranky"cranky"crazy"crazy"crumbling"crumbling"dangerous"dangerous"dangling"dangling"dazzling"dazzling"determined"determined"distracted"distracted"dreamy"dreamy"eager"eager"earnest"earnest"earning"earning"ecstatic"ecstatic"eerie"eerie"elastic"elastic"elated"elated"elegant"elegant"eloquent"eloquent"endangered"endangered"epic"epic"exciting"exciting"fasting"fasting"fervent"fervent"festive"festive"flamboyant"flamboyant"focused"focused"friendly"friendly"frosty"frosty"funny"funny"gallant"gallant"gifted"gifted"goofy"goofy"gracious"gracious"great"great"happy"happy"hardcore"hardcore"heuristic"heuristic"hopeful"hopeful"hungry"hungry"infallible"infallible"inspiring"inspiring"interesting"interesting"intelligent"intelligent"jolly"jolly"jovial"jovial"keen"keen"laughing"laughing"loving"loving"lucid"lucid"magical"magical"mystifying"mystifying"modest"modest"musing"musing"naughty"naughty"nervous"nervous""nice"nifty"nifty"noshing"noshing"nostalgic"nostalgic"objective"objective"obstinate"obstinate"optimistic"optimistic"peaceful"peaceful"pedantic"pedantic"pensive"pensive"practical"practical"priceless"priceless"quirky"quirky"quizzical"quizzical"rainy"rainy"realistic"realistic"recursing"recursing"ridiculous"ridiculous"righteous"righteous"rightful"rightful"relaxed"relaxed"reverent"reverent"romantic"romantic"rustic"rustic"rustling"rustling"rusty"rusty"sad"sad"serene"serene"sharp"sharp"shiny"shiny"silly"silly"sleepy"sleepy"sloppy"sloppy"spectacular"spectacular"stoic"stoic"strange"strange"stubborn"stubborn"stupefied"stupefied"suspicious"suspicious"sweet"sweet"tasty"tasty"tender"tender"terrifying"terrifying"thirsty"thirsty"toasty"toasty"trusting"trusting"unbridled"unbridled"unruffled"unruffled"upbeat"upbeat"vibrant"vibrant"victorious"victorious"vigilant"vigilant"vigorous"vigorous"vivid"vivid"wizardly"wizardly"wonderful"wonderful"wondrous"wondrous"xenodochial"xenodochial"youthful"youthful"zealous"zealous"zen"zen"albattani"albattani"allen"allen"almeida"almeida"antonelli"antonelli"agnesi"agnesi"archimedes"archimedes"ardinghelli"ardinghelli"aryabhata"aryabhata"austin"austin"babbage"babbage"banach"banach"banzai"banzai"bardeen"bardeen"bartik"bartik"bassi"bassi"beaver"beaver"bell"bell"benz"benz"bhabha"bhabha"bhaskara"bhaskara"black"black"blackburn"blackburn"blackwell"blackwell"bohr"bohr"booth"booth"borg"borg"bose"bose"bouman"bouman"boyd"boyd"brahmagupta"brahmagupta"brattain"brattain"brown"brown"buck"buck"burnell"burnell"cannon"cannon"carson"carson"cartwright"cartwright"carver"carver"cerf"cerf"chandrasekhar"chandrasekhar"chaplygin"chaplygin"chatelet"chatelet"chatterjee"chatterjee"chebyshev"chebyshev"cohen"cohen"chaum"chaum"clarke"clarke"colden"colden"cori"cori"cray"cray"curran"curran"curie"curie""darwin"davinci"davinci"dewdney"dewdney"dhawan"dhawan"diffie"diffie"dijkstra"dijkstra"dirac"dirac"driscoll"driscoll"dubinsky"dubinsky"easley"easley"edison"edison"einstein"einstein"elbakyan"elbakyan"elgamal"elgamal"elion"elion"ellis"ellis"engelbart"engelbart"euclid"euclid"euler"euler"faraday"faraday"feistel"feistel"fermat"fermat"fermi"fermi"feynman"feynman"franklin"franklin"gagarin"gagarin"galileo"galileo"galois"galois"ganguly"ganguly"gates"gates"gauss"gauss"germain"germain"goldberg"goldberg"goldstine"goldstine"goldwasser"goldwasser"golick"golick"goodall"goodall"gould"gould"greider"greider"grothendieck"grothendieck"haibt"haibt"hamilton"hamilton"haslett"haslett"hawking"hawking"hellman"hellman"heisenberg"heisenberg"hermann"hermann"herschel"herschel"hertz"hertz"heyrovsky"heyrovsky"hodgkin"hodgkin"hofstadter"hofstadter"hoover"hoover"hopper"hopper"hugle"hugle"hypatia"hypatia"ishizaka"ishizaka"jackson"jackson"jang"jang"jemison"jemison"jennings"jennings"jepsen"jepsen"johnson"johnson"joliot"joliot"jones"jones"kalam"kalam"kapitsa"kapitsa"kare"kare"keldysh"keldysh"keller"keller"kepler"kepler"khayyam"khayyam"khorana"khorana"kilby"kilby"kirch"kirch"knuth"knuth"kowalevski"kowalevski"lalande"lalande"lamarr"lamarr"lamport"lamport"leakey"leakey"leavitt"leavitt"lederberg"lederberg"lehmann"lehmann"lewin"lewin"lichterman"lichterman"liskov"liskov"lovelace"lovelace"lumiere"lumiere"mahavira"mahavira"margulis"margulis"matsumoto"matsumoto"maxwell"maxwell"mayer"mayer"mccarthy"mccarthy"mcclintock"mcclintock"mclaren"mclaren"mclean"mclean"mcnulty"mcnulty"mendel"mendel"mendeleev"mendeleev"meitner"meitner"meninsky"meninsky"merkle"merkle"mestorf"mestorf"mirzakhani"mirzakhani"moore"moore"morse"morse"murdock"murdock"moser"moser"napier"napier"nash"nash"neumann"neumann"newton"newton"nightingale"nightingale"nobel"nobel"noether"noether"northcutt"northcutt"noyce"noyce"panini"panini"pare"pare"pascal"pascal"pasteur"pasteur"payne"payne"perlman"perlman"pike"pike"poincare"poincare"poitras"poitras"proskuriakova"proskuriakova"ptolemy"ptolemy"raman"raman"ramanujan"ramanujan"ride"ride"montalcini"montalcini"ritchie"ritchie"rhodes"rhodes"robinson"robinson"roentgen"roentgen"rosalind"rosalind"rubin"rubin"saha"saha"sammet"sammet"sanderson"sanderson"satoshi"satoshi"shamir"shamir"shannon"shannon"shaw"shaw"shirley"shirley"shockley"shockley"shtern"shtern"sinoussi"sinoussi"snyder"snyder"solomon"solomon"spence"spence"stonebraker"stonebraker"sutherland"sutherland"swanson"swanson"swartz"swartz"swirles"swirles"taussig"taussig"tereshkova"tereshkova"tesla"tesla"tharp"tharp"thompson"thompson"torvalds"torvalds""tu"turing"turing"varahamihira"varahamihira"vaughan"vaughan"visvesvaraya"visvesvaraya"volhard"volhard"villani"villani"wescoff"wescoff"wilbur"wilbur"wiles"wiles"williams"williams"williamson"williamson"wilson"wilson"wing"wing"wozniak"wozniak"wright"wright"wu"wu"yalow"yalow"yonath"yonath"zhukovsky"zhukovsky" Muhammad ibn JÄbir al-á¸¤arrÄnÄ« al-BattÄnÄ« was a founding father of astronomy. https://en.wikipedia.org/wiki/Mu%E1%B8%A5ammad_ibn_J%C4%81bir_al-%E1%B8%A4arr%C4%81n%C4%AB_al-Batt%C4%81n%C4%AB Frances E. Allen, became the first female IBM Fellow in 1989. In 2006, she became the first female recipient of the ACM's Turing Award. https://en.wikipedia.org/wiki/Frances_E._Allen June Almeida - Scottish virologist who took the first pictures of the rubella virus - https://en.wikipedia.org/wiki/June_Almeida Kathleen Antonelli, American computer programmer and one of the six original programmers of the ENIAC - https://en.wikipedia.org/wiki/Kathleen_Antonelli Maria Gaetana Agnesi - Italian mathematician, philosopher, theologian and humanitarian. She was the first woman to write a mathematics handbook and the first woman appointed as a Mathematics Professor at a University. https://en.wikipedia.org/wiki/Maria_Gaetana_Agnesi Archimedes was a physicist, engineer and mathematician who invented too many things to list them here. https://en.wikipedia.org/wiki/Archimedes Maria Ardinghelli - Italian translator, mathematician and physicist - https://en.wikipedia.org/wiki/Maria_Ardinghelli Aryabhata - Ancient Indian mathematician-astronomer during 476-550 CE https://en.wikipedia.org/wiki/Aryabhata Wanda Austin - Wanda Austin is the President and CEO of The Aerospace Corporation, a leading architect for the US security space programs. https://en.wikipedia.org/wiki/Wanda_Austin Charles Babbage invented the concept of a programmable computer. https://en.wikipedia.org/wiki/Charles_Babbage. Stefan Banach - Polish mathematician, was one of the founders of modern functional analysis. https://en.wikipedia.org/wiki/Stefan_Banach Buckaroo Banzai and his mentor Dr. Hikita perfected the "oscillation overthruster", a device that allows one to pass through solid matter. - https://en.wikipedia.org/wiki/The_Adventures_of_Buckaroo_Banzai_Across_the_8th_Dimension John Bardeen co-invented the transistor - https://en.wikipedia.org/wiki/John_Bardeen Jean Bartik, born Betty Jean Jennings, was one of the original programmers for the ENIAC computer. https://en.wikipedia.org/wiki/Jean_Bartik Laura Bassi, the world's first female professor https://en.wikipedia.org/wiki/Laura_Bassi Hugh Beaver, British engineer, founder of the Guinness Book of World Records https://en.wikipedia.org/wiki/Hugh_Beaver Alexander Graham Bell - an eminent Scottish-born scientist, inventor, engineer and innovator who is credited with inventing the first practical telephone - https://en.wikipedia.org/wiki/Alexander_Graham_Bell Karl Friedrich Benz - a German automobile engineer. Inventor of the first practical motorcar. https://en.wikipedia.org/wiki/Karl_Benz Homi J Bhabha - was an Indian nuclear physicist, founding director, and professor of physics at the Tata Institute of Fundamental Research. Colloquially known as "father of Indian nuclear programme"- https://en.wikipedia.org/wiki/Homi_J._Bhabha Bhaskara II - Ancient Indian mathematician-astronomer whose work on calculus predates Newton and Leibniz by over half a millennium - https://en.wikipedia.org/wiki/Bh%C4%81skara_II#Calculus Sue Black - British computer scientist and campaigner. She has been instrumental in saving Bletchley Park, the site of World War II codebreaking - https://en.wikipedia.org/wiki/Sue_Black_(computer_scientist) Elizabeth Helen Blackburn - Australian-American Nobel laureate; best known for co-discovering telomerase. https://en.wikipedia.org/wiki/Elizabeth_Blackburn Elizabeth Blackwell - American doctor and first American woman to receive a medical degree - https://en.wikipedia.org/wiki/Elizabeth_Blackwell Niels Bohr is the father of quantum theory. https://en.wikipedia.org/wiki/Niels_Bohr. Kathleen Booth, she's credited with writing the first assembly language. https://en.wikipedia.org/wiki/Kathleen_Booth Anita Borg - Anita Borg was the founding director of the Institute for Women and Technology (IWT). https://en.wikipedia.org/wiki/Anita_Borg Satyendra Nath Bose - He provided the foundation for BoseâEinstein statistics and the theory of the BoseâEinstein condensate. - https://en.wikipedia.org/wiki/Satyendra_Nath_Bose Katherine Louise Bouman is an imaging scientist and Assistant Professor of Computer Science at the California Institute of Technology. She researches computational methods for imaging, and developed an algorithm that made possible the picture first visualization of a black hole using the Event Horizon Telescope. - https://en.wikipedia.org/wiki/Katie_Bouman Evelyn Boyd Granville - She was one of the first African-American woman to receive a Ph.D. in mathematics; she earned it in 1949 from Yale University. https://en.wikipedia.org/wiki/Evelyn_Boyd_Granville Brahmagupta - Ancient Indian mathematician during 598-670 CE who gave rules to compute with zero - https://en.wikipedia.org/wiki/Brahmagupta#Zero Walter Houser Brattain co-invented the transistor - https://en.wikipedia.org/wiki/Walter_Houser_Brattain Emmett Brown invented time travel. https://en.wikipedia.org/wiki/Emmett_Brown (thanks Brian Goff) Linda Brown Buck - American biologist and Nobel laureate best known for her genetic and molecular analyses of the mechanisms of smell. https://en.wikipedia.org/wiki/Linda_B._Buck Dame Susan Jocelyn Bell Burnell - Northern Irish astrophysicist who discovered radio pulsars and was the first to analyse them. https://en.wikipedia.org/wiki/Jocelyn_Bell_Burnell Annie Jump Cannon - pioneering female astronomer who classified hundreds of thousands of stars and created the system we use to understand stars today. https://en.wikipedia.org/wiki/Annie_Jump_Cannon Rachel Carson - American marine biologist and conservationist, her book Silent Spring and other writings are credited with advancing the global environmental movement. https://en.wikipedia.org/wiki/Rachel_Carson Dame Mary Lucy Cartwright - British mathematician who was one of the first to study what is now known as chaos theory. Also known for Cartwright's theorem which finds applications in signal processing. https://en.wikipedia.org/wiki/Mary_Cartwright George Washington Carver - American agricultural scientist and inventor. He was the most prominent black scientist of the early 20th century. https://en.wikipedia.org/wiki/George_Washington_Carver Vinton Gray Cerf - American Internet pioneer, recognised as one of "the fathers of the Internet". With Robert Elliot Kahn, he designed TCP and IP, the primary data communication protocols of the Internet and other computer networks. https://en.wikipedia.org/wiki/Vint_Cerf Subrahmanyan Chandrasekhar - Astrophysicist known for his mathematical theory on different stages and evolution in structures of the stars. He has won nobel prize for physics - https://en.wikipedia.org/wiki/Subrahmanyan_Chandrasekhar Sergey Alexeyevich Chaplygin (Russian: Ð¡ÐµÑÐ³ÐµÌÐ¹ ÐÐ»ÐµÐºÑÐµÌÐµÐ²Ð¸Ñ Ð§Ð°Ð¿Ð»ÑÌÐ³Ð¸Ð½; April 5, 1869 â October 8, 1942) was a Russian and Soviet physicist, mathematician, and mechanical engineer. He is known for mathematical formulas such as Chaplygin's equation and for a hypothetical substance in cosmology called Chaplygin gas, named after him. https://en.wikipedia.org/wiki/Sergey_Chaplygin Ãmilie du ChÃ¢telet - French natural philosopher, mathematician, physicist, and author during the early 1730s, known for her translation of and commentary on Isaac Newton's book Principia containing basic laws of physics. https://en.wikipedia.org/wiki/%C3%89milie_du_Ch%C3%A2telet Asima Chatterjee was an Indian organic chemist noted for her research on vinca alkaloids, development of drugs for treatment of epilepsy and malaria - https://en.wikipedia.org/wiki/Asima_Chatterjee Pafnuty Chebyshev - Russian mathematician. He is known fo his works on probability, statistics, mechanics, analytical geometry and number theory https://en.wikipedia.org/wiki/Pafnuty_Chebyshev Bram Cohen - American computer programmer and author of the BitTorrent peer-to-peer protocol. https://en.wikipedia.org/wiki/Bram_Cohen David Lee Chaum - American computer scientist and cryptographer. Known for his seminal contributions in the field of anonymous communication. https://en.wikipedia.org/wiki/David_Chaum Joan Clarke - Bletchley Park code breaker during the Second World War who pioneered techniques that remained top secret for decades. Also an accomplished numismatist https://en.wikipedia.org/wiki/Joan_Clarke Jane Colden - American botanist widely considered the first female American botanist - https://en.wikipedia.org/wiki/Jane_Colden Gerty Theresa Cori - American biochemist who became the third womanâand first American womanâto win a Nobel Prize in science, and the first woman to be awarded the Nobel Prize in Physiology or Medicine. Cori was born in Prague. https://en.wikipedia.org/wiki/Gerty_Cori Seymour Roger Cray was an American electrical engineer and supercomputer architect who designed a series of computers that were the fastest in the world for decades. https://en.wikipedia.org/wiki/Seymour_Cray This entry reflects a husband and wife team who worked together: Joan Curran was a Welsh scientist who developed radar and invented chaff, a radar countermeasure. https://en.wikipedia.org/wiki/Joan_Curran Samuel Curran was an Irish physicist who worked alongside his wife during WWII and invented the proximity fuse. https://en.wikipedia.org/wiki/Samuel_Curran Marie Curie discovered radioactivity. https://en.wikipedia.org/wiki/Marie_Curie. Charles Darwin established the principles of natural evolution. https://en.wikipedia.org/wiki/Charles_Darwin. Leonardo Da Vinci invented too many things to list here. https://en.wikipedia.org/wiki/Leonardo_da_Vinci. A. K. (Alexander Keewatin) Dewdney, Canadian mathematician, computer scientist, author and filmmaker. Contributor to Scientific American's "Computer Recreations" from 1984 to 1991. Author of Core War (program), The Planiverse, The Armchair Universe, The Magic Machine, The New Turing Omnibus, and more. https://en.wikipedia.org/wiki/Alexander_Dewdney Satish Dhawan - Indian mathematician and aerospace engineer, known for leading the successful and indigenous development of the Indian space programme. https://en.wikipedia.org/wiki/Satish_Dhawan Bailey Whitfield Diffie - American cryptographer and one of the pioneers of public-key cryptography. https://en.wikipedia.org/wiki/Whitfield_Diffie Edsger Wybe Dijkstra was a Dutch computer scientist and mathematical scientist. https://en.wikipedia.org/wiki/Edsger_W._Dijkstra. Paul Adrien Maurice Dirac - English theoretical physicist who made fundamental contributions to the early development of both quantum mechanics and quantum electrodynamics. https://en.wikipedia.org/wiki/Paul_Dirac Agnes Meyer Driscoll - American cryptanalyst during World Wars I and II who successfully cryptanalysed a number of Japanese ciphers. She was also the co-developer of one of the cipher machines of the US Navy, the CM. https://en.wikipedia.org/wiki/Agnes_Meyer_Driscoll Donna Dubinsky - played an integral role in the development of personal digital assistants (PDAs) serving as CEO of Palm, Inc. and co-founding Handspring. https://en.wikipedia.org/wiki/Donna_Dubinsky Annie Easley - She was a leading member of the team which developed software for the Centaur rocket stage and one of the first African-Americans in her field. https://en.wikipedia.org/wiki/Annie_Easley Thomas Alva Edison, prolific inventor https://en.wikipedia.org/wiki/Thomas_Edison Albert Einstein invented the general theory of relativity. https://en.wikipedia.org/wiki/Albert_Einstein Alexandra Asanovna Elbakyan (Russian: ÐÐ»ÐµÐºÑÐ°ÌÐ½Ð´ÑÐ° ÐÑÐ°ÌÐ½Ð¾Ð²Ð½Ð° Ð­Ð»Ð±Ð°ÐºÑÌÐ½) is a Kazakhstani graduate student, computer programmer, internet pirate in hiding, and the creator of the site Sci-Hub. Nature has listed her in 2016 in the top ten people that mattered in science, and Ars Technica has compared her to Aaron Swartz. - https://en.wikipedia.org/wiki/Alexandra_Elbakyan Taher A. ElGamal - Egyptian cryptographer best known for the ElGamal discrete log cryptosystem and the ElGamal digital signature scheme. https://en.wikipedia.org/wiki/Taher_Elgamal Gertrude Elion - American biochemist, pharmacologist and the 1988 recipient of the Nobel Prize in Medicine - https://en.wikipedia.org/wiki/Gertrude_Elion James Henry Ellis - British engineer and cryptographer employed by the GCHQ. Best known for conceiving for the first time, the idea of public-key cryptography. https://en.wikipedia.org/wiki/James_H._Ellis Douglas Engelbart gave the mother of all demos: https://en.wikipedia.org/wiki/Douglas_Engelbart Euclid invented geometry. https://en.wikipedia.org/wiki/Euclid Leonhard Euler invented large parts of modern mathematics. https://de.wikipedia.org/wiki/Leonhard_Euler Michael Faraday - British scientist who contributed to the study of electromagnetism and electrochemistry. https://en.wikipedia.org/wiki/Michael_Faraday Horst Feistel - German-born American cryptographer who was one of the earliest non-government researchers to study the design and theory of block ciphers. Co-developer of DES and Lucifer. Feistel networks, a symmetric structure used in the construction of block ciphers are named after him. https://en.wikipedia.org/wiki/Horst_Feistel Pierre de Fermat pioneered several aspects of modern mathematics. https://en.wikipedia.org/wiki/Pierre_de_Fermat Enrico Fermi invented the first nuclear reactor. https://en.wikipedia.org/wiki/Enrico_Fermi. Richard Feynman was a key contributor to quantum mechanics and particle physics. https://en.wikipedia.org/wiki/Richard_Feynman Benjamin Franklin is famous for his experiments in electricity and the invention of the lightning rod. Yuri Alekseyevich Gagarin - Soviet pilot and cosmonaut, best known as the first human to journey into outer space. https://en.wikipedia.org/wiki/Yuri_Gagarin Galileo was a founding father of modern astronomy, and faced politics and obscurantism to establish scientific truth.  https://en.wikipedia.org/wiki/Galileo_Galilei Ãvariste Galois - French mathematician whose work laid the foundations of Galois theory and group theory, two major branches of abstract algebra, and the subfield of Galois connections, all while still in his late teens. https://en.wikipedia.org/wiki/%C3%89variste_Galois Kadambini Ganguly - Indian physician, known for being the first South Asian female physician, trained in western medicine, to graduate in South Asia. https://en.wikipedia.org/wiki/Kadambini_Ganguly William Henry "Bill" Gates III is an American business magnate, philanthropist, investor, computer programmer, and inventor. https://en.wikipedia.org/wiki/Bill_Gates Johann Carl Friedrich Gauss - German mathematician who made significant contributions to many fields, including number theory, algebra, statistics, analysis, differential geometry, geodesy, geophysics, mechanics, electrostatics, magnetic fields, astronomy, matrix theory, and optics. https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss Marie-Sophie Germain - French mathematician, physicist and philosopher. Known for her work on elasticity theory, number theory and philosophy. https://en.wikipedia.org/wiki/Sophie_Germain Adele Goldberg, was one of the designers and developers of the Smalltalk language. https://en.wikipedia.org/wiki/Adele_Goldberg_(computer_scientist) Adele Goldstine, born Adele Katz, wrote the complete technical description for the first electronic digital computer, ENIAC. https://en.wikipedia.org/wiki/Adele_Goldstine Shafi Goldwasser is a computer scientist known for creating theoretical foundations of modern cryptography. Winner of 2012 ACM Turing Award. https://en.wikipedia.org/wiki/Shafi_Goldwasser James Golick, all around gangster. Jane Goodall - British primatologist, ethologist, and anthropologist who is considered to be the world's foremost expert on chimpanzees - https://en.wikipedia.org/wiki/Jane_Goodall Stephen Jay Gould was was an American paleontologist, evolutionary biologist, and historian of science. He is most famous for the theory of punctuated equilibrium - https://en.wikipedia.org/wiki/Stephen_Jay_Gould Carolyn Widney Greider - American molecular biologist and joint winner of the 2009 Nobel Prize for Physiology or Medicine for the discovery of telomerase. https://en.wikipedia.org/wiki/Carol_W._Greider Alexander Grothendieck - German-born French mathematician who became a leading figure in the creation of modern algebraic geometry. https://en.wikipedia.org/wiki/Alexander_Grothendieck Lois Haibt - American computer scientist, part of the team at IBM that developed FORTRAN - https://en.wikipedia.org/wiki/Lois_Haibt Margaret Hamilton - Director of the Software Engineering Division of the MIT Instrumentation Laboratory, which developed on-board flight software for the Apollo space program. https://en.wikipedia.org/wiki/Margaret_Hamilton_(scientist) Caroline Harriet Haslett - English electrical engineer, electricity industry administrator and champion of women's rights. Co-author of British Standard 1363 that specifies AC power plugs and sockets used across the United Kingdom (which is widely considered as one of the safest designs). https://en.wikipedia.org/wiki/Caroline_Haslett Stephen Hawking pioneered the field of cosmology by combining general relativity and quantum mechanics. https://en.wikipedia.org/wiki/Stephen_Hawking Martin Edward Hellman - American cryptologist, best known for his invention of public-key cryptography in co-operation with Whitfield Diffie and Ralph Merkle. https://en.wikipedia.org/wiki/Martin_Hellman Werner Heisenberg was a founding father of quantum mechanics. https://en.wikipedia.org/wiki/Werner_Heisenberg Grete Hermann was a German philosopher noted for her philosophical work on the foundations of quantum mechanics. https://en.wikipedia.org/wiki/Grete_Hermann Caroline Lucretia Herschel - German astronomer and discoverer of several comets. https://en.wikipedia.org/wiki/Caroline_Herschel Heinrich Rudolf Hertz - German physicist who first conclusively proved the existence of the electromagnetic waves. https://en.wikipedia.org/wiki/Heinrich_Hertz Jaroslav HeyrovskÃ½ was the inventor of the polarographic method, father of the electroanalytical method, and recipient of the Nobel Prize in 1959. His main field of work was polarography. https://en.wikipedia.org/wiki/Jaroslav_Heyrovsk%C3%BD Dorothy Hodgkin was a British biochemist, credited with the development of protein crystallography. She was awarded the Nobel Prize in Chemistry in 1964. https://en.wikipedia.org/wiki/Dorothy_Hodgkin Douglas R. Hofstadter is an American professor of cognitive science and author of the Pulitzer Prize and American Book Award-winning work Goedel, Escher, Bach: An Eternal Golden Braid in 1979. A mind-bending work which coined Hofstadter's Law: "It always takes longer than you expect, even when you take into account Hofstadter's Law." https://en.wikipedia.org/wiki/Douglas_Hofstadter Erna Schneider Hoover revolutionized modern communication by inventing a computerized telephone switching method. https://en.wikipedia.org/wiki/Erna_Schneider_Hoover Grace Hopper developed the first compiler for a computer programming language and  is credited with popularizing the term "debugging" for fixing computer glitches. https://en.wikipedia.org/wiki/Grace_Hopper Frances Hugle, she was an American scientist, engineer, and inventor who contributed to the understanding of semiconductors, integrated circuitry, and the unique electrical principles of microscopic materials. https://en.wikipedia.org/wiki/Frances_Hugle Hypatia - Greek Alexandrine Neoplatonist philosopher in Egypt who was one of the earliest mothers of mathematics - https://en.wikipedia.org/wiki/Hypatia Teruko Ishizaka - Japanese scientist and immunologist who co-discovered the antibody class Immunoglobulin E. https://en.wikipedia.org/wiki/Teruko_Ishizaka Mary Jackson, American mathematician and aerospace engineer who earned the highest title within NASA's engineering department - https://en.wikipedia.org/wiki/Mary_Jackson_(engineer) Yeong-Sil Jang was a Korean scientist and astronomer during the Joseon Dynasty; he invented the first metal printing press and water gauge. https://en.wikipedia.org/wiki/Jang_Yeong-sil Mae Carol Jemison -  is an American engineer, physician, and former NASA astronaut. She became the first black woman to travel in space when she served as a mission specialist aboard the Space Shuttle Endeavour - https://en.wikipedia.org/wiki/Mae_Jemison Betty Jennings - one of the original programmers of the ENIAC. https://en.wikipedia.org/wiki/ENIAC - https://en.wikipedia.org/wiki/Jean_Bartik Mary Lou Jepsen, was the founder and chief technology officer of One Laptop Per Child (OLPC), and the founder of Pixel Qi. https://en.wikipedia.org/wiki/Mary_Lou_Jepsen Katherine Coleman Goble Johnson - American physicist and mathematician contributed to the NASA. https://en.wikipedia.org/wiki/Katherine_Johnson IrÃ¨ne Joliot-Curie - French scientist who was awarded the Nobel Prize for Chemistry in 1935. Daughter of Marie and Pierre Curie. https://en.wikipedia.org/wiki/Ir%C3%A8ne_Joliot-Curie Karen SpÃ¤rck Jones came up with the concept of inverse document frequency, which is used in most search engines today. https://en.wikipedia.org/wiki/Karen_Sp%C3%A4rck_Jones A. P. J. Abdul Kalam - is an Indian scientist aka Missile Man of India for his work on the development of ballistic missile and launch vehicle technology - https://en.wikipedia.org/wiki/A._P._J._Abdul_Kalam Sergey Petrovich Kapitsa (Russian: Ð¡ÐµÑÐ³ÐµÌÐ¹ ÐÐµÑÑÐ¾ÌÐ²Ð¸Ñ ÐÐ°Ð¿Ð¸ÌÑÐ°; 14 February 1928 â 14 August 2012) was a Russian physicist and demographer. He was best known as host of the popular and long-running Russian scientific TV show, Evident, but Incredible. His father was the Nobel laureate Soviet-era physicist Pyotr Kapitsa, and his brother was the geographer and Antarctic explorer Andrey Kapitsa. - https://en.wikipedia.org/wiki/Sergey_Kapitsa Susan Kare, created the icons and many of the interface elements for the original Apple Macintosh in the 1980s, and was an original employee of NeXT, working as the Creative Director. https://en.wikipedia.org/wiki/Susan_Kare Mstislav Keldysh - a Soviet scientist in the field of mathematics and mechanics, academician of the USSR Academy of Sciences (1946), President of the USSR Academy of Sciences (1961â1975), three times Hero of Socialist Labor (1956, 1961, 1971), fellow of the Royal Society of Edinburgh (1968). https://en.wikipedia.org/wiki/Mstislav_Keldysh Mary Kenneth Keller, Sister Mary Kenneth Keller became the first American woman to earn a PhD in Computer Science in 1965. https://en.wikipedia.org/wiki/Mary_Kenneth_Keller Johannes Kepler, German astronomer known for his three laws of planetary motion - https://en.wikipedia.org/wiki/Johannes_Kepler Omar Khayyam - Persian mathematician, astronomer and poet. Known for his work on the classification and solution of cubic equations, for his contribution to the understanding of Euclid's fifth postulate and for computing the length of a year very accurately. https://en.wikipedia.org/wiki/Omar_Khayyam Har Gobind Khorana - Indian-American biochemist who shared the 1968 Nobel Prize for Physiology - https://en.wikipedia.org/wiki/Har_Gobind_Khorana Jack Kilby invented silicone integrated circuits and gave Silicon Valley its name. - https://en.wikipedia.org/wiki/Jack_Kilby Maria Kirch - German astronomer and first woman to discover a comet - https://en.wikipedia.org/wiki/Maria_Margarethe_Kirch Donald Knuth - American computer scientist, author of "The Art of Computer Programming" and creator of the TeX typesetting system. https://en.wikipedia.org/wiki/Donald_Knuth Sophie Kowalevski - Russian mathematician responsible for important original contributions to analysis, differential equations and mechanics - https://en.wikipedia.org/wiki/Sofia_Kovalevskaya Marie-Jeanne de Lalande - French astronomer, mathematician and cataloguer of stars - https://en.wikipedia.org/wiki/Marie-Jeanne_de_Lalande Hedy Lamarr - Actress and inventor. The principles of her work are now incorporated into modern Wi-Fi, CDMA and Bluetooth technology. https://en.wikipedia.org/wiki/Hedy_Lamarr Leslie B. Lamport - American computer scientist. Lamport is best known for his seminal work in distributed systems and was the winner of the 2013 Turing Award. https://en.wikipedia.org/wiki/Leslie_Lamport Mary Leakey - British paleoanthropologist who discovered the first fossilized Proconsul skull - https://en.wikipedia.org/wiki/Mary_Leakey Henrietta Swan Leavitt - she was an American astronomer who discovered the relation between the luminosity and the period of Cepheid variable stars. https://en.wikipedia.org/wiki/Henrietta_Swan_Leavitt Esther Miriam Zimmer Lederberg - American microbiologist and a pioneer of bacterial genetics. https://en.wikipedia.org/wiki/Esther_Lederberg Inge Lehmann - Danish seismologist and geophysicist. Known for discovering in 1936 that the Earth has a solid inner core inside a molten outer core. https://en.wikipedia.org/wiki/Inge_Lehmann Daniel Lewin - Mathematician, Akamai co-founder, soldier, 9/11 victim-- Developed optimization techniques for routing traffic on the internet. Died attempting to stop the 9-11 hijackers. https://en.wikipedia.org/wiki/Daniel_Lewin Ruth Lichterman - one of the original programmers of the ENIAC. https://en.wikipedia.org/wiki/ENIAC - https://en.wikipedia.org/wiki/Ruth_Teitelbaum Barbara Liskov - co-developed the Liskov substitution principle. Liskov was also the winner of the Turing Prize in 2008. - https://en.wikipedia.org/wiki/Barbara_Liskov Ada Lovelace invented the first algorithm. https://en.wikipedia.org/wiki/Ada_Lovelace (thanks James Turnbull) Auguste and Louis LumiÃ¨re - the first filmmakers in history - https://en.wikipedia.org/wiki/Auguste_and_Louis_Lumi%C3%A8re Mahavira - Ancient Indian mathematician during 9th century AD who discovered basic algebraic identities - https://en.wikipedia.org/wiki/Mah%C4%81v%C4%ABra_(mathematician) Lynn Margulis (b. Lynn Petra Alexander) - an American evolutionary theorist and biologist, science author, educator, and popularizer, and was the primary modern proponent for the significance of symbiosis in evolution. - https://en.wikipedia.org/wiki/Lynn_Margulis Yukihiro Matsumoto - Japanese computer scientist and software programmer best known as the chief designer of the Ruby programming language. https://en.wikipedia.org/wiki/Yukihiro_Matsumoto James Clerk Maxwell - Scottish physicist, best known for his formulation of electromagnetic theory. https://en.wikipedia.org/wiki/James_Clerk_Maxwell Maria Mayer - American theoretical physicist and Nobel laureate in Physics for proposing the nuclear shell model of the atomic nucleus - https://en.wikipedia.org/wiki/Maria_Mayer John McCarthy invented LISP: https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist) Barbara McClintock - a distinguished American cytogeneticist, 1983 Nobel Laureate in Physiology or Medicine for discovering transposons. https://en.wikipedia.org/wiki/Barbara_McClintock Anne Laura Dorinthea McLaren - British developmental biologist whose work helped lead to human in-vitro fertilisation. https://en.wikipedia.org/wiki/Anne_McLaren Malcolm McLean invented the modern shipping container: https://en.wikipedia.org/wiki/Malcom_McLean Kay McNulty - one of the original programmers of the ENIAC. https://en.wikipedia.org/wiki/ENIAC - https://en.wikipedia.org/wiki/Kathleen_Antonelli Gregor Johann Mendel - Czech scientist and founder of genetics. https://en.wikipedia.org/wiki/Gregor_Mendel Dmitri Mendeleev - a chemist and inventor. He formulated the Periodic Law, created a farsighted version of the periodic table of elements, and used it to correct the properties of some already discovered elements and also to predict the properties of eight elements yet to be discovered. https://en.wikipedia.org/wiki/Dmitri_Mendeleev Lise Meitner - Austrian/Swedish physicist who was involved in the discovery of nuclear fission. The element meitnerium is named after her - https://en.wikipedia.org/wiki/Lise_Meitner Carla Meninsky, was the game designer and programmer for Atari 2600 games Dodge 'Em and Warlords. https://en.wikipedia.org/wiki/Carla_Meninsky Ralph C. Merkle - American computer scientist, known for devising Merkle's puzzles - one of the very first schemes for public-key cryptography. Also, inventor of Merkle trees and co-inventor of the Merkle-DamgÃ¥rd construction for building collision-resistant cryptographic hash functions and the Merkle-Hellman knapsack cryptosystem. https://en.wikipedia.org/wiki/Ralph_Merkle Johanna Mestorf - German prehistoric archaeologist and first female museum director in Germany - https://en.wikipedia.org/wiki/Johanna_Mestorf Maryam Mirzakhani - an Iranian mathematician and the first woman to win the Fields Medal. https://en.wikipedia.org/wiki/Maryam_Mirzakhani Gordon Earle Moore - American engineer, Silicon Valley founding father, author of Moore's law. https://en.wikipedia.org/wiki/Gordon_Moore Samuel Morse - contributed to the invention of a single-wire telegraph system based on European telegraphs and was a co-developer of the Morse code - https://en.wikipedia.org/wiki/Samuel_Morse Ian Murdock - founder of the Debian project - https://en.wikipedia.org/wiki/Ian_Murdock May-Britt Moser - Nobel prize winner neuroscientist who contributed to the discovery of grid cells in the brain. https://en.wikipedia.org/wiki/May-Britt_Moser John Napier of Merchiston - Scottish landowner known as an astronomer, mathematician and physicist. Best known for his discovery of logarithms. https://en.wikipedia.org/wiki/John_Napier John Forbes Nash, Jr. - American mathematician who made fundamental contributions to game theory, differential geometry, and the study of partial differential equations. https://en.wikipedia.org/wiki/John_Forbes_Nash_Jr. John von Neumann - todays computer architectures are based on the von Neumann architecture. https://en.wikipedia.org/wiki/Von_Neumann_architecture Isaac Newton invented classic mechanics and modern optics. https://en.wikipedia.org/wiki/Isaac_Newton Florence Nightingale, more prominently known as a nurse, was also the first female member of the Royal Statistical Society and a pioneer in statistical graphics https://en.wikipedia.org/wiki/Florence_Nightingale#Statistics_and_sanitary_reform Alfred Nobel - a Swedish chemist, engineer, innovator, and armaments manufacturer (inventor of dynamite) - https://en.wikipedia.org/wiki/Alfred_Nobel Emmy Noether, German mathematician. Noether's Theorem is named after her. https://en.wikipedia.org/wiki/Emmy_Noether Poppy Northcutt. Poppy Northcutt was the first woman to work as part of NASAâs Mission Control. http://www.businessinsider.com/poppy-northcutt-helped-apollo-astronauts-2014-12?op=1 Robert Noyce invented silicone integrated circuits and gave Silicon Valley its name. - https://en.wikipedia.org/wiki/Robert_Noyce Panini - Ancient Indian linguist and grammarian from 4th century CE who worked on the world's first formal system - https://en.wikipedia.org/wiki/P%C4%81%E1%B9%87ini#Comparison_with_modern_formal_systems Ambroise Pare invented modern surgery. https://en.wikipedia.org/wiki/Ambroise_Par%C3%A9 Blaise Pascal, French mathematician, physicist, and inventor - https://en.wikipedia.org/wiki/Blaise_Pascal Louis Pasteur discovered vaccination, fermentation and pasteurization. https://en.wikipedia.org/wiki/Louis_Pasteur. Cecilia Payne-Gaposchkin was an astronomer and astrophysicist who, in 1925, proposed in her Ph.D. thesis an explanation for the composition of stars in terms of the relative abundances of hydrogen and helium. https://en.wikipedia.org/wiki/Cecilia_Payne-Gaposchkin Radia Perlman is a software designer and network engineer and most famous for her invention of the spanning-tree protocol (STP). https://en.wikipedia.org/wiki/Radia_Perlman Rob Pike was a key contributor to Unix, Plan 9, the X graphic system, utf-8, and the Go programming language. https://en.wikipedia.org/wiki/Rob_Pike Henri PoincarÃ© made fundamental contributions in several fields of mathematics. https://en.wikipedia.org/wiki/Henri_Poincar%C3%A9 Laura Poitras is a director and producer whose work, made possible by open source crypto tools, advances the causes of truth and freedom of information by reporting disclosures by whistleblowers such as Edward Snowden. https://en.wikipedia.org/wiki/Laura_Poitras Tatâyana Avenirovna Proskuriakova (Russian: Ð¢Ð°ÑÑÑÌÐ½Ð° ÐÐ²ÐµÐ½Ð¸ÌÑÐ¾Ð²Ð½Ð° ÐÑÐ¾ÑÐºÑÑÑÐºÐ¾ÌÐ²Ð°) (January 23 [O.S. January 10] 1909 â August 30, 1985) was a Russian-American Mayanist scholar and archaeologist who contributed significantly to the deciphering of Maya hieroglyphs, the writing system of the pre-Columbian Maya civilization of Mesoamerica. https://en.wikipedia.org/wiki/Tatiana_Proskouriakoff Claudius Ptolemy - a Greco-Egyptian writer of Alexandria, known as a mathematician, astronomer, geographer, astrologer, and poet of a single epigram in the Greek Anthology - https://en.wikipedia.org/wiki/Ptolemy C. V. Raman - Indian physicist who won the Nobel Prize in 1930 for proposing the Raman effect. - https://en.wikipedia.org/wiki/C._V._Raman Srinivasa Ramanujan - Indian mathematician and autodidact who made extraordinary contributions to mathematical analysis, number theory, infinite series, and continued fractions. - https://en.wikipedia.org/wiki/Srinivasa_Ramanujan Sally Kristen Ride was an American physicist and astronaut. She was the first American woman in space, and the youngest American astronaut. https://en.wikipedia.org/wiki/Sally_Ride Rita Levi-Montalcini - Won Nobel Prize in Physiology or Medicine jointly with colleague Stanley Cohen for the discovery of nerve growth factor (https://en.wikipedia.org/wiki/Rita_Levi-Montalcini) Dennis Ritchie - co-creator of UNIX and the C programming language. - https://en.wikipedia.org/wiki/Dennis_Ritchie Ida Rhodes - American pioneer in computer programming, designed the first computer used for Social Security. https://en.wikipedia.org/wiki/Ida_Rhodes Julia Hall Bowman Robinson - American mathematician renowned for her contributions to the fields of computability theory and computational complexity theory. https://en.wikipedia.org/wiki/Julia_Robinson Wilhelm Conrad RÃ¶ntgen - German physicist who was awarded the first Nobel Prize in Physics in 1901 for the discovery of X-rays (RÃ¶ntgen rays). https://en.wikipedia.org/wiki/Wilhelm_R%C3%B6ntgen Rosalind Franklin - British biophysicist and X-ray crystallographer whose research was critical to the understanding of DNA - https://en.wikipedia.org/wiki/Rosalind_Franklin Vera Rubin - American astronomer who pioneered work on galaxy rotation rates. https://en.wikipedia.org/wiki/Vera_Rubin Meghnad Saha - Indian astrophysicist best known for his development of the Saha equation, used to describe chemical and physical conditions in stars - https://en.wikipedia.org/wiki/Meghnad_Saha Jean E. Sammet developed FORMAC, the first widely used computer language for symbolic manipulation of mathematical formulas. https://en.wikipedia.org/wiki/Jean_E._Sammet Mildred Sanderson - American mathematician best known for Sanderson's theorem concerning modular invariants. https://en.wikipedia.org/wiki/Mildred_Sanderson Satoshi Nakamoto is the name used by the unknown person or group of people who developed bitcoin, authored the bitcoin white paper, and created and deployed bitcoin's original reference implementation. https://en.wikipedia.org/wiki/Satoshi_Nakamoto Adi Shamir - Israeli cryptographer whose numerous inventions and contributions to cryptography include the Ferge Fiat Shamir identification scheme, the Rivest Shamir Adleman (RSA) public-key cryptosystem, the Shamir's secret sharing scheme, the breaking of the Merkle-Hellman cryptosystem, the TWINKLE and TWIRL factoring devices and the discovery of differential cryptanalysis (with Eli Biham). https://en.wikipedia.org/wiki/Adi_Shamir Claude Shannon - The father of information theory and founder of digital circuit design theory. (https://en.wikipedia.org/wiki/Claude_Shannon) Carol Shaw - Originally an Atari employee, Carol Shaw is said to be the first female video game designer. https://en.wikipedia.org/wiki/Carol_Shaw_(video_game_designer) Dame Stephanie "Steve" Shirley - Founded a software company in 1962 employing women working from home. https://en.wikipedia.org/wiki/Steve_Shirley William Shockley co-invented the transistor - https://en.wikipedia.org/wiki/William_Shockley Lina Solomonovna Stern (or Shtern; Russian: ÐÐ¸Ð½Ð° Ð¡Ð¾Ð»Ð¾Ð¼Ð¾Ð½Ð¾Ð²Ð½Ð° Ð¨ÑÐµÑÐ½; 26 August 1878 â 7 March 1968) was a Soviet biochemist, physiologist and humanist whose medical discoveries saved thousands of lives at the fronts of World War II. She is best known for her pioneering work on bloodâbrain barrier, which she described as hemato-encephalic barrier in 1921. https://en.wikipedia.org/wiki/Lina_Stern FranÃ§oise BarrÃ©-Sinoussi - French virologist and Nobel Prize Laureate in Physiology or Medicine; her work was fundamental in identifying HIV as the cause of AIDS. https://en.wikipedia.org/wiki/Fran%C3%A7oise_Barr%C3%A9-Sinoussi Betty Snyder - one of the original programmers of the ENIAC. https://en.wikipedia.org/wiki/ENIAC - https://en.wikipedia.org/wiki/Betty_Holberton Cynthia Solomon - Pioneer in the fields of artificial intelligence, computer science and educational computing. Known for creation of Logo, an educational programming language.  https://en.wikipedia.org/wiki/Cynthia_Solomon Frances Spence - one of the original programmers of the ENIAC. https://en.wikipedia.org/wiki/ENIAC - https://en.wikipedia.org/wiki/Frances_Spence Michael Stonebraker is a database research pioneer and architect of Ingres, Postgres, VoltDB and SciDB. Winner of 2014 ACM Turing Award. https://en.wikipedia.org/wiki/Michael_Stonebraker Ivan Edward Sutherland - American computer scientist and Internet pioneer, widely regarded as the father of computer graphics. https://en.wikipedia.org/wiki/Ivan_Sutherland Janese Swanson (with others) developed the first of the Carmen Sandiego games. She went on to found Girl Tech. https://en.wikipedia.org/wiki/Janese_Swanson Aaron Swartz was influential in creating RSS, Markdown, Creative Commons, Reddit, and much of the internet as we know it today. He was devoted to freedom of information on the web. https://en.wikiquote.org/wiki/Aaron_Swartz Bertha Swirles was a theoretical physicist who made a number of contributions to early quantum theory. https://en.wikipedia.org/wiki/Bertha_Swirles Helen Brooke Taussig - American cardiologist and founder of the field of paediatric cardiology. https://en.wikipedia.org/wiki/Helen_B._Taussig Valentina Tereshkova is a Russian engineer, cosmonaut and politician. She was the first woman to fly to space in 1963. In 2013, at the age of 76, she offered to go on a one-way mission to Mars. https://en.wikipedia.org/wiki/Valentina_Tereshkova Nikola Tesla invented the AC electric system and every gadget ever used by a James Bond villain. https://en.wikipedia.org/wiki/Nikola_Tesla Marie Tharp - American geologist and oceanic cartographer who co-created the first scientific map of the Atlantic Ocean floor. Her work led to the acceptance of the theories of plate tectonics and continental drift. https://en.wikipedia.org/wiki/Marie_Tharp Ken Thompson - co-creator of UNIX and the C programming language - https://en.wikipedia.org/wiki/Ken_Thompson Linus Torvalds invented Linux and Git. https://en.wikipedia.org/wiki/Linus_Torvalds Youyou Tu - Chinese pharmaceutical chemist and educator known for discovering artemisinin and dihydroartemisinin, used to treat malaria, which has saved millions of lives. Joint winner of the 2015 Nobel Prize in Physiology or Medicine. https://en.wikipedia.org/wiki/Tu_Youyou Alan Turing was a founding father of computer science. https://en.wikipedia.org/wiki/Alan_Turing. Varahamihira - Ancient Indian mathematician who discovered trigonometric formulae during 505-587 CE - https://en.wikipedia.org/wiki/Var%C4%81hamihira#Contributions Dorothy Vaughan was a NASA mathematician and computer programmer on the SCOUT launch vehicle program that put America's first satellites into space - https://en.wikipedia.org/wiki/Dorothy_Vaughan Sir Mokshagundam Visvesvaraya - is a notable Indian engineer.  He is a recipient of the Indian Republic's highest honour, the Bharat Ratna, in 1955. On his birthday, 15 September is celebrated as Engineer's Day in India in his memory - https://en.wikipedia.org/wiki/Visvesvaraya Christiane NÃ¼sslein-Volhard - German biologist, won Nobel Prize in Physiology or Medicine in 1995 for research on the genetic control of embryonic development. https://en.wikipedia.org/wiki/Christiane_N%C3%BCsslein-Volhard CÃ©dric Villani - French mathematician, won Fields Medal, Fermat Prize and PoincarÃ© Price for his work in differential geometry and statistical mechanics. https://en.wikipedia.org/wiki/C%C3%A9dric_Villani Marlyn Wescoff - one of the original programmers of the ENIAC. https://en.wikipedia.org/wiki/ENIAC - https://en.wikipedia.org/wiki/Marlyn_Meltzer Sylvia B. Wilbur - British computer scientist who helped develop the ARPANET, was one of the first to exchange email in the UK and a leading researcher in computer-supported collaborative work. https://en.wikipedia.org/wiki/Sylvia_Wilbur Andrew Wiles - Notable British mathematician who proved the enigmatic Fermat's Last Theorem - https://en.wikipedia.org/wiki/Andrew_Wiles Roberta Williams, did pioneering work in graphical adventure games for personal computers, particularly the King's Quest series. https://en.wikipedia.org/wiki/Roberta_Williams Malcolm John Williamson - British mathematician and cryptographer employed by the GCHQ. Developed in 1974 what is now known as Diffie-Hellman key exchange (Diffie and Hellman first published the scheme in 1976). https://en.wikipedia.org/wiki/Malcolm_J._Williamson Sophie Wilson designed the first Acorn Micro-Computer and the instruction set for ARM processors. https://en.wikipedia.org/wiki/Sophie_Wilson Jeannette Wing - co-developed the Liskov substitution principle. - https://en.wikipedia.org/wiki/Jeannette_Wing Steve Wozniak invented the Apple I and Apple II. https://en.wikipedia.org/wiki/Steve_Wozniak The Wright brothers, Orville and Wilbur - credited with inventing and building the world's first successful airplane and making the first controlled, powered and sustained heavier-than-air human flight - https://en.wikipedia.org/wiki/Wright_brothers Chien-Shiung Wu - Chinese-American experimental physicist who made significant contributions to nuclear physics. https://en.wikipedia.org/wiki/Chien-Shiung_Wu Rosalyn Sussman Yalow - Rosalyn Sussman Yalow was an American medical physicist, and a co-winner of the 1977 Nobel Prize in Physiology or Medicine for development of the radioimmunoassay technique. https://en.wikipedia.org/wiki/Rosalyn_Sussman_Yalow Ada Yonath - an Israeli crystallographer, the first woman from the Middle East to win a Nobel prize in the sciences. https://en.wikipedia.org/wiki/Ada_Yonath Nikolay Yegorovich Zhukovsky (Russian: ÐÐ¸ÐºÐ¾Ð»Ð°ÌÐ¹ ÐÐ³Ð¾ÌÑÐ¾Ð²Ð¸Ñ ÐÑÐºÐ¾ÌÐ²ÑÐºÐ¸Ð¹, January 17 1847 â March 17, 1921) was a Russian scientist, mathematician and engineer, and a founding father of modern aero- and hydrodynamics. Whereas contemporary scientists scoffed at the idea of human flight, Zhukovsky was the first to undertake the study of airflow. He is often called the Father of Russian Aviation. https://en.wikipedia.org/wiki/Nikolay_Yegorovich_Zhukovsky GetRandomName generates a random name from the list of adjectives and surnames in this package formatted as "adjective_surname". For example 'focused_turing'. If retry is non-zero, a random integer between 0 and 10 will be added to the end of the name, e.g `focused_turing3`/Users/austinjaybecker/projects/abeck-go-testing/pkger/models.gocompssumChartvp"Check""CheckDeadman""CheckThreshold""Dashboard""Label""NotificationEndpoint""NotificationEndpointHTTP""NotificationEndpointPagerDuty""NotificationEndpointSlack""NotificationRule""Package""Task""Telegraf""Variable"invalid kind"invalid kind"unsupported kind provided"unsupported kind provided"json:"stateStatus"`json:"stateStatus"`json:"labelMappings"`json:"labelMappings"`json:"telegrafConfigs"`json:"telegrafConfigs"`json:"new"`json:"new"`json:"old"`json:"old"`json:"charts"`json:"charts"`json:"color"`json:"color"`"exists"json:"resourceName"`json:"resourceName"`json:"resourceTemplateMetaName"`json:"resourceTemplateMetaName"`json:"labelName"`json:"labelName"`json:"labelTemplateMetaName"`json:"labelTemplateMetaName"`json:"endpointID"`json:"endpointID"`json:"endpointName"`json:"endpointName"`json:"endpointType"`json:"endpointType"`json:"statusRules"`json:"statusRules"`json:"tagRules"`json:"tagRules"`json:"cron"`json:"cron"`json:"old,omitempty"`json:"old,omitempty"`json:"missingEnvRefs"`json:"missingEnvRefs"`json:"missingSecrets"`json:"missingSecrets"`json:"summaryTask"`json:"summaryTask"`json:"envReferences"`json:"envReferences"`json:"labelAssociations"`json:"labelAssociations"`json:"check"`json:"check"`json:"xPos"`json:"xPos"`json:"yPos"`json:"yPos"`json:"height"`json:"height"`json:"width"`json:"width"`Propsjson:"notificationEndpoint"`json:"notificationEndpoint"`json:"endpointTemplateMetaName"`json:"endpointTemplateMetaName"`json:"operator"`json:"operator"`json:"resourceField"`json:"resourceField"`json:"envRefKey"`json:"envRefKey"`json:"valueType"`json:"valueType"`json:"defaultValue"`json:"defaultValue"`json:"telegrafConfig"`json:"telegrafConfig"`json:"arguments"`json:"arguments"` Package kind types. Kinds is a list of known pkger kinds. Kind is a resource kind. String provides the kind in human readable form. OK validates the kind is valid. ResourceType converts a kind to a known resource type (if applicable). SafeID is an equivalent influxdb.ID that encodes safely with zero values (influxdb.ID == 0). Encode will safely encode the id. String prints a encoded string representation of the id. DiffIdentifier are the identifying fields for any given resource. Each resource dictates if the resource is new, to be removed, or will remain. IsNew indicates the resource is new to the platform. Diff is the result of a service DryRun call. The diff outlines what is new and or updated from the current state of the platform. HasConflicts provides a binary t/f if there are any changes within package after dry run is complete. DiffBucket is a diff of an individual bucket. DiffBucketValues are the varying values for a bucket. DiffCheckValues are the varying values for a check. MarshalJSON implementation here is forced by the embedded check value here. UnmarshalJSON decodes the check values. DiffCheck is a diff of an individual check. DiffDashboard is a diff of an individual dashboard. DiffDashboardValues are values for a dashboard. DiffChart is a diff of oa chart. Since all charts are new right now. the SummaryChart is reused here. DiffLabel is a diff of an individual label. DiffLabelValues are the varying values for a label. StateStatus indicates the status of a diff or summary resource DiffLabelMapping is a diff of an individual label mapping. A single resource may have multiple mappings to multiple labels. A label can have many mappings to other resources.func (d DiffLabelMapping) IsNew() bool {	return d.StateStatus == StateStatusNew DiffNotificationEndpointValues are the varying values for a notification endpoint. UnmarshalJSON decodes the notification endpoint. This is necessary unfortunately. DiffNotificationEndpoint is a diff of an individual notification endpoint. DiffNotificationRule is a diff of an individual notification rule. DiffNotificationRuleValues are the values for an individual rule. These 3 fields represent the relationship of the rule to the endpoint. DiffTask is a diff of an individual task. DiffTaskValues are the values for an individual task. DiffTelegraf is a diff of an individual telegraf. This resource is always new. DiffVariable is a diff of an individual variable. using omitempty here to signal there was no prev state with a nil DiffVariableValues are the varying values for a variable. Summary is a definition of all the resources that have or will be created from a pkg. SummaryIdentifier establishes the shared identifiers for a given resource within a template. SummaryBucket provides a summary of a pkg bucket. TODO: return retention rules? SummaryCheck provides a summary of a pkg check. SummaryDashboard provides a summary of a pkg dashboard. SummaryChart provides a summary of a pkg dashboard's chart. MarshalJSON marshals a summary chart. UnmarshalJSON unmarshals a view properties and other data. SummaryNotificationEndpoint provides a summary of a pkg notification endpoint. UnmarshalJSON unmarshals the notificatio endpoint. This is necessary b/c of the notification endpoint does not have a means ot unmarshal itself. Summary types for NotificationRules which provide a summary of a pkg notification rule. These fields represent the relationship of the rule to the endpoint. SummaryLabel provides a summary of a pkg label. SummaryLabelMapping provides a summary of a label mapped with a single resource. SummaryReference informs the consumer of required references for this resource. SummaryTask provides a summary of a task. SummaryTelegraf provides a summary of a pkg telegraf config. SummaryVariable provides a summary of a pkg variable./Users/austinjaybecker/projects/abeck-go-testing/pkger/parser.gopkgFnreaderFnbyteriPkgNamejPkgNameexistingAsspkgNamenewPkgvalidationOptssetupFnseiejmatchingfoundInPlatformirjrgraphFnsidentcheckKindsfailsnknotificationKindsteleiSelectednewVarvErrlbnestedLabelsnameRefdisplayNameRefresourceUniqueByNameuniqNameslegpresLegdpvErrspresentQueriespresentColorspresAxestableOptsResfieldOptReschartIdxckdashMetaNamerqtobjvaltypedefDurmParamsparamsErrparamsOpttParamstaskErrtaskOptexamplesastDuriFaceiFaceSlcnewResourcesstrSlcnewReskeyResresBodyseenErrsrootErrfieldPairseditgithub.com/influxdata/flux/ast/edit"github.com/influxdata/flux/ast/edit"github.com/influxdata/flux/parser"github.com/influxdata/flux/parser"invalid encoding provided"invalid encoding provided"invalid filepath provided"invalid filepath provided"byte stream"byte stream"bad response: address=%s status_code=%d body=%q"bad response: address=%s status_code=%d body=%q"raw.githubusercontent.com"raw.githubusercontent.com"github.com"github.com"failed to decode pkg source: %s"failed to decode pkg source: %s"DetectContentTypejson:"apiVersion" yaml:"apiVersion"`json:"apiVersion" yaml:"apiVersion"`json:"kind" yaml:"kind"`json:"kind" yaml:"kind"`json:"metadata" yaml:"metadata"`json:"metadata" yaml:"metadata"`json:"spec" yaml:"spec"`json:"spec" yaml:"spec"`json:"-" yaml:"-"`json:"-" yaml:"-"`attempted to encode a nil Template"attempted to encode a nil Template""resources"at least 1 kind must be provided"at least 1 kind must be provided"notificationKindtasks[%s].spec"tasks[%s].spec"influxdata.com/v2alpha2.task.name".task.name".params.name".params.name".task.every".task.every"field every is not duration"field every is not duration".task.offset".task.offset"apiVersioninvalid API version provided %q; must be 1 in [%s, %s]"invalid API version provided %q; must be 1 in [%s, %s]"name %q is invalid; %s"name %q is invalid; %s""associations"duplicate nested label: %q"duplicate nested label: %q"label %q does not exist in pkg"label %q does not exist in pkg"duplicate name: "duplicate name: "orientation"queries""axes"dashboards[%s].spec.charts[%d].queries[%d]"dashboards[%s].spec.charts[%d].queries[%d]"ParseSourceinvalid query source"invalid query source"GetOption%s.params.%s"%s.params.%s"%s.task.%s"%s.task.%s"DateTimeFromLiteralFloatFromLiteralIntegerFromLiteralStringFromLiteral[a-z0-9]([-a-z0-9]*[a-z0-9])?"[a-z0-9]([-a-z0-9]*[a-z0-9])?"a DNS-1123 label must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character"a DNS-1123 label must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character"^"^"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$"$"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$must be no more than %d characters"must be no more than %d characters"my-name"my-name"123-abc"123-abc" (regex used for validation is '" (regex used for validation is '"')"')" (e.g. " (e.g. " or " or "'"'"', "', "regex used for validation is '"regex used for validation is '"no kind provided"no kind provided"invalid chart kind provided: "invalid chart kind provided: "json:"fields" yaml:"fields"`json:"fields" yaml:"fields"`json:"idxs" yaml:"idxs"`json:"idxs" yaml:"idxs"`json:"reason" yaml:"reason"`json:"reason" yaml:"reason"`%s[%d]"%s[%d]"kind=%s field=%s reason=%q"kind=%s field=%s reason=%q" ReaderFn is used for functional inputs to abstract the individual entrypoints for the reader itself. Encoder is an encodes a type. Encoding describes the encoding for the raw package data. The encoding determines how the raw data is parsed. encoding types EncodingSource draws the encoding type by inferring it from the source. String provides the string representation of the encoding. ErrInvalidEncoding indicates the encoding is invalid type for the parser. Parse parses a pkg defined by the encoding and readerFns. As of writing this we can parse both a YAML, JSON, and Jsonnet formats of the Template model. FromFile reads a file from disk and provides a reader from it. not using os.Open to avoid having to deal with closing the file in here FromReader simply passes the reader along. Useful when consuming this from an HTTP request body. There are a number of other useful places for this functional input. FromString parses a pkg from a raw string value. This is very useful in tests. FromHTTPRequest parses a pkg from the request body of a HTTP request. This is very useful when using packages that are hosted.. highly unlikely to fall in here with supported content type detection as is forced to use this for loop b/c the yaml dependency does not decode multi documents. Object describes the metadata and raw spec for an entity of a package kind. Name returns the name of the kind. ObjectAssociation is an association for an object. The supported types at this time are KindLabel. AddAssociations adds an association to the object. SetMetadataName sets the metadata.name field. Template is the model for a package. The resources are more generic that one might expect at first glance. This was done on purpose. The way json/yaml/toml or w/e scripting you want to use, can have very different ways of parsing. The different parsers are limited for the parsers that do not come from the std lib (looking at you yaml/v2). This allows us to parse it and leave the matching to another power, the graphing of the package is handled within itself. indicates the pkg has been parsed and all resources graphed accordingly Encode is a helper for encoding the pkg correctly. note: we prevent the internal field from being changed by enabling access 		 to the sources via the exported method here. Summary returns a package Summary that describes all the resources and associations the pkg contains. It is very useful for informing users of the changes that will take place when this pkg would be applied. ensure zero values for arrays aren't returned, but instead we always returning an initialized slice. Contains identifies if a pkg contains a given object identified by its kind and metadata.Name (MetaName) field. Combine combines pkgs together. Is useful when you want to take multiple disparate pkgs and compile them into one to take advantage of the parser and service guarantees. ValidateOptFn provides a means to disable desired validation checks. ValidWithoutResources ignores the validation check for minimum number of resources. This is useful for the service Create to ignore this and allow the creation of a pkg without resources. ValidSkipParseError ignores the validation check from the  of resources. This is useful for the service Create to ignore this and allow the creation of a pkg without resources. Validate will graph all resources and validate every thing is in a useful form. labelMappings returns the mappings that will be created for valid pairs of labels and resources of which all have IDs. If a resource does not exist yet, a label mapping will not be returned for it. sort by res type ASC, then res name ASC, then label name ASC labels are first, this is to validate associations with other resources todo: what is the business goal wrt having unique names? (currently duplicates are allowed) override defaults here maybe? a signed duration is represented by a UnaryExpression. it is the only unary expression allowed. dns1123LabelMaxLength is a label's max length in DNS (RFC 1123) isDNS1123Label tests for a string that conforms to the definition of a label in DNS (RFC 1123). regexError returns a string explanation of a regex validation failure. Resource is a pkger Resource kind. It can be one of any of available kinds that are supported. Name returns the name of the resource. ParseError is the error from parsing the given package. The ParseError behavior provides a list of resources that failed and all validations that failed for that resource. A resource can multiple errors, and a parseErr can have multiple resources which themselves can have multiple validation failures. NewParseError creates a new parse error from existing validation errors. resourceErr describes the error for a particular resource. In which it may have numerous validation and association errors. used to provide a means to == or != in the map lookup to remove duplicate errors ValidationErr represents an error during the parsing of a package. IsParseErr inspects a given error to determine if it is a parseErr. If a parseErr it is, it will return it along with the confirmation boolean. If the error is not a parseErr it will return nil values for the parseErr, making it unsafe to use./Users/austinjaybecker/projects/abeck-go-testing/pkger/parser_models.goiThreshspacedKindqueryRefsqIdxvalidatorFnvalidatorFnsptrToFloat64clrtMapcErraxexpectedAxesmAxesiLabelssRuleErrstagErrssjtjtranslatorqueryBodywriteLinelastImportIdxquerySansImportstaskOptsminLengthmetadata.name"metadata.name"spec.name"spec.name""apiVersion""language""metadata""operator""prefix""suffix""spec""retentionRules"json:"type" yaml:"type"`json:"type" yaml:"type"`json:"everySeconds" yaml:"everySeconds"`json:"everySeconds" yaml:"everySeconds"`seconds must be a minimum of "seconds must be a minimum of "type must be "expire"`type must be "expire"`"everySeconds""allValues""reportZero""staleTime""statusMessageTemplate""thresholds""timeSince"duration value must be provided that is >= 5s (seconds)"duration value must be provided that is >= 5s (seconds)"must provide a non zero value"must provide a non zero value"must provide a template; ex. "Check: ${ r._check_name } is: ${ r._level }"`must provide a template; ex. "Check: ${ r._check_name } is: ${ r._level }"`must be 1 of [active, inactive]"must be 1 of [active, inactive]"must provide at least 1 threshold entry"must provide at least 1 threshold entry""inside_range""outside_range"must be 1 in [CRIT, WARN, INFO, OK]; got=%q"must be 1 in [CRIT, WARN, INFO, OK]; got=%q"must be 1 in [Lesser, Greater, Inside_Range, Outside_Range]; got=%q"must be 1 in [Lesser, Greater, Inside_Range, Outside_Range]; got=%q"min must be < max"min must be < max""single_stat""single_stat_plus_line""charts"spec.charts[%d].queries[%d].params.%s"spec.charts[%d].queries[%d].params.%s""binCount""binSize""colors""decimalPlaces""domain""fillColumns""geom""height""legend""noteOnEmpty""position""shade""hoverDimension""fieldOptions""tableOptions""tickPrefix""tickSuffix""timeFormat""ySeriesColumns""upperColumn""mainColumn""lowerColumn""width""xCol""generateXAxisTicks""xTotalTicks""xTickStart""xTickStep""xPos""yCol""generateYAxisTicks""yTotalTicks""yTickStart""yTickStep""yPos""legendColorizeRows""legendOpacity""legendOrientationThreshold"invalid position supplied %q; valid positions is one of [overlaid, stacked]"invalid position supplied %q; valid positions is one of [overlaid, stacked]"monotoneX"monotoneX"type not found"type not found"type provided is not supported"type provided is not supported"%s: %q"%s: %q""fieldName""verticalTimeAxis""wrapping""fixFirstColumn"single-line"single-line""truncate""wrap"chart table option should 1 in ["single-line", "truncate", "wrap"]`chart table option should 1 in ["single-line", "truncate", "wrap"]`"hex"json:"id,omitempty" yaml:"id,omitempty"`json:"id,omitempty" yaml:"id,omitempty"`json:"name,omitempty" yaml:"name,omitempty"`json:"name,omitempty" yaml:"name,omitempty"`json:"type,omitempty" yaml:"type,omitempty"`json:"type,omitempty" yaml:"type,omitempty"`json:"hex,omitempty" yaml:"hex,omitempty"`json:"hex,omitempty" yaml:"hex,omitempty"`json:"value,omitempty" yaml:"value,omitempty"`json:"value,omitempty" yaml:"value,omitempty"`type not found: %q"type not found: %q"a color must have a hex value provided"a color must have a hex value provided"json:"query" yaml:"query"`json:"query" yaml:"query"`SetPropertySetOptionjson:"base,omitempty" yaml:"base,omitempty"`json:"base,omitempty" yaml:"base,omitempty"`json:"label,omitempty" yaml:"label,omitempty"`json:"label,omitempty" yaml:"label,omitempty"`json:"prefix,omitempty" yaml:"prefix,omitempty"`json:"prefix,omitempty" yaml:"prefix,omitempty"`json:"scale,omitempty" yaml:"scale,omitempty"`json:"scale,omitempty" yaml:"scale,omitempty"`json:"suffix,omitempty" yaml:"suffix,omitempty"`json:"suffix,omitempty" yaml:"suffix,omitempty"`json:"domain,omitempty" yaml:"domain,omitempty"`json:"domain,omitempty" yaml:"domain,omitempty"`axis not found: %q"axis not found: %q""orientation"json:"orientation,omitempty" yaml:"orientation,omitempty"`json:"orientation,omitempty" yaml:"orientation,omitempty"`spec.%s[%d].name"spec.%s[%d].name""OPTIONS"must be valid url"must be valid url"not a valid status; valid statues are one of [active, inactive]"not a valid status; valid statues are one of [active, inactive]"must be provide"must be provide"http method must be a valid HTTP verb"http method must be a valid HTTP verb"must provide non empty string"must provide non empty string"invalid type provided %q; valid type is 1 in [%s, %s, %s]"invalid type provided %q; valid type is 1 in [%s, %s, %s]""currentLevel""endpointName""messageTemplate""previousLevel""statusRules""tagRules"spec.endpointName"spec.endpointName"must be provided"must be provided"notification endpoint %q does not exist in pkg"notification endpoint %q does not exist in pkg"must be 1 in [active, inactive]; got=%q"must be 1 in [active, inactive]; got=%q"must provide at least 1"must provide at least 1"must be 1 in [equal]; got=%q"must be 1 in [equal]; got=%q""cron"spec.params.%s"spec.params.%s"spec.task.%s"spec.task.%s"must provide if cron field is not provided"must provide if cron field is not provided"must provide if every field is not provided"must provide if every field is not provided"import\s+\".*\"`import\s+\".*\"`name: %q"name: %q"cron: %q"cron: %q"every: %s"every: %s"offset: %s"offset: %s"option task = { %s }"option task = { %s }"no config provided"no config provided""selected"spec.%s[%d]"spec.%s[%d]"map variable must have at least 1 key/val pair"map variable must have at least 1 key/val pair"constant variable must have a least 1 value provided"constant variable must have a least 1 value provided"query variable must provide a query string"query variable must provide a query string"query variable language must be either "influxql" or "flux"; got %q`query variable language must be either "influxql" or "flux"; got %q`"envRef""secretRef""bool"booleanliteral"booleanliteral"durationliteral"durationliteral"float"float"floatliteral"floatliteral""int"integerliteral"integerliteral"stringliteral"stringliteral"datetimeliteral"datetimeliteral"env-"env-"BooleanLiteralFromValue-0m"-0m"FloatLiteralFromValueIntegerLiteralFromValueStringLiteralFromValueDateTimeLiteralFromValuemust be a string of at least %d chars in length"must be a string of at least %d chars in length" TODO: this feels very odd to me, will need to follow up with  team to better understand this chartKind identifies what kind of chart is eluded too. Each chart kind has their own requirements for what constitutes a chart. available chart kinds at the time of writing, there's nothing to validate for markdown types chart kind specific validations using reference for Value here so we can set to nil and it will be ignored during encoding, keeps our exported pkgs clear of unneeded entries. TODO:  - verify templates are desired  - template colors so references can be shared TODO: looks like much of these are actually getting defaults in  the UI. looking at system charts, seeing lots of failures for missing  color types or no colors at all. this is required by the API, super nasty. Will be super challenging for anyone outside org to figure out how to do this within an hour of looking at the API :sadpanda:. Would be ideal to let the API translate the arguments into this required form instead of forcing that complexity on the caller. this zero value check is for situations where we want to marshal/unmarshal a variable and not have the invalid args blow up during unmarshalling. When that validation is decoupled from the unmarshalling, we can clean this up. key used to reference parameterized field/Users/austinjaybecker/projects/abeck-go-testing/pkger/service.gotelegrafSVCupdateStackFndeletedStackorgIDOptsettersstCreateuninstalledStackupdatedStackmExistingNamesmExistingResourcessetterexistingResourcesresGenmKindsnewResGenresourceKindFiltersresourceTypeGensseenKindsfindsstateLabelMappingsstateBktstateDashexistingLabeliExistingexistingEndpointsfindEndpointmExistingByIDmExistingByNameexistingSecretstemplateSecretsstateTeleteleConfigsexistingVarsmIDsmNamesstateLabelsByResNamemsgFmtsLabelstLabelassociatedResourceexistingLabelstemplateLabelstemplateResourceLabelsappliersendpointAppruleAppsecondaryinfluxBucketrollbackFnnewNameinfluxCheckcheckStubinfluxDashboardicellicellsinfluxLabelrollBackLabelscreatLabelinfluxEndpointrollbackEndpointsendpointApplierendpointRollbackFnruleApplierruleRollbackFnstateEndpointsinfluxRuleruleCreateexistingRuleFnunknownStatusrollbackSecretsnewTasknewFluxnewStatusupdatedTaskrollbackTelegrafsupdatedConfiginfluxVarrollBackVarsupdatedVarrollbackMappingsinfluxMappingstateMappingstackResourcesendpointAssociationChangednewAsshasChangeslatestEventstateSumstateLabelsreqLimitmErrserrArg"influxdata.com/v2alpha1""influxdata.com/v2alpha2""stack"organization dependency does not exist for id[%q]"organization dependency does not exist for id[%q]"unable to update stack after uninstalling resources"unable to update stack after uninstalling resources"you do not have access to given stack ID"you do not have access to given stack ID"orgID provided must not be zero"orgID provided must not be zero"finding "finding "finding labels"finding labels"(?:https://raw.githubusercontent.com/influxdata/community-templates/master/)(?P<name>\w+)(?:/.*)`(?:https://raw.githubusercontent.com/influxdata/community-templates/master/)(?P<name>\w+)(?:/.*)`failed to dry run notification endpoints"failed to dry run notification endpoints"failed to find notification endpoint %q dependency for notification rule %q"failed to find notification endpoint %q dependency for notification rule %q"failed to find labels mappings for %s resource[%q]"failed to find labels mappings for %s resource[%q]"reading stack"reading stack"json:"resourceTemplateName"`json:"resourceTemplateName"`failed to delete created stack"failed to delete created stack"failed to update stack"failed to update stack"failed to setup notification generator"failed to setup notification generator"rolling back removed bucket"rolling back removed bucket"rolling back existing bucket to previous state"rolling back existing bucket to previous state"rolling back new bucket"rolling back new bucket"error for bucket[%q]: %s"error for bucket[%q]: %s"error for check[%q]: %s"error for check[%q]: %s"rolling back removed dashboard"rolling back removed dashboard"rolling back new dashboard"rolling back new dashboard"error for dashboard[%q]: %s"error for dashboard[%q]: %s"error for label[%q]: %s"error for label[%q]: %s"updating"updating"creating"creating"failed to rollback removed endpoint"failed to rollback removed endpoint"failed to rollback updated endpoint"failed to rollback updated endpoint"failed to rollback created endpoint"failed to rollback created endpoint"error for notification endpoint[%q]: %s"error for notification endpoint[%q]: %s"notification rule endpoint dependency does not exist; endpointName=%q"notification rule endpoint dependency does not exist; endpointName=%q"notification_rules"notification_rules"failed to find dependency"failed to find dependency"failed to roll back endpoints"failed to roll back endpoints"failed to find endpoint dependency to rollback existing notification rule"failed to find endpoint dependency to rollback existing notification rule"failed to rollback created notification rule"failed to rollback created notification rule"failed to rollback updated notification rule"failed to rollback updated notification rule"error for notification rule[%q]: %s"error for notification rule[%q]: %s"failed to rollback removed task"failed to rollback removed task"failed to rollback updated task"failed to rollback updated task"failed to rollback created task"failed to rollback created task"error for task[%q]: %s"error for task[%q]: %s"rolling back removed telegraf config"rolling back removed telegraf config"rolling back updated telegraf config"rolling back updated telegraf config"rolling back created telegraf config"rolling back created telegraf config"error for variable[%q]: %s"error for variable[%q]: %s"rolling back removed variable"rolling back removed variable"rolling back updated variable"rolling back updated variable"rolling back created variable"rolling back created variable"removed_label_mapping"removed_label_mapping"%s:%s:%s"%s:%s:%s"error for label mapping: resource_type=%s resource_id=%s label_id=%s err=%s"error for label mapping: resource_type=%s resource_id=%s label_id=%s err=%s"label_mapping"label_mapping"label_resource_id_pairs=[%s] err="unable to delete label"`label_resource_id_pairs=[%s] err="unable to delete label"`failed to parse url"failed to parse url"no labels found for name: "no labels found for name: "failed to %s %s[%q]"failed to %s %s[%q]"panic applying "panic applying "stack_trace"stack_trace"Reflectpanic: %s paniced"panic: %s paniced"failed to delete "failed to delete "failed to apply resource"failed to apply resource"resource_type=%q err=%q`resource_type=%q err=%q`
	metadata_name=%q err_msg=%q"\n\tmetadata_name=%q err_msg=%q"url invalid for entry %q"url invalid for entry %q" APIVersion marks the current APIVersion for influx packages. Stack is an identifier for stateful application of a package(s). This stack will map created resources from the template(s) to existing resources on the platform. This stack is updated only after side effects of applying a template. If the template is applied, and no changes are had, then the stack is not updated. StackResource is a record for an individual resource side effect generated from applying a template. StackResourceAssociation associates a stack resource with another stack resource. StackUpdate provides a means to update an existing stack. SVC is the packages service interface. SVCMiddleware is a service middleware func. ServiceSetterFn is a means of setting dependencies on the Service type. WithLogger sets the logger for the service. WithIDGenerator sets the id generator for the service. WithTimeGenerator sets the time generator for the service. WithStore sets the store for the service. WithBucketSVC sets the bucket service. WithCheckSVC sets the check service. WithDashboardSVC sets the dashboard service. WithLabelSVC sets the label service. WithNotificationEndpointSVC sets the endpoint notification service. WithNotificationRuleSVC sets the endpoint rule service. WithOrganizationService sets the organization service for the service. WithSecretSVC sets the secret service. WithTaskSVC sets the task service. WithTelegrafSVC sets the telegraf service. WithVariableSVC sets the variable service. Store is the storage behavior the Service depends on. Service provides the template business logic including all the dependencies to make this resource sausage. internal dependencies external service dependencies NewService is a constructor for a template Service. InitStack will create a new stack for the given user and its given org. The stack can be created with urls that point to the location of packages that are included as part of the stack when it is applied. UninstallStack will remove all resources associated with the stack. DeleteStack removes a stack and all the resources that have are associated with the stack. providing empty template will remove all applied resources ListFilter are filter options for filtering stacks from being returned. ListStacks returns a list of stacks. ReadStack returns a stack that matches the given id. UpdateStack updates the stack by the given parameters. ExportOptFn is a functional input for setting the template fields. ExportOpt are the options for creating a new package. ExportByOrgIDOpt identifies an org to export resources for and provides multiple filtering options. ExportWithExistingResources allows the create method to clone existing resources. ExportWithAllOrgResources allows the create method to clone all existing resources for the given organization. ExportWithStackID provides an export for the given stack ID. Export will produce a templates from the parameters provided. ImpactSummary represents the impact the application of a template will have on the system. pull name `name` from community url https://raw.githubusercontent.com/influxdata/community-templates/master/name/name_template.yml so here's the deal, when we have issues with the parsing validation, we continue to do the diff anyhow. any resource that does not have a name will be skipped, and won't bleed into the dry run here. We can now return a error (parseErr) and valid diff/summary. grab em all marked true since it exists in the platform if label is found in state then we track the mapping and mark it existing otherwise we continue on now we add labels that do not exist ApplyOpt is an option for applying a package. ActionSkipResource provides an action from the consumer to use the template with modifications to the resource kind and template name that will be applied. ActionSkipKind provides an action from the consumer to use the template with modifications to the resource kinds will be applied. ApplyOptFn updates the ApplyOpt per the functional option. ApplyWithEnvRefs provides env refs to saturate the missing reference fields in the template. ApplyWithTemplate provides a template to the application/dry run. ApplyWithResourceSkip provides an action skip a resource in the application of a template. ApplyWithKindSkip provides an action skip a kidn in the application of a template. ApplyWithSecrets provides secrets to the platform that the template will need. ApplyWithStackID associates the application of a template with a stack. from before the template were applied. if stackID is not provided, a stack will be provided for the application. each grouping here runs for its entirety, then returns an error that is indicative of running all appliers provided. For instance, the labels may have 1 variable fail and one of the buckets fails. The errors aggregate so the caller will be informed of both the failed label variable the failed bucket. the groupings here allow for steps to occur before exiting. The first step is adding the dependencies, resources that are associated by other resources. Then the primary resources. Here we get all the errors associated with them. If those are all good, then we run the secondary(dependent) resources which rely on the primary resources having been created. adds secrets that are referenced it the template, this allows user to provide data that does not rest in the template. deps for primary resources primary resources, can have relationships to labels this has to be run after the above primary resources, because it relies on notification endpoints already being applied. secondary resources this last grouping relies on the above 2 steps having completely successfully TODO: fixup error stub out userID since we're always using hte http client which will fill it in for us with the token feels a bit broken that is required. TODO: look into this userID requirement here we have to couple the endpoints to rules b/c of the dependency here when rolling back a deleted endpoint and rule. This forces the endpoints to be rolled back first so the reference for the rule has settled. The dependency has to be available before rolling back notification rules. setting status to unknown b/c these resources for two reasons:	1. we have no ability to find status via the Service, only to set it...	2. we have no way of inspecting an existing rule and pulling status from it	3. since this is a fallback condition, we set things to inactive as a user		is likely to follow up this failure by fixing their template up then reapplying when an existing variable (referenced in stack) has been deleted by a user then the resource is created anew to get it back to the expected state. this block here does 2 things, it does not write a mapping when one exists. it also avoids having to worry about deleting an existing mapping since it will not be passed to the delete function below b/c it is never added to the list of mappings that is referenced in the delete call. these are the case where a deletion happens and is rolled back creating a new resource. when resource is not to be removed this is a nothing burger, as it should be rolled back to previous state. TODO: would be ideal to extend find variables to allow for a name matcher  since names are unique for vars within an org. In the meanwhile, make large  limit returned vars, should be more than enough for the time being. cannot reuse the shared variable from for loop since we're using concurrency b/c that temp var gets recycled between iterations TODO: clean up apply errors to inform the user in an actionable way/Users/austinjaybecker/projects/abeck-go-testing/pkger/service_auth.go MWAuth is an auth service middleware for the packager domain./Users/austinjaybecker/projects/abeck-go-testing/pkger/service_logging.gopotentialFieldsfailed to init stack"failed to init stack""urls"failed to uninstall stack"failed to uninstall stack"failed to delete stack"failed to delete stack"failed to list stacks"failed to list stacks""stackIDs""names"failed to read stack"failed to read stack""desc"failed to export template"failed to export template"exported template"exported template"failed to dry run template"failed to dry run template"template dry run successful"template dry run successful"failed to apply template"failed to apply template"template apply successful"template apply successful"label_mappings"label_mappings"num_"num_" MWLogging adds logging functionality for the service./Users/austinjaybecker/projects/abeck-go-testing/pkger/service_metrics.gobyStacknumOrgIDsinstalled"installed"Total number of templates installed by name."Total number of templates installed by name."init_stack"init_stack"uninstall_stack"uninstall_stack"delete_stack"delete_stack"list_stacks"list_stacks"read_stack"read_stack"update_stack"update_stack"num_org_ids"num_org_ids"by_stack"by_stack"dry_run"dry_run"telegraf_configs"telegraf_configs"template_export"template_export"Metrics for resources being exported"Metrics for resources being exported"template_count"template_count"Number of installations per template"Number of installations per template"blob"blob" Installed template count metrics MWMetrics is a metrics service middleware for the notification endpoint service. safe to ignore the failed type assertion, a zero value provides a nil slice, so no worries. keep /account/repo as base, then append the blob to it/Users/austinjaybecker/projects/abeck-go-testing/pkger/service_models.goactsreconcileFnreconcilersassForRemovalmStackAssmLabelMetaNameToIDlabeleridSetFnnewIdentitynewCheckoldDiffiv labels are done first to validate dependencies are accounted for. when a label is skipped by an action, this will still be accurate for hte individual labels, and cascades to the resources that are associated to a label. if associations agree => do nothing if associations are new (in state not in stack) => do nothing if associations are not in state and in stack => add them for removal we want to keep associations that are from previous application and are not changing all associations that are in the stack but not in the state fall into here and are marked for removal. setObjectID sets the id for the resource graphed from the object the key identifies. addObjectForRemoval sets the id for the resource graphed from the object the key identifies. The metaName and kind are used as the unique identifier, when calling this it will overwrite any existing value if one exists. If desired, check for the value by using the Contains method. IsNew identifies state status as new to the platform. defaulting zero value to identify as new IsExisting identifies state status as existing in the platform. IsRemoval identifies state status as existing resource that will be removed from the platform./Users/austinjaybecker/projects/abeck-go-testing/pkger/service_tracing.gonum_stacks"num_stacks" MWTracing adds tracing functionality for the service./Users/austinjaybecker/projects/abeck-go-testing/pkger/store.gostoreKVoptionalFieldFilterFnorgIDEncodeddecodeEntFnentityBucketidBytesorgIDBytesstEntentEventEventsstackResentResourcesjson:"sources,omitempty"`json:"sources,omitempty"`json:"urls,omitempty"`json:"urls,omitempty"`json:"resources,omitempty"`json:"resources,omitempty"`json:"associations,omitempty"`json:"associations,omitempty"`org id does not match"org id does not match" this embedding is for stacks that were created before events, this should stay for some time. StoreKV is a store implementation that uses a kv store backing. NewStoreKV creates a new StoreKV entity. This does not initialize the store. You will want to init it if you want to have this init donezo at startup. If not it'll lazy load the buckets as they are used. CreateStack will create a new stack. If collisions are found will fail. existing data before stacks are event sourced have this shape. since the stackIDs are a filter, if it is not found, we just continue on. If the user wants to verify the existence of a particular stack then it would be upon them to use the ReadByID call. ReadStackByID reads a stack by the provided ID. UpdateStack updates a stack. DeleteStack deletes a stack by id. ensure backwards compatibility. All existing fields will be associated with a createEvent, regardless if they are or not/Users/austinjaybecker/projects/abeck-go-testing/predicate/Users/austinjaybecker/projects/abeck-go-testing/predicate/logical.goLogicalAndLogicalNodeLogicalOperatorNodeComparisonNodeTypeLiteralTagRuleNodespecialKeylogicalOpdatatypesgithub.com/influxdata/influxdb/v2/storage/reads/datatypes"github.com/influxdata/influxdb/v2/storage/reads/datatypes"the logical operator %q is invalid"the logical operator %q is invalid"json:"children"`json:"children"`Err in Child %d, err: %s"Err in Child %d, err: %s"NodeTypeLogicalExpressionNode_Logical_ LogicalOperator is a string type of logical operator. LogicalOperators Value returns the node logical type. LogicalNode is a node type includes a logical expression with other nodes. ToDataType convert a LogicalNode to datatypes.Node.openParenunscanscanIgnoreWhitespaceparseLogicalNodeparseTagRuleNodepeekTok/Users/austinjaybecker/projects/abeck-go-testing/predicate/parser.gotokNextcurrParenWSNUMBERINTEGERNAMEIDENTLPARENthe logical operator OR is not supported yet at position %d"the logical operator OR is not supported yet at position %d"extra ( seen"extra ( seen"RPARENextra ) seen"extra ) seen"bad logical expression, at position %d"bad logical expression, at position %d"bad tag key, at position %d"bad tag key, at position %d"scanRegularTagValueNEQREGEXoperator: %q at position: %d is not supported yet"operator: %q at position: %d is not supported yet"invalid operator %q at position: %d"invalid operator %q at position: %d"DURATIONVALbad tag value: %q, at position %d"bad tag value: %q, at position %d" a fixed buffer ring last read token last read pos last read literal parser of the predicate will convert such a statement `(a = "a" or b!="b") and c ! =~/efg/` to the predicate node buffer index buffer size scan returns the next token from the underlying scanner. If a token has been unscanned then read that instead. If we have a token on the buffer, then return it. Move buffer position forward and save the token. curr returns the last read token. scanIgnoreWhitespace scans the next non-whitespace token. Parse the predicate statement. scan the key scan the value peekRune returns the next rune that would be read by the scanner./Users/austinjaybecker/projects/abeck-go-testing/predicate/predicate.goNewProtobufPredicate Node is a predicate node. New predicate from a node/Users/austinjaybecker/projects/abeck-go-testing/predicate/tag_rule.gospecialcompareNode_RegexValueRegexValueNode_StringValueComparisonEqualComparisonNotEqualOperator %s is not supported for delete predicate yet"Operator %s is not supported for delete predicate yet"Unsupported operator: %s"Unsupported operator: %s"NodeTypeComparisonExpressionNode_Comparison_ComparisonNodeTypeTagRefNode_TagRefValueTagRefValue TagRuleNode is a node type of a single tag rule. NodeTypeLiteral convert a TagRuleNode to a nodeTypeLiteral. NodeComparison convert influxdb.Operator to Node_Comparison. ToDataType convert a TagRuleNode to datatypes.Node./Users/austinjaybecker/projects/abeck-go-testing/prometheus/Users/austinjaybecker/projects/abeck-go-testing/prometheus/auth_service.goAddLabelsDecodeExpfmtEncodeExpfmtEncodeJSONEncodeLineProtocolExpfmtNewMatcherRemoveLabelsRenameFamiliesfamilySorterinfluxCollectorlabelPairSorterlabelPairsnsPerMillisecondsrequestCountrequestDuration"prometheus"0.001setAuthorizationStatus"setAuthorizationStatus" AuthorizationService manages authorizations. NewAuthorizationService creates an instance of AuthorizationService. TODO: what to make these values TODO(desa): determine what spacing these buckets should have. FindAuthorizationByID returns an authorization given a id, records function call latency, and counts function calls. FindAuthorizationByToken returns an authorization given a token, records function call latency, and counts function calls. FindAuthorizations returns authorizations given a filter, records function call latency, and counts function calls. CreateAuthorization creates an authorization, records function call latency, and counts function calls. DeleteAuthorization deletes an authorization, records function call latency, and counts function calls. UpdateAuthorization updates the status and description. PrometheusCollectors returns all authorization service prometheus collectors.influxInfoDescinfluxUptimeDescFromTo/Users/austinjaybecker/projects/abeck-go-testing/prometheus/codec.gofamiliesmtsFmtProtoDelimapplication/vnd.google.protobuf; proto=io.prometheus.client.MetricFamily; encoding=delimitedMetricType_GAUGEMetricType_COUNTERMetricType_UNTYPED Encoder transforms metric families into bytes. Encode encodes metrics into bytes. Expfmt encodes metric families into prometheus exposition format. Encode encodes metrics into prometheus exposition format bytes. DecodeExpfmt decodes the reader of format into metric families. EncodeExpfmt encodes the metrics family (defaults to expfmt.FmtProtoDelim). JSON encodes metric families into JSON. Encode encodes metrics JSON bytes. This not always works as some prometheus values are NaN or Inf. DecodeJSON decodes a JSON array of metrics families. EncodeJSON encodes the metric families to JSON. just in case the definition of time.Nanosecond changes from 1. LineProtocol encodes metric families into influxdb line protocol. Encode encodes metrics into line protocol format bytes. EncodeLineProtocol converts prometheus metrics into line protocol./Users/austinjaybecker/projects/abeck-go-testing/prometheus/filter.golpsfilteredFamiliesgithub.com/golang/protobuf/proto"github.com/golang/protobuf/proto"protobuf:"bytes,1,rep,name=label" json:"label,omitempty"`protobuf:"bytes,1,rep,name=label" json:"label,omitempty"` Filter filters the metrics from Gather using Matcher. Gather filters all metrics to only those that match the Matcher. Matcher is used to match families of prometheus metrics. family name to label/value NewMatcher returns a new matcher. Family helps constuct match by adding a metric family to match to. prometheus metrics labels are sorted by label name. Match returns all metric families that match. L is used with Family to create a series of label pairs for matching. Labels are string representations of a set of prometheus label pairs that are used to match to metric. Match checks if the metric's labels matches this set of labels. match empty string so no labels can be matched. labelPairs is used to serialize a portion of dto.Metric into a serializable string./Users/austinjaybecker/projects/abeck-go-testing/prometheus/influx.goprocIDinfluxdb_info"influxdb_info"Information about the influxdb environment."Information about the influxdb environment."cpus"cpus"NumCPUinfluxdb_uptime_seconds"influxdb_uptime_seconds"influxdb process uptime in seconds"influxdb process uptime in seconds" NewInfluxCollector returns a collector which exports influxdb process metrics./Users/austinjaybecker/projects/abeck-go-testing/prometheus/metric_recorder.gorequest_count"request_count"Total number of query requests"Total number of query requests"Count of bytes received"Count of bytes received"response_bytes"response_bytes"Count of bytes returned"Count of bytes returned" EventRecorder implements http/metric.EventRecorder. It is used to collect http api metrics. NewEventRecorder returns an instance of a metric event recorder. Subsystem is expected to be descriptive of the type of metric being recorded. Possible values may include write, query, task, dashboard, etc. The general structure of the metrics produced from the metric recorder should be http_<subsystem>_request_count{org_id=<org_id>, status=<status>, endpoint=<endpoint>} ... http_<subsystem>_request_bytes{org_id=<org_id>, status=<status>, endpoint=<endpoint>} ... http_<subsystem>_response_bytes{org_id=<org_id>, status=<status>, endpoint=<endpoint>} ... Record metric records the request count, response bytes, and request bytes with labels for the org, endpoint, and status. PrometheusCollectors exposes the prometheus collectors associated with a metric recorder./Users/austinjaybecker/projects/abeck-go-testing/prometheus/sort.go labelPairSorter implements sort.Interface. It is used to sort a slice of dto.LabelPair pointers. familySorter implements sort.Interface. It is used to sort a slice of dto.MetricFamily pointers./Users/austinjaybecker/projects/abeck-go-testing/prometheus/transformer.gorenamed Transformer modifies prometheus metrics families. Transform updates the metrics family AddLabels adds labels to all metrics. It will overwrite the label if it already exists. Transform adds labels to the metrics. Filter out labels to add Add all new labels to the metric RemoveLabels adds labels to all metrics. It will overwrite Transform removes labels from the metrics. Filter out labels RenameFamilies changes the name of families to another name Transform renames metric families names./Users/austinjaybecker/projects/abeck-go-testing/query/Users/austinjaybecker/projects/abeck-go-testing/query/bridges.goAddDialectMappingsConditionalLoggingContextWithRequestFromBucketServiceFromOrganizationServiceFromSecretServiceGroupModeByGroupModeNoneLoggingProxyQueryServiceLoggingProxyQueryServiceOptionNewLoggingProxyQueryServiceNewNoContentDialectNewNoContentWithErrorDialectNoContentDialectTypeNoContentEncoderNoContentWErrDialectTypeNoContentWithErrorEncoderProxyQueryServiceAsyncBridgeQueryServiceProxyBridgeREPLQuerierRequestFromContextRequestHeaderOptionRequireMetadataKeySecretLookupSetReturnNoContentToGroupModeactiveContextKeyasyncStatsResultIteratorbufferedReadClosernewBufferedReadCloserasristatsReadyNewResultIteratorFromQueryDefaultEncoderConfig QueryServiceBridge implements the QueryService interface while consuming the AsyncQueryService interface. Check returns the status of this query service.  Since this bridge consumes an AsyncQueryService, which is not available over the network, this check always passes. QueryServiceProxyBridge implements QueryService while consuming a ProxyQueryService interface. The buffered reader and any error that has been encountered when reading. Channel that is closed when stats have been written. Statistics gathered from calling the proxy query service. This field must not be read until statsReady is closed. Peek into the read. If there is an error before reading any bytes, do not use the result decoder and use the error that is returned as the error for this result iterator. Only an error if this is not an EOF. At least one byte could be read so create a result iterator using the reader. ProxyQueryServiceAsyncBridge implements ProxyQueryService while consuming an AsyncQueryService Release the results and collect the statistics regardless of the error. REPLQuerier implements the repl.Querier interface while consuming a QueryService Authorization is the authorization to provide for all requests OrganizationID is the ID to provide for all requests Query will pack a query to be sent to a remote server for execution.  deps may be safely ignored since they will be correctly initialized on the server side. bufferedReadCloser is a bufio.Reader that implements io.ReadCloser. newBufferedReadCloser constructs a new bufferedReadCloser.SampledResponseSizeRedactproxyQueryServicequeryLoggernowFunctionrequireMetadataKeySetNowFunctionForTestingResultEncodercsvWriterEncodeErrorerrorEncoder/Users/austinjaybecker/projects/abeck-go-testing/query/control/Users/austinjaybecker/projects/abeck-go-testing/query/control/controller.goCompilingErroredFinishedQueueingerrorCollectingResulterrorCollectingTableIteratorhandleFluxErrorisFinishedStatelabelCompileErrorlabelQueueErrorlabelRuntimeErrorlabelSuccessnewControllerMetricsorgLabelminMemoryisCompletecctxlvsunusederrMsgscurrentCtxcurrentStatenewStateecrgithub.com/influxdata/flux/execute/table"github.com/influxdata/flux/execute/table"ConcurrencyQuota must be positive"ConcurrencyQuota must be positive"MemoryBytesQuotaPerQuery must be positive"MemoryBytesQuotaPerQuery must be positive"InitialMemoryBytesQuotaPerQuery must be positive"InitialMemoryBytesQuotaPerQuery must be positive"MaxMemoryBytes must be positive"MaxMemoryBytes must be positive"MaxMemoryBytes must be greater than or equal to the ConcurrencyQuota * InitialMemoryBytesQuotaPerQuery: %d < %d (%d * %d)"MaxMemoryBytes must be greater than or equal to the ConcurrencyQuota * InitialMemoryBytesQuotaPerQuery: %d < %d (%d * %d)"QueueSize must be positive"QueueSize must be positive"invalid controller config"invalid controller config"Starting query controller"Starting query controller"concurrency_quota"concurrency_quota"initial_memory_bytes_quota_per_query"initial_memory_bytes_quota_per_query"memory_bytes_quota_per_query"memory_bytes_quota_per_query"max_memory_bytes"max_memory_bytes"queue_size"queue_size"WithQueryTracingEnabledcompile_errorqueue_errorquery controller shutdown"query controller shutdown"Unavailablepanic: %v"panic: %v"panic during compile"panic during compile"Internalfailed to transition query to compiling state"failed to transition query to compiling state"astPkgsbuiltinsfinalizedRegisterPackageRegisterPackageValueReplacePackageValueregisterPackageValuePreludenewScopeForStdlibcompilePackagesFinalizecompilation failed"compilation failed"LoggingProgramfailed to transition query to queueing state"failed to transition query to queueing state"ResourceExhaustedqueue length exceeded"queue length exceeded"panic during program start"panic during program start"impossible state transition"impossible state transition"AstProgramcompileOptionsPhysicalOptionphysicalPlannerdefaultMemoryLimitdisableValidationlogicalphysicalexternplanOptionsPlanSpecprocessResultsreadMetadataProfilerGetResultGetSortedResultOperatorProfilerOperatorProfilingResultoperatorProfilingResultAggregateoperationTyperesultCountresultMinresultMaxresultSumresultMeanchInchOutcloseIncomingChannelgetTableBuilderAstProfilerstfProfilerGetAstgetSpecupdateProfilersupdateOptsProfilerResultNewProfilerResultNewSliceResultIteratorTRANSITION"created""compiling""queueing""executing"errored"errored""finished"InheritFailedPreconditionAbortedOutOfRangeUnimplementedPermissionDeniedUnauthenticated Package control keeps track of resources and manages queries. The Controller manages the resources available to each query by managing the memory allocation and concurrency usage of each query. The Controller will compile a program by using the passed in language and it will start the program using the ResourceManager. It will guarantee that each program that is started has at least one goroutine that it can use with the dispatcher and it will ensure a minimum amount of memory is available before the program runs. Other goroutines and memory usage is at the will of the specific resource strategy that the Controller is using. The Controller also provides visibility into the lifetime of the query and its current resource usage. orgLabel is the metric label to use in the controller Controller provides a central location to manage all incoming queries. The controller is responsible for compiling, queueing, and executing queries. ConcurrencyQuota is the number of queries that are allowed to execute concurrently. InitialMemoryBytesQuotaPerQuery is the initial number of bytes allocated for a query when it is started. If this is unset, then the MemoryBytesQuotaPerQuery will be used. MemoryBytesQuotaPerQuery is the maximum number of bytes (in table memory) a query is allowed to use at any given time. A query may not be able to use its entire quota of memory if requesting more memory would conflict with the maximum amount of memory that the controller can request. MaxMemoryBytes is the maximum amount of memory the controller is allowed to allocated to queries. If this is unset, then this number is ConcurrencyQuota * MemoryBytesQuotaPerQuery. This number must be greater than or equal to the ConcurrencyQuota * InitialMemoryBytesQuotaPerQuery. This number may be less than the ConcurrencyQuota * MemoryBytesQuotaPerQuery. QueueSize is the number of queries that are allowed to be awaiting execution before new queries are rejected. MetricLabelKeys is a list of labels to add to the metrics produced by the controller. The value for a given key will be read off the context. The context value must be a string or an implementation of the Stringer interface. complete will fill in the defaults, validate the configuration, and return the new Config. Validate will validate that the controller configuration is valid. Query satisfies the AsyncQueryService while ensuring the request is propagated on the context. Set the request on the context so platform specific Flux operations can retrieve it later. Set the org label value for controller metricslint:ignore SA1029 this is a temporary ignore until we have time to create an appropriate type The controller injects the dependencies for each incoming request. Add per-transformation spans if the feature flag is set. query submits a query for execution returning immediately. Done must be called on any returned Query objects. Lock the queries mutex for the rest of this method. Query controller was shutdown between when we started creating the query and ending it. executeQuery will execute a compiled program and wait for its completion. This may happen if the query was cancelled (either because the client cancelled it, or because the controller is shutting down) In the case of cancellation, SetErr() should reset the error to an appropriate message. Record unused memory before start. waitForQuery will wait until the query is done. Queries reports the active queries. Shutdown will signal to the Controller that it should not accept any new queries and that it should finish executing any existing queries. This will return once the Controller's run loop has been exited and all queries have been finished or until the Context has been canceled. Mark that the controller is shutdown so it does not accept new queries. Cancel all of the currently active queries. Wait for query processing goroutines to finish. Wait for all of the queries to be cleaned up or until the context is done. Query represents a single request. query state. The stateMu protects access for the group below. ID reports an ephemeral unique ID for the query. Cancel will stop the query execution. Call the cancel function to signal that execution should be interrupted. Results returns a channel that will deliver the query results. It's possible that the channel is closed before any results arrive. In particular, if a query's context or the query itself is canceled, the query may close the results channel before any results are computed. The query may also have an error during execution so the Err() function should be used to check if an error happened. Done signals to the Controller that this query is no longer being used and resources related to the query may be freed. This must only be invoked once. All done calls should block until the first done call succeeds. Lock the state mutex and transition to the finished state. Then force the query to cancel to tell it to stop executing. We transition to the new state first so that we do not enter the canceled state at any point (as we have not been canceled). Ensure that all of the results have been drained. It is ok to read this as the user has already indicated they don't care about the results. When this is closed, it tells us an error has been set or the results have finished being pumped. Do nothing with the results. No other goroutines should be modifying state at this point so we can do things that would be unsafe in another context. Mark the program as being done and copy out the error if it exists. TODO(jsternberg): The underlying program never returns this so maybe their interface should change? Merge the metadata from the program into the controller stats. Retrieve the runtime errors that have been accumulated. Mark the query as finished so it is removed from the query map. Release the additional memory associated with this query. Record unused memory after finish. Count query request. Statistics reports the statistics for the query. This method must be called after Done. It will block until the query has been finalized unless a context is given. State reports the current state of the query. If the query is a non-finished state, check the context to see if we have been interrupted. The query has been canceled so report to the outside world that we have been canceled. Do NOT attempt to change the internal state variable here. It is a minefield. Leave the normal query execution to figure that out. The context has not been canceled. transitionTo will transition from one state to another. If a list of current states is given, then the query must be in one of those states for the transition to succeed. This method must be called with a lock and it must be called from within the run loop. If we are transitioning to a non-finished state, the query may have been canceled. If the query was canceled, then we need to transition to the canceled state Transition to the canceled state and report that we failed to transition to the desired state. Find the current state in the list of current states. We are transitioning to a new state. Close the current span (if it exists). Invoke the cancel function to ensure that we have signaled that the query should be done. The user is supposed to read the entirety of the tables returned before we end up in a finished state, but user error may have caused this not to happen so there's no harm to canceling multiple times. If we are transitioning to a finished state from a non-finished state, finish the parent span. Transition to the new state. Start a new span and set a new context. This state is not tracked so do not create a new span or context for it. Use the parent context if one is needed. Err reports any error the query may have encountered. setErr marks this query with an error. If the query was canceled, then the error is ignored. This will mark the query as ready so setResults must not be called if this method is invoked. We may have this get called when the query is canceled. If that is the case, transition to the canceled state instead and record the error from that since the error we received is probably wrong. Close the ready channel to report that no results will be sent. pump will read from the executing query results and pump the results to our destination. When there are no more results, then this will close our own results channel. When our context is canceled, we need to propagate that cancel signal down to the executing program just in case it is waiting for a cancel signal and is ignoring the passed in context. We want this signal to only be sent once and we want to continue draining the results until the underlying program has actually been finished so we copy this to a new channel and set it to nil when it has been closed. It is possible for the underlying query to misbehave. We have to continue pumping results even if this is the case, but if the query has been canceled or finished with done, nobody is going to read these values so we need to avoid blocking. Signal to the underlying executor that the query has been canceled. Usually, the signal on the context is likely enough, but this explicitly signals just in case. Set the done channel to nil so we don't do this again and we continue to drain the results. If we get here, then any running queries should have been cancelled in controller.Shutdown(). tryCompile attempts to transition the query into the Compiling state. tryQueue attempts to transition the query into the Queueing state. tryExec attempts to transition the query into the Executing state. State is the query state. Created indicates the query has been created. Compiling indicates that the query is in the process of executing the compiler associated with the query. Queueing indicates the query is waiting inside of the scheduler to be executed. Executing indicates that the query is currently executing. Errored indicates that there was an error when attempting to execute a query within any state inside of the controller. Finished indicates that the query has been marked as Done and it is awaiting removal from the Controller or has already been removed. Canceled indicates that the query was signaled to be canceled. A canceled query must still be released with Done. handleFluxError will take a flux.Error and convert it into an influxdb.Error. It will match certain codes to the equivalent in influxdb. If the error is any other type of error, it will return the error untouched. TODO(jsternberg): This likely becomes a public function, but this is just an initial implementation so playing it safe by making it package local for now. If we are inheriting the error code, influxdb doesn't have an equivalent of this so we need to retrieve the error code from the wrapped error which has already been translated to an influxdb error (if possible). These don't really map correctly, but we want them to show up as 4XX so until influxdb error codes are updated for more types of failures, mapping these to invalid. Everything else is treated as an internal error which is set above.sideEffectsSideEffectsImportPackageObjectColListTableBuildercolumnBuilderSetNilAppendBoolsGrowBoolsAppendIntsGrowIntsAppendUIntsGrowUIntsAppendFloatsGrowFloatsAppendStringsGrowStringsAppendTimesGrowTimescolMetanrowsNRowsNColsAddColLevelColumnsSetUIntAppendUIntAppendValueAppendNilcheckColGetRowSliceColumnsClearDataFilesLength/Users/austinjaybecker/projects/abeck-go-testing/query/control/memory.goamountCompareAndSwapInt64query hit hard limit"query hit hard limit"not enough capacity"not enough capacity" initialBytesQuotaPerQuery is the initial amount of memory allocated for each query. It does not count against the memory pool. memoryBytesQuotaPerQuery is the maximum amount of memory that may be allocated to each query. unusedMemoryBytes is the amount of memory that may be used when a query requests more memory. This value is only used when unlimited is set to false. unlimited indicates that the memory manager should indicate there is an unlimited amount of free memory available. createAllocator will construct an allocator and memory manager for the given query. Use an anonymous function to ensure the value is copied. queryMemoryManager is a memory manager for a specific query. RequestMemory will determine if the query can be given more memory when it is requested. Note: This function accesses the memoryManager whose attributes may be modified concurrently. Atomic operations are used to keep it lockless. The data associated with this specific query are only invoked from within a lock so they are safe to modify. Second Note: The errors here are discarded anyway so don't worry too much about the specific message or structure. It can be determined statically if we are going to violate the memoryBytesQuotaPerQuery. We do not have the capacity for this query to be given more memory. The memory allocator will only request the bare amount of memory it needs, but it will probably ask for more memory so, if possible, give it more so it isn't repeatedly calling this method. Reserve this memory for our own use. The unused value has changed so someone may have taken the memory that we wanted. Retry. Successfully reserved the memory so update our own internal counter for the limit. giveMemory will determine an appropriate amount of memory to give a query based on what it wants and how much it has allocated in the past. It will always return a number greater than or equal to want. If we can safely double the limit, then just do that. Doubling the limit sends us over the quota. Determine what would be our maximum amount. If we can't double because there isn't enough space in unused, maybe we can just use everything. Otherwise we have already determined we can give the wanted number of bytes so just give that. Not implemented. There is no problem with invoking this method, but the controller won't recognize that the memory has been declared as returned. Release will release all of the allocated memory to the memory manager./Users/austinjaybecker/projects/abeck-go-testing/query/control/metrics.go"compile_error""queue_error""control"functions_total"functions_total"Count of functions in queries"Count of functions in queries"function"function"all_active"all_active"Number of queries in all states"Number of queries in all states"compiling_active"compiling_active"Number of queries actively compiling"Number of queries actively compiling"compiler_type"compiler_type"queueing_active"queueing_active"Number of queries actively queueing"Number of queries actively queueing"executing_active"executing_active"Number of queries actively executing"Number of queries actively executing"memory_unused_bytes"memory_unused_bytes"The free memory as seen by the internal memory manager"The free memory as seen by the internal memory manager"all_duration_seconds"all_duration_seconds"Histogram of total times spent in all query states"Histogram of total times spent in all query states"compiling_duration_seconds"compiling_duration_seconds"Histogram of times spent compiling queries"Histogram of times spent compiling queries"queueing_duration_seconds"queueing_duration_seconds"Histogram of times spent queueing queries"Histogram of times spent queueing queries"/Users/austinjaybecker/projects/abeck-go-testing/query/dependency.goallBucketsmissing request on context"missing request on context" FromBucketService wraps an influxdb.BucketService in the BucketLookup interface. BucketLookup converts Flux bucket lookups into influxdb.BucketService calls. Lookup returns the bucket id and its existence given an org id and bucket name. LookupName returns an bucket name given its organization ID and its bucket ID. FromOrganizationService wraps a influxdb.OrganizationService in the OrganizationLookup interface. OrganizationLookup converts organization name lookups into influxdb.OrganizationService calls. Lookup returns the organization ID and its existence given an organization name. LookupName returns an organization name given its ID. SecretLookup wraps the influxdb.SecretService to perform lookups based on the organization in the context. FromSecretService wraps a influxdb.OrganizationService in the OrganizationLookup interface. LoadSecret loads the secret associated with the key in the current organization context./Users/austinjaybecker/projects/abeck-go-testing/query/encode.goencErrno-content"no-content"no-content-with-error"no-content-with-error"NewResultEncodertext/csv; charset=utf-8"text/csv; charset=utf-8" AddDialectMappings adds the mappings for the no-content dialects. NoContentDialect is a dialect that provides an Encoder that discards query results. When invoking `dialect.Encoder().Encode(writer, results)`, `results` get consumed, while the `writer` is left intact. It is an HTTPDialect that sets the response status code to 204 NoContent. Consume and discard results. Do not write anything. NoContentWithErrorDialect is a dialect that provides an Encoder that discards query results, but it encodes runtime errors from the Flux query in CSV format. To discover if there was any runtime error in the query, one should check the response size. If it is equal to zero, then no error was present. Otherwise one can decode the response body to get the error. For example: _, err = csv.NewResultDecoder(csv.ResultDecoderConfig{}).Decode(bytes.NewReader(res)) if err != nil {   // we got some runtime error Make sure we release results. Remember, it is safe to call `Release` multiple times. Consume and discard results, but keep an eye on errors. If there is an error, then encode it in the response. Now Release in order to populate the error, if present./Users/austinjaybecker/projects/abeck-go-testing/query/fluxlang/Users/austinjaybecker/projects/abeck-go-testing/query/fluxlang/service.godefaultServiceastPkggithub.com/influxdata/flux/complete"github.com/influxdata/flux/complete"github.com/influxdata/flux/interpreter"github.com/influxdata/flux/interpreter"NewCompleter Package language exposes the flux parser as an interface. DefaultService is the default language service./Users/austinjaybecker/projects/abeck-go-testing/query/influxql/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/compiler.goAddCompilerMappingsCSVGzipJSONPrettyMsgpackNewCompilerNewTranspilerNonecreateFunctionCursorcreateVarRefCursordurationLiteralerrDatabaseNameRequiredevalBuilderfunctionCursorgroupInfogroupVisitoridentifyGroupsisTransformationjoinCursormapCursornewQueryResultnewQueryTablenewTranspilerStateparseFunctionpipeCursorqueryTableresponseIteratorseriesIteratortagsCursortranspilerStatevarRefCursorhdlgithub.com/influxdata/flux/plan"github.com/influxdata/flux/plan"json:"cluster,omitempty"`json:"cluster,omitempty"`json:"now,omitempty"`json:"now,omitempty"`CompileOptionWithLogPlanOptsCompileAST AddCompilerMappings adds the influxql specific compiler mappings. Compiler is the transpiler to convert InfluxQL to a Flux specification. Compile transpiles the query into a Program.assignmentsmapFieldsmapFieldtranspiletranspileShowTagValuestranspileShowDatabasestranspileShowRetentionPoliciestranspileSelectrequireImporttranslateRowsToColumnsQueryRawJSONexcludeevalneedNormalizationcreateCursorresultIdx/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/config.go Config modifies the behavior of the Transpiler. Bucket is the name of a bucket to use instead of the db/rp from the query. If bucket is empty then the dbrp mapping is used. FallbackToDBRP if true will use the naming convention of `db/rp` for a bucket name when an mapping is not found/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/cursor.gorange_github.com/influxdata/flux/execute"github.com/influxdata/flux/execute"unimplemented: only one source is allowed"unimplemented: only one source is allowed"unimplemented: source must be a measurement"unimplemented: source must be a measurement"DefaultValueColLabel cursor is holds known information about the current stream. It maps the influxql ast information to the attributes on a table. Expr is the AST expression that produces this table. Keys returns all of the expressions that this cursor contains. Value returns the string that can be used to access the computed expression. If this cursor does not produce the expression, this returns false for the second return argument. varRefCursor contains a cursor for a single variable. This is usually the raw value coming from the database and points to the default value column property. createVarRefCursor creates a new cursor from a variable reference using the sources in the transpilerState. TODO(jsternberg): Support multiple sources. Only support a direct measurement. Subqueries are not supported yet. Create the from spec and add it to the list of operations. If the maximum is not set and we have a windowing function, then the end time will be set to now. If these are the same variable reference (by pointer), then they are equal. pipeCursor wraps a cursor with a new expression while delegating all calls to the wrapped cursor./Users/austinjaybecker/projects/abeck-go-testing/query/influxql/dialect.go AddDialectMappings adds the influxql specific dialect mappings. Dialect describes the output format of InfluxQL queries. TimeFormat is the format of the timestamp; defaults to RFC3339Nano. Encoding is the format of the results; defaults to JSON. Chunks is the number of points per chunk encoding batch; defaults to 0 or no chunking. Compression is the compression of the result output; defaults to None. TimeFormat specifies the format of the timestamp in the query results. RFC3339Nano is the default format for timestamps for InfluxQL. Hour formats time as the number of hours in the unix epoch. Minute formats time as the number of minutes in the unix epoch. Second formats time as the number of seconds in the unix epoch. Millisecond formats time as the number of milliseconds in the unix epoch. Microsecond formats time as the number of microseconds in the unix epoch. Nanosecond formats time as the number of nanoseconds in the unix epoch. CompressionFormat is the format to compress the query results. None does not compress the results and is the default. Gzip compresses the query results with gzip. EncodingFormat is the output format for the query response content. JSON marshals the response to JSON octets. JSONPretty marshals the response to JSON octets with indents. CSV marshals the response to CSV. Msgpack has a similar structure as the  JSON response. Used?/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/errors.godatabase name required"database name required"/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/function.gofunctionReftimeValuenormalizeunimplemented: count(distinct)"unimplemented: count(distinct)"unimplemented: wildcard function"unimplemented: wildcard function"unimplemented: wildcard regex function"unimplemented: wildcard regex function"expected float argument in %s()"expected float argument in %s()"unimplemented function: %q"unimplemented function: %q"undefined variable: %s"undefined variable: %s"argument unit must be a duration type"argument unit must be a duration type""unit"exact_mean"exact_mean"percentile function requires two arguments field_key and N"percentile function requires two arguments field_key and N"argument N must be a float type"argument N must be a float type"argument N must be between 0 and 100"argument N must be between 0 and 100"exact_selector"exact_selector"quantile"quantile""drop"DefaultTimeColLabelDefaultStartColLabel TODO(ethan): more to be added here. function contains the prototype for invoking a function. TODO(jsternberg): This should do a lot more heavy lifting, but it mostly just pre-validates that we know the function exists. The cursor creation should be done by this struct, but it isn't at the moment. parseFunction parses a call AST and creates the function for it. createFunctionCursor creates a new cursor that calls a function on one of the columns and returns the result. TODO(ethan): https://github.com/influxdata/influxdb/issues/10733 to enable this. elapsed has an optional unit parameter, default to 1ns https://docs.influxdata.com/influxdb/v1.7/query_language/functions/#elapsed If we have been told to normalize the time, we do it here. err checked in caller/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/group.gocondErrlit2windowOffsetwindowEverywindowStartunimplemented: distinct expression"unimplemented: distinct expression"unimplemented: field wildcard"unimplemented: field wildcard"unimplemented: field regex wildcard"unimplemented: field regex wildcard"first argument to %q must be a variable"first argument to %q must be a variable"unimplemented: joining fields within a cursor"unimplemented: joining fields within a cursor"unable to evaluate condition"unable to evaluate condition""window"using GROUP BY requires at least one aggregate function"using GROUP BY requires at least one aggregate function"unimplemented: dimension regex wildcards"unimplemented: dimension regex wildcards"by"by"keep"keep" TODO(jsternberg): Identify duplicates so they are a single common instance. TODO(jsternberg): Identify math functions so we visit their arguments instead of recording them. identifyGroups will identify the groups for creating data access cursors. Attempt to take the calls and variables and put them into groups. If any of the calls are not selectors, we have an error message. All of the functions are selectors. If we have more than 1, then we have another error message. Otherwise, we create a single group. Always a selector if we are here. We do not have any auxiliary fields so each of the function calls goes into its own group. If there is exactly one group and that contains a selector or a transformation function, then mark it does not need normalization. Create all of the cursors for every variable reference. TODO(jsternberg): Determine which of these cursors are from fields and which are tags. TODO(jsternberg): This should be validated and figured out somewhere else. TODO(jsternberg): Establish which variables in the condition are tags and which are fields. We need to create the references to fields here so they can be joined. Walk through the condition for every variable reference. There will be no function calls here. If the variable reference is in any of the cursors, it is definitely a field and we do not have to inspect it further. This may be a field or a tag. If it is a field, we need to create the cursor and add it to the listing of cursors so it can be joined before we evaluate the condition. Add this variable name to the listing of tags. Join the cursors using an inner join. TODO(jsternberg): We need to differentiate between various join types and this needs to be except: ["_field"] rather than joining on the _measurement. This also needs to specify what the time column should be. Evaluate the conditional and insert a filter if a condition exists. // Generate a filter expression by evaluating the condition and wrapping it in a filter op. Group together the results. If a function call is present, evaluate the function call. If there was a window operation, we now need to undo that and sort by the start column so they stay in the same table and are joined in the correct order. If we do not have a function, but we have a field option, return the appropriate error message if there is something wrong with the flux. TODO(jsternberg): Fill needs to be somewhere and it's probably here somewhere. Move this to the correct location once we've figured it out. Maintain a set of the dimensions we have encountered. This is so we don't duplicate groupings, but we still maintain the listing of tags in the tags slice so it is deterministic.TODO set windowStart Do not add a group call for wildcard, which means group by everything Perform the grouping by the tags we found. There is always a group by because there is always something to group in influxql. TODO(jsternberg): A wildcard will skip this step. tagsCursor is a pseudo-cursor that can be used to access tags within the cursor./Users/austinjaybecker/projects/abeck-go-testing/query/influxql/join.govarNametableNameononExprjoin"join""on" Iterate through each cursor and each expression within each cursor to assign them an id. Perform a variable assignment and use it for the table name. Combine the table name with the name to access this attribute so we can know what it will be mapped to. Construct the expression for the on parameter./Users/austinjaybecker/projects/abeck-go-testing/query/influxql/map.gomappedNamesymreturnMemberExprnumber of columns does not match the number of fields"number of columns does not match the number of fields""rename"unimplemented math function: %q"unimplemented math function: %q"missing symbol for %s"missing symbol for %s"RegexpLiteralunimplemented: %T"unimplemented: %T"NotEqualOperatorLessThanEqualOperatorADDRegexpMatchOperatorNotRegexpMatchOperatorunimplemented binary expression: %s"unimplemented binary expression: %s" mapCursor holds the mapping of expressions to specific fields that happens at the end of the transpilation. TODO(jsternberg): This abstraction might be useful for subqueries, but we only need the expr at the moment so just hold that. mapFields will take the list of symbols and maps each of the operations using the column names. TODO(jsternberg): This scenario should not be possible. Replace the use of ColumnNames with a more statically verifiable list of columns when we process the fields from the select statement instead of doing this in the future. Skip past any time columns. TODO(jsternberg): Handle the other expressions by turning them into an equivalent expression. evalBuilder is used for namespacing the logical and eval wrapping functions./Users/austinjaybecker/projects/abeck-go-testing/query/influxql/math.go isMathFunction returns true if the call is a math function./Users/austinjaybecker/projects/abeck-go-testing/query/influxql/response.go all of this code is copied more or less verbatim from the influxdb repo. we copy instead of sharing because we want to prevent inadvertent breaking changes introduced by the transpiler vs the actual InfluxQL engine. By copying the code, we'll be able to detect more explicitly that the results generated by the transpiler diverge from InfluxQL./Users/austinjaybecker/projects/abeck-go-testing/query/influxql/response_iterator.golengarrowgithub.com/apache/arrow/go/arrow/array"github.com/apache/arrow/go/arrow/array"github.com/influxdata/flux/arrow"github.com/influxdata/flux/arrow"Float64BuildernullBitmapreserveunsafeAppendBoolsToBitmapunsafeSetValidUnsafeAppendBoolToBitmaprawDataAppendNullUnsafeAppendAppendValuesNewArrayNewFloat64ArraynewDataNewFloatBuilderunsupported type %T found in column %s of type %s"unsupported type %T found in column %s of type %s"Int64BuilderNewInt64ArrayNewIntBuilderUint64BuilderNewUint64ArrayNewUintBuilderBinaryBuilderBinaryDataTypeint32BufferBuilderbufferBuilderAdvanceunsafeAppendbyteBufferBuilderAppendStringValuesDataLenDataCapReserveDataNewBinaryArrayappendNextOffsetNewStringBuilderBooleanBuilderNewBooleanArrayNewBoolBuildercould not parse string %q as time: %v"could not parse string %q as time: %v"unsupported type %T found in column %s"unsupported type %T found in column %s"invalid type %T found in column %s"invalid type %T found in column %s"ColIdxtable invalid: missing group column %q"table invalid: missing group column %q"InvalidValueunsupported value kind %T"unsupported value kind %T"NewGroupKeymust have at least one value"must have at least one value" responseIterator implements flux.ResultIterator for a Response. NewResponseIterator constructs a flux.ResultIterator from a Response. More returns true if there are results left to iterate through. It is used to implement flux.ResultIterator. Next retrieves the next flux.Result. Release is a noop. Err returns an error if the response contained an error. seriesIterator is a simple wrapper for Result that implements flux.Result and flux.TableIterator. Name returns the results statement id. It is used to implement flux.Result. Tables returns the original as a flux.TableIterator. Do iterates through the series of a Result. It is used to implement flux.TableIterator. queryTable implements flux.Table and flux.ColReader. Data in a column is laid out in the following way:   [ r.row.Columns... , r.tagKeys()... , r.row.Name ] Key constructs the flux.GroupKey for a Row from the rows tags and measurement. It is used to implement flux.Table and flux.ColReader. plus one is for measurement tags returns the tag keys for a Row. Cols returns the columns for a row where the data is laid out in the following way: rename the time column Do applies f to itself. This is because Row is a flux.ColReader. It is used to implement flux.Table. Empty returns true if a Row has no values. Len returns the length or r.row.Values It is used to implement flux.ColReader. Bools returns the values in column index j as bools. It will panic if the column is not a []bool. Ints returns the values in column index j as ints. It will panic if the column is not a []int64. UInts returns the values in column index j as ints. It will panic if the column is not a []uint64. Floats returns the values in column index j as floats. It will panic if the column is not a []float64. Strings returns the values in column index j as strings. It will panic if the column is not a []string. Times returns the values in column index j as values.Times. It will panic if the column is not a []values.Time./Users/austinjaybecker/projects/abeck-go-testing/query/influxql/result.goresultColMapunable to parse statement id from result name: %s"unable to parse statement id from result name: %s"unsupported column type: %s"unsupported column type: %s" MultiResultEncoder encodes results as InfluxQL JSON format. Encode writes a collection of results to the influxdb 1.X http response format. Expectations/Assumptions:  1.  Each result will be published as a 'statement' in the top-level list of results. The result name      will be interpreted as an integer and used as the statement id.  2.  If the _measurement name is present in the group key, it will be used as the result name instead      of as a normal tag.  3.  All columns in the group key must be strings and they will be used as tags. There is no current way      to have a tag and field be the same name in the results.      TODO(jsternberg): For full compatibility, the above must be possible.  4.  All other columns are fields and will be output in the order they are found.      TODO(jsternberg): This function currently requires the first column to be a time field, but this isn't      a strict requirement and will be lifted when we begin to work on transpiling meta queries. Skip any columns that aren't strings. They are extra ones that flux includes by default like the start and end times that we do not care about. If the field key was not removed by a previous operation, we explicitly ignore it here when encoding the result back. TODO: resultColMap should be constructed from query metadata once it is provided. for now we know that an influxql query ALWAYS has time first, so we put this placeholder here to catch this most obvious requirement.  Column orderings should be explicitly determined from the ordering given in the original flux. Preallocate the number of rows for the response to make this section of code easier to read. Find a time column which should exist in the output. Fill in the values for each column./Users/austinjaybecker/projects/abeck-go-testing/query/influxql/service.goinfluxql query service does not support the '%s' compiler type"influxql query service does not support the '%s' compiler type"no endpoint found for cluster %s"no endpoint found for cluster %s"unexpected http status: %s"unexpected http status: %s" Endpoint contains the necessary information to connect to a specific cluster. Service is a client for the influxdb 1.x endpoint that implements the QueryService for the influxql compiler type. Endpoints maps a cluster name to the influxdb 1.x endpoint. Query will execute a query for the influxql.Compiler type against an influxdb 1.x endpoint, and return results using the default decoder. Decode the response into the JSON structure. Return a result iterator using the response. QueryRawJSON will execute a query for the influxql.Compiler type against an influxdb 1.x endpoint, and return the body of the response as a byte array. Verify that this is an influxql query in the compiler. Lookup the endpoint information for the cluster. Prepare the HTTP request. Perform the request and look at the status code./Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/aggregates.goAggregateTestFixtureNewFixtureRegisterFixtureSelectorTestaggregateFuncNamesallFixturesaltBucketIDcollectionfixtureselectorFuncNamesspectestsSELECT %s(value) FROM db0..cpu`SELECT %s(value) FROM db0..cpu`package main

`package main

`from(bucketID: "%s")`from(bucketID: "%s")`
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> `
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> `()
	|> map(fn: (r) => ({r with _time: 1970-01-01T00:00:00Z}))
	|> rename(columns: {_value: "`()
	|> map(fn: (r) => ({r with _time: 1970-01-01T00:00:00Z}))
	|> rename(columns: {_value: "`"})
	|> yield(name: "0")
`"})
	|> yield(name: "0")
`/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/aggregates_with_condition.goSELECT %s(value) FROM db0..cpu WHERE host = 'server01'`SELECT %s(value) FROM db0..cpu WHERE host = 'server01'`from(bucketID: "%s"`from(bucketID: "%s"`)
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> filter(fn: (r) => r["host"] == "server01")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> `)
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> filter(fn: (r) => r["host"] == "server01")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> `/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/aggregates_with_groupby.goSELECT %s(value) FROM db0..cpu GROUP BY host`SELECT %s(value) FROM db0..cpu GROUP BY host`)
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field", "host"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "host", "_time", "_value"])
	|> `)
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field", "host"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "host", "_time", "_value"])
	|> `/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/aggregates_with_window.goSELECT %s(value) FROM db0..cpu WHERE time >= now() - 10m GROUP BY time(1m)`SELECT %s(value) FROM db0..cpu WHERE time >= now() - 10m GROUP BY time(1m)`)
	|> range(start: 2010-09-15T08:50:00Z, stop: 2010-09-15T09:00:00Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> window(every: 1m)
	|> `)
	|> range(start: 2010-09-15T08:50:00Z, stop: 2010-09-15T09:00:00Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> window(every: 1m)
	|> `()
	|> map(fn: (r) => ({r with _time: r._start}))
	|> window(every: inf)
	|> rename(columns: {_value: "`()
	|> map(fn: (r) => ({r with _time: r._start}))
	|> window(every: inf)
	|> rename(columns: {_value: "`/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/aggregates_with_window_offset.goSELECT %s(value) FROM db0..cpu WHERE time >= now() - 10m GROUP BY time(5m, 12m)`SELECT %s(value) FROM db0..cpu WHERE time >= now() - 10m GROUP BY time(5m, 12m)`)
	|> range(start: 2010-09-15T08:50:00Z, stop: 2010-09-15T09:00:00Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> window(every: 5m, start: 1970-01-01T00:02:00Z)
	|> `)
	|> range(start: 2010-09-15T08:50:00Z, stop: 2010-09-15T09:00:00Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> window(every: 5m, start: 1970-01-01T00:02:00Z)
	|> `/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/doc.go Package spectests the influxql transpiler specification tests./Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/multiple_aggregates.go TODO(ethan): https://github.com/influxdata/flux/issues/2594	RegisterFixture(		NewFixture(			`SELECT mean(value), max(value) FROM db0..cpu`,			`package maint0 = from(bucketID: "")	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")	|> group(columns: ["_measurement", "_start"], mode: "by")	|> mean()	|> duplicate(column: "_start", as: "_time")t1 = from(bucketID: "")	|> max()	|> drop(columns: ["_time"])join(tables: {t0: t0, t1: t1}, on: ["_time", "_measurement"])	|> map(fn: (r) => ({_time: r._time, mean: r["t0__value"], max: r["t1__value"]}), mergeKey: true)	|> yield(name: "0")`,		),	)/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/multiple_statements.goSELECT mean(value) FROM db0..cpu; SELECT max(value) FROM db0..cpu`SELECT mean(value) FROM db0..cpu; SELECT max(value) FROM db0..cpu`package main

from(bucketID: "")
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> mean()
	|> map(fn: (r) => ({r with _time: 1970-01-01T00:00:00Z}))
	|> rename(columns: {_value: "mean"})
	|> yield(name: "0")
from(bucketID: "")
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> max()
	|> rename(columns: {_value: "max"})
	|> yield(name: "1")
`package main

from(bucketID: "")
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> mean()
	|> map(fn: (r) => ({r with _time: 1970-01-01T00:00:00Z}))
	|> rename(columns: {_value: "mean"})
	|> yield(name: "0")
from(bucketID: "")
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> max()
	|> rename(columns: {_value: "max"})
	|> yield(name: "1")
`/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/raw.goSELECT value FROM db0..cpu`SELECT value FROM db0..cpu`package main

from(bucketID: "")
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> rename(columns: {_value: "value"})
	|> yield(name: "0")
`package main

from(bucketID: "")
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> rename(columns: {_value: "value"})
	|> yield(name: "0")
`/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/raw_with_condition.goSELECT value FROM db0..cpu WHERE host = 'server01'`SELECT value FROM db0..cpu WHERE host = 'server01'`package main

from(bucketID: "")
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> filter(fn: (r) => r["host"] == "server01")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> rename(columns: {_value: "value"})
	|> yield(name: "0")
`package main

from(bucketID: "")
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> filter(fn: (r) => r["host"] == "server01")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> rename(columns: {_value: "value"})
	|> yield(name: "0")
`/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/raw_with_regex_condition.goSELECT value FROM db0..cpu WHERE host =~ /.*er01/`SELECT value FROM db0..cpu WHERE host =~ /.*er01/`package main

from(bucketID: "")
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> filter(fn: (r) => r["host"] =~ /.*er01/)
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> rename(columns: {_value: "value"})
	|> yield(name: "0")
`package main

from(bucketID: "")
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> filter(fn: (r) => r["host"] =~ /.*er01/)
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> rename(columns: {_value: "value"})
	|> yield(name: "0")
`/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/retention_policy.goSELECT value FROM db0.alternate.cpu`SELECT value FROM db0.alternate.cpu`
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> rename(columns: {_value: "value"})
	|> yield(name: "0")
`
	|> range(start: 1677-09-21T00:12:43.145224194Z, stop: 2262-04-11T23:47:16.854775806Z)
	|> filter(fn: (r) => r._measurement == "cpu" and r._field == "value")
	|> group(columns: ["_measurement", "_start", "_stop", "_field"], mode: "by")
	|> keep(columns: ["_measurement", "_start", "_stop", "_field", "_time", "_value"])
	|> rename(columns: {_value: "value"})
	|> yield(name: "0")
`/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/selectors.go()
	|> rename(columns: {_value: "`()
	|> rename(columns: {_value: "`/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/show_databases.gopackage main

import v1 "influxdata/influxdb/v1"

v1.databases()
	|> rename(columns: {databaseName: "name"})
	|> keep(columns: ["name"])
	|> yield(name: "0")
`package main

import v1 "influxdata/influxdb/v1"

v1.databases()
	|> rename(columns: {databaseName: "name"})
	|> keep(columns: ["name"])
	|> yield(name: "0")
`/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/show_retention_policies.goSHOW RETENTION POLICIES ON telegraf`SHOW RETENTION POLICIES ON telegraf`package main

import v1 "influxdata/influxdb/v1"

v1.databases()
	|> filter(fn: (r) => r.databaseName == "telegraf")
	|> rename(columns: {retentionPolicy: "name", retentionPeriod: "duration"})
	|> set(key: "shardGroupDuration", value: "0")
	|> set(key: "replicaN", value: "2")
	|> keep(columns: ["name", "duration", "shardGroupDuration", "replicaN", "default"])
	|> yield(name: "0")
`package main

import v1 "influxdata/influxdb/v1"

v1.databases()
	|> filter(fn: (r) => r.databaseName == "telegraf")
	|> rename(columns: {retentionPolicy: "name", retentionPeriod: "duration"})
	|> set(key: "shardGroupDuration", value: "0")
	|> set(key: "replicaN", value: "2")
	|> keep(columns: ["name", "duration", "shardGroupDuration", "replicaN", "default"])
	|> yield(name: "0")
`/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/show_tag_values.goSHOW TAG VALUES ON "db0" WITH KEY = "host"`SHOW TAG VALUES ON "db0" WITH KEY = "host"`package main

from(bucketID: "")
	|> range(start: -1h)
	|> keyValues(keyColumns: ["host"])
	|> group(columns: ["_measurement", "_key"], mode: "by")
	|> distinct()
	|> group(columns: ["_measurement"], mode: "by")
	|> rename(columns: {_key: "key", _value: "value"})
	|> yield(name: "0")
`package main

from(bucketID: "")
	|> range(start: -1h)
	|> keyValues(keyColumns: ["host"])
	|> group(columns: ["_measurement", "_key"], mode: "by")
	|> distinct()
	|> group(columns: ["_measurement"], mode: "by")
	|> rename(columns: {_key: "key", _value: "value"})
	|> yield(name: "0")
`/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/show_tag_values_in_list.goSHOW TAG VALUES ON "db0" WITH KEY IN ("host", "region")`SHOW TAG VALUES ON "db0" WITH KEY IN ("host", "region")`package main

from(bucketID: "")
	|> range(start: -1h)
	|> keyValues(keyColumns: ["host", "region"])
	|> group(columns: ["_measurement", "_key"], mode: "by")
	|> distinct()
	|> group(columns: ["_measurement"], mode: "by")
	|> rename(columns: {_key: "key", _value: "value"})
	|> yield(name: "0")
`package main

from(bucketID: "")
	|> range(start: -1h)
	|> keyValues(keyColumns: ["host", "region"])
	|> group(columns: ["_measurement", "_key"], mode: "by")
	|> distinct()
	|> group(columns: ["_measurement"], mode: "by")
	|> rename(columns: {_key: "key", _value: "value"})
	|> yield(name: "0")
`/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/show_tag_values_multiple_measurements.goSHOW TAG VALUES ON "db0" FROM "cpu", "mem", "gpu" WITH KEY = "host"`SHOW TAG VALUES ON "db0" FROM "cpu", "mem", "gpu" WITH KEY = "host"`package main

from(bucketID: "")
	|> range(start: -1h)
	|> filter(fn: (r) => r._measurement == "cpu" or (r._measurement == "mem" or r._measurement == "gpu"))
	|> keyValues(keyColumns: ["host"])
	|> group(columns: ["_measurement", "_key"], mode: "by")
	|> distinct()
	|> group(columns: ["_measurement"], mode: "by")
	|> rename(columns: {_key: "key", _value: "value"})
	|> yield(name: "0")
`package main

from(bucketID: "")
	|> range(start: -1h)
	|> filter(fn: (r) => r._measurement == "cpu" or (r._measurement == "mem" or r._measurement == "gpu"))
	|> keyValues(keyColumns: ["host"])
	|> group(columns: ["_measurement", "_key"], mode: "by")
	|> distinct()
	|> group(columns: ["_measurement"], mode: "by")
	|> rename(columns: {_key: "key", _value: "value"})
	|> yield(name: "0")
`/Users/austinjaybecker/projects/abeck-go-testing/query/influxql/spectests/testing.goaltMappingwantASTfixturesplatformtesting"github.com/andreyvit/diff"github.com/influxdata/influxdb/v2/testing"github.com/influxdata/influxdb/v2/testing"db0"db0""alternate"aaaaaaaaaaaaaaaa"aaaaaaaaaaaaaaaa"bbbbbbbbbbbbbbbb"bbbbbbbbbbbbbbbb"cccccccccccccccc"cccccccccccccccc"found parser errors in the want text: %s"found parser errors in the want text: %s""cluster"%s:%d: unexpected error: %s"%s:%d: unexpected error: %s"LineDiffunexpected ast at %s:%d
%s"unexpected ast at %s:%d\n%s"2010-09-15T09:00:00Z"2010-09-15T09:00:00Z" Fixture is a structure that will run tests. Encode both of these to JSON and compare the results./Users/austinjaybecker/projects/abeck-go-testing/query/influxql/transpiler.gotxtfilterExprkeyColumnsdefaultRPdeclunknown statement type %T"unknown statement type %T"unimplemented: tag key operand: %s"unimplemented: tag key operand: %s"unsupported operand: %s"unsupported operand: %s"unsupported literal type: %T"unsupported literal type: %T""keyValues""keyColumns"_key"_key"databaseName"databaseName""retentionPolicy"retentionPeriod"retentionPeriod"shardGroupDuration"shardGroupDuration""0"replicaN"replicaN"unable to transpile: at least one non-time field must be queried"unable to transpile: at least one non-time field must be queried"unable to transpile: db and rp mappings need to be created by some way"unable to transpile: db and rp mappings need to be created by some way"unable to transpile: database is required"unable to transpile: database is required"t%d"t%d" Package influxql implements the transpiler for executing influxql queries in the 2.0 query engine. Transpiler converts InfluxQL queries into a query spec. Parse the text of the query. Stamp the current time using the now time. While the ShowTagValuesStatement contains a sources section and those sources are measurements, they do not actually contain the database and we do not factor in retention policies. So we are always going to use the default retention policy when evaluating which bucket we are querying and we do not have to consult the sources in the statement. TODO(jsternberg): Read the range from the condition expression. 1.x doesn't actually do this so it isn't urgent to implement this functionality so we can use the default range. If we have a list of sources, look through it and add each of the measurement names. TODO(jsternberg): Add the condition filter for the where clause. Create the key values op spec from the Group by the measurement and key, find distinct values, then group by the measurement to join all of the different keys together. Finish by renaming the columns. This is static. Clone the select statement and omit the time from the list of column names. Join the cursors together on the measurement name. TODO(jsternberg): This needs to join on all remaining group keys. Map each of the fields into another cursor. This evaluates any lingering expressions. TODO(jsternberg): Actually evaluate the type against the schema. Use the bucket inteasd of dbrp mapping if it exists. use `db/rp` naming convention use mapping bucket id requireImport will ensure the import is included in the file and return the variable used to access the package. Check to see if the import already exists. Append the import to the file./Users/austinjaybecker/projects/abeck-go-testing/query/logger.go Logger persists metadata about executed queries. Log captures a query and any relevant metadata for the query execution. Time is the time the query was completed OrganizationID is the ID of the organization that requested the query TraceID is the ID of the trace related to this query Sampled specifies whether the trace for TraceID was chosen for permanent storage by the sampling mechanism of the tracer Error is any error encountered by the query ProxyRequest is the query request ResponseSize is the size in bytes of the query response Statistics is a set of statistics about the query execution Redact removes any sensitive information before logging Make shallow copy of request Make shallow copy of authorization Redact authorization token Apply redacted authorization Apply redacted request/Users/austinjaybecker/projects/abeck-go-testing/query/logging.golpqsmetadataKeyQueryLogging panic"QueryLogging panic" LoggingProxyQueryService wraps a ProxyQueryService and logs the queries. If this is set then logging happens only if this key is present in the metadata. LoggingProxyQueryServiceOption provides a way to modify the behavior of LoggingProxyQueryService. ConditionalLogging returns a LoggingProxyQueryServiceOption that only logs if the passed in function returns true. Thus logging can be controlled by a request-scoped attribute, e.g., a feature flag. Query executes and logs the query. Logging is conditional, and we are not logging this request. Just invoke the wrapped service directly. Enforce requireMetadataKey, if set./Users/austinjaybecker/projects/abeck-go-testing/query/mock/Users/austinjaybecker/projects/abeck-go-testing/query/mock/logger.goNewQueryQueryLoggerLogFnSetResults/Users/austinjaybecker/projects/abeck-go-testing/query/mock/service.gogithub.com/influxdata/flux/metadata"github.com/influxdata/flux/metadata"Mock Proxy Query Service"Mock Proxy Query Service"Mock Query Service"Mock Query Service"call to query.Statistics() before the query has been finished"call to query.Statistics() before the query has been finished" ProxyQueryService mocks the idpe QueryService for testing. Query writes the results of the query request. QueryService mocks the idep QueryService for testing. AsyncQueryService mocks the idep QueryService for testing. Query is a mock implementation of a flux.Query. It contains controls to ensure that the flux.Query object is used correctly. Note: Query will only return one result, specified by calling the SetResults method. NewQuery constructs a new asynchronous query. Cancel closes the results channel. Err will return an error if one was set. Statistics will return Statistics. Unlike the normal flux.Query, this will panic if it is called before Done./Users/austinjaybecker/projects/abeck-go-testing/query/promql/Users/austinjaybecker/projects/abeck-go-testing/query/promql/gen.goAggregateExprAllowInvalidUTF8ArgKindAvgKindBottomKindClonerCountKindCountValuesKindDurationKindEntrypointExprKindGlobalStoreIdentifierKindInitStateLabelMatcherMatchKindMaxExpressionsMaxKindMemoizeMinKindNewAggregateExprNewIdentifierListNewLabelMatcherNewLabelMatchesNewNumberNewRangeOpNewSelectorNewWhereOperationNumberKindParseFileParsePromQLParseReaderQuantileKindQueryBuilderRegexMatchRegexNoMatchSelectorSelectorKindStdVarKindStdevKindStringKindSumKindToOperatorKindTopKindUnknownOpKindactionExprandCodeExprandExpranyMatchercharClassMatcherchoiceExprchoiceNoMatcherrInvalidEncodingerrInvalidEntrypointerrListerrMaxExprCnterrNoRulegrammarlabeledExprlistJoinlitMatchernewParsernotCodeExprnotExproneOrMoreExproperatorLookupparserErrorrangeTablerecoveryExprreservedWordsresultTupleruleRefExprsavepointseqExprstateCodeExprstoreDictthrowExprtoIfaceSliceunicodeClassesvalidateUnicodeEscapezeroOrMoreExprzeroOrOneExprpromqlgo:generate env GO111MODULE=on go run github.com/mna/pigeon -o promql.go promql.pegglobalStoreonGrammar1onComment1onIdentifier1onIdentifierName1onStringLiteral2onStringLiteral18onDoubleStringEscape5onSingleStringEscape5onOctalEscape6onHexEscape6onLongUnicodeEscape2onLongUnicodeEscape13onShortUnicodeEscape2onShortUnicodeEscape9onCharClassMatcher2onCharClassMatcher15onCharClassEscape5onUnicodeClassEscape5onUnicodeClassEscape13onUnicodeClassEscape19onNumber1onInteger3onLabelBlock2onLabelBlock8onNanoSecondUnits1onMicroSecondUnits1onMilliSecondUnits1onSecondUnits1onMinuteUnits1onHourUnits1onDayUnits1onWeekUnits1onYearUnits1onDuration1onLabelOperators2onLabelOperators4onLabelOperators6onLabelOperators8onLabelMatch1onLabelMatches1onLabelMatchesRest1onLabelList2onLabelList7onLabelListRest1onVectorSelector1onRange1onOffset1onCountValueOperator1onBinaryAggregateOperators1onUnaryAggregateOperators1onAggregateBy1onAggregateWithout1onAggregateExpression2onAggregateExpression22onAggregateExpression42onAggregateExpression62onAggregateExpression82onAggregateExpression97dedupeExprCntChoiceAltCntmemoizememovstackrstackmaxFailPosmaxFailExpectedmaxFailInvertExpectedmaxExprCntentrypointallowInvalidUTF8recoveryStackcallonGrammar1callonComment1callonIdentifier1callonIdentifierName1callonStringLiteral2callonStringLiteral18callonDoubleStringEscape5callonSingleStringEscape5callonOctalEscape6callonHexEscape6callonLongUnicodeEscape2callonLongUnicodeEscape13callonShortUnicodeEscape2callonShortUnicodeEscape9callonCharClassMatcher2callonCharClassMatcher15callonCharClassEscape5callonUnicodeClassEscape5callonUnicodeClassEscape13callonUnicodeClassEscape19callonNumber1callonInteger3callonLabelBlock2callonLabelBlock8callonNanoSecondUnits1callonMicroSecondUnits1callonMilliSecondUnits1callonSecondUnits1callonMinuteUnits1callonHourUnits1callonDayUnits1callonWeekUnits1callonYearUnits1callonDuration1callonLabelOperators2callonLabelOperators4callonLabelOperators6callonLabelOperators8callonLabelMatch1callonLabelMatches1callonLabelMatchesRest1callonLabelList2callonLabelList7callonLabelListRest1callonVectorSelector1callonRange1callonOffset1callonCountValueOperator1callonBinaryAggregateOperators1callonUnaryAggregateOperators1callonAggregateBy1callonAggregateWithout1callonAggregateExpression2callonAggregateExpression22callonAggregateExpression42callonAggregateExpression62callonAggregateExpression82callonAggregateExpression97pushVpopVpushRecoverypopRecoveryaddErraddErrAtfailAtcloneStaterestoreStatesliceFromgetMemoizedsetMemoizedbuildRulesTableparseRuleparseExprparseActionExprparseAndCodeExprparseAndExprparseAnyMatcherparseCharClassMatcherincChoiceAltCntparseChoiceExprparseLabeledExprparseLitMatcherparseNotCodeExprparseNotExprparseOneOrMoreExprparseRecoveryExprparseRuleRefExprparseSeqExprparseStateCodeExprparseThrowExprparseZeroOrMoreExprparseZeroOrOneExprDurWithoutByQuerySpecrecoverExprfailureLabelRangeTableRange16LoHiStrideRange32R16R32LatinOffsetbasicLatinCharscharsrangesclassesinvertedLabelMatchersalternatives/Users/austinjaybecker/projects/abeck-go-testing/query/promql/promql.goconversionnanosunitsopervectoroldMaxExprCntoldEntrypointoldChoiceNoMatcholdStatscloseErrcleanedfltuplemaxFailExpectedMapstartRulelastSepsepactValchraltIchoiceIdentlabGrammar"Grammar"300311"grammar""Comment"331AggregateExpression"AggregateExpression"353VectorSelector"VectorSelector"370SourceChar"SourceChar"416419433435436EOL"EOL"440"Identifier""ident"519IdentifierName"IdentifierName"692709IdentifierStart"IdentifierStart"725IdentifierPart"IdentifierPart"776794[\pL_]"[\\pL_]""L"801818836[\p{Nd}]"[\\p{Nd}]"Nd"Nd""StringLiteral"846862864868DoubleStringChar"DoubleStringChar"886892896SingleStringChar"SingleStringChar"913919`"`"923RawStringChar"RawStringChar"938108410861088109211121118113011341154116011701174118912601279128212881295130113141319DoubleStringEscape"DoubleStringEscape"13381357136013661373137913921397SingleStringEscape"SingleStringEscape"14161432143314371449147014721478CommonEscapeSequence"CommonEscapeSequence"15071509152215281593161416161622165116531666167217381761SingleCharEscape"SingleCharEscape"1780OctalEscape"OctalEscape"1794HexEscape"HexEscape"1806LongUnicodeEscape"LongUnicodeEscape"1826ShortUnicodeEscape"ShortUnicodeEscape"18451864187018761882188818941900190619111925OctalDigit"OctalDigit"193619471964197719901996205720692073HexDigit"HexDigit"2082209721032116212221892214"U"221822272236224522542263227222812383238924022408247124972501251025192528263026362649265527192732[0-7]"[0-7]"DecimalDigit"DecimalDigit"27382753[0-9]"[0-9]"27592770[0-9a-f]i"[0-9a-f]i"CharClassMatcher"CharClassMatcher"2781280028042806ClassCharRange"ClassCharRange"2823ClassChar"ClassChar"28352840UnicodeClassEscape"UnicodeClassEscape"286228662908291229142917292329392945301730343044304830583070307330793086309231053110CharClassEscape"CharClassEscape"312631443146315231813182318832013207327332943307SingleCharUnicodeClass"SingleCharUnicodeClass"3336333733433356336234333437344334583622362636433649365537353760[LMNCPZS]"[LMNCPZS]""Number"377237813786"Integer"379437963800Digit"Digit"385238623868NonZeroDigit"NonZeroDigit"388139453960[1-9]"[1-9]"39663974LabelBlock"LabelBlock"398139943998"block"4004LabelMatches"LabelMatches"4017404940534066NanoSecondUnits"NanoSecondUnits"41314149MicroSecondUnits"MicroSecondUnits"42544273427442814289Î¼s"Î¼s"MilliSecondUnits"MilliSecondUnits"43984417SecondUnits"SecondUnits"45244538MinuteUnits"MinuteUnits"45754589HourUnits"HourUnits"46264638DayUnits"DayUnits"46734684WeekUnits"WeekUnits"48924904YearUnits"YearUnits"51155127DurationUnits"DurationUnits"532453415359537853975411542554375448546054725483"dur"54875495"units"5501Operators"Operators"56505662566856745680%"%"5686569256995706<="<="5713<"<"5719>=">="5726>">"5732=~"=~"5739!~"!~"57465752LabelOperators"LabelOperators"5757577558115849588959205928LabelMatch"LabelMatch"5939595259585964__"__"5967597059855988"match"5996601261026117612361346137"rest"6142LabelMatchesRest"LabelMatchesRest"62216241624562486254LabelList"LabelList"629263046305("("630963126344634863516357636363666371LabelListRest"LabelListRest"638663896455647264766479648565186535"metric"654265536557656365756578"rng"6582"Range"658965926599667766856689669266966705670867376746675667596763CountValueOperator"CountValueOperator"67976818count_values"count_values"BinaryAggregateOperators"BinaryAggregateOperators"690369316935topk"topk"6945bottomk"bottomk"6958UnaryAggregateOperators"UnaryAggregateOperators"706070867090709971087117"avg"71267138stdvar"stdvar"7150AggregateOperators"AggregateOperators"7249727072917318AggregateBy"AggregateBy"73437357736373667373738373867391keep_common"keep_common"AggregateWithout"AggregateWithout"26175047523without"without"753475377544AggregateGroup"AggregateGroup"2687656767376872707705271772777307750775377577760"param"77667780778377877790"vector"779778127815781978227828277797679797999800280088024802780318034804080548057806180648071808680892838225822882548257826182658271827882818285828882958310831383178320832628984678470849684998505852185248528853285388545854885528555856285778580295870987128737874087448747875487698772877687798785299887888818906890989158931893489388941894889638966303904690519053Whitespace"Whitespace"906690723049083908730691009113[ \t\r]"[ \\t\\r]"'\r'91219127EOS"EOS"30891329138914191479149SingleLineComment"SingleLineComment"916891749177310918291889189identifier is a reserved word"identifier is a reserved word"Unquotestring literal not terminated"string literal not terminated"invalid escape character"invalid escape character"invalid octal escape"invalid octal escape"invalid hexadecimal escape"invalid hexadecimal escape"invalid Unicode escape"invalid Unicode escape"character class not terminated"character class not terminated"invalid Unicode class escape"invalid Unicode class escape"unicode class not terminated"unicode class not terminated"code block not terminated"code block not terminated"grammar has no rule"grammar has no rule"invalid entrypoint"invalid entrypoint"invalid encoding"invalid encoding"max number of expresssions parsed"max number of expresssions parsed"%d:%d [%d]"%d:%d [%d]"%s %d:%d:%d: %s [%#U]
"%s %d:%d:%d: %s [%#U]\n"%d:%d (%d)"%d:%d (%d)"rule "rule "!"!"DecodeRune"cloneState""restoreState"panic handler"panic handler"!."!."no match found, expected: "no match found, expected: ""or"%s %s %s"%s %s %s"parseRule "parseRule "MATCH"MATCH"unknown expression type %T"unknown expression type %T""parseActionExpr""parseAndCodeExpr""parseAndExpr""parseAnyMatcher""parseCharClassMatcher"%s %d:%d"%s %d:%d""parseChoiceExpr""parseLabeledExpr""parseLitMatcher"%q%s"%q%s""parseNotCodeExpr""parseNotExpr""parseOneOrMoreExpr"parseRecoveryExpr ("parseRecoveryExpr ("parseRuleRefExpr "parseRuleRefExpr "%s: invalid rule: missing name"%s: invalid rule: missing name"undefined rule: %s"undefined rule: %s""parseSeqExpr""parseStateCodeExpr""parseThrowExpr""parseZeroOrMoreExpr""parseZeroOrOneExpr"CategoriesScriptsinvalid Unicode class: %s"invalid Unicode class: %s" Code generated by pigeon; DO NOT EDIT. Package promql implements a promql parser to build flux query specifications from promql. DO NOT EDIT: This file is auto generated by the pigeon PEG parser generator.lint:file-ignore SA6001 Ignore all unused code, it's generated Prometheus doesn't support nanoseconds, but, influx does Prometheus always assumes exactly 24 hours in a day https://github.com/prometheus/common/blob/61f87aac8082fa8c3c5655c7608d7478d46ac2ad/model/time.go#L180 Prometheus always assumes exactly 7 days in a week Prometheus always assumes 365 days errNoRule is returned when the grammar to parse has no rule. errInvalidEntrypoint is returned when the specified entrypoint rule does not exit. errInvalidEncoding is returned when the source is not properly utf8-encoded. errMaxExprCnt is used to signal that the maximum number of expressions have been parsed. Option is a function that can set an option on the parser. It returns the previous setting as an Option. MaxExpressions creates an Option to stop parsing after the provided number of expressions have been parsed, if the value is 0 then the parser will parse for as many steps as needed (possibly an infinite number). The default for maxExprCnt is 0. Entrypoint creates an Option to set the rule name to use as entrypoint. The rule name must have been specified in the -alternate-entrypoints if generating the parser with the -optimize-grammar flag, otherwise it may have been optimized out. Passing an empty string sets the entrypoint to the first rule in the grammar. The default is to start parsing at the first rule in the grammar. Statistics adds a user provided Stats struct to the parser to allow the user to process the results after the parsing has finished. Also the key for the "no match" counter is set. Example usage:     input := "input"     stats := Stats{}     _, err := Parse("input-file", []byte(input), Statistics(&stats, "no match"))     if err != nil {         log.Panicln(err)     b, err := json.MarshalIndent(stats.ChoiceAltCnt, "", "  ")     fmt.Println(string(b)) Debug creates an Option to set the debug flag to b. When set to true, debugging information is printed to stdout while parsing. The default is false. Memoize creates an Option to set the memoize flag to b. When set to true, the parser will cache all results so each expression is evaluated only once. This guarantees linear parsing time even for pathological cases, at the expense of more memory and slower times for typical cases. AllowInvalidUTF8 creates an Option to allow invalid UTF-8 bytes. Every invalid UTF-8 byte is treated as a utf8.RuneError (U+FFFD) by character class matchers and is matched by the any matcher. The returned matched value, c.text and c.offset are NOT affected. Recover creates an Option to set the recover flag to b. When set to true, this causes the parser to recover from panics and convert it to an error. Setting it to false can be useful while debugging to access the full stack trace. The default is true. GlobalStore creates an Option to set a key to a certain value in the globalStore. InitState creates an Option to set a key to a certain value in the global "state" store. ParseFile parses the file identified by filename. ParseReader parses the data from r using filename as information in the error messages. Parse parses the data from b using filename as information in the position records a position in the text. savepoint stores all state required to go back to this point in the parser. start position of the match raw text of the match state is a store for arbitrary key,value pairs that the user wants to be tied to the backtracking of the parser. This is always rolled back if a parsing rule fails. globalStore is a general store for the user to store arbitrary key-value pairs that they need to manage and that they do not want tied to the backtracking of the parser. This is only modified by the user and never rolled back by the parser. It is always up to the user to keep this in a consistent state. the AST types... errList cumulates the errors found by the parser. parserError wraps an error with a prefix indicating the rule in which the error occurred. The original error is stored in the Inner field. Error returns the error message. newParser creates a parser with the specified input source and options. start rule is rule [0] unless an alternate entrypoint is specified setOptions applies the options to the parser. Stats stores some statistics, gathered during parsing ExprCnt counts the number of expressions processed during parsing This value is compared to the maximum number of expressions allowed (set by the MaxExpressions option). ChoiceAltCnt is used to count for each ordered choice expression, which alternative is used how may times. These numbers allow to optimize the order of the ordered choice expression to increase the performance of the parser The outer key of ChoiceAltCnt is composed of the name of the rule as well as the line and the column of the ordered choice. The inner key of ChoiceAltCnt is the number (one-based) of the matching alternative. For each alternative the number of matches are counted. If an ordered choice does not match, a special counter is incremented. The name of this counter is set with the parser option Statistics. For an alternative to be included in ChoiceAltCnt, it has to match at least once. memoization table for the packrat algorithm: map[offset in source] map[expression or rule] {value, match} rules table, maps the rule identifier to the rule node variables stack, map of label to value rule stack, allows identification of the current rule in errors parse fail max number of expressions to be parsed entrypoint for the parser recovery expression stack, keeps track of the currently available recovery expression, these are traversed in reverse push a variable set on the vstack. create new empty slot in the stack slice to 1 more get the last args set empty map, all good pop a variable set from the vstack. if the map is not empty, clear it GC that map push a recovery expression with its labels to the recoveryStack pop a recovery expression from the recoveryStack process fail if parsing fails and not inverted or parsing succeeds and invert is set read advances the parser to the next rune. see utf8.DecodeRune restore parser position to the savepoint pt. Cloner is implemented by any value that has a Clone method, which returns a copy of the value. This is mainly used for types which are not passed by value (e.g map, slice, chan) or structs that contain such types. This is used in conjunction with the global state feature to create proper copies of the state to allow the parser to properly restore the state in the case of backtracking. clone and return parser current state. restore parser current state to the state storeDict. every restoreState should applied only one time for every cloned state get the slice of bytes from the savepoint start to the current position. TODO : not super critical but this could be generated panic can be used in action code to stop parsing immediately and return the panic as an error. advance to first rune If parsing fails, but no errors have been recorded, the expected values for the farthest parser position are returned as error. EOF - see utf8.DecodeRune can't match EOF try to match in the list of available chars try to match in the list of ranges try to match in the list of Unicode classes We increment altI by 1, so the keys do not start at 0 dummy assignment to prevent compile error if optimized did not match once, no match whether it matched or not, consider it a match cannot happen/Users/austinjaybecker/projects/abeck-go-testing/query/promql/query.gounable to build as %t is not a QueryBuilder"unable to build as %t is not a QueryBuilder"/Users/austinjaybecker/projects/abeck-go-testing/query/promql/types.goaggsemanticuniversegithub.com/influxdata/flux/semantic"github.com/influxdata/flux/semantic"github.com/influxdata/flux/stdlib/universe"github.com/influxdata/flux/stdlib/universe"json:"string,omitempty"`json:"string,omitempty"`json:"dur,omitempty"`json:"dur,omitempty"`json:"val,omitempty"`json:"val,omitempty"`json:"kind,omitempty"`json:"kind,omitempty"`json:"label_matchers,omitempty"`json:"label_matchers,omitempty"`FromOpSpecNameOrID"where"RangeOpSpecIsRelativeRelativeAbsoluteStartColumnStopColumnIdentifierExpression_metric"_metric"unknown label match kind %d"unknown label match kind %d"FilterOpSpecResolvedFunctionFunctionParametersFunctionParameterDefaultsGetFunctionBodyExpressionFnOnEmptyjson:"without,omitempty"`json:"without,omitempty"`json:"by,omitempty"`json:"by,omitempty"`unable to merge using `without`"unable to merge using `without`""merge"GroupOpSpecjson:"arg,omitempty"`json:"arg,omitempty"`unable to run %d yet"unable to run %d yet"CountOpSpecAggregateConfigDefaultCostReadArgsSumOpSpecunknown Op kind %d"unknown Op kind %d"json:"selector,omitempty"`json:"selector,omitempty"`json:"aggregate,omitempty"`json:"aggregate,omitempty"`json:"source,omitempty"`json:"source,omitempty"`unable to represent comments in the AST"unable to represent comments in the AST" MatchKind is an enum for label matching types. Possible MatchKinds. TODO: Change this to a UUID TODO: Change to ASTcase TopKind:	return &flux.Operation{		ID:   "top",		Spec: &universe.TopOpSpec{}, // TODO: Top doesn't have arg yet	}, nilcase MinKind:		ID:   "min",		Spec: &universe.MinOpSpec{},case MaxKind:		ID:   "max",		Spec: &universe.MaxOpSpec{},case AvgKind:		ID:   "mean",		Spec: &universe.MeanOpSpec{},case StdevKind:		ID:   "stddev",		Spec: &universe.StddevOpSpec{},TypTypeMutateTypTypeGroupCardinalityDiskGPUMEMNETGetAllGetArrayGetFloatGetFunctionGetRequiredGetRequiredArrayGetRequiredArrayAllowEmptyGetRequiredBoolGetRequiredFloatGetRequiredFunctionGetRequiredIntGetRequiredObjectGetRequiredStringlistUnusedGetRequiredTimeGetRequiredDurationIsPipeMutateIsPipeDefaultTypeMutateDefaultType/Users/austinjaybecker/projects/abeck-go-testing/query/promql/unicode.goUnquoteChar552960xD800573430xDFFFASCII_Hex_Digit"ASCII_Hex_Digit"Arabic"Arabic"Armenian"Armenian"Avestan"Avestan"Balinese"Balinese"Bamum"Bamum"Bassa_Vah"Bassa_Vah"Batak"Batak"Bengali"Bengali"Bidi_Control"Bidi_Control"Bopomofo"Bopomofo"Brahmi"Brahmi"Braille"Braille"Buginese"Buginese"Buhid"Buhid""C"Canadian_Aboriginal"Canadian_Aboriginal"Carian"Carian"Caucasian_Albanian"Caucasian_Albanian"Cc"Cc"Cf"Cf"Chakma"Chakma"Cham"Cham"Cherokee"Cherokee"Co"Co"Common"Common"Coptic"Coptic"Cs"Cs"Cuneiform"Cuneiform"Cypriot"Cypriot"Cyrillic"Cyrillic"Dash"Dash""Deprecated"Deseret"Deseret"Devanagari"Devanagari"Diacritic"Diacritic"Duployan"Duployan"Egyptian_Hieroglyphs"Egyptian_Hieroglyphs"Elbasan"Elbasan"Ethiopic"Ethiopic"Extender"Extender"Georgian"Georgian"Glagolitic"Glagolitic"Gothic"Gothic"Grantha"Grantha"Greek"Greek"Gujarati"Gujarati"Gurmukhi"Gurmukhi"Han"Han"Hangul"Hangul"Hanunoo"Hanunoo"Hebrew"Hebrew"Hex_Digit"Hex_Digit"Hiragana"Hiragana"Hyphen"Hyphen"IDS_Binary_Operator"IDS_Binary_Operator"IDS_Trinary_Operator"IDS_Trinary_Operator"Ideographic"Ideographic"Imperial_Aramaic"Imperial_Aramaic"Inherited"Inherited"Inscriptional_Pahlavi"Inscriptional_Pahlavi"Inscriptional_Parthian"Inscriptional_Parthian"Javanese"Javanese"Join_Control"Join_Control"Kaithi"Kaithi"Kannada"Kannada"Katakana"Katakana"Kayah_Li"Kayah_Li"Kharoshthi"Kharoshthi"Khmer"Khmer"Khojki"Khojki"Khudawadi"Khudawadi"Lao"Lao"Latin"Latin"Lepcha"Lepcha"Limbu"Limbu"Linear_A"Linear_A"Linear_B"Linear_B"Lisu"Lisu"Ll"Ll"Lm"Lm""Lo"Logical_Order_Exception"Logical_Order_Exception"Lt"Lt"Lu"Lu"Lycian"Lycian"Lydian"Lydian""M"Mahajani"Mahajani"Malayalam"Malayalam"Mandaic"Mandaic"Manichaean"Manichaean"Mc"Mc""Me"Meetei_Mayek"Meetei_Mayek"Mende_Kikakui"Mende_Kikakui"Meroitic_Cursive"Meroitic_Cursive"Meroitic_Hieroglyphs"Meroitic_Hieroglyphs"Miao"Miao"Mn"Mn"Modi"Modi"Mongolian"Mongolian"Mro"Mro"Myanmar"Myanmar""N"Nabataean"Nabataean"New_Tai_Lue"New_Tai_Lue"Nko"Nko"Nl"Nl"No"No"Noncharacter_Code_Point"Noncharacter_Code_Point"Ogham"Ogham"Ol_Chiki"Ol_Chiki"Old_Italic"Old_Italic"Old_North_Arabian"Old_North_Arabian"Old_Permic"Old_Permic"Old_Persian"Old_Persian"Old_South_Arabian"Old_South_Arabian"Old_Turkic"Old_Turkic"Oriya"Oriya"Osmanya"Osmanya"Other_Alphabetic"Other_Alphabetic"Other_Default_Ignorable_Code_Point"Other_Default_Ignorable_Code_Point"Other_Grapheme_Extend"Other_Grapheme_Extend"Other_ID_Continue"Other_ID_Continue"Other_ID_Start"Other_ID_Start"Other_Lowercase"Other_Lowercase"Other_Math"Other_Math"Other_Uppercase"Other_Uppercase""P"Pahawh_Hmong"Pahawh_Hmong"Palmyrene"Palmyrene"Pattern_Syntax"Pattern_Syntax"Pattern_White_Space"Pattern_White_Space"Pau_Cin_Hau"Pau_Cin_Hau"Pc"Pc"Pd"Pd"Pe"Pe"Pf"Pf"Phags_Pa"Phags_Pa"Phoenician"Phoenician"Pi"Pi"Po"Po"Ps"Ps"Psalter_Pahlavi"Psalter_Pahlavi"Quotation_Mark"Quotation_Mark"Radical"Radical"Rejang"Rejang"Runic"Runic""S"STerm"STerm"Samaritan"Samaritan"Saurashtra"Saurashtra"Sc"Sc"Sharada"Sharada"Shavian"Shavian"Siddham"Siddham"Sinhala"Sinhala"Sk"Sk"Sm"Sm"So"So"Soft_Dotted"Soft_Dotted"Sora_Sompeng"Sora_Sompeng"Sundanese"Sundanese"Syloti_Nagri"Syloti_Nagri"Syriac"Syriac"Tagalog"Tagalog"Tagbanwa"Tagbanwa"Tai_Le"Tai_Le"Tai_Tham"Tai_Tham"Tai_Viet"Tai_Viet"Takri"Takri"Tamil"Tamil"Telugu"Telugu"Terminal_Punctuation"Terminal_Punctuation"Thaana"Thaana"Thai"Thai"Tibetan"Tibetan"Tifinagh"Tifinagh"Tirhuta"Tirhuta"Ugaritic"Ugaritic"Unified_Ideograph"Unified_Ideograph"Vai"Vai"Variation_Selector"Variation_Selector"Warang_Citi"Warang_Citi"White_Space"White_Space"Yi"Yi""Z"Zl"Zl"Zp"Zp"Zs"Zs" validateUnicodeEscape checks that the provided escape sequence is a valid Unicode escape sequence./Users/austinjaybecker/projects/abeck-go-testing/query/query.go/Users/austinjaybecker/projects/abeck-go-testing/query/querytest/Users/austinjaybecker/projects/abeck-go-testing/query/querytest/compiler.goMakeFromInfluxJSONCompilerReplaceFromRulejsonFilequerytestgithub.com/influxdata/flux/stdlib/influxdata/influxdb"github.com/influxdata/flux/stdlib/influxdata/influxdb"github.com/influxdata/flux/stdlib/influxdata/influxdb/v1"github.com/influxdata/flux/stdlib/influxdata/influxdb/v1"AddLogicalRules"ReplaceFromRule"PatFromKindFromInfluxJSONProcedureSpec MakeFromInfluxJSONCompiler returns a compiler that replaces all From operations with FromJSON./Users/austinjaybecker/projects/abeck-go-testing/query/request.gowithErrorvisitorcreateCompilercreateDialect"Prefer""return-no-content""return-no-content-with-error"json:"compiler"`json:"compiler"`json:"compiler_type"`json:"compiler_type"`unsupported compiler type %q"unsupported compiler type %q"json:"request"`json:"request"`json:"dialect_type"`json:"dialect_type"`unsupported dialect type %q"unsupported dialect type %q" Request represents the query to run. Options to mutate the header associated to this Request can be specified via `WithOption` or associated methods. One should always `Request.ApplyOptions()` before encoding and sending the request. Scope Command Compiler converts the query to a specification to run against the data. compilerMappings maps compiler types to creation methods SetReturnNoContent sets the header for a Request to return no content. RequestHeaderOption is a function that mutates the header associated to a Request. WithOption adds a RequestHeaderOption to this Request. WithReturnNoContent makes this Request return no content. ApplyOptions applies every option added to this Request to the given header. WithCompilerMappings sets the query type mappings on the request. UnmarshalJSON populates the request from the JSON data. WithCompilerMappings must have been called or an error will occur. ContextWithRequest returns a new context with a reference to the request. RequestFromContext retrieves a *Request from a context. If not request exists on the context nil is returned. ProxyRequest specifies a query request and the dialect for the results. Request is the basic query request Dialect is the result encoder dialectMappings maps dialect types to creation methods WithCompilerMappings sets the compiler type mappings on the request. WithDialectMappings sets the dialect type mappings on the request. WithCompilerMappings and WithDialectMappings must have been called or an error will occur./Users/austinjaybecker/projects/abeck-go-testing/query/service.goflux is not configured; cannot parse"flux is not configured; cannot parse"flux is not configured; cannot evaluate"flux is not configured; cannot evaluate" QueryService represents a type capable of performing queries. Query submits a query for execution returning a results iterator. Cancel must be called on any returned results to free resources. AsyncQueryService represents a service for performing queries where the results are delivered asynchronously. Query submits a query for execution returning immediately. ProxyQueryService performs queries and encodes the result into a writer. Query performs the requested query and encodes the results into w. The number of bytes written to w is returned __independent__ of any error. Parse will take flux source code and produce a package. If there are errors when parsing, the first error is returned. An ast.Package may be returned when a parsing error occurs, but it may be null if parsing didn't even occur. This will return an error if the FluxLanguageService is nil. EvalAST will evaluate and run an AST./Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/experimental/Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/experimental/to.goExperimentalToKindGetTablePointsMetadataLabelAndOffsetNewToTransformationTablePointsMetadataToOpSpecToProcedureSpecToTransformationcreateToOpSpeccreateToTransformationdefaultFieldColLabeldefaultMeasurementColLabeldefaultStartColLabeldefaultStopColLabeldefaultTimeColLabelnewToProceduretoSignatureBucketsAccessedbfreadBucketswriteBucketsExecutionNodeSetLabelDatasetTransformationDatasetIDRetractTableUpdateProcessingTimeUpdateWatermarkTriggerSpecTriggerKindAddTransformationSetTriggerSpecTableBuilderCacheTableBuilderForEachBuilderBufferedPointsWriterwriteTablebIDisTagtagmaptimestampOffsettmdfieldVallaopointNamegithub.com/influxdata/flux/stdlib/experimental"github.com/influxdata/flux/stdlib/experimental"experimental-tojson:"host"`json:"host"`MustLookupBuiltinType"to"MustValueCreateOperationSpecAdministrationTableObjectAddParentFromArgsAddParentFunctionValueWithSideEffectNewOperationSpecRegisterOpSpecCreateProcedureSpecRegisterProcedureSpecWithSideEffectCreateTransformationAccumulationModeStreamContextResolveTimeRegisterTransformationcannot provide both `bucket` and `bucketID` parameters to the `to` function"cannot provide both `bucket` and `bucketID` parameters to the `to` function"cannot provide both `org` and `orgID` parameters to the `to` function"cannot provide both `org` and `orgID` parameters to the `to` function"invalid spec type %T"invalid spec type %T"tableBuilderCacheGroupLookupgroupKeyListgroupKeyListElementkgInsertAtlastIndexLookupOrCreatecreateOrSetInGroupnewKeyGrouptriggerSpeclookupStateDiscardTableExpireTableForEachWithContextNewTableBuilderCachedatasetTransformationSetDataCacheTriggerContextTableContextWatermarkCurrentProcessingTimeTriggeredaccModewatermarkprocessingTimeevalTriggerstriggerTableexpireTableNewDatasetGetStorageDependenciesfailed to look up organization %q"failed to look up organization %q"failed to look up bucket %q in org %q"failed to look up bucket %q in org %q"NewBufferedPointsWriterDefaultBufferSizeDefaultMeasurementColLabelDefaultStopColLabelMeasurementNameFieldKeyTagValueOffsetTimestampOffsetfound column %q in the group key; experimental.to() expects pivoted data"found column %q in the group key; experimental.to() expects pivoted data"group key column %q has type %v; type %v is required"group key column %q has type %v; type %v is required"required column %q not in group key"required column %q not in group key"column %q has type string; type %s is required"column %q has type string; type %s is required"input table is missing required column %q"input table is missing required column %q"AddTableColsValueForRowBasicFloatBasicIntBasicUintBasicStringBasicBoolunsupported field type %v"unsupported field type %v"AppendRecord ToKind is the kind for the `to` flux function ToOpSpec is the flux.OperationSpec for the `to` flux function. ReadArgs reads the args from flux.Arguments into the op spec Kind returns the kind for the ToOpSpec function. BucketsAccessed returns the buckets accessed by the spec. ToProcedureSpec is the procedure spec for the `to` flux function. Kind returns the kind for the procedure spec for the `to` flux function. Copy clones the procedure spec for `to` flux function. ToTransformation is the transformation for the `to` flux function. RetractTable retracts the table for the transformation for the `to` flux function. NewToTransformation returns a new *ToTransformation with the appropriate fields set. Get organization name and ID No org or orgID provided as an arg, use the orgID from the context Get bucket name and ID User will have specified exactly one in the ToOpSpec. Process does the actual work for the ToTransformation. UpdateWatermark updates the watermark for the transformation for the `to` flux function. UpdateProcessingTime updates the processing time for the transformation for the `to` flux function. Finish is called after the `to` flux function's transformation is done processing. TablePointsMetadata stores state needed to write the points from one table. The tags in the table (final element is left as nil, to be replaced by field name) The offset in tags where to store the field name The column offset in the input table where the _time column is stored The labels and offsets of all the fields in the table Find measurement, tags Loop over all columns to find fields and _time Skip this iteration if field value is nulltableStateIDer/Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/influxdb/Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/influxdb/buckets.goBucketsDecoderBucketsKindFromStorageProcedureSpecFromStorageRuleGroupWindowAggregateTransposeRuleHostLookupLocalBucketsProcedureSpecLocalBucketsRuleMergeFiltersRuleNewStaticLookupPushDownBareAggregateRulePushDownFilterRulePushDownGroupAggregateRulePushDownGroupRulePushDownRangeRulePushDownReadTagKeysRulePushDownReadTagValuesRulePushDownWindowAggregateByTimeRulePushDownWindowAggregateRuleReadFilterSourceReadGroupPhysKindReadGroupPhysSpecReadGroupSourceReadRangePhysKindReadRangePhysSpecReadTagKeysPhysKindReadTagKeysPhysSpecReadTagKeysSourceReadTagValuesPhysKindReadTagValuesPhysSpecReadTagValuesSourceReadWindowAggregatePhysKindReadWindowAggregatePhysSpecReadWindowAggregateSourceSortedPivotRuleStaticLookupSwitchFillImplRuleSwitchSchemaMutationImplRuleToKindToStoragePredicateaddTagsFromTableargsReaderasSchemaMutationProcedureSpeccanPushGroupedAggregatecanPushWindowedAggregatecreateBucketsSourcecreateReadFilterSourcecreateReadGroupSourcecreateReadTagKeysSourcecreateReadTagValuesSourcecreateReadWindowAggregateSourcedefaultFieldMappingdependenciesKeyfieldFunctionVisitorfieldValuePropertyinvalidTagKeysForTagValuesisFieldisPushableBinaryPredicateisPushableExprisPushableFieldOperatorisPushableTagOperatorisPushableUnaryPredicateisPushableWindowisValidTagKeyForTagValuesmergePredicatesnewFieldFunctionVisitoropLabelreadFilterSourcereadGroupSourcereadTagKeysSourcereadTagValuesSourcereadWindowAggregateSourcerewritePushableExprtoComparisonOperatortoOptoStoragePredicateHelpervalidateMemberExprwindowMergeablePushAggswindowPushableAggsbdFetchgkkbdsidprSpecfromSpecinfluxdata/influxdb.localBuckets"influxdata/influxdb.localBuckets"RegisterSourceRegisterPhysicalRulesno buckets found in organization %v"no buckets found in organization %v"GroupKeyBuilderSetKeyValuegkbAddKeyValueNewGroupKeyBuilderNewStringNewColListTableBuilderSourceDecoderCreateSourceFromDecoderinfluxdata/influxdb.LocalBucketsRule"influxdata/influxdb.LocalBucketsRule"BucketsProcedureSpecSetOrgSetHostPostPhysicalValidatebuckets cannot list from a separate organization; please specify a host or remove the organization"buckets cannot list from a separate organization; please specify a host or remove the organization"predecessorssuccessorsshallowCopylpnCreateLogicalNodelocalBuckets"localBuckets" the dependencies used for FromKind are adequate for what we need here so there's no need to inject custom dependencies for buckets()MeasurementColumnTagColumnsFieldFnHostsWatchprocessTablesprocessTablereadSpecWindowProcedureSpecWindowSpecRowMapFndynamicFnrecordNametypeofimplicitTagColumnsHasZeroLookupBucketIDTimeBoundsLatestEarliestNFieldsNTagsPlanDetailscapturedrowParamSchemaMutationProcedureSpecSchemaMutationSchemaMutatorBuilderContextTableColumnsTableKeyColIdxMapColMapMutateMutatorMutationsRowMapPreparedFnpreparedFnreturnType/Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/influxdb/dependencies.godepSbucketLookupSvcfdepsmetricLabelKeysorgLookupSvcDepsWrappedDepsNewDefaultDependencies/Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/influxdb/from.goinfluxDBFrom"influxDBFrom"cannot submit unbounded read to %q; try bounding 'from' with a call to 'range'"cannot submit unbounded read to %q; try bounding 'from' with a call to 'range'" FromStorageProcedureSpec is a logical operation representing any read from storage. However as a logical operation, it doesn't specify how data is to be read from storage. It is the query planner's job to determine the optimal read strategy and to convert this logical operation into the appropriate physical operation. Logical operations cannot be executed by the query engine. So if this operation is still around post physical planning, it means that a 'range' could not be pushed down to storage. Storage does not support unbounded reads, and so this query must not be validated./Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/influxdb/metrics.goinfluxdb_source"influxdb_source"read_request_duration_seconds"read_request_duration_seconds"Histogram of times spent in read requests"Histogram of times spent in read requests" NewMetrics produces a new metrics objects for an influxdb source. Currently it just collects the duration of read requests into a histogram. ctxLabelKeys is a list of labels to add to the produced metrics. In addition, produced metrics will be labeled with the orgID and type of operation requested. PrometheusCollectors satisfies the PrometheusCollector interface. if metrics happens to be nil here (such as for a test), then let's not panic./Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/influxdb/operators.gopredecessorBounds"ReadRangePhysKind""ReadGroupPhysKind""ReadWindowAggregatePhysKind""ReadTagKeysPhysKind""ReadTagValuesPhysKind"GroupMode: %v, GroupKeys: %v, AggregateMethod: "%s""GroupMode: %v, GroupKeys: %v, AggregateMethod: \"%s\""could not find bucket %q"could not find bucket %q"invalid bucket id"invalid bucket id"no bucket name or id have been specified"no bucket name or id have been specified"ConvertTimeevery = %v, aggregates = %v, createEmpty = %v, timeColumn = "%s""every = %v, aggregates = %v, createEmpty = %v, timeColumn = \"%s\"" Filter is the filter to use when calling into storage. It must be possible to push down this filter. Determine bucketID TimeBounds implements plan.BoundsAwareProcedureSpec./Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/influxdb/rules.gogrpfromNoderangeSpecmergedPredicatemergedNodebodyExprfilterSpecnewFilterSpecnewFromSpecnotPushableparamNamepushabledistinctSpeckeepNodekeepSpeckeysNodekeysSpecdistinctNodegroupNodegroupSpecuebelokrokmemberExpridExprpushableOperatorsmoreOperatorspivotSpecminSpecmaxSpecmeanSpeccountSpecsumSpecfirstSpeclastSpecfnNodewindowSpecwindowNodemutatorduplicateNodeduplicateSpecduplicateSpecOkwindowAggregateNodewindowAggregateSpecnewFnNodegroupKeysnewFromNodenewGroupNodeRegisterLogicalRulesinfluxdata/influxdb.FromStorageRule"influxdata/influxdb.FromStorageRule"FromProcedureSpecreads from the storage engine cannot read from a separate organization; please specify a host or remove the organization"reads from the storage engine cannot read from a separate organization; please specify a host or remove the organization"fromStorage"fromStorage""PushDownGroupRule"GroupKindGroupProcedureSpecPhysicalPlanNodePhysicalProcedureSpecPhysicalAttributesRequiredAttrsOutputAttrsppnCreateUniquePhysicalNode"ReadGroup""PushDownRangeRule"RangeKindRangeProcedureSpecReadRange"ReadRange""PushDownFilterRule"FilterKindFilterProcedureSpecKeepEmptyTablesPartitionPredicatesMergeToPhysicalNode"PushDownReadTagKeysRule"DistinctKindSchemaMutationKindKeysKindDistinctProcedureSpecKeysProcedureSpecKeepOpSpec"ReadTagKeys""PushDownReadTagValuesRule""ReadTagValues"NotOperatorExistsOperator"SortedPivotRule"PivotKindpivotPivotProcedureSpecRowKeyColumnKeyValueColumnIsSortedByFuncIsKeyColumnFuncisSortedByisKeyColumn"PushDownWindowAggregateRule"MeanKindFirstKindLastKindOneOfWindowKindMinProcedureSpecSelectorConfigMaxProcedureSpecMeanProcedureSpecCountProcedureSpecReAggregateSpecSumProcedureSpecFirstProcedureSpecLastProcedureSpec"ReadWindowAggregate""PushDownWindowAggregateByTimeRule"DuplicateOpSpecConvertDurationNsecsReadWindowAggregateByTime"ReadWindowAggregateByTime""PushDownBareAggregateRule"ConvertDuration"GroupWindowAggregateTransposeRule"ContainsStrReplaceNode"PushDownGroupAggregateRule"ReadGroupAggregate"ReadGroupAggregate""SwitchFillImplRule"FillKindUseDeprecatedImpl"SwitchSchemaMutationImplRule"DualImplProcedureSpecUseDeprecated PushDownGroupAggregateRule{}, PushDownGroupRule pushes down a group operation to storage Storage can only group by tag keys. Note the columns _start and _stop are ok since all tables coming from storage will have the same _start and _values. PushDownRangeRule pushes down a range filter to storage Pattern matches 'from |> range' Rewrite converts 'from |> range' into 'ReadRange' PushDownFilterRule is a rule that pushes filters into from procedures to be evaluated in the storage layer. This rule is likely to be replaced by a more generic rule when we have a better framework for pushing filters, etc into sources. Cannot push down when keeping empty tables. I would expect that type checking would catch this, but just to be safe... Nothing could be pushed down, no rewrite can happen Convert the pushable expression to a storage predicate. If the filter has already been set, then combine the existing predicate with the new one. Copy the specification and set the predicate. All predicates could be pushed down, so eliminate the filter PushDownReadTagKeysRule matches 'ReadRange |> keys() |> keep() |> distinct()'. The 'from()' must have already been merged with 'range' and, optionally, may have been merged with 'filter'. If any other properties have been set on the from procedure, this rule will not rewrite anything. Retrieve the nodes and specs for all of the predecessors. A filter spec would have already been merged into the from spec if it existed so we will take that one when constructing our own replacement. We do not care about it at the moment though which is why it is not in the pattern. The schema mutator needs to correspond to a keep call on the column specified by the keys procedure. We have a keep mutator, but it uses a function or it retains more than one column so it does not match what we want. We are not keeping the value column so this optimization will not work. The distinct spec should keep only the value column. We have passed all of the necessary prerequisites so construct the procedure spec. PushDownReadTagValuesRule matches 'ReadRange |> keep(columns: [tag]) |> group() |> distinct(column: tag)'. All of the values need to be grouped into the same table. The column that distinct is for will be the tag key. on the tag key column. isValidTagKeyForTagValues returns true if the given key can be used in a tag values call. isPushableExpr determines if a predicate expression can be pushed down into the storage layer. TODO(jsternberg): We should be able to rewrite `not r.host == "tag"` to `r.host != "tag"` but that is beyond what we do right now. Manual testing seems to indicate that (at least right now) we can only handle predicates of the form <fn param>.<property> <op> <literal> and the literal must be on the RHS. If the predicate is a string literal, we are comparing for equality, it is a tag, and it is empty, then it is not pushable. This is because the storage engine does not consider there a difference between a tag with an empty value and a non-existant tag. We have made the decision that a missing tag is null and not an empty string, so empty string isn't something that can be returned from the storage layer. The string literal is pushable if the operator is != because != "" will evaluate to true with everything that has a tag value and false when the tag value is null. rewritePushableExpr will rewrite the expression for the storage layer. Fields can be filtered by anything that tags can be filtered by, plus range operators. SortedPivotRule is a rule that optimizes a pivot when it is directly after an influxdb from. The only thing that disqualifies this from being sorted is if the _value column is mentioned or if the tag does not exist. Everything else is a tag. Even if the tag does not exist, this is still considered sorted since sorting doesn't depend on a tag existing. We are already sorted. Everything else would be a tag if it existed. The transformation itself will catch if the column does not exist. Push Down of window aggregates. ReadRangePhys |> window |> { min, max, mean, count, sum } Check the aggregate function spec. Require the operation on _value and check the feature flag associated with the aggregate function. every and period must be equal every.isNegative must be false offset.isNegative must be false timeColumn: must be "_time" startColumn: must be "_start" stopColumn: must be "_stop" createEmpty: must be false Rule passes. PushDownWindowAggregateWithTimeRule will match the given pattern. ReadWindowAggregatePhys |> duplicate |> window(every: inf) If this pattern matches and the arguments to duplicate are matching time column names, it will set the time column on the spec. The As field must be the default time value and the column must be start or stop. window(every: inf) Cannot rewrite if already was rewritten. PushDownBareAggregateRule is a rule that allows pushing down of aggregates that are directly over a ReadRange source. GroupWindowAggregateTransposeRule will match the given pattern. ReadGroupPhys |> window |> { min, max, count, sum } This pattern will use the PushDownWindowAggregateRule to determine if the ReadWindowAggregatePhys operation is available before it will rewrite the above. This rewrites the above to: ReadWindowAggregatePhys |> group(columns: ["_start", "_stop", ...]) |> { min, max, sum } The count aggregate uses sum to merge the results. This only works with GroupModeBy. It is the case that ReadGroup, which we depend on as a predecessor, only works with GroupModeBy so it should be impossible to fail this condition, but we add this here for extra protection. Perform the rewrite by replacing each of the nodes. Replace the window node with a group node first. Attach the existing function node to the new group node. Replace the spec for the function if needed. No replacement required. The procedure is idempotent so we can use it over and over again and get the same result. Push Down of group aggregates. ReadGroupPhys |> { count } Cannot push down multiple aggregates ReadGroup() -> count => ReadGroup(count) ReadGroup() -> sum => ReadGroup(sum) ReadGroup() -> first => ReadGroup(first) ReadGroup() -> last => ReadGroup(last) ReadGroup() -> min => ReadGroup(min) ReadGroup() -> max => ReadGroup(max)/Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/influxdb/source.goctxWithSpanbufTableIsQueryTracingEnabledsource-"source-"influxdb/scanned-bytes"influxdb/scanned-bytes"influxdb/scanned-values"influxdb/scanned-values"BufferedTableBufferNCopyTablereadFilter"readFilter"nil bounds passed to from"nil bounds passed to from"readTagKeys"readTagKeys"readTagValues"readTagValues" Track the number of bytes and values scanned. There is more than one transformation so we need to copy the table for each transformation./Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/influxdb/storage.gomissing reader dependency"missing reader dependency"missing bucket lookup dependency"missing bucket lookup dependency"missing organization lookup dependency"missing organization lookup dependency" A nil channel always blocks, since hosts never change this is appropriate./Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/influxdb/storage_predicate.goobjectNamepredicatesat least one predicate is needed"at least one predicate is needed"LogicalOrunknown logical operator %v"unknown logical operator %v"left hand side"left hand side"right hand side"right hand side"Node_IntegerValueNode_BooleanValueNode_FloatValueunknown object %q"unknown object %q"MeasurementKeyValueKeyNodeTypeFieldRefNode_FieldRefValueFieldRefValueduration literals not supported in storage predicates"duration literals not supported in storage predicates"time literals not supported in storage predicates"time literals not supported in storage predicates"unsupported semantic expression type %T"unsupported semantic expression type %T"ComparisonRegexComparisonNotRegexStartsWithOperatorComparisonStartsWithComparisonLessComparisonLessEqualComparisonGreaterComparisonGreaterEqualunknown operator %v"unknown operator %v" ToStoragePredicate will convert a FunctionExpression into a predicate that can be sent down to the storage layer. Nest the predicates backwards. This way we get a tree like this: a AND (b AND c) Sanity check that the object is the objectName identifierValueLengthMutateMonthsMutateNanosecondsMutateNegative/Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/influxdb/to.gofieldFnhttpOKkafkaOKtoDepstoSpeccolVisitorexprNoderecordParamexcludeColumnstagIdxvalueTimemstatspointTimemeasurementNamemeasurementStatstimeColIdxtimeColLabelfieldColumnIdxfieldValueMappingvalueColumnIdxkafkagithub.com/influxdata/flux/compiler"github.com/influxdata/flux/compiler"github.com/influxdata/flux/stdlib/kafka"github.com/influxdata/flux/stdlib/kafka"influxdata/influxdb/to"influxdata/influxdb/to"json:"timeColumn"`json:"timeColumn"`json:"measurementColumn"`json:"measurementColumn"`json:"tagColumns"`json:"tagColumns"`json:"fieldFn"`json:"fieldFn"`influxdata/influxdb"influxdata/influxdb"timeColumn"timeColumn"measurementColumn"measurementColumn"tagColumns"tagColumns""fieldFn"ResolveFunctionbrokers"brokers"specify at most one of url, brokers in the same `to` function"specify at most one of url, brokers in the same `to` function"ToKafkaOpSpecBrokersNameColumnValueColumnsMsgBufSizecannot return storage dependencies; storage dependencies are unimplemented"cannot return storage dependencies; storage dependencies are unimplemented"NewRowMapFnToScopeYou must specify org and bucket"You must specify org and bucket"missing points writer dependency"missing points writer dependency"SearchStringsno time column detected"no time column detected"column %s of type %s is not of type %s"column %s of type %s is not of type %s"invalid type for tag column"invalid type for tag column"timestamp missing from block"timestamp missing from block"no column with label %s exists"no column with label %s exists"table has no _field column"table has no _field column"table has no _value column"table has no _value column"NewObjectNewObjectType TODO(jlapacik) remove this once we have execute.DefaultFieldColLabel argsReader is an interface for OperationSpec that have the same method to read args. Get organization ID Get bucket ID If no tag columns are specified, by default we exclude _field, _value and _measurement from being tag columns. If a field function is specified then we exclude any column that is referenced in the function expression from being a tag column. Walk the field function expression and record which columns are referenced. None of these columns will be used as tag columns. fieldFunctionVisitor implements semantic.Visitor. fieldFunctionVisitor is used to walk the the field function expression of the `to` operation and to record all referenced columns. This visitor is only used when no tag columns are provided as input to the `to` func. A field function is of the form `(r) => { Function Body }`, and it returns an object mapping field keys to values for each row r of the input. Visit records every column that is referenced in `Function Body`. These columns are either directly or indirectly used as value columns and as such need to be recorded so as not to be used as tag columns. ToDependencies contains the dependencies for executing the `to` function. Validate returns an error if any required field is unset. cache tag columns do time prepare field function if applicable and record the number of values to write per row +2 for field key, value Gather the timestamp and the tags. skip rows with null timestamp TODO(docmerlin): instead of doing this sort of thing, it would be nice if we had a way that allocated a lot less./Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/influxdb/v1/Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/influxdata/influxdb/v1/databases.goDatabasesDecoderDatabasesDependenciesDatabasesKindGetDatabasesDependenciesLocalDatabasesProcedureSpecLocalDatabasesRulecreateDatabasesSourcedatabaseInfoinfluxdata/influxdb/v1.localDatabases"influxdata/influxdb/v1.localDatabases"no 1.x databases found"no 1.x databases found"bucketId"bucketId"missing all databases lookup dependency"missing all databases lookup dependency"missing buckets lookup dependency"missing buckets lookup dependency"influxdata/influxdb.LocalDatabasesRule"influxdata/influxdb.LocalDatabasesRule"DatabasesProcedureSpeclocalDatabases"localDatabases"/Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/packages.gostdlibgithub.com/influxdata/influxdb/v2/query/stdlib/experimental"github.com/influxdata/influxdb/v2/query/stdlib/experimental"github.com/influxdata/influxdb/v2/query/stdlib/influxdata/influxdb/v1"github.com/influxdata/influxdb/v2/query/stdlib/influxdata/influxdb/v1"github.com/influxdata/influxdb/v2/query/stdlib/testing"github.com/influxdata/influxdb/v2/query/stdlib/testing" Import all stdlib packages/Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/testing/Users/austinjaybecker/projects/abeck-go-testing/query/stdlib/testing/testing.goFluxEndToEndFeatureFlagsFluxEndToEndSkipListPerTestFeatureFlagMap"universe"cov"cov"Reason TBD"Reason TBD"covariance"covariance"cumulative_sum_default"cumulative_sum_default"cumulative_sum_noop"cumulative_sum_noop"drop_non_existent"drop_non_existent"highestAverage"highestAverage"highestMax"highestMax"histogram_normalize"histogram_normalize"histogram_quantile"histogram_quantile"join_across_measurements"join_across_measurements"join_agg"join_agg"keep_non_existent"keep_non_existent"key_values"key_values"key_values_host_name"key_values_host_name"lowestAverage"lowestAverage"selector_preserve_time"selector_preserve_time""shift"shift_negative_duration"shift_negative_duration"task_per_line"task_per_line"union_heterogeneous"union_heterogeneous""unique"fill_bool"fill_bool"failed to read meta data: panic: interface conversion: interface {} is nil, not uint64"failed to read meta data: panic: interface conversion: interface {} is nil, not uint64"fill_float"fill_float"fill_int"fill_int"fill_string"fill_string"fill_time"fill_time"fill_uint"fill_uint"window_null"window_null"failed to read meta data: panic: interface conversion: interface {} is nil, not float64"failed to read meta data: panic: interface conversion: interface {} is nil, not float64"group_nulls"group_nulls"unbounded test"unbounded test"integral_columns"integral_columns"join_missing_on_col"join_missing_on_col"join_use_previous"join_use_previous"unbounded test (https://github.com/influxdata/flux/issues/2996)"unbounded test (https://github.com/influxdata/flux/issues/2996)"rowfn_with_import"rowfn_with_import"group key mismatch"group key mismatch"column order mismatch"column order mismatch"simple_max"simple_max"_stop missing from expected output"_stop missing from expected output"time bounds mismatch (engine uses now() instead of bounds on input table)"time bounds mismatch (engine uses now() instead of bounds on input table)"difference_columns"difference_columns"data write/read path loses columns x and y"data write/read path loses columns x and y""keys"failed to read metadata"failed to read metadata"group_except"group_except"group_ungroup"group_ungroup"pivot_mean"pivot_mean"histogram_quantile_minvalue"histogram_quantile_minvalue"failed to read meta data: no column with label _measurement exists"failed to read meta data: no column with label _measurement exists"increase"increase"failed to read meta data: table has no _value column"failed to read meta data: table has no _value column"string_max"string_max"error: invalid use of function: *functions.MaxSelector has no implementation for type string (https://github.com/influxdata/platform/issues/224)"error: invalid use of function: *functions.MaxSelector has no implementation for type string (https://github.com/influxdata/platform/issues/224)"null_as_value"null_as_value"null not supported as value in influxql (https://github.com/influxdata/platform/issues/353)"null not supported as value in influxql (https://github.com/influxdata/platform/issues/353)"string_interp"string_interp"string interpolation not working as expected in flux (https://github.com/influxdata/platform/issues/404)"string interpolation not working as expected in flux (https://github.com/influxdata/platform/issues/404)"to functions are not supported in the testing framework (https://github.com/influxdata/flux/issues/77)"to functions are not supported in the testing framework (https://github.com/influxdata/flux/issues/77)"covariance_missing_column_1"covariance_missing_column_1"need to support known errors in new test framework (https://github.com/influxdata/flux/issues/536)"need to support known errors in new test framework (https://github.com/influxdata/flux/issues/536)"covariance_missing_column_2"covariance_missing_column_2"drop_before_rename"drop_before_rename"drop_referenced"drop_referenced"yield requires special test case (https://github.com/influxdata/flux/issues/535)"yield requires special test case (https://github.com/influxdata/flux/issues/535)"window_group_mean_ungroup"window_group_mean_ungroup"window trigger optimization modifies sort order of its output tables (https://github.com/influxdata/flux/issues/1067)"window trigger optimization modifies sort order of its output tables (https://github.com/influxdata/flux/issues/1067)"median_column"median_column"failing in different ways (https://github.com/influxdata/influxdb/issues/13909)"failing in different ways (https://github.com/influxdata/influxdb/issues/13909)"dynamic_query"dynamic_query"tableFind does not work in e2e tests: https://github.com/influxdata/influxdb/issues/13975"tableFind does not work in e2e tests: https://github.com/influxdata/influxdb/issues/13975"to_int"to_int"dateTime conversion issue: https://github.com/influxdata/influxdb/issues/14575"dateTime conversion issue: https://github.com/influxdata/influxdb/issues/14575"to_uint"to_uint"holt_winters_panic"holt_winters_panic"Expected output is an empty table which breaks the testing framework (https://github.com/influxdata/influxdb/issues/14749)"Expected output is an empty table which breaks the testing framework (https://github.com/influxdata/influxdb/issues/14749)"map_nulls"map_nulls"to cannot write null values"to cannot write null values"alignTime"alignTime"experimental/array"experimental/array"test not meant to be consumed by influxdb"test not meant to be consumed by influxdb"from_group"from_group"experimental/geo"experimental/geo"filterRowsNotStrict"filterRowsNotStrict"filterRowsStrict"filterRowsStrict"gridFilterLevel"gridFilterLevel"gridFilter"gridFilter"groupByArea"groupByArea"filterRowsPivoted"filterRowsPivoted"shapeDataWithFilter"shapeDataWithFilter"shapeData"shapeData"test run before to() is finished: https://github.com/influxdata/influxdb/issues/13975"test run before to() is finished: https://github.com/influxdata/influxdb/issues/13975"replaceAllString"replaceAllString"http_endpoint"http_endpoint"need ability to test side effects in e2e tests: (https://github.com/influxdata/flux/issues/1723)"need ability to test side effects in e2e tests: (https://github.com/influxdata/flux/issues/1723)"influxdata/influxdb/schema"influxdata/influxdb/schema"show_tag_keys"show_tag_keys"failing due to bug in test, unskip this after upgrading from Flux v0.91.0"failing due to bug in test, unskip this after upgrading from Flux v0.91.0"state_changes_big_any_to_any"state_changes_big_any_to_any"state_changes_big_info_to_ok"state_changes_big_info_to_ok"state_changes_big_ok_to_info"state_changes_big_ok_to_info"state_changes_any_to_any"state_changes_any_to_any"state_changes_info_to_any"state_changes_info_to_any"state_changes_invalid_any_to_any"state_changes_invalid_any_to_any"state_changes"state_changes"Cannot inject custom deps into the test framework so the secrets don't lookup correctly"Cannot inject custom deps into the test framework so the secrets don't lookup correctly"internal/promql"internal/promql"testing/chronograf"testing/chronograf"aggregate_window_count"aggregate_window_count"flakey test: https://github.com/influxdata/influxdb/issues/18463"flakey test: https://github.com/influxdata/influxdb/issues/18463"testing/kapacitor"testing/kapacitor"fill_default"fill_default"unknown field type for f1"unknown field type for f1"testing/pandas"testing/pandas"extract_regexp_findStringIndex"extract_regexp_findStringIndex"pandas. map does not correctly handled returned arrays (https://github.com/influxdata/flux/issues/1387)"pandas. map does not correctly handled returned arrays (https://github.com/influxdata/flux/issues/1387)"partition_strings_splitN"partition_strings_splitN"testing/promql"testing/promql"emptyTable"emptyTable"tests a source"tests a source""year"flakey test: https://github.com/influxdata/influxdb/issues/15667"flakey test: https://github.com/influxdata/influxdb/issues/15667"extrapolatedRate_counter_rate"extrapolatedRate_counter_rate"option "testing.loadStorage" reassigned: https://github.com/influxdata/flux/issues/3155"option \"testing.loadStorage\" reassigned: https://github.com/influxdata/flux/issues/3155"extrapolatedRate_nocounter"extrapolatedRate_nocounter"extrapolatedRate_norate"extrapolatedRate_norate"linearRegression_nopredict"linearRegression_nopredict"linearRegression_predict"linearRegression_predict"testing/influxql"testing/influxql"invalid test data requires loadStorage to be overridden. See https://github.com/influxdata/flux/issues/3145"invalid test data requires loadStorage to be overridden. See https://github.com/influxdata/flux/issues/3145"failing since split with Flux upgrade: https://github.com/influxdata/influxdb/issues/19568"failing since split with Flux upgrade: https://github.com/influxdata/influxdb/issues/19568"contrib/RohanSreerama5/naiveBayesClassifier"contrib/RohanSreerama5/naiveBayesClassifier"bayes"bayes"error calling tableFind: "error calling tableFind: " TODO(adam) determine the reason for these test failures. it appears these occur when writing the input data.  `to` may not be null safe. these may just be missing calls to range() in the tests.  easy to fix in a new PR. the following tests have a difference between the CSV-decoded input table, and the storage-retrieved version of that table failed to read meta data errors: the CSV encoding is incomplete probably due to data schema errors.  needs more detailed investigation to find root cause of error "filter_by_regex":             "failed to read metadata", "filter_by_tags":              "failed to read metadata",/Users/austinjaybecker/projects/abeck-go-testing/query/storage.gofluxModereadGroup(%s)"readGroup(%s)"readWindow(%s)"readWindow(%s)"unknown group mode: "unknown group mode: " StorageReader is an interface for reading tables from the storage subsystem. Window and the WindowEvery/Offset should be mutually exclusive. If you set either the WindowEvery or Offset with nanosecond values, then the Window will be ignored TableIterator is a table iterator that also keeps track of cursor statistics from the storage engine. GroupModeNone merges all series into a single group. GroupModeBy produces a table for each unique value of the specified GroupKeys. ToGroupMode accepts the group mode from Flux and produces the appropriate storage group mode./Users/austinjaybecker/projects/abeck-go-testing/query.go TODO(desa): These files are possibly a temporary. This is needed as a part of the source work that is being done. See https://github.com/influxdata/platform/issues/594 for more info. SourceQuery is a query for a source. FluxLanguageService is a service for interacting with flux code. Completer will return a flux completer./Users/austinjaybecker/projects/abeck-go-testing/rand/Users/austinjaybecker/projects/abeck-go-testing/rand/id.goNewOrgBucketIDgenerateRandomBytesgenerateRandomStringsanitize0x5C0x2C OrgBucketID creates an id that does not have ascii backslash, commas, or spaces.  Used to create IDs for organizations and buckets. It is implemented without those characters because orgbucket pairs are placed in the old measurement field.  Measurement was interpreted as a string delimited with commas.  Therefore, to continue to use the underlying storage engine we need to sanitize ids. Safe for concurrent use by multiple goroutines. NewOrgBucketID creates an influxdb.IDGenerator that creates random numbers seeded with seed.  Ascii backslash, comma, and space are manipulated by incrementing. Typically, seed with `time.Now().UnixNano()` Seed allows one to override the current seed. Typically, this override is done for tests. ID generates an ID that does not have backslashes, commas, or spaces. these bytes must be remove here to prevent the need to escape/unescape.  See the models package for additional detail.    \     ,     " "/Users/austinjaybecker/projects/abeck-go-testing/rand/token.goURLEncoding TODO: rename to token.go TokenGenerator implements platform.TokenGenerator. NewTokenGenerator creates an instance of an platform.TokenGenerator. Token returns a new string token of size t.size./Users/austinjaybecker/projects/abeck-go-testing/resource/Users/austinjaybecker/projects/abeck-go-testing/resource/noop/Users/austinjaybecker/projects/abeck-go-testing/resource/noop/resource_logger.go/Users/austinjaybecker/projects/abeck-go-testing/resource/resource.go"put" Package resource defines an interface for recording changes to InfluxDB resources. A resource is an entity in our system, e.g. an organization, task or bucket. A change includes the creation, update or deletion of a resource. Logger records changes to resources. Log a change to a resource. Change to a resource. Type of change. ResourceID of the changed resource. ResourceType that was changed. OrganizationID of the organization owning the changed resource. UserID of the user changing the resource. ResourceBody after the change. Time when the resource was changed. Type of  change. Create a resource. Put a resource. Update a resource. Delete a resource/Users/austinjaybecker/projects/abeck-go-testing/scraper.goscraper target not found"scraper target not found""ListTargets""AddTarget""GetTargetByID""RemoveTarget""UpdateTarget"json:"allowInsecure,omitempty"`json:"allowInsecure,omitempty"`json:"ids"`json:"ids"` ErrScraperTargetNotFound is the error msg for a missing scraper target. ops for ScraperTarget Store ScraperTarget is a target to scrape ScraperTargetStoreService defines the crud service for ScraperTarget. ScraperTargetFilter represents a set of filter that restrict the returned results. ScraperType defines the scraper methods. Scraper types PrometheusScraperType parses metrics from a prometheus endpoint. ValidScraperType returns true is the type string is valid/Users/austinjaybecker/projects/abeck-go-testing/secret/Users/austinjaybecker/projects/abeck-go-testing/secret/http_client.godecodeSecretKeydecodeSecretValueencodeSecretKeyencodeSecretValuenewSecretsResponsesecretBucketsecretsDeleteBodysecretsResponseload secret is not implemented for http"load secret is not implemented for http"put secret is not implemented for http"put secret is not implemented for http"/api/v2/orgs/%s/secrets"/api/v2/orgs/%s/secrets"put secrets is not implemented for http"put secrets is not implemented for http"/api/v2/orgs/%s/secrets/delete"/api/v2/orgs/%s/secrets/delete" LoadSecret is not implemented for http PutSecret is not implemented for http. GetSecretKeys get all secret keys mathing an org ID via HTTP. PutSecrets is not implemented for http. PatchSecrets will update the existing secret with new via http. DeleteSecret removes a single secret via HTTP.idLookupKeyhandleGetSecretshandlePatchSecretshandleDeleteSecretsdecodeOrgID/Users/austinjaybecker/projects/abeck-go-testing/secret/http_server.go/delete"/delete" NewHandler creates a new handler for the secret service TODO: this shouldn't be a post to delete handleGetSecrets is the HTTP handler for the GET /api/v2/orgs/:id/secrets route. handleGetPatchSecrets is the HTTP handler for the PATCH /api/v2/orgs/:id/secrets route. handleDeleteSecrets is the HTTP handler for the DELETE /api/v2/orgs/:id/secrets route./Users/austinjaybecker/projects/abeck-go-testing/secret/middleware_auth.go AuthedSvc wraps a influxdb.AuthedSvc and authorizes actions NewAuthedService constructs an instance of an authorizing secret service./Users/austinjaybecker/projects/abeck-go-testing/secret/middleware_logging.gofailed to load secret"failed to load secret"secret load"secret load"failed to get secret keys"failed to get secret keys"secret get keys"secret get keys"failed to put secret"failed to put secret"secret put"secret put"failed to put secrets"failed to put secrets"secret puts"secret puts"failed to patch secret"failed to patch secret"secret patch"secret patch"failed to delete secret"failed to delete secret"secret delete"secret delete" Logger is a logger service middleware for secrets NewLogger returns a logging service middleware for the User Service./Users/austinjaybecker/projects/abeck-go-testing/secret/middleware_metrics.goload_secret"load_secret"get_secret_keys"get_secret_keys"put_secret"put_secret"put_secrets"put_secrets"patch_secrets"patch_secrets"delete_secret"delete_secret" SecreteService is a metrics middleware system for the secret service NewMetricService creates a new secret metrics middleware/Users/austinjaybecker/projects/abeck-go-testing/secret/service.go NewService creates a new service implementation for secrets put secretes expects to replace all existing secretes/Users/austinjaybecker/projects/abeck-go-testing/secret/storage.gosecret not foundprovided key is too short to contain an ID (please report this error)"provided key is too short to contain an ID (please report this error)" Storage is a store translation layer between the data storage unit and the service layer. NewStore creates a new storage system GetSecret Returns the value of a secret ListSecrets returns a list of secret keys We've reached the end of the keyspace for the provided orgID PutSecret sets a secret in the db. DeleteSecret removes a secret for the db store the secret value base64 encoded so that it's marginally better than plaintext/Users/austinjaybecker/projects/abeck-go-testing/secret.go"secret not found"secret: "secret: " ErrSecretNotFound is the error msg for a missing secret. SecretService a service for storing and retrieving secrets. SecretField contains a key string, and value pointer. String returns the key of the secret. MarshalJSON implement the json marshaler interface. UnmarshalJSON implement the json unmarshaler interface./Users/austinjaybecker/projects/abeck-go-testing/semaphore.gottlownership not acquired"ownership not acquired" ErrNoAcquire is returned when it was not possible to acquire ownership of the semaphore. DefaultLeaseTTL is used when a specific lease TTL is not requested. A Semaphore provides an API for requesting ownership of an expirable semaphore. Acquired semaphores have an expiration. If they're not released or kept alive during this period then they will expire and ownership of the semaphore will be lost. TODO(edd): add AcquireTTL when needed. It should block. TryAcquire attempts to acquire ownership of the semaphore. TryAcquire must not block. Failure to get ownership of the semaphore should be signalled to the caller via the return of the ErrNoAcquire error. A Lease represents ownership over a semaphore. It gives the owner the ability to extend ownership over the semaphore or release ownership of the semaphore. TTL returns the duration of time remaining before the lease expires. Release terminates ownership of the semaphore by revoking the lease. KeepAlive extends the lease back to the original TTL. NopSemaphore is a Semaphore that always hands out leases./Users/austinjaybecker/projects/abeck-go-testing/session/Users/austinjaybecker/projects/abeck-go-testing/session/errors.goErrUnauthorizedWithTokenGeneratorpermissionFromMappingsessionIDsessionIndexKeystoreIndexstorePrefix ErrUnauthorized when a session request is unauthorized usually due to password mismatch/Users/austinjaybecker/projects/abeck-go-testing/session/http_server.gopasswordsSvc Prefix is necessary to mount the router as a resource handler SignInResourceHandler allows us to return 2 different resource handler for the appropriate mounting location SignOutResourceHandler allows us to return 2 different resource handler/Users/austinjaybecker/projects/abeck-go-testing/session/middleware_logging.gofailed to session find"failed to session find"session find"session find"failed to session expire"failed to session expire"session expire"session expire"failed to session create"failed to session create"session create"session create"failed to session renew"failed to session renew"session renew"session renew" SessionLogger is a logger service middleware for sessions NewSessionLogger returns a logging service middleware for the User Service. FindSession calls the underlying session service and logs the results of the request ExpireSession calls the underlying session service and logs the results of the request CreateSession calls the underlying session service and logs the results of the request RenewSession calls the underlying session service and logs the results of the request/Users/austinjaybecker/projects/abeck-go-testing/session/middleware_metrics.gofind_session"find_session"expire_session"expire_session"create_session"create_session"renew_session"renew_session" SessionMetrics is a metrics middleware system for the session service NewSessionMetrics creates a new session metrics middleware FindSession calls the underlying session service and tracks RED metrics for the call ExpireSession calls the underlying session service and tracks RED metrics for the call CreateSession calls the underlying session service and tracks RED metrics for the call RenewSession calls the underlying session service and tracks RED metrics for the call/Users/austinjaybecker/projects/abeck-go-testing/session/service.gopmssession is nil"session is nil" Service implements the influxdb.SessionService interface and handles communication between session and the necessary user and urm services ServiceOption is a functional option for configuring a *Service WithSessionLength configures the length of the session with the provided duration when the resulting option is called on a *Service. WithIDGenerator overrides the default ID generator with the one provided to this function when called on a *Service WithTokenGenerator overrides the default token generator with the one NewService creates a new session service WithMaxPermissionFunc sets the useAuthorizationsForMaxPermissions function which can trigger whether or not max permissions uses the users authorizations to derive maximum permissions. FindSession finds a session based on the session key TODO: We want to be able to store permissions in the session but the contract provided by urm's doesn't give us enough information to quickly repopulate our session permissions on updates so we are required to pull the permissions every time we find the session. ExpireSession removes a session from the system CreateSession for now we are not storing the permissions because we need to pull them every time we find so we might as well keep the session stored small RenewSession update the sessions expiration time if we got 100 mappings we probably need to pull more pages account for paginated results/Users/austinjaybecker/projects/abeck-go-testing/session/storage.gosessionBytessessionsv2/"sessionsv2/"sessionsindexv2/"sessionsindexv2/"session not found NewStorage creates a new storage system FindSessionByKey use a given key to retrieve the stored session FindSessionByID use a provided id to retrieve the stored session CreateSession creates a new session create session use a minute time just so the session will expire if we fail to set the expiration later create index RefreshSession updates the expiration time of a session. no need to recreate the session if we aren't extending the expiration DeleteSession removes the session and index from storage/Users/austinjaybecker/projects/abeck-go-testing/session.go"session not found""FindSession""ExpireSession""CreateSession""RenewSession"json:"expiresAt"`json:"expiresAt"` ErrSessionNotFound is the error messages for a missing sessions. ErrSessionExpired is the error message for expired sessions. RenewSessionTime is the the time to extend session, currently set to 5min. DefaultSessionLength is the default session length on initial creation. OpFindSession represents the operation that looks for sessions. OpExpireSession represents the operation that expires sessions. OpCreateSession represents the operation that creates a session for a given user. OpRenewSession = "RenewSession" SessionAuthorizationKind defines the type of authorizer Session is a user session. ID is only required for auditing purposes. Expired returns an error if the session is expired. PermissionSet returns the set of permissions associated with the session. Identifier returns the sessions ID and is used for auditing. EphemeralAuth generates an Authorization that is not stored but at the user's max privs. SessionService represents a service for managing user sessions. TODO: update RenewSession to take a ID instead of a session. By taking a session object it could be confused to update more things about the session/Users/austinjaybecker/projects/abeck-go-testing/snowflake/Users/austinjaybecker/projects/abeck-go-testing/snowflake/id.goErrGlobalIDBadValGlobalMachineIDSetGlobalMachineIDWithMachineIDglobalmachineIDglobalID must be a number between (inclusive) 0 and 1023"globalID must be a number between (inclusive) 0 and 1023" ErrGlobalIDBadVal means that the global machine id value wasn't properly set. SetGlobalMachineID returns the global machine id.  This number is limited to a number between 0 and 1023 inclusive. GlobalMachineID returns the global machine id.  This number is limited to a number between 0 and 1023 inclusive. NewDefaultIDGenerator returns an *IDGenerator that uses the currently set global machine ID. If you change the global machine id, it will not change the id in any generators that have already been created. IDGenerator holds the ID generator. IDGeneratorOp is an option for an IDGenerator. WithMachineID uses the low 12 bits of machineID to set the machine ID for the snowflake ID. NewIDGenerator returns a new IDGenerator.  Optionally you can use an IDGeneratorOp. to use a specific Generator ID returns the next platform.ID from an IDGenerator./Users/austinjaybecker/projects/abeck-go-testing/source/Users/austinjaybecker/projects/abeck-go-testing/source/bucket.gogithub.com/influxdata/influxdb/v2/http/influxdb"github.com/influxdata/influxdb/v2/http/influxdb"self source type not implemented"self source type not implemented"unsupported source type %s"unsupported source type %s" NewBucketService creates a bucket service from a source. TODO(fntlnz): this is supposed to call a bucket service directly locally, we are letting it err for now since we have some refactoring to do on how services are instantiated/Users/austinjaybecker/projects/abeck-go-testing/source/query.go NewQueryService creates a bucket service from a source. TODO(fntlnz): this is supposed to call a query service directly locally, This is an influxd that calls another influxd, the query path is /v1/query - in future /v2/query it basically is the same as Self but on an external influxd. This is an InfluxDB 1.7 source, which supports both InfluxQL and Flux queries/Users/austinjaybecker/projects/abeck-go-testing/source.go"v2""DefaultSource""FindSourceByID""FindSources""CreateSource""UpdateSource""DeleteSource"json:"metaURL,omitempty"`json:"metaURL,omitempty"` ErrSourceNotFound is an error message when a source does not exist. SourceType is a string for types of sources. V2SourceType is an InfluxDBv2 type. V1SourceType is an InfluxDBv1 type. SelfSourceType is the source hosting the UI. Source is an external Influx with time series data. TODO(desa): do we still need default? TODO(desa): do sources belong OrganizationID is the organization ID that resource belongs to Type specifies which kinds of source (enterprise vs oss vs 2.0) InsecureSkipVerify as true means any certificate presented by the source is accepted V1SourceFields are the fields for connecting to a 1.0 source (oss or enterprise) SourceFields is used to authorize against an influx 2.0 source. Token is the 2.0 authorization token associated with a source ops for sources. SourceService is a service for managing sources. DefaultSourceFindOptions are the default find options for sources SourceUpdate represents updates to a source. Apply applies an update to a source./Users/austinjaybecker/projects/abeck-go-testing/status.goinvalid status: must be %v or %v"invalid status: must be %v or %v" Status defines if a resource is active or inactive. Active status means that the resource can be used. Inactive status means that the resource cannot be used. Valid determines if a Status value matches the enum. Ptr returns the pointer of that status./Users/austinjaybecker/projects/abeck-go-testing/storage/Users/austinjaybecker/projects/abeck-go-testing/storage/bucket_service.goErrEngineClosedUnable to cleanup bucket after create failed"Unable to cleanup bucket after create failed" BucketService wraps an existing influxdb.BucketService implementation. BucketService ensures that when a bucket is deleted, all stored data associated with the bucket is either removed, or marked to be removed via a future compaction. NewBucketService returns a new BucketService for the provided EngineSchema, which typically will be an Engine. The data is dropped first from the storage engine. If this fails for any reason, then the bucket will still be available in the future to retrieve the orgID, which is needed for the engine./Users/austinjaybecker/projects/abeck-go-testing/storage/config.goprecreatorgithub.com/influxdata/influxdb/v2/v1/services/precreator"github.com/influxdata/influxdb/v2/v1/services/precreator"github.com/influxdata/influxdb/v2/v1/services/retention"github.com/influxdata/influxdb/v2/v1/services/retention" Config holds the configuration for an Engine. NewConfig initialises a new config for an Engine./Users/austinjaybecker/projects/abeck-go-testing/storage/engine.gocutoffconsistencyLevelretErrnewDBImultierrgithub.com/influxdata/influxdb/v2/tsdb/engine"github.com/influxdata/influxdb/v2/tsdb/engine"github.com/influxdata/influxdb/v2/tsdb/index/inmem"github.com/influxdata/influxdb/v2/tsdb/index/inmem""go.uber.org/multierr"engine is closed"engine is closed"WritePointsRequestAddPointWriteStatisticsPointWriteReqPointWriteReqLocalSubWriteOKSubWriteDropsubPointsAddWriteSubscriberWritePointsPrivilegedwriteToShardNewPointsWriterClose() called on already-closed engine"Close() called on already-closed engine"error closing shard precreator service: %w"error closing shard precreator service: %w"error closing retention service: %w"error closing retention service: %w"error closing TSDB store: %w"error closing TSDB store: %w"error closing points writer: %w"error closing points writer: %w"bucket dbi for %q not found during restore"bucket dbi for %q not found during restore"bucket must have 1 retention policy; attempting to restore %d retention policies"bucket must have 1 retention policy; attempting to restore %d retention policies" ErrEngineClosed is returned when a caller attempts to use the engine while it's closed. ErrNotImplemented is returned for APIs that are temporarily not implemented. closing returns the zero value when the engine is shutting down. Option provides a set NewEngine initialises a new storage engine, including a series file, index and TSM engine. Copy TSDB configuration. WithLogger sets the logger on the Store. It must be called before Open. Open opens the store and all underlying resources. It returns an error if any of the underlying systems fail to open. Already open EnableCompactions allows the series file, index, & underlying engine to compact. DisableCompactions disables compactions in the series file, index, & engine. Close closes the store and all underlying resources. It returns an error if any of the underlying systems fail to close. Unusual if an engine is closed more than once, so note it. Already closed WritePoints writes the provided points to the engine. The Engine expects all points to have been correctly validated by the caller. However, WritePoints will determine if any tag key-pairs are missing, or if there are any field type conflicts. Rosalie was here lockdown 2020 Appropriate errors are returned in those cases.TODO - remember to add back unicode validation... A value of zero ensures the ShardGroupDuration is adjusted to an appropriate value based on the specified   duration DeleteBucket deletes an entire bucket from the storage engine. DeleteBucketRange deletes an entire range of data from the storage engine. DeleteBucketRangePredicate deletes data within a bucket from the storage engine. Any data deleted must be in [min, max], and the key must match the predicate if provided. Replace KV store data and remove all existing shard data. Create new shards based on the restored KV data. Generate shard ID mapping. Update data. Create shards. Path returns the path of the engine's base directory.ShardMappingMapPoint/Users/austinjaybecker/projects/abeck-go-testing/storage/flux/Users/austinjaybecker/projects/abeck-go-testing/storage/flux/reader.goGroupCursorErroraggregateCountGroupsIntegeraggregateFirstGroupsBooleanaggregateFirstGroupsFloataggregateFirstGroupsIntegeraggregateFirstGroupsStringaggregateFirstGroupsUnsignedaggregateLastGroupsBooleanaggregateLastGroupsFloataggregateLastGroupsIntegeraggregateLastGroupsStringaggregateLastGroupsUnsignedaggregateMaxGroupsFloataggregateMaxGroupsIntegeraggregateMaxGroupsUnsignedaggregateMinGroupsFloataggregateMinGroupsIntegeraggregateMinGroupsUnsignedaggregateSumGroupsFloataggregateSumGroupsIntegeraggregateSumGroupsUnsignedbooleanAggregateMethodbooleanEmptyWindowSelectorTablebooleanGroupTablebooleanTablebooleanWindowSelectorTablebooleanWindowTablecolReaderconvertGroupModedefaultGroupKeyForSeriesdefaultMaxLengthForTagsCachedetermineAggregateMethoddetermineBooleanAggregateMethoddetermineFloatAggregateMethoddetermineIntegerAggregateMethoddetermineStringAggregateMethoddetermineTableColsForGroupdetermineTableColsForSeriesdetermineTableColsForWindowAggregatedetermineUnsignedAggregateMethodfilterIteratorfloatAggregateMethodfloatEmptyWindowSelectorTablefloatGroupTablefloatTablefloatWindowSelectorTablefloatWindowTablegetColumnValuesgroupIteratorgroupKeyForGroupgroupKeyForWindowintegerAggregateMethodintegerEmptyWindowSelectorTableintegerGroupTableintegerTableintegerWindowSelectorTableintegerWindowTableisAggregateCountisSelectornewBooleanEmptyWindowSelectorTablenewBooleanGroupTablenewBooleanTablenewBooleanWindowSelectorTablenewBooleanWindowTablenewFloatEmptyWindowSelectorTablenewFloatGroupTablenewFloatTablenewFloatWindowSelectorTablenewFloatWindowTablenewIntegerEmptyWindowSelectorTablenewIntegerGroupTablenewIntegerTablenewIntegerWindowSelectorTablenewIntegerWindowTablenewStringEmptyWindowSelectorTablenewStringGroupTablenewStringTablenewStringWindowSelectorTablenewStringWindowTablenewTablenewTagsCachenewUnsignedEmptyWindowSelectorTablenewUnsignedGroupTablenewUnsignedTablenewUnsignedWindowSelectorTablenewUnsignedWindowTablesplitWindowsstartColIdxstopColIdxstorageTablestoreReaderstringAggregateMethodstringEmptyWindowSelectorTablestringGroupTablestringTablestringWindowSelectorTablestringWindowTabletagKeysIteratortagValuesIteratortagsCacheunsignedAggregateMethodunsignedEmptyWindowSelectorTableunsignedGroupTableunsignedTableunsignedWindowSelectorTableunsignedWindowTablevalueColIdxvalueColIdxWithoutTimewindowAggregateIteratorwindowTableRowwindowTableSplitterstartColumnstopColumnlrumaxLengthgetBoundsFromCachereplaceBoundscreateBoundsgetTagFromCachetouchOrReplaceTagmaintainLRUcreateTaghandleReadtypedCurbndsgihasTimeColwaiaggKindtableFnfillValuevalueIdxunsigned"unsigned"schema collision: cannot group %s and %s types together"schema collision: cannot group %s and %s types together"MarshalAnycolBufscancelledusedisCancelledallocateBufferreadTagsappendTagsappendBoundscloseDonetoArrowBufferunreachable: %T"unreachable: %T"cannot have group mode none with group key values"cannot have group mode none with group key values"AggregateTypeNoneadvanceCursorAggregate_AggregateType_valueunknown aggregate type %q"unknown aggregate type %q"GroupNoneinvalid group mode: "invalid group mode: "AggregateTypeMinAggregateTypeMaxAggregateTypeFirstAggregateTypeLastnextTSidxInArrcreateNextBufferTimesgetWindowBoundsFornextAtisInWindownextBuffermergeValuesrangeStartrangeStopwindowStopstartTimesstopTimesstartStopTimesarrowBuilder GroupCursorError is returned when two different cursor types are read for the same table. NewReader returns a new storageflux reader Setup read request these resources must be closed if not nil on return no data for series key + field combination table owns these resources and is responsible for closing them aggregates remove the _time column The group without aggregate or with selector (min, max, first, last) case: _start, _stop, _time, _value + tags The group aggregate case: Only the group keys + _value are needed. Note that `groupKey` will contain _start, _stop, plus any group columns specified. _start and _stop will always be in the first two slots, see: groupKeyForGroup() For the group aggregate case the output does not contain a _time column. Also note that if in the future we will add support for mean, then it should also fall onto this branch. No matter this has aggregate, selector, or neither, the first two columns are always _start and _stop For the group without aggregate or with selector case: Aggregate has no _time From now on, only include group keys that are not _start and _stop. which are already included as the first two columns This highly depends on the implementation of groupKeyForGroup() which put _start and _stop into the first two slots. the starting columns index for other group key columns is 3 (1+j) isSelector returns true if given a procedure kind that represents a selector operator. Note hasTimeCol == true means that aggregateWindow() was called. Because aggregateWindow() ultimately removes empty tables we don't bother creating them here. Add the _start and _stop columns that come from storage. Construct the table and add to the reference count so we can free the table later. Release the references to the arrays held by the builder.TableBuffergetTimeColumnIndex/Users/austinjaybecker/projects/abeck-go-testing/storage/flux/table.gen.gostartTstopTsubEverystartBstopBappendNullappendValuetimestampsNewIntMakeDurationMaxPointsPerBlockAggregateTypeCountunsupported for aggregate count: Float"unsupported for aggregate count: Float"AggregateTypeSumunknown/unimplemented aggregate type: %v"unknown/unimplemented aggregate type: %v"unsupported for aggregate count: Unsigned"unsupported for aggregate count: Unsigned"unsupported for aggregate count: String"unsupported for aggregate count: String"unsupported for aggregate sum: String"unsupported for aggregate sum: String"unsupported for aggregate min: String"unsupported for aggregate min: String"unsupported for aggregate max: String"unsupported for aggregate max: String"unsupported for aggregate count: Boolean"unsupported for aggregate count: Boolean"unsupported for aggregate sum: Boolean"unsupported for aggregate sum: Boolean"unsupported for aggregate min: Boolean"unsupported for aggregate min: Boolean"unsupported for aggregate max: Boolean"unsupported for aggregate max: Boolean" Source: table.gen.go.tmpl *********** Float *********** Retrieve the buffer for the data to avoid allocating additional slices. If the buffer is still being used because the references were retained, then we will allocate a new buffer. window table createNextBufferTimes will read the timestamps from the array cursor and construct the values for the next buffer. There are no more windows when the start time is greater than or equal to the stop time. Create a buffer with the buffer size. TODO(jsternberg): Calculate the exact size with max points as the maximum. Retrieve the next buffer so we can copy the timestamps. Copy over the timestamps from the next buffer and adjust times for the boundaries. nextAt will retrieve the next value that can be used with the given stop timestamp. If no values can be used with the timestamp, it will return the default value and false. isInWindow will check if the given time at stop can be used within the window stop time for ts. The ts may be a truncated stop time because of a restricted boundary while stop will be the true stop time returned by storage. This method checks if the stop time is a valid stop time for that interval. This calculation is different from the calculation of the window itself. For example, for a 10 second window that starts at 20 seconds, we would include points between [20, 30). The stop time for this interval would be 30, but because the stop time can be truncated, valid stop times range from anywhere between (20, 30]. The storage engine will always produce 30 as the end time but we may have truncated the stop time because of the boundary and this is why we are checking for this range instead of checking if the two values are equal. nextBuffer will ensure the array cursor is filled and will return true if there is at least one value that can be read from it. Discard the current array cursor if we have exceeded it. Retrieve the next array cursor if needed. appendValues will scan the timestamps and append values that match those timestamps from the buffer. Create the timestamps for the next window. This table implementation will not have any empty windows. This table implementation may contain empty windows in addition to non-empty windows. The first window should start at the beginning of the time range. If the current timestamp falls within the current window, append the value to the builder, otherwise append a null value. If the current array is non-empty and has been read in its entirety, call Next(). The last window should stop at the end of the time range. group table For group aggregates, we will try to get all the series and all table buffers within those series all at once and merge them into one row when this advance() function is first called. At the end of this process, t.advanceCursor() already returns false and t.cur becomes nil. But we still need to return true to indicate that there is data to be returned. The second time when we call this advance(), t.cur is already nil, so we directly return false. handle the group without aggregate case determineFloatAggregateMethod returns the method for aggregating returned points within the same group. The incoming points are the ones returned for each series and the method returned here will aggregate the aggregates. For group count and sum, the timestamp here is always math.MaxInt64. their final result does not contain _time, so this timestamp value can be anything and it won't matter. TODO(sgc): error or skip? *********** Integer *********** determineIntegerAggregateMethod returns the method for aggregating *********** Unsigned *********** determineUnsignedAggregateMethod returns the method for aggregating *********** String *********** determineStringAggregateMethod returns the method for aggregating *********** Boolean *********** determineBooleanAggregateMethod returns the method for aggregating/Users/austinjaybecker/projects/abeck-go-testing/storage/flux/table.goStoreInt32LoadInt32CompareAndSwapInt32table already used"table already used"CheckColTypeNewFloatNewUintNewBoolgo:generate env GO111MODULE=on go run github.com/benbjohnson/tmpl -data=@types.tmpldata table.gen.go.tmpl cache of the tags on the current series. len(tags) == len(colMeta) Mark this table as having been used. If this doesn't succeed, then this has already been invoked somewhere else. If an error occurred during initialization, that is returned here. Mark the table as having been used. If this has already been done, then nothing needs to be done. allocateBuffer will allocate a suitable buffer for the table implementations to use. If the existing buffer is not used anymore, then it may be reused. The allocated buffer can be accessed at colBufs or through the returned colReader. The current buffer is still being used so we should generate a new one. readTags populates b.tags with the provided tags In the case of group aggregate, tags that are not referenced in group() are not included in the result, but readTags () still get a complete tag list. Here is just to skip the tags that should not present in the result. appendTags fills the colBufs for the tag columns with the tag value. appendBounds fills the colBufs for the time bounds/Users/austinjaybecker/projects/abeck-go-testing/storage/flux/tags_cache.gocarrcontainer/list"container/list""github.com/apache/arrow/go/arrow"github.com/apache/arrow/go/arrow/memory"github.com/apache/arrow/go/arrow/memory"NewSliceDataNewInt64DataNewInt64BuilderNewBinaryDataNewBinaryBuilderBinaryTypes defaultMaxLengthForTagsCache is the default maximum number of tags that will be memoized when retrieving tags from the tags cache. startColumn is a special slot for holding the start column. stopColumn is a special slot for holding the stop column. tags holds cached arrays for various tag values. An lru is used to keep track of the least recently used item in the cache so that it can be ejected. An lru is used here because we cannot be certain if tag values are going to be used again and we do not want to retain a reference that may have already been released. This makes an lru a good fit since it will be more likely to eject a value that is not going to be used again than another data structure. The increase in performance by reusing arrays for tag values is dependent on the order of the tags coming out of storage. It is possible that a value will be reused but could get ejected from the cache before it would be reused. The map contains the tag **values** and not the tag keys. An array can get shared among two different tag keys that have the same value. newTagsCache will create a tags cache that will retain the last sz entries. If zero, the default will be used. GetBounds will return arrays that match with the bounds. If an array that is within the cache works with the bounds and can be sliced to the length, a reference to it will be returned. Retrieve the columns from the cache if they exist. If we could not retrieve an array from the cache, create one here outside of the lock. Record that we will need to replace the values in the cache. No need to retrieve the write lock. Return now since we retrieved all values from getBoundsFromCache will return an array of values if the array in the cache is of the appropriate size. This must be called from inside of a lock. If the lengths do not match, but the cached array is less than the desired array, then we can use slice. NewSlice will automatically create a new reference to the passed in array so we do not need to manually retain. replaceBounds will examine the array and replace it if the length of the array is greater than the current array or if there isn't an array in the cache. This must be called from inside of a write lock. The cached value is longer so just keep it. createBounds will create an array of times for the given time with the given length. DO NOT CALL THIS METHOD IN A LOCK. It is slow and will probably cause lock contention. GetTag returns a binary arrow array that contains the value repeated l times. If an array with a length greater than or equal to the length and with the same value exists in the cache, a reference to the data will be retained and returned. Otherwise, the allocator will be used to construct a new column. Attempt to retrieve the array from the cache. The array is not in the cache so create it. getTagFromCache will return an array of values with the specified value at the specified length. If there is no cache entry or the entry is not large enough for the specified length, then this returns false. Slice will automatically create a new reference to the touchOrReplaceTag will update the LRU cache to have the value specified by the array as the most recently used entry. If the cache entry does not exist or the current array in the cache is shorter than this one, it will replace the array. If the array in the cache is longer to or equal to the current tag, then do not touch it. Retain this array again and release our previous reference to the other array. Move this element to the front of the lru. maintainLRU will ensure the lru cache maintains the appropriate length by ejecting the least recently used value from the cache until the cache is the appropriate size. This function must be called from inside of a lock. createTag will create a new array for a tag with the given length. Release will release all references to cached tag columns./Users/austinjaybecker/projects/abeck-go-testing/storage/flux/window.gowtsarrsstartIdxstopIdxNewEmptyTablemissing %q column from window splitter"missing %q column from window splitter"%q column must be of type time"%q column must be of type time"table already read"table already read"unimplemented column type: %s"unimplemented column type: %s" splitWindows will split a windowTable by creating a new table from each row and modifying the group key to use the start and stop values from that row. Retrieve the start and stop columns for splitting the windows. Iterate through each time to produce a table using the start and stop values. Rewrite the group key using the new time. Produce an empty table if the value is null and this is a selector. Produce a slice for each column into a new table buffer. Wrap these into a single table and execute. getColumnValues returns the array from the column reader as an array.Interface./Users/austinjaybecker/projects/abeck-go-testing/storage/mocks/Users/austinjaybecker/projects/abeck-go-testing/storage/mocks/EngineSchema.goMockEngineSchemaMockEngineSchemaMockRecorderNewMockEngineSchema"UpdateBucketRetentionPeriod" Source: github.com/influxdata/influxdb/v2/storage (interfaces: EngineSchema) MockEngineSchema is a mock of EngineSchema interface MockEngineSchemaMockRecorder is the mock recorder for MockEngineSchema NewMockEngineSchema creates a new mock instance UpdateBucketRetentionPeriod mocks base method UpdateBucketRetentionPeriod indicates an expected call of UpdateBucketRetentionPeriod/Users/austinjaybecker/projects/abeck-go-testing/storage/points_writer.gopointswriterlogging bucket not found: %q"logging bucket not found: %q"write_errors"write_errors" PointsWriter describes the ability to write points into a storage engine. LoggingPointsWriter wraps an underlying points writer but writes logs to another bucket when an error occurs. Wrapped points writer. Errored writes from here will be logged. Service used to look up logging bucket. Name of the bucket to log to. WritePoints writes points to the underlying PointsWriter. Logs on error. Write to underlying writer and exit immediately if successful. Attempt to lookup log bucket. Log error to bucket. WritePoints writes the points to the underlying PointsWriter. Large write, empty buffer. Write directly from p to avoid copy. Available returns how many models.Points are unused in the buffer. Buffered returns the number of models.Points that have been written into the current buffer. Flush writes any buffered data to the underlying PointsWriter./Users/austinjaybecker/projects/abeck-go-testing/storage/reads/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/aggregate_resultset.goBooleanEmptyArrayCursorEvalExprBoolExprHasKeyFloatEmptyArrayCursorGroupOptionGroupOptionNilSortLoHasFieldValueKeyIntegerEmptyArrayCursorIsTrueBooleanLiteralKeyMergerModuloNewFilteredResultSetNewGroupResultSetNewLimitSeriesCursorNewWindowAggregateResultSetNilSortHiNilSortLoNodeToExprNodeVisitorPredicateToExprStringResultSetToLineProtocolRewriteExprRemoveFieldValueSeriesRowStringEmptyArrayCursorUnsignedEmptyArrayCursorWalkChildrenWalkNodeWindowStartWindowStoparrayCursorTypeastExprbooleanArrayFilterCursorbooleanEmptyArrayCursorbooleanLimitArrayCursorbooleanMultiShardArrayCursorbooleanWindowCountArrayCursorbooleanWindowFirstArrayCursorbooleanWindowLastArrayCursorconvertNsecscursorContextcursorToLineProtocolevalExprfieldReffloatArrayFilterCursorfloatEmptyArrayCursorfloatLimitArrayCursorfloatMultiShardArrayCursorfloatWindowCountArrayCursorfloatWindowFirstArrayCursorfloatWindowLastArrayCursorfloatWindowMaxArrayCursorfloatWindowMeanArrayCursorfloatWindowMinArrayCursorfloatWindowSumArrayCursorgroupByCursorgroupByNextGroupgroupNoneCursorgroupNoneNextGroupgroupResultSethasRefsintegerArrayFilterCursorintegerEmptyArrayCursorintegerLimitArrayCursorintegerMultiShardArrayCursorintegerWindowCountArrayCursorintegerWindowFirstArrayCursorintegerWindowLastArrayCursorintegerWindowMaxArrayCursorintegerWindowMeanArrayCursorintegerWindowMinArrayCursorintegerWindowSumArrayCursorlimitSeriesCursormultiShardArrayCursorsmultiShardCursorsnewAggregateArrayCursornewBooleanFilterArrayCursornewBooleanLimitArrayCursornewBooleanWindowCountArrayCursornewBooleanWindowFirstArrayCursornewBooleanWindowLastArrayCursornewFloatFilterArrayCursornewFloatLimitArrayCursornewFloatWindowCountArrayCursornewFloatWindowFirstArrayCursornewFloatWindowLastArrayCursornewFloatWindowMaxArrayCursornewFloatWindowMeanArrayCursornewFloatWindowMinArrayCursornewFloatWindowSumArrayCursornewIntegerFilterArrayCursornewIntegerLimitArrayCursornewIntegerWindowCountArrayCursornewIntegerWindowFirstArrayCursornewIntegerWindowLastArrayCursornewIntegerWindowMaxArrayCursornewIntegerWindowMeanArrayCursornewIntegerWindowMinArrayCursornewIntegerWindowSumArrayCursornewLimitArrayCursornewMultiShardArrayCursorsnewStringFilterArrayCursornewStringLimitArrayCursornewStringWindowCountArrayCursornewStringWindowFirstArrayCursornewStringWindowLastArrayCursornewUnsignedFilterArrayCursornewUnsignedLimitArrayCursornewUnsignedWindowCountArrayCursornewUnsignedWindowFirstArrayCursornewUnsignedWindowLastArrayCursornewUnsignedWindowMaxArrayCursornewUnsignedWindowMeanArrayCursornewUnsignedWindowMinArrayCursornewUnsignedWindowSumArrayCursornewWindowAggregateArrayCursornewWindowCountArrayCursornewWindowFirstArrayCursornewWindowLastArrayCursornewWindowMaxArrayCursornewWindowMeanArrayCursornewWindowMinArrayCursornewWindowSumArrayCursornodeToExprVisitorpredicateExpressionPrinterresultSetsingleValuestringArrayFilterCursorstringEmptyArrayCursorstringLimitArrayCursorstringMultiShardArrayCursorstringWindowCountArrayCursorstringWindowFirstArrayCursorstringWindowLastArrayCursortagsBufferunsignedArrayFilterCursorunsignedEmptyArrayCursorunsignedLimitArrayCursorunsignedMultiShardArrayCursorunsignedWindowCountArrayCursorunsignedWindowFirstArrayCursorunsignedWindowLastArrayCursorunsignedWindowMaxArrayCursorunsignedWindowMeanArrayCursorunsignedWindowMinArrayCursorunsignedWindowSumArrayCursorwindowAggregateResultSetnAggsCursorIteratorsSortKeySeriesTagsValueCondseriesCursorseriesRowarrayCursorseveryDuroffsetDurperiodDuraggregate_window_every"aggregate_window_every"aggregate_type"aggregate_type"attempt to create a windowAggregateResultSet with %v aggregate functions"attempt to create a windowAggregateResultSet with %v aggregate functions"nextArrayCursor The following is an optimization where in the case of a single window, the selector `last` is implemented as a descending array cursor followed by a limit array cursor that selects only the first point, i.e the point with the largest timestamp, from the descending array cursor. assume window was passed in and translate protobuf window to execute.Window nanosecond values were passed in and need to be converted to windows This means to aggregate over whole series for the query's time range See the equivalent method in *resultSet.Stats.seriesRowskmMergeTagKeysMergeKeysnilSortnewSeriesCursorFnnextGroupFnseriesHasPointsgroupNoneSortgroupBySortremappop2allFound/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/array_cursor.gen.goresLenaccrowIdxwindowHasPointstsAccunsupported input type for sum aggregate: %s"unsupported input type for sum aggregate: %s"unsupported for aggregate min: %T"unsupported for aggregate min: %T"unsupported for aggregate max: %T"unsupported for aggregate max: %T"unsupported input type for mean aggregate: %s"unsupported input type for mean aggregate: %s"expected float cursor"expected float cursor"NEXTWINDOWSMaxFloat641.7976931348623157081e+308179769313486231570814527423731704356798070567525844996598917476803157260780028538760589558632766878171540458953514382464234321326889464182768467546703537516986049910576551282076245490090389328944075868508455133942304583236903222948165808559332123348274797826204144723168738177180919299881250404026184124858368-1.7976931348623157081e+308-179769313486231570814527423731704356798070567525844996598917476803157260780028538760589558632766878171540458953514382464234321326889464182768467546703537516986049910576551282076245490090389328944075868508455133942304583236903222948165808559332123348274797826204144723168738177180919299881250404026184124858368expected integer cursor"expected integer cursor"expected unsigned cursor"expected unsigned cursor"expected string cursor"expected string cursor"expected boolean cursor"expected boolean cursor" Source: array_cursor.gen.go.tmpl MaxPointsPerBlock is the maximum number of points in an encoded block in a TSM file. It should match the value in the tsm1 package, but we don't want to import it. ******************** Float Array Cursor Clear buffered timestamps & values if we make it through a cursor. The break above will skip this if a cursor is partially read. Window array cursors assume that every != 0 && every != MaxInt64. Such a cursor will panic in the first case and possibly overflow in the second. enumerate windows new window detected, close the current window do not generate a point for empty windows the output array is full, save the remaining points in the input array in tmp. they will be processed in the next call to Next() start the new window get the next chunk write the final point Integer Array Cursor Unsigned Array Cursor String Array Cursor Boolean Array Cursor/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/array_cursor.goascAggregateTypeMeaninvalid aggregate"invalid aggregate" TODO(sgc): should be validated higher up/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/datatypes/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/datatypes/gen.goAggregate_AggregateType_nameCapabilitiesResponseCapabilityDataTypeBooleanDataTypeFloatDataTypeIntegerDataTypeStringDataTypeUnsignedErrIntOverflowPredicateErrIntOverflowStorageCommonErrInvalidLengthPredicateErrInvalidLengthStorageCommonErrUnexpectedEndOfGroupPredicateErrUnexpectedEndOfGroupStorageCommonFieldTypeBooleanFieldTypeFloatFieldTypeIntegerFieldTypeUndefinedFieldTypeUnsignedFrameTypePointsFrameTypeSeriesHintNoPointsHintNoSeriesHintNoneMeasurementFieldsRequestMeasurementFieldsResponseMeasurementFieldsResponse_FieldTypeMeasurementFieldsResponse_FieldType_nameMeasurementFieldsResponse_FieldType_valueMeasurementFieldsResponse_MessageFieldMeasurementNamesRequestMeasurementTagKeysRequestMeasurementTagValuesRequestNodeTypeParenExpressionNode_Comparison_nameNode_Comparison_valueNode_Logical_nameNode_Logical_valueNode_Type_nameNode_Type_valueNode_UnsignedValueReadGroupRequest_Group_nameReadGroupRequest_Group_valueReadGroupRequest_HintFlagsReadGroupRequest_HintFlags_nameReadGroupRequest_HintFlags_valueReadResponseReadResponse_BooleanPointsFrameReadResponse_DataTypeReadResponse_DataType_nameReadResponse_DataType_valueReadResponse_FloatPointsFrameReadResponse_FrameReadResponse_FrameTypeReadResponse_FrameType_nameReadResponse_FrameType_valueReadResponse_Frame_BooleanPointsReadResponse_Frame_FloatPointsReadResponse_Frame_GroupReadResponse_Frame_IntegerPointsReadResponse_Frame_SeriesReadResponse_Frame_StringPointsReadResponse_Frame_UnsignedPointsReadResponse_GroupFrameReadResponse_IntegerPointsFrameReadResponse_SeriesFrameReadResponse_StringPointsFrameReadResponse_UnsignedPointsFrameStringValuesResponseencodeVarintPredicateencodeVarintStorageCommonfileDescriptor_715e4bf4cdf1f73dfileDescriptor_87cba9804b436f42isReadResponse_Frame_DataskipPredicateskipStorageCommonsovPredicatesovStorageCommonsozPredicatesozStorageCommonxxx_messageInfo_Aggregatexxx_messageInfo_CapabilitiesResponsexxx_messageInfo_Capabilityxxx_messageInfo_Durationxxx_messageInfo_MeasurementFieldsRequestxxx_messageInfo_MeasurementFieldsResponsexxx_messageInfo_MeasurementFieldsResponse_MessageFieldxxx_messageInfo_MeasurementNamesRequestxxx_messageInfo_MeasurementTagKeysRequestxxx_messageInfo_MeasurementTagValuesRequestxxx_messageInfo_Nodexxx_messageInfo_Predicatexxx_messageInfo_ReadFilterRequestxxx_messageInfo_ReadGroupRequestxxx_messageInfo_ReadResponsexxx_messageInfo_ReadResponse_BooleanPointsFramexxx_messageInfo_ReadResponse_FloatPointsFramexxx_messageInfo_ReadResponse_Framexxx_messageInfo_ReadResponse_GroupFramexxx_messageInfo_ReadResponse_IntegerPointsFramexxx_messageInfo_ReadResponse_SeriesFramexxx_messageInfo_ReadResponse_StringPointsFramexxx_messageInfo_ReadResponse_UnsignedPointsFramexxx_messageInfo_ReadWindowAggregateRequestxxx_messageInfo_StringValuesResponsexxx_messageInfo_Tagxxx_messageInfo_TagKeysRequestxxx_messageInfo_TagValuesRequestxxx_messageInfo_TimestampRangexxx_messageInfo_Windowgo:generate protoc -I ../../../internal -I . --plugin ../../../scripts/protoc-gen-gogofaster --gogofaster_out=Mgoogle/protobuf/empty.proto=github.com/gogo/protobuf/types,Mgoogle/protobuf/any.proto=github.com/gogo/protobuf/types,plugins=grpc:. storage_common.proto predicate.protoIntegerPointsUnsignedPointsFeaturesCapsGetDataGetGroupGetSeriesGetFloatPointsGetIntegerPointsGetUnsignedPointsGetBooleanPointsGetStringPointsBooleanPointsFloatPointsStringPoints/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/datatypes/hintflags.goenumsEnumValueMapinfluxdata.platform.storage.ReadRequest_HintFlags"influxdata.platform.storage.ReadRequest_HintFlags"HINT_NONE"HINT_NONE"/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/datatypes/key_types.go/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/datatypes/predicate.pb.goencoding_binarymath_bitsLOGICAL_EXPRESSION"LOGICAL_EXPRESSION"COMPARISON_EXPRESSION"COMPARISON_EXPRESSION"PAREN_EXPRESSION"PAREN_EXPRESSION"TAG_REF"TAG_REF"LITERAL"LITERAL"FIELD_REF"FIELD_REF"EQUAL"EQUAL"NOT_EQUAL"NOT_EQUAL"STARTS_WITH"STARTS_WITH"REGEX"REGEX"NOT_REGEX"NOT_REGEX""LT""LTE""GT""GTE""AND""OR"protobuf:"varint,1,opt,name=node_type,json=nodeType,proto3,enum=influxdata.platform.storage.Node_Type" json:"nodeType"`protobuf:"varint,1,opt,name=node_type,json=nodeType,proto3,enum=influxdata.platform.storage.Node_Type" json:"nodeType"`protobuf:"bytes,2,rep,name=children,proto3" json:"children,omitempty"`protobuf:"bytes,2,rep,name=children,proto3" json:"children,omitempty"`protobuf:"bytes,3,opt,name=string_value,json=stringValue,proto3,oneof" json:"string_value,omitempty"`protobuf:"bytes,3,opt,name=string_value,json=stringValue,proto3,oneof" json:"string_value,omitempty"`protobuf:"varint,4,opt,name=bool_value,json=boolValue,proto3,oneof" json:"bool_value,omitempty"`protobuf:"varint,4,opt,name=bool_value,json=boolValue,proto3,oneof" json:"bool_value,omitempty"`protobuf:"varint,5,opt,name=int_value,json=intValue,proto3,oneof" json:"int_value,omitempty"`protobuf:"varint,5,opt,name=int_value,json=intValue,proto3,oneof" json:"int_value,omitempty"`protobuf:"varint,6,opt,name=uint_value,json=uintValue,proto3,oneof" json:"uint_value,omitempty"`protobuf:"varint,6,opt,name=uint_value,json=uintValue,proto3,oneof" json:"uint_value,omitempty"`protobuf:"fixed64,7,opt,name=float_value,json=floatValue,proto3,oneof" json:"float_value,omitempty"`protobuf:"fixed64,7,opt,name=float_value,json=floatValue,proto3,oneof" json:"float_value,omitempty"`protobuf:"bytes,8,opt,name=regex_value,json=regexValue,proto3,oneof" json:"regex_value,omitempty"`protobuf:"bytes,8,opt,name=regex_value,json=regexValue,proto3,oneof" json:"regex_value,omitempty"`protobuf:"bytes,9,opt,name=tag_ref_value,json=tagRefValue,proto3,oneof" json:"tag_ref_value,omitempty"`protobuf:"bytes,9,opt,name=tag_ref_value,json=tagRefValue,proto3,oneof" json:"tag_ref_value,omitempty"`protobuf:"bytes,10,opt,name=field_ref_value,json=fieldRefValue,proto3,oneof" json:"field_ref_value,omitempty"`protobuf:"bytes,10,opt,name=field_ref_value,json=fieldRefValue,proto3,oneof" json:"field_ref_value,omitempty"`protobuf:"varint,11,opt,name=logical,proto3,enum=influxdata.platform.storage.Node_Logical,oneof" json:"logical,omitempty"`protobuf:"varint,11,opt,name=logical,proto3,enum=influxdata.platform.storage.Node_Logical,oneof" json:"logical,omitempty"`protobuf:"varint,12,opt,name=comparison,proto3,enum=influxdata.platform.storage.Node_Comparison,oneof" json:"comparison,omitempty"`protobuf:"varint,12,opt,name=comparison,proto3,enum=influxdata.platform.storage.Node_Comparison,oneof" json:"comparison,omitempty"`protobuf:"bytes,1,opt,name=root,proto3" json:"root,omitempty"`protobuf:"bytes,1,opt,name=root,proto3" json:"root,omitempty"`influxdata.platform.storage.Node_Type"influxdata.platform.storage.Node_Type"influxdata.platform.storage.Node_Comparison"influxdata.platform.storage.Node_Comparison"influxdata.platform.storage.Node_Logical"influxdata.platform.storage.Node_Logical"influxdata.platform.storage.Node"influxdata.platform.storage.Node"influxdata.platform.storage.Predicate"influxdata.platform.storage.Predicate"predicate.proto"predicate.proto"littleEndianLittleEndianLen64proto: Node: wiretype end group for non-group"proto: Node: wiretype end group for non-group"proto: Node: illegal tag %d (wire type %d)"proto: Node: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field NodeType"proto: wrong wireType = %d for field NodeType"proto: wrong wireType = %d for field Children"proto: wrong wireType = %d for field Children"proto: wrong wireType = %d for field StringValue"proto: wrong wireType = %d for field StringValue"proto: wrong wireType = %d for field BooleanValue"proto: wrong wireType = %d for field BooleanValue"proto: wrong wireType = %d for field IntegerValue"proto: wrong wireType = %d for field IntegerValue"proto: wrong wireType = %d for field UnsignedValue"proto: wrong wireType = %d for field UnsignedValue"proto: wrong wireType = %d for field FloatValue"proto: wrong wireType = %d for field FloatValue"proto: wrong wireType = %d for field RegexValue"proto: wrong wireType = %d for field RegexValue"proto: wrong wireType = %d for field TagRefValue"proto: wrong wireType = %d for field TagRefValue"proto: wrong wireType = %d for field FieldRefValue"proto: wrong wireType = %d for field FieldRefValue"proto: wrong wireType = %d for field Logical"proto: wrong wireType = %d for field Logical"proto: wrong wireType = %d for field Comparison"proto: wrong wireType = %d for field Comparison"proto: Predicate: wiretype end group for non-group"proto: Predicate: wiretype end group for non-group"proto: Predicate: illegal tag %d (wire type %d)"proto: Predicate: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Root"proto: wrong wireType = %d for field Root"proto: unexpected end of group"proto: unexpected end of group" source: predicate.proto Logical operators apply to boolean values and combine to produce a single boolean result.	*Node_StringValue	*Node_BooleanValue	*Node_IntegerValue	*Node_UnsignedValue	*Node_FloatValue	*Node_RegexValue	*Node_TagRefValue	*Node_FieldRefValue	*Node_Logical_	*Node_Comparison_ XXX_OneofWrappers is for the internal use of the proto package. 869 bytes of a gzipped FileDescriptorProto/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/datatypes/storage_common.pb.gof15num1dAtA17j16dAtA19j18baseImapEntrySizebyteLenelementCountpackedLenintStringLenmapkeypostStringIndexmapkeystringLenmapkeymapmsglenpostmsgIndexentryPreIndexmapkeymapvalueGROUP_NONE"GROUP_NONE"GROUP_BY"GROUP_BY"HINT_NO_POINTS"HINT_NO_POINTS"HINT_NO_SERIES"HINT_NO_SERIES"HINT_SCHEMA_ALL_TIME"HINT_SCHEMA_ALL_TIME"NONE"NONE"SUM"SUM"COUNT"COUNT"MIN"MIN"MAX"MAX"FIRST"FIRST"LAST"LAST"MEAN"MEAN"SERIES"SERIES"POINTS"POINTS"FLOAT"FLOAT""INTEGER"UNSIGNED"UNSIGNED"BOOLEAN"BOOLEAN"UNDEFINED"UNDEFINED"protobuf:"bytes,1,opt,name=read_source,json=readSource,proto3" json:"read_source,omitempty"`protobuf:"bytes,1,opt,name=read_source,json=readSource,proto3" json:"read_source,omitempty"`protobuf:"bytes,2,opt,name=range,proto3" json:"range"`protobuf:"bytes,2,opt,name=range,proto3" json:"range"`protobuf:"bytes,3,opt,name=predicate,proto3" json:"predicate,omitempty"`protobuf:"bytes,3,opt,name=predicate,proto3" json:"predicate,omitempty"`protobuf:"bytes,4,rep,name=group_keys,json=groupKeys,proto3" json:"group_keys,omitempty"`protobuf:"bytes,4,rep,name=group_keys,json=groupKeys,proto3" json:"group_keys,omitempty"`protobuf:"varint,5,opt,name=group,proto3,enum=influxdata.platform.storage.ReadGroupRequest_Group" json:"group,omitempty"`protobuf:"varint,5,opt,name=group,proto3,enum=influxdata.platform.storage.ReadGroupRequest_Group" json:"group,omitempty"`protobuf:"bytes,6,opt,name=aggregate,proto3" json:"aggregate,omitempty"`protobuf:"bytes,6,opt,name=aggregate,proto3" json:"aggregate,omitempty"`protobuf:"fixed32,7,opt,name=hints,proto3,casttype=HintFlags" json:"hints,omitempty"`protobuf:"fixed32,7,opt,name=hints,proto3,casttype=HintFlags" json:"hints,omitempty"`protobuf:"varint,1,opt,name=type,proto3,enum=influxdata.platform.storage.Aggregate_AggregateType" json:"type,omitempty"`protobuf:"varint,1,opt,name=type,proto3,enum=influxdata.platform.storage.Aggregate_AggregateType" json:"type,omitempty"`protobuf:"bytes,1,rep,name=frames,proto3" json:"frames"`protobuf:"bytes,1,rep,name=frames,proto3" json:"frames"`protobuf_oneof:"data"`protobuf_oneof:"data"`protobuf:"bytes,7,opt,name=group,proto3,oneof" json:"group,omitempty"`protobuf:"bytes,7,opt,name=group,proto3,oneof" json:"group,omitempty"`protobuf:"bytes,1,opt,name=series,proto3,oneof" json:"series,omitempty"`protobuf:"bytes,1,opt,name=series,proto3,oneof" json:"series,omitempty"`protobuf:"bytes,2,opt,name=float_points,json=floatPoints,proto3,oneof" json:"float_points,omitempty"`protobuf:"bytes,2,opt,name=float_points,json=floatPoints,proto3,oneof" json:"float_points,omitempty"`protobuf:"bytes,3,opt,name=integer_points,json=integerPoints,proto3,oneof" json:"integer_points,omitempty"`protobuf:"bytes,3,opt,name=integer_points,json=integerPoints,proto3,oneof" json:"integer_points,omitempty"`protobuf:"bytes,4,opt,name=unsigned_points,json=unsignedPoints,proto3,oneof" json:"unsigned_points,omitempty"`protobuf:"bytes,4,opt,name=unsigned_points,json=unsignedPoints,proto3,oneof" json:"unsigned_points,omitempty"`protobuf:"bytes,5,opt,name=boolean_points,json=booleanPoints,proto3,oneof" json:"boolean_points,omitempty"`protobuf:"bytes,5,opt,name=boolean_points,json=booleanPoints,proto3,oneof" json:"boolean_points,omitempty"`protobuf:"bytes,6,opt,name=string_points,json=stringPoints,proto3,oneof" json:"string_points,omitempty"`protobuf:"bytes,6,opt,name=string_points,json=stringPoints,proto3,oneof" json:"string_points,omitempty"`protobuf:"bytes,1,rep,name=tag_keys,json=tagKeys,proto3" json:"tag_keys,omitempty"`protobuf:"bytes,1,rep,name=tag_keys,json=tagKeys,proto3" json:"tag_keys,omitempty"`protobuf:"bytes,2,rep,name=partition_key_vals,json=partitionKeyVals,proto3" json:"partition_key_vals,omitempty"`protobuf:"bytes,2,rep,name=partition_key_vals,json=partitionKeyVals,proto3" json:"partition_key_vals,omitempty"`protobuf:"bytes,1,rep,name=tags,proto3" json:"tags"`protobuf:"bytes,1,rep,name=tags,proto3" json:"tags"`protobuf:"varint,2,opt,name=data_type,json=dataType,proto3,enum=influxdata.platform.storage.ReadResponse_DataType" json:"data_type,omitempty"`protobuf:"varint,2,opt,name=data_type,json=dataType,proto3,enum=influxdata.platform.storage.ReadResponse_DataType" json:"data_type,omitempty"`protobuf:"fixed64,1,rep,packed,name=timestamps,proto3" json:"timestamps,omitempty"`protobuf:"fixed64,1,rep,packed,name=timestamps,proto3" json:"timestamps,omitempty"`protobuf:"fixed64,2,rep,packed,name=values,proto3" json:"values,omitempty"`protobuf:"fixed64,2,rep,packed,name=values,proto3" json:"values,omitempty"`protobuf:"varint,2,rep,packed,name=values,proto3" json:"values,omitempty"`protobuf:"varint,2,rep,packed,name=values,proto3" json:"values,omitempty"`protobuf:"bytes,2,rep,name=values,proto3" json:"values,omitempty"`protobuf:"bytes,2,rep,name=values,proto3" json:"values,omitempty"`protobuf:"bytes,1,rep,name=features,proto3" json:"features,omitempty"`protobuf:"bytes,1,rep,name=features,proto3" json:"features,omitempty"`protobuf:"bytes,1,rep,name=caps,proto3" json:"caps,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`protobuf:"bytes,1,rep,name=caps,proto3" json:"caps,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`protobuf:"varint,1,opt,name=start,proto3" json:"start,omitempty"`protobuf:"varint,1,opt,name=start,proto3" json:"start,omitempty"`protobuf:"varint,2,opt,name=end,proto3" json:"end,omitempty"`protobuf:"varint,2,opt,name=end,proto3" json:"end,omitempty"`protobuf:"bytes,1,opt,name=tags_source,json=tagsSource,proto3" json:"tags_source,omitempty"`protobuf:"bytes,1,opt,name=tags_source,json=tagsSource,proto3" json:"tags_source,omitempty"`protobuf:"bytes,4,opt,name=tag_key,json=tagKey,proto3" json:"tag_key,omitempty"`protobuf:"bytes,4,opt,name=tag_key,json=tagKey,proto3" json:"tag_key,omitempty"`protobuf:"bytes,1,rep,name=values,proto3" json:"values,omitempty"`protobuf:"bytes,1,rep,name=values,proto3" json:"values,omitempty"`protobuf:"bytes,1,opt,name=source,proto3" json:"source,omitempty"`protobuf:"bytes,1,opt,name=source,proto3" json:"source,omitempty"`protobuf:"bytes,2,opt,name=measurement,proto3" json:"measurement,omitempty"`protobuf:"bytes,2,opt,name=measurement,proto3" json:"measurement,omitempty"`protobuf:"bytes,3,opt,name=range,proto3" json:"range"`protobuf:"bytes,3,opt,name=range,proto3" json:"range"`protobuf:"bytes,4,opt,name=predicate,proto3" json:"predicate,omitempty"`protobuf:"bytes,4,opt,name=predicate,proto3" json:"predicate,omitempty"`protobuf:"bytes,3,opt,name=tag_key,json=tagKey,proto3" json:"tag_key,omitempty"`protobuf:"bytes,3,opt,name=tag_key,json=tagKey,proto3" json:"tag_key,omitempty"`protobuf:"bytes,4,opt,name=range,proto3" json:"range"`protobuf:"bytes,4,opt,name=range,proto3" json:"range"`protobuf:"bytes,5,opt,name=predicate,proto3" json:"predicate,omitempty"`protobuf:"bytes,5,opt,name=predicate,proto3" json:"predicate,omitempty"`protobuf:"bytes,1,rep,name=fields,proto3" json:"fields"`protobuf:"bytes,1,rep,name=fields,proto3" json:"fields"`protobuf:"varint,2,opt,name=type,proto3,enum=influxdata.platform.storage.MeasurementFieldsResponse_FieldType" json:"type,omitempty"`protobuf:"varint,2,opt,name=type,proto3,enum=influxdata.platform.storage.MeasurementFieldsResponse_FieldType" json:"type,omitempty"`protobuf:"fixed64,3,opt,name=timestamp,proto3" json:"timestamp,omitempty"`protobuf:"fixed64,3,opt,name=timestamp,proto3" json:"timestamp,omitempty"`protobuf:"varint,4,opt,name=WindowEvery,proto3" json:"WindowEvery,omitempty"`protobuf:"varint,4,opt,name=WindowEvery,proto3" json:"WindowEvery,omitempty"`protobuf:"varint,6,opt,name=Offset,proto3" json:"Offset,omitempty"`protobuf:"varint,6,opt,name=Offset,proto3" json:"Offset,omitempty"`protobuf:"bytes,5,rep,name=aggregate,proto3" json:"aggregate,omitempty"`protobuf:"bytes,5,rep,name=aggregate,proto3" json:"aggregate,omitempty"`protobuf:"bytes,7,opt,name=window,proto3" json:"window,omitempty"`protobuf:"bytes,7,opt,name=window,proto3" json:"window,omitempty"`protobuf:"bytes,1,opt,name=every,proto3" json:"every,omitempty"`protobuf:"bytes,1,opt,name=every,proto3" json:"every,omitempty"`protobuf:"bytes,2,opt,name=offset,proto3" json:"offset,omitempty"`protobuf:"bytes,2,opt,name=offset,proto3" json:"offset,omitempty"`protobuf:"varint,1,opt,name=nsecs,proto3" json:"nsecs,omitempty"`protobuf:"varint,1,opt,name=nsecs,proto3" json:"nsecs,omitempty"`protobuf:"varint,2,opt,name=months,proto3" json:"months,omitempty"`protobuf:"varint,2,opt,name=months,proto3" json:"months,omitempty"`protobuf:"varint,3,opt,name=negative,proto3" json:"negative,omitempty"`protobuf:"varint,3,opt,name=negative,proto3" json:"negative,omitempty"`influxdata.platform.storage.ReadGroupRequest_Group"influxdata.platform.storage.ReadGroupRequest_Group"influxdata.platform.storage.ReadGroupRequest_HintFlags"influxdata.platform.storage.ReadGroupRequest_HintFlags"influxdata.platform.storage.Aggregate_AggregateType"influxdata.platform.storage.Aggregate_AggregateType"influxdata.platform.storage.ReadResponse_FrameType"influxdata.platform.storage.ReadResponse_FrameType"influxdata.platform.storage.ReadResponse_DataType"influxdata.platform.storage.ReadResponse_DataType"influxdata.platform.storage.MeasurementFieldsResponse_FieldType"influxdata.platform.storage.MeasurementFieldsResponse_FieldType"influxdata.platform.storage.ReadFilterRequest"influxdata.platform.storage.ReadFilterRequest"influxdata.platform.storage.ReadGroupRequest"influxdata.platform.storage.ReadGroupRequest"influxdata.platform.storage.Aggregate"influxdata.platform.storage.Aggregate"influxdata.platform.storage.Tag"influxdata.platform.storage.Tag"influxdata.platform.storage.ReadResponse"influxdata.platform.storage.ReadResponse"influxdata.platform.storage.ReadResponse.Frame"influxdata.platform.storage.ReadResponse.Frame"influxdata.platform.storage.ReadResponse.GroupFrame"influxdata.platform.storage.ReadResponse.GroupFrame"influxdata.platform.storage.ReadResponse.SeriesFrame"influxdata.platform.storage.ReadResponse.SeriesFrame"influxdata.platform.storage.ReadResponse.FloatPointsFrame"influxdata.platform.storage.ReadResponse.FloatPointsFrame"influxdata.platform.storage.ReadResponse.IntegerPointsFrame"influxdata.platform.storage.ReadResponse.IntegerPointsFrame"influxdata.platform.storage.ReadResponse.UnsignedPointsFrame"influxdata.platform.storage.ReadResponse.UnsignedPointsFrame"influxdata.platform.storage.ReadResponse.BooleanPointsFrame"influxdata.platform.storage.ReadResponse.BooleanPointsFrame"influxdata.platform.storage.ReadResponse.StringPointsFrame"influxdata.platform.storage.ReadResponse.StringPointsFrame"influxdata.platform.storage.Capability"influxdata.platform.storage.Capability"influxdata.platform.storage.CapabilitiesResponse"influxdata.platform.storage.CapabilitiesResponse"influxdata.platform.storage.CapabilitiesResponse.CapsEntry"influxdata.platform.storage.CapabilitiesResponse.CapsEntry"influxdata.platform.storage.TimestampRange"influxdata.platform.storage.TimestampRange"influxdata.platform.storage.TagKeysRequest"influxdata.platform.storage.TagKeysRequest"influxdata.platform.storage.TagValuesRequest"influxdata.platform.storage.TagValuesRequest"influxdata.platform.storage.StringValuesResponse"influxdata.platform.storage.StringValuesResponse"influxdata.platform.storage.MeasurementNamesRequest"influxdata.platform.storage.MeasurementNamesRequest"influxdata.platform.storage.MeasurementTagKeysRequest"influxdata.platform.storage.MeasurementTagKeysRequest"influxdata.platform.storage.MeasurementTagValuesRequest"influxdata.platform.storage.MeasurementTagValuesRequest"influxdata.platform.storage.MeasurementFieldsRequest"influxdata.platform.storage.MeasurementFieldsRequest"influxdata.platform.storage.MeasurementFieldsResponse"influxdata.platform.storage.MeasurementFieldsResponse"influxdata.platform.storage.MeasurementFieldsResponse.MessageField"influxdata.platform.storage.MeasurementFieldsResponse.MessageField"influxdata.platform.storage.ReadWindowAggregateRequest"influxdata.platform.storage.ReadWindowAggregateRequest"influxdata.platform.storage.Window"influxdata.platform.storage.Window"influxdata.platform.storage.Duration"influxdata.platform.storage.Duration"storage_common.proto"storage_common.proto"proto: ReadFilterRequest: wiretype end group for non-group"proto: ReadFilterRequest: wiretype end group for non-group"proto: ReadFilterRequest: illegal tag %d (wire type %d)"proto: ReadFilterRequest: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field ReadSource"proto: wrong wireType = %d for field ReadSource"proto: wrong wireType = %d for field Range"proto: wrong wireType = %d for field Range"proto: wrong wireType = %d for field Predicate"proto: wrong wireType = %d for field Predicate"proto: ReadGroupRequest: wiretype end group for non-group"proto: ReadGroupRequest: wiretype end group for non-group"proto: ReadGroupRequest: illegal tag %d (wire type %d)"proto: ReadGroupRequest: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field GroupKeys"proto: wrong wireType = %d for field GroupKeys"proto: wrong wireType = %d for field Group"proto: wrong wireType = %d for field Group"proto: wrong wireType = %d for field Aggregate"proto: wrong wireType = %d for field Aggregate"proto: wrong wireType = %d for field Hints"proto: wrong wireType = %d for field Hints"proto: Aggregate: wiretype end group for non-group"proto: Aggregate: wiretype end group for non-group"proto: Aggregate: illegal tag %d (wire type %d)"proto: Aggregate: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Type"proto: wrong wireType = %d for field Type"proto: Tag: wiretype end group for non-group"proto: Tag: wiretype end group for non-group"proto: Tag: illegal tag %d (wire type %d)"proto: Tag: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Value"proto: wrong wireType = %d for field Value"proto: ReadResponse: wiretype end group for non-group"proto: ReadResponse: wiretype end group for non-group"proto: ReadResponse: illegal tag %d (wire type %d)"proto: ReadResponse: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Frames"proto: wrong wireType = %d for field Frames"proto: Frame: wiretype end group for non-group"proto: Frame: wiretype end group for non-group"proto: Frame: illegal tag %d (wire type %d)"proto: Frame: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Series"proto: wrong wireType = %d for field Series"proto: wrong wireType = %d for field FloatPoints"proto: wrong wireType = %d for field FloatPoints"proto: wrong wireType = %d for field IntegerPoints"proto: wrong wireType = %d for field IntegerPoints"proto: wrong wireType = %d for field UnsignedPoints"proto: wrong wireType = %d for field UnsignedPoints"proto: wrong wireType = %d for field BooleanPoints"proto: wrong wireType = %d for field BooleanPoints"proto: wrong wireType = %d for field StringPoints"proto: wrong wireType = %d for field StringPoints"proto: GroupFrame: wiretype end group for non-group"proto: GroupFrame: wiretype end group for non-group"proto: GroupFrame: illegal tag %d (wire type %d)"proto: GroupFrame: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field TagKeys"proto: wrong wireType = %d for field TagKeys"proto: wrong wireType = %d for field PartitionKeyVals"proto: wrong wireType = %d for field PartitionKeyVals"proto: SeriesFrame: wiretype end group for non-group"proto: SeriesFrame: wiretype end group for non-group"proto: SeriesFrame: illegal tag %d (wire type %d)"proto: SeriesFrame: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Tags"proto: wrong wireType = %d for field Tags"proto: wrong wireType = %d for field DataType"proto: wrong wireType = %d for field DataType"proto: FloatPointsFrame: wiretype end group for non-group"proto: FloatPointsFrame: wiretype end group for non-group"proto: FloatPointsFrame: illegal tag %d (wire type %d)"proto: FloatPointsFrame: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Timestamps"proto: wrong wireType = %d for field Timestamps"proto: wrong wireType = %d for field Values"proto: wrong wireType = %d for field Values"proto: IntegerPointsFrame: wiretype end group for non-group"proto: IntegerPointsFrame: wiretype end group for non-group"proto: IntegerPointsFrame: illegal tag %d (wire type %d)"proto: IntegerPointsFrame: illegal tag %d (wire type %d)"proto: UnsignedPointsFrame: wiretype end group for non-group"proto: UnsignedPointsFrame: wiretype end group for non-group"proto: UnsignedPointsFrame: illegal tag %d (wire type %d)"proto: UnsignedPointsFrame: illegal tag %d (wire type %d)"proto: BooleanPointsFrame: wiretype end group for non-group"proto: BooleanPointsFrame: wiretype end group for non-group"proto: BooleanPointsFrame: illegal tag %d (wire type %d)"proto: BooleanPointsFrame: illegal tag %d (wire type %d)"proto: StringPointsFrame: wiretype end group for non-group"proto: StringPointsFrame: wiretype end group for non-group"proto: StringPointsFrame: illegal tag %d (wire type %d)"proto: StringPointsFrame: illegal tag %d (wire type %d)"proto: Capability: wiretype end group for non-group"proto: Capability: wiretype end group for non-group"proto: Capability: illegal tag %d (wire type %d)"proto: Capability: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Features"proto: wrong wireType = %d for field Features"proto: CapabilitiesResponse: wiretype end group for non-group"proto: CapabilitiesResponse: wiretype end group for non-group"proto: CapabilitiesResponse: illegal tag %d (wire type %d)"proto: CapabilitiesResponse: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Caps"proto: wrong wireType = %d for field Caps"proto: TimestampRange: wiretype end group for non-group"proto: TimestampRange: wiretype end group for non-group"proto: TimestampRange: illegal tag %d (wire type %d)"proto: TimestampRange: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field End"proto: wrong wireType = %d for field End"proto: TagKeysRequest: wiretype end group for non-group"proto: TagKeysRequest: wiretype end group for non-group"proto: TagKeysRequest: illegal tag %d (wire type %d)"proto: TagKeysRequest: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field TagsSource"proto: wrong wireType = %d for field TagsSource"proto: TagValuesRequest: wiretype end group for non-group"proto: TagValuesRequest: wiretype end group for non-group"proto: TagValuesRequest: illegal tag %d (wire type %d)"proto: TagValuesRequest: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field TagKey"proto: wrong wireType = %d for field TagKey"proto: StringValuesResponse: wiretype end group for non-group"proto: StringValuesResponse: wiretype end group for non-group"proto: StringValuesResponse: illegal tag %d (wire type %d)"proto: StringValuesResponse: illegal tag %d (wire type %d)"proto: MeasurementNamesRequest: wiretype end group for non-group"proto: MeasurementNamesRequest: wiretype end group for non-group"proto: MeasurementNamesRequest: illegal tag %d (wire type %d)"proto: MeasurementNamesRequest: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Source"proto: wrong wireType = %d for field Source"proto: MeasurementTagKeysRequest: wiretype end group for non-group"proto: MeasurementTagKeysRequest: wiretype end group for non-group"proto: MeasurementTagKeysRequest: illegal tag %d (wire type %d)"proto: MeasurementTagKeysRequest: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Measurement"proto: wrong wireType = %d for field Measurement"proto: MeasurementTagValuesRequest: wiretype end group for non-group"proto: MeasurementTagValuesRequest: wiretype end group for non-group"proto: MeasurementTagValuesRequest: illegal tag %d (wire type %d)"proto: MeasurementTagValuesRequest: illegal tag %d (wire type %d)"proto: MeasurementFieldsRequest: wiretype end group for non-group"proto: MeasurementFieldsRequest: wiretype end group for non-group"proto: MeasurementFieldsRequest: illegal tag %d (wire type %d)"proto: MeasurementFieldsRequest: illegal tag %d (wire type %d)"proto: MeasurementFieldsResponse: wiretype end group for non-group"proto: MeasurementFieldsResponse: wiretype end group for non-group"proto: MeasurementFieldsResponse: illegal tag %d (wire type %d)"proto: MeasurementFieldsResponse: illegal tag %d (wire type %d)"proto: MessageField: wiretype end group for non-group"proto: MessageField: wiretype end group for non-group"proto: MessageField: illegal tag %d (wire type %d)"proto: MessageField: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Timestamp"proto: wrong wireType = %d for field Timestamp"proto: ReadWindowAggregateRequest: wiretype end group for non-group"proto: ReadWindowAggregateRequest: wiretype end group for non-group"proto: ReadWindowAggregateRequest: illegal tag %d (wire type %d)"proto: ReadWindowAggregateRequest: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field WindowEvery"proto: wrong wireType = %d for field WindowEvery"proto: wrong wireType = %d for field Offset"proto: wrong wireType = %d for field Offset"proto: wrong wireType = %d for field Window"proto: wrong wireType = %d for field Window"proto: Window: wiretype end group for non-group"proto: Window: wiretype end group for non-group"proto: Window: illegal tag %d (wire type %d)"proto: Window: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Every"proto: wrong wireType = %d for field Every"proto: Duration: wiretype end group for non-group"proto: Duration: wiretype end group for non-group"proto: Duration: illegal tag %d (wire type %d)"proto: Duration: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Nsecs"proto: wrong wireType = %d for field Nsecs"proto: wrong wireType = %d for field Months"proto: wrong wireType = %d for field Months"proto: wrong wireType = %d for field Negative"proto: wrong wireType = %d for field Negative" source: storage_common.proto GroupNone returns all series as a single group. The single GroupFrame.TagKeys will be the union of all tag keys. GroupBy returns a group for each unique value of the specified GroupKeys. TODO(jlapacik): This field is only used in unit tests. Specifically the two tests in group_resultset_test.go. This field should be removed and the tests that depend on it refactored. HintSchemaAllTime performs schema queries without using time ranges GroupKeys specifies a list of tag keys used to order the data. It is dependent on the Group property to determine its behavior. Response message for ReadFilter and ReadGroup Types that are valid to be assigned to Data:	*ReadResponse_Frame_Group	*ReadResponse_Frame_Series	*ReadResponse_Frame_FloatPoints	*ReadResponse_Frame_IntegerPoints	*ReadResponse_Frame_UnsignedPoints	*ReadResponse_Frame_BooleanPoints	*ReadResponse_Frame_StringPoints TagKeys PartitionKeyVals is the values of the partition key for this group, order matching ReadGroupRequest.GroupKeys Features contains the specific features supported by this capability. Capabilities contains the set of capabilities supported by the storage engine. It is a map of method names to the detailed capability information for the method. Specifies a continuous range of nanosecond timestamps. Start defines the inclusive lower bound. End defines the exclusive upper bound. TagKeysRequest is the request message for Storage.TagKeys. TagValuesRequest is the request message for Storage.TagValues. Response message for Storage.TagKeys, Storage.TagValues Storage.MeasurementNames, Storage.MeasurementTagKeys and Storage.MeasurementTagValues. MeasurementNamesRequest is the request message for Storage.MeasurementNames. MeasurementTagKeysRequest is the request message for Storage.MeasurementTagKeys. MeasurementTagValuesRequest is the request message for Storage.MeasurementTagValues. MeasurementFieldsRequest is the request message for Storage.MeasurementFields. MeasurementFieldsResponse is the response message for Storage.MeasurementFields. 1891 bytes of a gzipped FileDescriptorProto/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/gen.gogo:generate env GO111MODULE=on go run github.com/benbjohnson/tmpl -data=@array_cursor.gen.go.tmpldata array_cursor.gen.go.tmplgo:generate env GO111MODULE=on go run github.com/benbjohnson/tmpl -data=@array_cursor.gen.go.tmpldata -o=array_cursor_gen_test.go array_cursor_test.gen.go.tmpl/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/group_resultset.goallTimerowKeytagsBuf'\000' GroupOptionNilSortLo configures nil values to be sorted lower than any other value NilSort values determine the lexicographical order of nil values in the partition key nil sorts lowest nil sorts highest seriesHasPoints reads the first block of TSM data to verify the series has points for the time range of the query. TODO(sgc): this is expensive. Storage engine must provide efficient time range queries of series keys. TODO(sgc): store error for sort key separators separate sort key values with ascii null character/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/influxql_eval.gorhsirhsfUnsignedLiteralBITWISE_ANDBITWISE_ORBITWISE_XORMULDIVMOD evalExpr evaluates expr against a map. When the LHS is nil and the RHS is a boolean, implicitly cast the nil to false. Implicit cast of the RHS nil to false when the LHS is a boolean. Evaluate if both sides are simple types. Try the rhs as a float64 or int64 Try as a float64 to see if a float cast is required./Users/austinjaybecker/projects/abeck-go-testing/storage/reads/influxql_expr.go TODO(sgc): build expression evaluator that does not use influxql AST Valuer is the interface that wraps the Value() method. Value returns the value and existence flag for a given key./Users/austinjaybecker/projects/abeck-go-testing/storage/reads/influxql_predicate.goinvalid expression"invalid expression"parenExpression expects one child"parenExpression expects one child"comparisonExpression expects two children"comparisonExpression expects two children"startsWith not implemented"startsWith not implemented"invalid comparison operator"invalid comparison operator"unexpected literal type"unexpected literal type"stack empty"stack empty" NodeToExpr transforms a predicate node to an influxql.Expr. TODO(edd): It would be preferable if RewriteRegexConditions was a package level function in influxql. TODO(sgc): rewrite to anchored RE, as index does not support startsWith yet TODO(sgc): consider hashing the RegexValue and cache compiled version/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/keymerger.go tagsKeyMerger is responsible for determining a merged set of tag keys no new tags back up the pointers/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/modulo.godividendmodulusbegmod WindowStart calculates the start time of a window given a timestamp, the window period (every), and the offset starting from the epoch. Note that the normalized offset value can fall on either side of the normalized timestamp. If it lies to the left we know it represents the start time. Otherwise it represents the stop time, in which case we decrement by the window period to get the start time. WindowStop calculates the stop time of a window given a timestamp, normalized timestamp. If it lies to the right we know it represents the stop time. Otherwise it represents the start time, in which case we increment by the window period to get the stop time./Users/austinjaybecker/projects/abeck-go-testing/storage/reads/predicate.go[none]"[none]" AND " AND " OR " OR "( "( " )" )"startsWith"startsWith"'<''>''\'''$'Quote NodeVisitor can be called by Walk to traverse the Node hierarchy. The Visit() function is called once per node./Users/austinjaybecker/projects/abeck-go-testing/storage/reads/resultset.go Close closes the result set. Close is idempotent. Next returns true if there are more results available. Stats returns the stats for the underlying cursors. Available after resultset has been scanned./Users/austinjaybecker/projects/abeck-go-testing/storage/reads/resultset_lineprotocol.goccurmissing measurement / field"missing measurement / field"AppendQuote ResultSetToLineProtocol transforms rs to line protocol and writes the output to wr. take first and last elements which are measurement and field keys/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/series_cursor.go measurement name unmodified series tags/Users/austinjaybecker/projects/abeck-go-testing/storage/reads/store.go Next advances the ResultSet to the next cursor. It returns false when there are no more cursors. Cursor returns the most recent cursor after a call to Next. Tags returns the tags for the most recent cursor after a call to Next. Close releases any resources allocated by the ResultSet. Err returns the first error encountered by the ResultSet. Next advances the GroupResultSet and returns the next GroupCursor. It returns nil if there are no more groups. Close releases any resources allocated by the GroupResultSet. Err returns the first error encountered by the GroupResultSet. Next advances to the next cursor. Next will return false when there are no more cursors in the current group. Keys returns the union of all tag key names for all series produced by this GroupCursor. PartitionKeyVals returns the values of all tags identified by the keys specified in ReadRequest#GroupKeys. The tag values values will appear in the same order as the GroupKeys. When the datatypes.GroupNone strategy is specified, PartitionKeyVals will be nil. Close releases any resources allocated by the GroupCursor. Err returns the first error encountered by the GroupCursor. WindowAggregate will invoke a ReadWindowAggregateRequest against the Store./Users/austinjaybecker/projects/abeck-go-testing/storage/reads/tagsbuffer.go/Users/austinjaybecker/projects/abeck-go-testing/storage/readservice/Users/austinjaybecker/projects/abeck-go-testing/storage/readservice/service.go NewProxyQueryService returns a proxy query service based on the given queryController suitable for the storage read service./Users/austinjaybecker/projects/abeck-go-testing/storage/retention.go A BucketFinder is responsible for providing access to buckets via a filter./Users/austinjaybecker/projects/abeck-go-testing/tag.gotagPairOperator is invalid"Operator is invalid""equal"notequal"notequal"equalregex"equalregex"notequalregex"notequalregex"unrecognized operator"unrecognized operator"^[a-zA-Z0-9_]+:[a-zA-Z0-9_]+$`^[a-zA-Z0-9_]+:[a-zA-Z0-9_]+$`tag must be in form key:value`tag must be in form key:value`tag must contain a key and a value"tag must contain a key and a value" Operator is an Enum value of operators. Valid returns invalid error if the operator is invalid. operators ToOperator converts a string into its equivalent Operator. String returns the string value of the operator. MarshalJSON implements json.Marshal interface. Tag is a tag key-value pair. NewTag generates a tag pair from a string in the format key:value. Valid returns an error if the tagpair is missing fields QueryParam converts a Tag to a string query parameter/Users/austinjaybecker/projects/abeck-go-testing/task/Users/austinjaybecker/projects/abeck-go-testing/task/backend/Users/austinjaybecker/projects/abeck-go-testing/task/backend/analytical_storage.goIsUnrecoverableNewAnalyticalRunStorageNewStoragePointsWriterRecorderNotifyCoordinatorOfExistingStoragePointsWriterRecorderfinishedAtFieldlogFieldrequestedAtFieldrunIDFieldrunReaderscheduledForFieldstartedAtFieldstatusTagtaskIDTagparsedAfterTimeparsedBeforeTimeconstructedTimeFilterfilterPartittrrunAuthrunSystemBucketIDrunsScriptcompleteRuncompleteRunscrMapcurrentRunsfindRunScriptsfreadTablereadRunsrequestedlogBytes"runID""scheduledFor""startedAt""finishedAt""requestedAt"|> filter(fn: (r) => r.runID > %q)`|> filter(fn: (r) => r.runID > %q)`failed parsing after time: %s"failed parsing after time: %s"failed parsing before time: %s"failed parsing before time: %s"given after time must be prior to before time"given after time must be prior to before time"|> filter(fn: (r) =>time(v: r["scheduledFor"]) > %s and time(v: r["scheduledFor"]) < %s)`|> filter(fn: (r) =>time(v: r["scheduledFor"]) > %s and time(v: r["scheduledFor"]) < %s)`from(bucketID: %q)
	  |> range(start: -14d)
	  |> filter(fn: (r) => r._field != "status")
	  |> filter(fn: (r) => r._measurement == "runs" and r.taskID == %q)
	  %s
	  |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
	  %s
	  |> group(columns: ["taskID"])
	  |> sort(columns:["scheduledFor"], desc: true)
	  |> limit(n:%d)

	  `from(bucketID: %q)
	  |> range(start: -14d)
	  |> filter(fn: (r) => r._field != "status")
	  |> filter(fn: (r) => r._measurement == "runs" and r.taskID == %q)
	  %s
	  |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
	  %s
	  |> group(columns: ["taskID"])
	  |> sort(columns:["scheduledFor"], desc: true)
	  |> limit(n:%d)

	  `run-reader"run-reader"unexpected internal error while decoding run response: %v"unexpected internal error while decoding run response: %v"run not found"run not found"from(bucketID: %q)
	|> range(start: -14d)
	|> filter(fn: (r) => r._field != "status")
	|> filter(fn: (r) => r._measurement == "runs" and r.taskID == %q)
	|> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
	|> group(columns: ["taskID"])
	|> filter(fn: (r) => r.runID == %q)
	  `from(bucketID: %q)
	|> range(start: -14d)
	|> filter(fn: (r) => r._field != "status")
	|> filter(fn: (r) => r._measurement == "runs" and r.taskID == %q)
	|> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
	|> group(columns: ["taskID"])
	|> filter(fn: (r) => r.runID == %q)
	  `found multiple runs with id "found multiple runs with id "Failed to parse runID"Failed to parse runID"Failed to parse taskID"Failed to parse taskID"Failed to parse startedAt time"Failed to parse startedAt time"Failed to parse requestedAt time"Failed to parse requestedAt time"Failed to parse scheduledFor time"Failed to parse scheduledFor time"Failed to parse finishedAt time"Failed to parse finishedAt time"Failed to parse log data"Failed to parse log data"log_bytes"log_bytes" RunRecorder is a type which records runs into an influxdb backed storage mechanism NewAnalyticalRunStorage creates a new analytical store with access to the necessary systems for storing data and to act as a middleware NewAnalyticalStorage creates a new analytical store with access to the necessary systems for storing data and to act as a middleware (deprecated) First attempt to use the TaskService, then append additional analytical's logs to the list add historical logs to the transactional logs. First attempt to use the TaskService, then append additional analytical's runs to the list if we reached the limit lets stop here creates flux script to filter based on time, if given the data will be stored for 7 days in the system bucket so pulling 14d's is sufficient. At this point we are behind authorization so we are faking a read only permission to the org's system bucket remove any kv runs that exist in the list of completed runs track the current runs if we find a complete run that matches a current run the current run is out dated and should be removed. First see if it is in the existing TaskService. If not pull it from analytical storage. check the taskService to see if the run is on its list try finding the run (in our system or underlying) if we dont have a full enough data set we fail here./Users/austinjaybecker/projects/abeck-go-testing/task/backend/check_task_error.goerrStringcould not find bucket"could not find bucket"could not parse Flux script"could not parse Flux script"filesystem service uninitialized"filesystem service uninitialized" IsUnrecoverable takes in an error and determines if it is permanent (requiring user intervention to fix) missing bucket requires user intervention to resolve unparsable Flux requires user intervention to resolve Flux script uses an API that attempts to read the filesystem/Users/austinjaybecker/projects/abeck-go-testing/task/backend/coordinator/Users/austinjaybecker/projects/abeck-go-testing/task/backend/coordinator/coordinator.goDefaultLimitNewSchedulableTaskSchedulableTaskWithLimitOptlsceffCroninvalid cron or every"invalid cron or every"NewSchedule DefaultLimit is the maximum number of tasks that a given taskd server can own Executor is an abstraction of the task executor with only the functions needed by the coordinator Coordinator is the intermediary between the scheduling/executing system and the rest of the task system SchedulableTask is a wrapper around the Task struct, giving it methods to make it compatible with the Scheduler Schedule takes the time a Task is scheduled for and returns a Schedule object Offset returns a time.Duration for the Task's offset property LastScheduled parses the task's LatestCompleted value as a Time object NewSchedulableTask transforms an influxdb task to a schedulable task type TaskCreated asks the Scheduler to schedule the newly created task func new schedulable task catch errors from offset and last scheduled TaskUpdated releases the task if it is being disabled, and schedules it otherwise if disabling the task, release it before schedule updateTaskDeleted asks the Scheduler to release the deleted task RunCancelled speaks directly to the executor to cancel a task run RunRetried speaks directly to the executor to re-try a task run immediately RunForced speaks directly to the Executor to run a task immediately/Users/austinjaybecker/projects/abeck-go-testing/task/backend/coordinator.gocoordFailed to set latestCompleted"Failed to set latestCompleted" TaskService is a type on which tasks can be listed Coordinator is a type with a single method which is called when a task has been created NotifyCoordinatorOfExisting lists all tasks by the provided task service and for each task it calls the provided coordinators task created method If we missed a Create Action TaskNotifyCoordinatorOfExisting lists all tasks by the provided task service and for TODO(docmerlin): this is temporary untill the executor queue is persistent/Users/austinjaybecker/projects/abeck-go-testing/task/backend/executor/Users/austinjaybecker/projects/abeck-go-testing/task/backend/executor/executor.goConcurrencyLimitMultiLimitNewASTCompilerNewExecutorMetricsNewFluxCompilerNewRunCollectorWithMaxWorkersWithNonSystemCompilerBuilderWithSystemCompilerBuilderdefaultMaxWorkersexhaustResultIteratorslastSuccessOptionmaxPromisesrunCollectorworkerworkerMakerwmiidisSampledbuildCompilerrunErrexternBytestasks.lastSuccessTime"tasks.lastSuccessTime"Failed to enqueue run: %s"Failed to enqueue run: %s"failed to fail create run: AddRunLog:"failed to fail create run: AddRunLog:"failed to fail create run: UpdateRunState:"failed to fail create run: UpdateRunState:"failed to fail create run: FinishRun:"failed to fail create run: FinishRun:"Task limit reached: %s"Task limit reached: %s"Run canceled"Run canceled"Started task from script: %q"Started task from script: %q"Completed(%s)"Completed(%s)"Execution failed"Execution failed"Task encountered unrecoverable error, requires admin action: %v"Task encountered unrecoverable error, requires admin action: %v"Completed successfully"Completed successfully"Failed to finish run"Failed to finish run"Error exhausting result iterator"Error exhausting result iterator"trace_id=%s is_sampled=%t"trace_id=%s is_sampled=%t"ParseToJSON MultiLimit allows us to create a single limit func that applies more then one limit. LimitFunc is a function the executor will use to WithMaxWorkers specifies the number of workers used by the Executor. CompilerBuilderFunc is a function that yields a new flux.Compiler. The context.Context provided can be assumed to be an authorized context. CompilerBuilderTimestamps contains timestamps which should be provided along with a Task query. WithSystemCompilerBuilder is an Executor option that configures a CompilerBuilderFunc to be used when compiling queries for System Tasks. WithNonSystemCompilerBuilder is an Executor option that configures a CompilerBuilderFunc to be used when compiling queries for non-System Tasks (Checks and Notifications). WithFlagger is an Executor option that allows us to use a feature flagger in the executor NewExecutor creates a new task executor noop Executor it a task specific executor that works with the new scheduler system. currentPromises are all the promises we are made that have not been fulfilled keep a pool of promise's we have in queue keep a pool of execution workers. SetLimitFunc sets the limit func for this task executor Execute is a executor to satisfy the needs of tasks PromisedExecute begins execution for the tasks id with a specific scheduledFor time. When we execute we will first build a run for the scheduledFor time, We then want to add to the queue anything that was manually queued to run. If the queue is full the call to execute should hang and apply back pressure to the caller We then start a worker to work the newly queued jobs. create promises for any manual runs see if have available workers we have reached our worker limit and we cannot start any more. fire up some workers if the worker is nil all the workers are busy and one of them will pick up the work we enqueued. don't forget to put the worker back when we are done remove a struct from the worker limit to another worker to work Cancel a run of a specific task. find the promise call cancel on it. create promise insert promise into queue to be worked when the queue gets full we will hand and apply back pressure to the scheduler insert the promise into the registry exhaustResultIterators is used to exhaust the result of a flux query loop until we have no more work to do in the promise queue check to see if we can execute the promiseQueue has been closed if nothing is left in the queue we are done check to make sure we are below the limits. add to the run log sleep If done the promise was canceled execute the promise close promise done channel and set appropriate error remove promise from registry trace add to run log update run status add to metrics log error TODO (al): once user notification system is put in place, this code should be uncommented if we get an error that requires user intervention to fix, deactivate the task and alert the user inactive := string(backend.TaskInactive) w.te.ts.UpdateTask(p.ctx, p.task.ID, influxdb.TaskUpdate{Status: &inactive}) and add to run logs start Assume the error should not be part of the runResult. Drain the result iterator. Consume the full iterator so that we don't leak outstanding iterators. log the trace id and whether or not it was sampled into the run log RunsActive returns the current number of workers, which is equivalent to the number of runs actively running WorkersBusy returns the percent of total workers that are busy PromiseQueueUsage returns the percent of the Promise Queue that is currently filled promise represents a promise the executor makes to finish a run's execution asynchronously. ID is the id of the run that was created Cancel is used to cancel a executing query call cancelfunc wait for ctx.Done or p.Done Done provides a channel that closes on completion of a promise Error returns the error resulting from a run execution. If the execution is not complete error waits on Done(). exhaustResultIterators drains all the iterators from a flux query Result. NewASTCompiler parses a Flux query string into an AST representation. NewFluxCompiler wraps a Flux query string in a raw-query representation. TODO(brett): This mitigates an immediate problem where Checks/Notifications breaks when sending Now, and system Tasks do not break when sending Now. We are currently sending C+N through using Flux Compiler and Tasks as AST Compiler until we come to the root cause. Removing Now here will keep the system humming along normally until we are able to locate the root cause and use Flux Compiler for all Task types. It turns out this is due to the exclusive nature of the stop time in Flux "from" and that we weren't including the left-hand boundary of the range check for notifications. We're shipping a fix soon in https://github.com/influxdata/influxdb/pull/19392 Once this has merged, we can send Now again. Now: now,totalRunsActiveworkersBusypromiseQueueUsage/Users/austinjaybecker/projects/abeck-go-testing/task/backend/executor/executor_metrics.gotaskType"executor"total_runs_complete"total_runs_complete"Total number of runs completed across all tasks, split out by success or failure."Total number of runs completed across all tasks, split out by success or failure."task_type"task_type"SummaryOptsObjectivesAgeBucketsBufCapNewSummaryVecrun_queue_delta"run_queue_delta"The duration in seconds between a run being due to start and actually starting."The duration in seconds between a run being due to start and actually starting."0.050.0500000000000000027763602879701896397/720575940379279360.90.90000000000000002228106479329266893/90071992547409920.010.0100000000000000002085764607523034235/5764607523034234880.990.989999999999999991124458563631096791/4503599627370496run_duration"run_duration"The duration in seconds between a run starting and finishing."The duration in seconds between a run starting and finishing."errors_counter"errors_counter"The number of errors thrown by the executor with the type of error (ex. Invalid, Internal, etc.)"The number of errors thrown by the executor with the type of error (ex. Invalid, Internal, etc.)"errorType"errorType"unrecoverable_counter"unrecoverable_counter"The number of errors by taskID that must be manually resolved or have the task deactivated."The number of errors by taskID that must be manually resolved or have the task deactivated."manual_runs_counter"manual_runs_counter"Total number of manual runs scheduled to run by task ID"Total number of manual runs scheduled to run by task ID"resume_runs_counter"resume_runs_counter"Total number of runs resumed by task ID"Total number of runs resumed by task ID"run_latency_seconds"run_latency_seconds"Records the latency between the time the run was due to run and the time the task started execution, by task type"Records the latency between the time the run was due to run and the time the task started execution, by task type"task_executor_workers_busy"task_executor_workers_busy"Percent of total available workers that are currently busy"Percent of total available workers that are currently busy"task_executor_total_runs_active"task_executor_total_runs_active"Total number of workers currently running tasks"Total number of workers currently running tasks"task_executor_promise_queue_usage"task_executor_promise_queue_usage"Percent of the promise queue that is currently full"Percent of the promise queue that is currently full" NewRunCollector returns a collector which exports influxdb process metrics. StartRun store the delta time between when a run is due to start and actually starting. schedule interval duration = (time task was scheduled to run) - (time it actually ran) FinishRun adjusts the metrics to indicate a run is no longer in progress for the given task ID. LogError increments the count of errors by error code. LogUnrecoverableError increments the count of unrecoverable errors, which require admin intervention to resolve or deactivate This count is separate from the errors count so that the errors metric can be used to identify only internal, rather than user errors and so that unrecoverable errors can be quickly identified for deactivation Describe returns all descriptions associated with the run collector. Collect returns the current state of all metrics of the run collector./Users/austinjaybecker/projects/abeck-go-testing/task/backend/executor/limits.gorunirunjSliceStable ConcurrencyLimit creates a concurrency limit func that uses the executor to determine if the task has exceeded the concurrency limit. sort by scheduledFor time because we want to make sure older scheduled for times are higher priority no need to keep looping. this run isn't currently running. but we have more run's then the concurrency allows/Users/austinjaybecker/projects/abeck-go-testing/task/backend/executor/mock/Users/austinjaybecker/projects/abeck-go-testing/task/backend/executor/mock/permission_service.goMockPermissionServiceMockPermissionServiceMockRecorderMockPromiseMockPromiseMockRecorderNewMockPermissionServiceNewMockPromise"FindPermissionForUser""Cancel""Done""Error" Source: executor.go MockPermissionService is a mock of PermissionService interface MockPermissionServiceMockRecorder is the mock recorder for MockPermissionService NewMockPermissionService creates a new mock instance FindPermissionForUser mocks base method FindPermissionForUser indicates an expected call of FindPermissionForUser MockPromise is a mock of Promise interface MockPromiseMockRecorder is the mock recorder for MockPromise NewMockPromise creates a new mock instance ID mocks base method ID indicates an expected call of ID Cancel mocks base method Cancel indicates an expected call of Cancel Done mocks base method Done indicates an expected call of Done Error mocks base method Error indicates an expected call of Error/Users/austinjaybecker/projects/abeck-go-testing/task/backend/middleware/Users/austinjaybecker/projects/abeck-go-testing/task/backend/middleware/check_middleware.goWithNowFuncfromTasktoTaskschedule task failed: %s
	cleanup also failed: %s"schedule task failed: %s\n\tcleanup also failed: %s" CoordinatingCheckService acts as a CheckService decorator that handles coordinating the api request with the required task control actions asynchronously via a message dispatcher NewCheckService constructs a new coordinating check service CreateCheck Creates a check and Publishes the change it can be scheduled. UpdateCheck Updates a check and publishes the change so the task owner can act on the update if the update is to activate and the previous task was inactive we should add a "latest completed" update this allows us to see not run the task for inactive time PatchCheck Updates a check and publishes the change so the task owner can act on the update DeleteCheck delete the check and publishes the change, to allow the task owner to find out about this change faster./Users/austinjaybecker/projects/abeck-go-testing/task/backend/middleware/middleware.go Coordinator is a type which is used to react to task related actions CoordinatingTaskService acts as a TaskService decorator that handles coordinating the api request New constructs a new coordinating task service CreateTask Creates a task in the existing task service and Publishes the change so any TaskD service can lease it. UpdateTask Updates a task and publishes the change so the task owner can act on the update DeleteTask delete the task and publishes the change, to allow the task owner to find out about this change faster. CancelRun Cancel the run and publish the cancellation. RetryRun calls retry on the task service and publishes the retry. ForceRun create the forced run in the task system and publish to the pubSub./Users/austinjaybecker/projects/abeck-go-testing/task/backend/middleware/notification_middleware.go CoordinatingNotificationRuleStore acts as a NotificationRuleStore decorator that handles coordinating the api request NewNotificationRuleStore constructs a new coordinating notification service CreateNotificationRule Creates a notification and Publishes the change it can be scheduled. UpdateNotificationRule Updates a notification and publishes the change so the task owner can act on the update PatchNotificationRule Updates a notification and publishes the change so the task owner can act on the update DeleteNotificationRule delete the notification and publishes the change, to allow the task owner to find out about this change faster./Users/austinjaybecker/projects/abeck-go-testing/task/backend/middleware/options.go Option is a functional option for the coordinating task service WithNowFunc sets the now func used to derive time/Users/austinjaybecker/projects/abeck-go-testing/task/backend/run_recorder.goRun missing critical fields"Run missing critical fields" StoragePointsWriterRecorder is an implementation of RunRecorder which writes runs via an implementation of storage PointsWriter NewStoragePointsWriterRecorder configures and returns a new *StoragePointsWriterRecorder Record formats the provided run as a models.Point and writes the resulting point to an underlying storage.PointsWriter log an error if we have incomplete data on finish TODO - fix/Users/austinjaybecker/projects/abeck-go-testing/task/backend/schedulable_task_service.gocould not update last scheduled for task; Err: %v"could not update last scheduled for task; Err: %v" UpdateTaskService provides an API to update the LatestScheduled time of a task SchedulableTaskService implements the SchedulableService interface NewSchedulableTaskService initializes a new SchedulableTaskService given an UpdateTaskService UpdateLastScheduled uses the task service to store the latest time a task was scheduled to run/Users/austinjaybecker/projects/abeck-go-testing/task/backend/scheduler/Users/austinjaybecker/projects/abeck-go-testing/task/backend/scheduler/noop_scheduler.goErrUnrecoverableNewSchedulerMetricsValidateScheduleWithMaxConcurrentWorkersWithTimedegreeBtreeSchedulednewExecutingTasks NoopScheduler is a no-op scheduler. It is used when we don't want the standard scheduler to run (e.g. when "--no-tasks" flag is present). Schedule is a mocked Scheduler.Schedule method. Release is a mocked Scheduler.Release method. Stop is a mocked stop method./Users/austinjaybecker/projects/abeck-go-testing/task/backend/scheduler/scheduler.goeveryStringlastScheduledAtunparsed"github.com/influxdata/cron"ParseUTC@every "@every "TrimPrefixerror unrecoverable error on task run "error unrecoverable error on task run "error unrecoverable error on task run"error unrecoverable error on task run" ID duplicates the influxdb ID so users of the scheduler don't have to import influxdb for the ID. Executor is a system used by the scheduler to actually execute the scheduleable item. Execute is used to execute run's for any schedulable object. the executor can go through manual runs, clean currently running, and then create a new run based on `now`. if Now is zero we can just do the first 2 steps (This is how we would trigger manual runs). Errors returned from the execute request imply that this attempt has failed and should be put back in scheduler and re executed at a alter time. We will add scheduler specific errors so the time can be configurable. Schedulable is the interface that encapsulates work that is to be executed on a specified schedule. ID is the unique identifier for this Schedulable Schedule defines the frequency for which this Schedulable should be queued for execution. Offset defines a negative or positive duration that should be added to the scheduled time, resulting in the instance running earlier or later than the scheduled time. LastScheduled specifies last time this Schedulable was queued for execution. SchedulableService encapsulates the work necessary to schedule a job UpdateLastScheduled notifies the instance that it was scheduled for execution at the specified time Align create to the hour/minute We cannot align a invalid time drop nanoseconds and align Schedule is an object a valid schedule of runs Next returns the next time after from that a schedule should trigger on. ValidSchedule returns an error if the cron string is invalid. Scheduler is a example interface of a Scheduler. // todo(lh): remove this once we start building the actual scheduler Schedule adds the specified task to the scheduler. Release removes the specified task from the scheduler./Users/austinjaybecker/projects/abeck-go-testing/task/backend/scheduler/scheduler_metrics.gote"scheduler"NewCountertotal_execution_calls"total_execution_calls"Total number of executions across all tasks."Total number of executions across all tasks."total_schedule_calls"total_schedule_calls"Total number of schedule requests."Total number of schedule requests."total_schedule_fails"total_schedule_fails"Total number of schedule requests that fail to schedule."Total number of schedule requests that fail to schedule."total_execute_failure"total_execute_failure"Total number of times an execution has failed."Total number of times an execution has failed."total_release_calls"total_release_calls"Total number of release requests."Total number of release requests."NewSummaryschedule_delay"schedule_delay"The duration between when a Item should be scheduled and when it is told to execute."The duration between when a Item should be scheduled and when it is told to execute."execute_delta"execute_delta"task_scheduler_current_execution"task_scheduler_current_execution"Number of tasks currently being executed"Number of tasks currently being executed" TODO(docmerlin): fix this metric/Users/austinjaybecker/projects/abeck-go-testing/task/backend/scheduler/treescheduler.gotoReAddwhenFromNowpreExecbItemit2newNextexecutor must be a non-nil function"executor must be a non-nil function"schedulerLoopexecutor panicked"executor panicked" degreeBtreeScheduled is the btree degree for the btree internal to the tree scheduler. it is purely a performance tuning parameter, but required by github.com/google/btree TODO(docmerlin): find the best number for this, its purely a perf optimization defaultMaxWorkers is a constant that sets the default number of maximum workers for a TreeScheduler TreeScheduler is a Scheduler based on a btree. It calls Executor in-order per ID.  That means you are guaranteed that for a specific ID, - The scheduler should, after creation, automatically call ExecutorFunc, when a task should run as defined by its Schedulable. - the scheduler's should not be able to get into a state where blocks Release and Schedule indefinitely. - Schedule should add a Schedulable to being scheduled, and Release should remove a task from being scheduled. - Calling of ExecutorFunc should be serial in time on a per taskID basis. I.E.: the run at 12:00 will go before the run at 12:01. Design: The core of the scheduler is a btree keyed by time, a nonce, and a task ID, and a map keyed by task ID and containing a nonce and a time (called a uniqueness index from now on). The map is to ensure task uniqueness in the tree, so we can replace or delete tasks in the tree. Scheduling in the tree consists of a main loop that feeds a fixed set of workers, each with their own communication channel. Distribution is handled by hashing the TaskID (to ensure uniform distribution) and then distributing over those channels evenly based on the hashed ID.  This is to ensure that all tasks of the same ID go to the same worker.The workers call ExecutorFunc handle any errors and update the LastScheduled time internally and also via the Checkpointer. The main loop: The main loop waits on a time.Timer to grab the task with the minimum time.  Once it successfully grabs a task ready to trigger, it will start walking the btree from the item nearest Putting a task into the scheduler: Adding a task to the scheduler acquires a write lock, grabs the task from the uniqueness map, and replaces the item in the uniqueness index and btree.  If new task would trigger sooner than the current soonest triggering task, it replaces the Timer when added to the scheduler.  Finally it releases the write lock. Removing a task from the scheduler: Removing a task from the scheduler acquires a write lock, deletes the task from the uniqueness index and from the btree, then releases the lock.  We do not have to readjust the time on delete, because, if the minimum task isn't ready yet, the main loop just resets the timer and keeps going. we need this index so we can delete items from the scheduled ErrorFunc is a function for error handling.  It is a good way to inject logging into a TreeScheduler. WithOnErrorFn is an option that sets the error function that gets called when there is an error in a TreeScheduler. its useful for injecting logging or special error handling. WithMaxConcurrentWorkers is an option that sets the max number of concurrent workers that a TreeScheduler will use. WithTime is an option for NewScheduler that allows you to inject a clock.Clock from ben johnson's github.com/benbjohnson/clock library, for testing purposes. NewScheduler gives us a new TreeScheduler and SchedulerMetrics when given an  Executor, a SchedulableService, and zero or more options. Schedulers should be initialized with this function. apply options because a stopped timer will wait forever, this allows us to wait for items to be added before triggering. close workchans this for loop is a work around to the way clock's mock works when you reset duration 0 in a different thread than you are calling your clock.Set grab a new item, because there could be a different item at the top of the queue grab a new item, because there could be a different item at the top of the queue after processing we can reset without a stop because we know it is fired here itemList is a list of items for deleting and inserting.  We have to do them separately instead of just a re-add, because usually the items key must be changed between the delete and insert Reset the length of the slice in preparation of the next iterator. we want it to panic if things other than Items are populating the scheduler, as it is something we can't recover from. distribute to the right worker. we just hash so that the number is uniformly distributed in this error case we can't schedule next, so we have to drop the task When gives us the next time the scheduler will run a task. delete the old task run time Release releases a task. Release also cancels the running task. Task deletion would be faster if the tree supported deleting ranges. work does work from the channel and checkpoints it. report the difference between when the item was supposed to be scheduled and now execute report how long execution took TODO(docmerlin): we can increase performance by making the call to UpdateLastScheduled async Schedule put puts a Schedulable on the TreeScheduler.last:   sch.LastScheduled().Unix(), insert the new task run time Item is a task in the scheduler. Less tells us if one Item is less than another/Users/austinjaybecker/projects/abeck-go-testing/task/backend/task.go TaskControlService is a low-level controller interface, intended to be passed to task executors and schedulers, which allows creation, completion, and status updates of runs. CreateRun creates a run with a scheduled for time. StartManualRun pulls a manual run from the list and moves it to currently running. FinishRun removes runID from the list of running tasks and if its `ScheduledFor` is later then last completed update it./Users/austinjaybecker/projects/abeck-go-testing/task/mock/Users/austinjaybecker/projects/abeck-go-testing/task/mock/executor.goNewTaskControlServicehangingFornextExecuteErrExecutedChanFailNextCallToExecutecould not add task ID to executedChan"could not add task ID to executedChan" Package mock contains mock implementations of different task interfaces. Forced error for next call to Execute. FailNextCallToExecute causes the next call to e.Execute to unconditionally return err.totalRunsCreatedfinishedRunsSetTaskSetManualRunsCreatedForTotalRunsCreatedForTaskPollForNumberCreatedFinishedRunFinishedRuns/Users/austinjaybecker/projects/abeck-go-testing/task/mock/task_control_service.goqrsactualCountnumAttemptsrun state called without a run"run state called without a run"cannot add a log to a non existent run"cannot add a log to a non existent run"2000000did not see count of %d created run(s) for task with ID %s in time, instead saw %d"did not see count of %d created run(s) for task with ID %s in time, instead saw %d" TaskControlService is a mock implementation of TaskControlService (used by NewScheduler). Map of stringified task ID to last ID used for run. Map of stringified, concatenated task and platform ID, to runs that have been created. Map of stringified task ID to task meta. Map of task ID to total number of runs created for that task. SetTask sets the task. SetTask must be called before CreateNextRun, for a given task ID. nothing TotalRunsCreatedForTask returns the number of runs created for taskID. PollForNumberCreated blocks for a small amount of time waiting for exactly the given count of created and unfinished runs for the given task ID. If the expected number isn't found in time, it returns an error. Because the scheduler and executor do a lot of state changes asynchronously, this is useful in test. we sleep even on first so it becomes more likely that we catch when too many are produced. we return created anyways, to make it easier to debug/Users/austinjaybecker/projects/abeck-go-testing/task/options/Users/austinjaybecker/projects/abeck-go-testing/task/options/flux_deps.goErrDuplicateIntervalFieldErrMultipleTaskOptionsDefinedErrNoASTFileErrNoTaskOptionsDefinedMustParseDurationcheckNatureerrMissingRequiredTaskOptionerrParseTaskOptionFielderrTaskInvalidDurationerrTaskOptionNotObjectExpressionevalASTextractConcurrencyOptionextractFnextractNameOptionextractOffsetOptionextractRetryOptionextractScheduleOptionsfileSystemgrabTaskOptionASThasDuplicateOptionsmaxConcurrencymaxRetrynewDepsoptConcurrencyoptCronoptEveryoptNameoptOffsetoptRetrytaskOptionExtractorsurlValidatorvalidateOptionNamesfilesystemfluxhttpfluxurlgithub.com/influxdata/flux/dependencies/filesystem"github.com/influxdata/flux/dependencies/filesystem"github.com/influxdata/flux/dependencies/http"github.com/influxdata/flux/dependencies/http"github.com/influxdata/flux/dependencies/secret"github.com/influxdata/flux/dependencies/secret"github.com/influxdata/flux/dependencies/url"github.com/influxdata/flux/dependencies/url"NopCloser/Users/austinjaybecker/projects/abeck-go-testing/task/options/options.goasmtfluxASTobjExprvanameExprnameStrcronExprStrcronErrcronExpreveryErreveryExproffsetExprVoffsetErroffsetExprconcurExprconcurIntretryExprretryIntdurNodeoffsetValconcurrencyValretryValcrValcronOKdurTypeseveryOKeveryValnameValoptObjectcronPresenteveryPresentjson:"concurrency,omitempty"`json:"concurrency,omitempty"`json:"retry,omitempty"`json:"retry,omitempty"`"OptionStatement""VariableAssignment"cron or every"cron or every"cron or every is required"cron or every is required"name required"name required"must specify exactly one of either cron or every"must specify exactly one of either cron or every"cron invalid: "cron invalid: "every option must be at least 1 second"every option must be at least 1 second"every option must be expressible as whole seconds"every option must be expressible as whole seconds"offset option must be expressible as whole seconds"offset option must be expressible as whole seconds"concurrency must be at least 1"concurrency must be at least 1"concurrency exceeded max of %d"concurrency exceeded max of %d"retry must be at least 1"retry must be at least 1"retry exceeded max of %d"retry exceeded max of %d"invalid options: %s"invalid options: %s"unexpected kind: got %q expected %q"unexpected kind: got %q expected %q"unknown task option(s): %s. valid options are %s"unknown task option(s): %s. valid options are %s" Package options provides ways to extract the task-related options from a Flux script. Options are the task-related options that can be specified in a Flux script. Name is a non optional name designator for each task. Cron is a cron style time schedule that can be used in place of Every. Every represents a fixed period to repeat execution. this can be unmarshaled from json as a string i.e.: "1d" will unmarshal as 1 day Duration is a time span that supports the same units as the flux parser's time duration, as well as negative length time spans. Parse parses a string into a Duration. MustParseDuration parses a string and returns a duration.  It panics if there is an error. UnmarshalText unmarshals text into a Duration. MarshalText marshals text into a Duration. IsZero checks if each segment of the duration is zero, it doesn't check if the Duration sums to zero, just if each internal duration is zero. DurationFrom gives us a time.Duration from a time. Currently because of how flux works, this is just an approfimation for any time unit larger than hours. Add adds the duration to a time. Clear clears out all options in the options struct, it us useful if you wish to reuse it. IsZero tells us if the options has been zeroed out. All the task option names we accept. contains is a helper function to see if an array of strings contains a string we preallocate two keys for the map, as that is how many we will use at maximum (offset and every) FromScriptAST extracts Task options from a Flux script using only the AST (no evaluation of the script). Using AST here allows us to avoid having to contend with functions that aren't available in some parsing contexts (within Gateway for example). hasDuplicateOptions determines whether or not there are multiple assignments to the same option variable. TODO(brett): This will be superceded by edit.HasDuplicateOptions once its available. FromScript extracts Options from a Flux script. TODO(desa): should be dependencies.NewEmpty(), but for now we'll hack things together pull options from the program scope check to make sure task is an object Validate returns an error if the options aren't valid. They're both present or both missing. For now, allowing negative offset delays. Maybe they're useful for forecasting? EffectiveCronString returns the effective cron string of the options. If the cron option was specified, it is returned. If the every option was specified, it is converted into a cron string using "@every". Otherwise, the empty string is returned. The value of the offset option is not considered. TODO(docmerlin): create an EffectiveCronStringFrom(t time.Time) string, that works from a unit of time. Do not use this if you haven't checked for validity already. we can ignore errors here because we have already checked for validity. checkNature returns a clean error of got and expected dont match. validateOptionNames returns an error if any keys in the option object o do not match an expected option name. Known option. Nothing to do. parse will take flux source code and produce a package./Users/austinjaybecker/projects/abeck-go-testing/task/options/options_errors.gofailed to parse field '%s' in task options"failed to parse field '%s' in task options"missing required option: %s"missing required option: %s"invalid duration in task %s"invalid duration in task %s"task option expected to be object literal, but found %q"task option expected to be object literal, but found %q"cannot use both cron and every in task options"cannot use both cron and every in task options"no task options defined"no task options defined"multiple task options defined"multiple task options defined"expected parsed file, but found none"expected parsed file, but found none" errParseTaskOptionField is returned when we fail to parse a single field in task options. errMissingRequiredTaskOption is returned when we a required option is missing. errTaskInvalidDuration is returned when an "every" or "offset" option is invalid in a task. errTaskOptionNotObjectExpression is returned when the type of an task option value is not an object literal expression./Users/austinjaybecker/projects/abeck-go-testing/task/options/strconv.go ParseSignedDuration is a helper wrapper around parser.ParseSignedDuration. We use it because we need to clear the basenode, but flux does not. TODO(jsternberg): This is copied from an internal package in flux to break a dependency on the parser package where this method is exposed. Consider exposing this properly in flux./Users/austinjaybecker/projects/abeck-go-testing/task/servicetest/Users/austinjaybecker/projects/abeck-go-testing/task/servicetest/servicetest.goBackendComponentFactorySystemTestCredsTestTaskServicescriptDifferentNamescriptFmttestLogsAcrossStoragetestManualRuntestRetryAcrossStoragetestRunStoragetestTaskCRUDtestTaskConcurrencytestTaskFindTasksAfterPagingtestTaskFindTasksPagingtestTaskOptionsUpdateFulltestTaskRunstestTaskTypetestUpdatecategorysystestCategoryfoundTasksactiveTasksauthorizedCtxfindTaskfindTasksByStatusinactiveTasksorigIDtc2tsktaskNameexpectedFluxsavedTaskfNoOffsetwithoutOffsetcaearliestCAearliestUAlatestCAlatestUArc2st2st3err0err1foundRun0rc0rc1expLine1expLine2log1Timelog2TimeaCtxerr2runsCreatedcreateTaskChcreateWgextraWgidMunumTaskstaskIDsfoundRun1runsLimit2smashedsmashtskCowtskPiginfluxdbmockservicetestCredsFuncCallFinishRuntransactional"transactional"analytical"analytical"TransactionalTaskService"TransactionalTaskService"Task CRUD"Task CRUD"FindTasks paging"FindTasks paging"FindTasks after paging"FindTasks after paging"Task Update Options Full"Task Update Options Full"Task Runs"Task Runs"Task Concurrency"Task Concurrency"skipping in short mode"skipping in short mode"Task Updates"Task Updates"Task Manual Run"Task Manual Run"Task Type"Task Type"AnalyticalTaskService"AnalyticalTaskService"Task Run Storage"Task Run Storage"Task RetryRun"Task RetryRun"task Log Storage"task Log Storage"option task = {
	name: "task #%d",
	cron: "* * * * *",
	offset: 5s,
	concurrency: 100,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")no task ID set"no task ID set"failed to find task by id %s"failed to find task by id %s""Created"FindTasks with Organization filter"FindTasks with Organization filter"FindTasks with Organization name filter"FindTasks with Organization name filter"FindTasks with User filter"FindTasks with User filter"task #0"task #0"* * * * *"* * * * *"got: %+#v"got: %+#v"expected %s task to be consistant: -got/+want: %s"expected %s task to be consistant: -got/+want: %s"failed to limit tasks: expected: 1, got : %d"failed to limit tasks: expected: 1, got : %d"expected at least 1 task: got 0"expected at least 1 task: got 0"after task included in task list"after task included in task list"expected to find %d active tasks, found: %d"expected to find %d active tasks, found: %d"expected to find %d inactive tasks, found: %d"expected to find %d inactive tasks, found: %d"task ID unexpectedly changed during update, from %s to %s"task ID unexpectedly changed during update, from %s to %s"wrong flux from update; want %q, got %q"wrong flux from update; want %q, got %q"expected task to be created active, got %q"expected task to be created active, got %q"flux unexpected updated: %s"flux unexpected updated: %s"expected task status to be inactive, got %q"expected task status to be inactive, got %q"option task = {
	name: "task-changed #98",
	cron: "* * * * *",
	offset: 5s,
	concurrency: 100,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")"option task = {\n\tname: \"task-changed #98\",\n\tcron: \"* * * * *\",\n\toffset: 5s,\n\tconcurrency: 100,\n}\n\nfrom(bucket: \"b\")\n\t|> to(bucket: \"two\", orgID: \"000000000000000\")"task-changed #98"task-changed #98"expected task status to be active, got %q"expected task status to be active, got %q"option task = {
	name: "task-changed #98",
	offset: 5s,
	concurrency: 100,
	every: 30s,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")"option task = {\n\tname: \"task-changed #98\",\n\toffset: 5s,\n\tconcurrency: 100,\n\tevery: 30s,\n}\n\nfrom(bucket: \"b\")\n\t|> to(bucket: \"two\", orgID: \"000000000000000\")"30s"30s"option task = {
	name: "task-changed #%d",
	offset: 5s,
	concurrency: 100,
	cron: "* * * * *",
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")expected %v, got %v"expected %v, got %v"option task = {
	name: "Task %03d",
	cron: "* * * * *",
	concurrency: 100,
	offset: 10s,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")`option task = {
	name: "Task %03d",
	cron: "* * * * *",
	concurrency: 100,
	offset: 10s,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")`FindTasks: %v"FindTasks: %v"unexpected len(taksks), -got/+exp: %v"unexpected len(taksks), -got/+exp: %v"Task 004"Task 004"option task = {
	name: "some-unique-task-name",
	cron: "* * * * *",
	concurrency: 100,
	offset: 10s,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")`option task = {
	name: "some-unique-task-name",
	cron: "* * * * *",
	concurrency: 100,
	offset: 10s,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")`some-unique-task-name"some-unique-task-name"expected %#v, found %#v"expected %#v, found %#v"option task = {
	name: "task-Options-Update",
	cron: "* * * * *",
	concurrency: 100,
	offset: 10s,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")`option task = {
	name: "task-Options-Update",
	cron: "* * * * *",
	concurrency: 100,
	offset: 10s,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")`update task and delete offset"update task and delete offset"option task = {name: "task-Options-Update", concurrency: 100, every: 10s}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")`option task = {name: "task-Options-Update", concurrency: 100, every: 10s}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")`10s"10s"update task with different offset option"update task with different offset option"option task = {
	name: "task-Options-Update",
	concurrency: 100,
	every: 10s,
	offset: 10s,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")`option task = {
	name: "task-Options-Update",
	concurrency: 100,
	every: 10s,
	offset: 10s,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")`option task = {
	name: "task-Options-Update",
	every: 10s,
	concurrency: 100,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")`option task = {
	name: "task-Options-Update",
	every: 10s,
	concurrency: 100,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")`removing offset failed"removing offset failed"-1000000000expected a non-zero LatestScheduled on created task"expected a non-zero LatestScheduled on created task"createdAt not accurate, expected %s to be between %s and %s"createdAt not accurate, expected %s to be between %s and %s"latest completed not accurate, expected: ~%s, got %s"latest completed not accurate, expected: ~%s, got %s"executed task has not updated latest complete: expected %s > %s"executed task has not updated latest complete: expected %s > %s"executed task has not updated last run status"executed task has not updated last run status"executed task has updated last run error on success"executed task has updated last run error on success"error message"error message"last message"last message"executed task has not updated last run error on failed"executed task has not updated last run error on failed"updatedAt not accurate, expected %s to be between %s and %s"updatedAt not accurate, expected %s to be between %s and %s"updatedAt not accurate after pulling new task, expected %s to be between %s and %s"updatedAt not accurate after pulling new task, expected %s to be between %s and %s"expected latest scheduled to update, expected: %v, got: %v"expected latest scheduled to update, expected: %v, got: %v"FindRuns and FindRunByID"FindRuns and FindRunByID"failed to error with out of bounds run limit: %d"failed to error with out of bounds run limit: %d"wrong task ID on created task: got %s, want %s"wrong task ID on created task: got %s, want %s"wrong task ID on created task run: got %s, want %s"wrong task ID on created task run: got %s, want %s"expected 1 run, got %#v"expected 1 run, got %#v"expected 1 run, got %v"expected 1 run, got %v"retrieved wrong run ID; want %s, got %s"retrieved wrong run ID; want %s, got %s"unexpectedStartedAt; want %s, got %s"unexpectedStartedAt; want %s, got %s"unexpected run status; want %s, got %s"unexpected run status; want %s, got %s"expected empty FinishedAt, got %q"expected empty FinishedAt, got %q"expected %s but got %s instead"expected %s but got %s instead"difference between listed run and found run: %s"difference between listed run and found run: %s"FindRunsByTime"FindRunsByTime"40000000001970-01-01T00:01:17Z"1970-01-01T00:01:17Z"expected: 1970-01-01T00:01:17Z, got %s"expected: 1970-01-01T00:01:17Z, got %s"subsequent force should have been rejected; failed to error: %s"subsequent force should have been rejected; failed to error: %s""FindLogs"entry 1"entry 1"unexpected log: -got/+want: %s"unexpected log: -got/+want: %s"entry 2"entry 2"450error creating task: %v"error creating task: %v"Concurrently deleted %d tasks"Concurrently deleted %d tasks"error finding tasks: %v"error finding tasks: %v"error deleting task: %v"error deleting task: %v"Concurrently created %d runs"Concurrently created %d runs"253339232461253339232469error creating next run: %v"error creating next run: %v"force run returned a different scheduled for time expected: %s, got %s"force run returned a different scheduled for time expected: %s, got %s"expected 1 manual run: got %d"expected 1 manual run: got %d"manual run missmatch: %s"manual run missmatch: %s"-10-100000000003000000000expected 2 runs, got %v"expected 2 runs, got %v"expected 3 runs, got %v"expected 3 runs, got %v"unexpected StartedAt; want %s, got %s"unexpected StartedAt; want %s, got %s"unexpected FinishedAt; want %s, got %s"unexpected FinishedAt; want %s, got %s"expected retrying run that doesn't exist to return %v, got %v"expected retrying run that doesn't exist to return %v, got %v"wrong task ID on retried run: got %s, want %s"wrong task ID on retried run: got %s, want %s""scheduled"expected new retried run to have status of scheduled"expected new retried run to have status of scheduled"wrong scheduledFor on task: got %s, want %s"wrong scheduledFor on task: got %s, want %s"run already queued"run already queued"subsequent retry should have been rejected with %v; got %v"subsequent retry should have been rejected with %v; got %v"0-0"0-0"0-1"0-1"0-2"0-2"1-0"1-0"1-1"1-1"1-2"1-2"1-3"1-3"log: %+v
"log: %+v\n"failed to get all logs: expected: 7 got: %d"failed to get all logs: expected: 7 got: %d"0-00-10-21-01-11-21-3"0-00-10-21-01-11-21-3"log contents not acceptable, expected: %q, got: %q"log contents not acceptable, expected: %q, got: %q"failed to get all logs: expected: 4 got: %d"failed to get all logs: expected: 4 got: %d"1-01-11-21-3"1-01-11-21-3"failed to get all logs: expected: 3 got: %d"failed to get all logs: expected: 3 got: %d"0-00-10-2"0-00-10-2"-user"-user"-org"-org"`option task = {
	name: "task #%d",
	cron: "* * * * *",
	offset: 5s,
	concurrency: 100,
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")``option task = {
	name: "task-changed #%d",
	offset: 5s,
	concurrency: 100,
	cron: "* * * * *",
}

from(bucket: "b")
	|> to(bucket: "two", orgID: "000000000000000")`cows"cows"pigs"pigs"received a task with a type when sending no type restriction"received a task with a type when sending no type restriction"tasks: %+v
"tasks: %+v\n"failed to return tasks by type, expected 1, got %d"failed to return tasks by type, expected 1, got %d"failed to return tasks with wildcard, expected 3, got %d"failed to return tasks with wildcard, expected 3, got %d" Package servicetest provides tests to ensure that implementations of platform/task/backend.Store and platform/task/backend.LogReader meet the requirements of influxdb.TaskService. Consumers of this package must import query/builtin. This package does not import it directly, to avoid requiring it too early. BackendComponentFactory is supplied by consumers of the adaptertest package, to provide the values required to constitute a PlatformAdapter. The provided context.CancelFunc is called after the test, and it is the implementer's responsibility to clean up after that is called. If creating the System fails, the implementer should call t.Fatal. TestTaskService should be called by consumers of the servicetest package. This will call fn once to create a single influxdb.TaskService used across all subtests in TestTaskService. We're running the subtests in parallel, but if we don't use this wrapper, the defer cancel() call above would return before the parallel subtests completed. Running the subtests in parallel might make them slightly faster, but more importantly, it should exercise concurrency to catch data races. TestCreds encapsulates credentials needed for a system to properly work with tasks. Authorizer returns an authorizer for the credentials in the struct System  as in "system under test" encapsulates the required parts of a influxdb.TaskAdapter Used in the Creds function to create valid organizations, users, tokens, etc. Set this context, to be used in tests, so that any spawned goroutines watching Ctx.Done() will clean up after themselves. TaskService is the task service we would like to test Override for accessing credentials for an individual test. Callers can leave this nil and the test will create its own random IDs for each test. However, if the system needs to verify credentials, the caller should set this value and return valid IDs and a valid token. It is safe if this returns the same values every time it is called. Toggles behavior between KV and archive storage because FinishRun() deletes runs after completion Create a task. TODO: replace with ErrMissingOwner test // should not be able to create a task without a token noToken := influxdb.TaskCreate{ 	OrganizationID: cr.OrgID, 	Flux:           fmt.Sprintf(scriptFmt, 0), 	// OwnerID:          cr.UserID, // should fail _, err = sys.TaskService.CreateTask(authorizedCtx, noToken) if err != influxdb.ErrMissingToken { 	t.Fatalf("expected error missing token, got: %v", err) Look up a task the different ways we can. Map of method name to found task. Find by ID should return the right task. Check limits Check after because this test runs concurrently we can only guarantee we at least 2 tasks when using after we can check to make sure the after is not in the list Check task status filter Update task: script only. Update task: status only. Update task: reactivate status and update script. Update task: just update an option. Update task: switch to every. Update task: just cron. // Update task: use a new token on the context and modify some other option. // Ensure the authorization doesn't change -- it did change at one time, which was bug https://github.com/influxdata/influxdb/issues/12218. newAuthz := &influxdb.Authorization{OrgID: cr.OrgID, UserID: cr.UserID, Permissions: influxdb.OperPermissions()} if err := sys.I.CreateAuthorization(sys.Ctx, newAuthz); err != nil { 	t.Fatal(err) newAuthorizedCtx := icontext.SetAuthorizer(sys.Ctx, newAuthz) f, err = sys.TaskService.UpdateTask(newAuthorizedCtx, origID, influxdb.TaskUpdate{Options: options.Options{Name: "foo"}}) if f.Name != "foo" { 	t.Fatalf("expected name to update to foo, got %s", f.Name) if f.AuthorizationID != authzID { 	t.Fatalf("expected authorization ID to remain %v, got %v", authzID, f.AuthorizationID) // Now actually update to use the new token, from the original authorization context. f, err = sys.TaskService.UpdateTask(authorizedCtx, origID, influxdb.TaskUpdate{Token: newAuthz.Token}) if f.AuthorizationID != newAuthz.ID { 	t.Fatalf("expected authorization ID %v, got %v", newAuthz.ID, f.AuthorizationID) Delete task. Task should not be returned. find tasks using name which are after first 10 last page should be empty one more than expected pagesCreate a new task with a Cron and Offset optionUpdate the task to remove the Offset option, and change Cron to EveryRetrieve the task again to ensure the options are now Every, without Cron or Offset round to remove monotonic clock Script is set to run every minute. The platform adapter is currently hardcoded to schedule after "now", which makes timing of runs somewhat difficult. check run filter errors This should guarantee we can make two runs. Update the run state to Started; normally the scheduler would do this. Mark the second run finished. Limit 1 should only return the earlier run. Look for a run that doesn't exist. look for a taskID that doesn't exist. set to one hour before now because of bucket retention policy create runs to put into Context setting run in memory to match the fields in Context Analytical storage does not store run at Forcing the same run before it's executed should be rejected. This should guarantee we can make a run. Create two runs. Add a log for the first run. Ensure it is returned when filtering logs by run ID. Add a log for the second run. Ensure both returned when filtering logs by task ID. Arbitrarily chosen to get a reasonable count of concurrent creates and deletes. Since this test is run in parallel with other tests, we need to keep a whitelist of IDs that are okay to delete. This only matters when the creds function returns an identical user/org from another test. Signal for non-creator goroutines to stop. Get all the tasks, and delete the first one we find. Check if we need to quit. Get all the tasks. Check again if we need to quit. Was the retrieved task an ID we're allowed to delete? Task was in whitelist. Delete it from the TaskService. We could remove it from the taskIDs map, but this test is short-lived enough that clearing out the map isn't really worth taking the lock again. Wait just a tiny bit. Create a run for the last task we found. The script should run every minute, so use max now. This may have errored due to the task being deleted. Check if the task still exists. It was deleted. Just continue. Otherwise, we were able to find the task, so something went wrong here. Start adding tasks. Done adding. Wait for cleanup. Create 3rd run and test limiting to 2 runs Unspecified limit returns all three runs, sorted by most recently scheduled first. TODO (al): handle empty finishedAt if runs[0].FinishedAt != "" { 	t.Fatalf("expected empty FinishedAt, got %q", runs[0].FinishedAt) Script is set to run every minute. Non-existent ID should return the right error. Update the run state to Started then Failed; normally the scheduler would do this. Now retry the run. Retrying a run which has been queued but not started, should be rejected. Create several run logs in both rc0 and rc1 We can then finalize rc1 and ensure that both the transactional (currently running logs) can be found with analytical (completed) logs. t.Helper() Create a tasks get system tasks (or task's with no type) get filtered tasks get all tasks/Users/austinjaybecker/projects/abeck-go-testing/task.gojooldFluxeditFuncparsedPKGoptsExprtaskOptionsjson:"runAt"`json:"runAt"`json:"runID,omitempty"`json:"runID,omitempty"`missing flux"missing flux"missing orgID and org"missing orgID and org"invalid task status: %q"invalid task status: %q"json:"flux,omitempty"`json:"flux,omitempty"`cannot specify both every and cron"cannot specify both every and cron"every: %s is invalid"every: %s is invalid"offset: %s, %s is invalid, the largest unit supported is h"offset: %s, %s is invalid, the largest unit supported is h"cannot update task without content"cannot update task without content"flux parser is not configured; updating a task requires the flux parser to be set"flux parser is not configured; updating a task requires the flux parser to be set"internal error in flux engine; unable to parse"internal error in flux engine; unable to parse"cannot specify both cron and every"cannot specify both cron and every"option assignment must be variable assignment"option assignment must be variable assignment"value is is %s, not an object expression"value is is %s, not an object expression"OptionFnunable to edit option"unable to edit option"DeletePropertyunknown RunStatus: %d"unknown RunStatus: %d"previous retry for start=%s end=%s has not yet finished"previous retry for start=%s end=%s has not yet finished"Sscanf TODO(jsteenb2): make these constants of type Status TaskSystemType is the type set in tasks' for all crud requests Task is a task. ð EffectiveCron returns the effective cron string of the options. Run is a record createId when a run of a task is scheduled. ScheduledFor is the Now time used in the task's query RunAt is the time the task is scheduled to be run, which is ScheduledFor + Offset StartedAt is the time the executor begins running the task FinishedAt is the time the executor finishes running the task RequestedAt is the time the coordinator told the scheduler to schedule the task Log represents a link to a log resource TaskService represents a service for managing one-off and recurring tasks. TaskCreate is the set of values to create a task. not to be set through a web request but rather used by a http service using tasks backend. TaskUpdate represents updates to a task. Options updates override any options set in the Flux field. LatestCompleted us to set latest completed on startup to skip task catchup Options gets unmarshalled from json as if it was flat, with the same level as Flux and Status. when we unmarshal this gets unmarshalled from flat key-values this is a type so we can marshal string into durations nicely safeParseSource calls the Flux parser.ParseSource function and is guaranteed not to panic. UpdateFlux updates the TaskUpdate to go from updating options to updating a flux string, that now has those updated options in it. It zeros the options in the TaskUpdate. modify in the keys and values that already are in the ast add in new keys and values to the ast TaskFilter represents a set of filters that restrict the returned results QueryParams Converts TaskFilter fields to url query params. RunFilter represents a set of filters that restrict the returned results Task ID is required for listing runs. LogFilter represents a set of filters that restrict the returned log results. Task ID is required. The optional Run ID limits logs to a single run. RequestStillQueuedError is returned when attempting to retry a run which has not yet completed. Unix timestamps matching existing request's start and end. ParseRequestStillQueuedError attempts to parse a RequestStillQueuedError from msg. If msg is formatted correctly, the resultant error is returned; otherwise it returns nil./Users/austinjaybecker/projects/abeck-go-testing/task_errors.gorunsInFrontrun canceled"run canceled"task not claimed"task not claimed"task already claimed"task already claimed"no matching runs found"no matching runs found"invalid id"invalid id"task not found"task not found"run key not found"run key not found"cannot have negative page limit"cannot have negative page limit"cannot use page size larger then %d"cannot use page size larger then %d"run limit is out of bounds, must be between 1 and 500"run limit is out of bounds, must be between 1 and 500"cannot create task with invalid ownerID"cannot create task with invalid ownerID"could not parse Flux script; Err: %v"could not parse Flux script; Err: %v"taskExecutor"taskExecutor"unexpected error from queryd; Err: %v"unexpected error from queryd; Err: %v"Error exhausting result iterator; Err: %v"Error exhausting result iterator; Err: %v"unexpected error in tasks; Err: %v"unexpected error in tasks; Err: %v"unexpected error retrieving task bucket; Err: %v"unexpected error retrieving task bucket; Err: %v""taskBucket"unexpected error parsing time; Err: %v"unexpected error parsing time; Err: %v"taskCron"taskCron"invalid options; Err: %v"invalid options; Err: %v""taskOptions"unable to marshal JSON; Err: %v"unable to marshal JSON; Err: %v"taskScheduler"taskScheduler"could not execute task run; Err: %v"could not execute task run; Err: %v"could not execute task, concurrency limit reached, runs in front: %d"could not execute task, concurrency limit reached, runs in front: %d" ErrRunCanceled is returned from the RunResult when a Run is Canceled.  It is used mostly internally. ErrTaskNotClaimed is returned when attempting to operate against a task that must be claimed but is not. ErrTaskAlreadyClaimed is returned when attempting to operate against a task that must not be claimed but is. ErrNoRunsFound is returned when searching for a range of runs, but none are found. ErrInvalidTaskID error object for bad id's ErrTaskNotFound indicates no task could be found for given parameters. ErrRunNotFound is returned when searching for a single run that doesn't exist. ErrPageSizeTooLarge indicates the page size is too large. This error is only used in the kv task service implementation. The name of this error may lead it to be used in a place that is not useful. The TaskMaxPageSize is the only one at 500, the rest at 100. This would likely benefit from a more specific name since those limits aren't shared globally. ErrOutOfBoundsLimit is returned with FindRuns is called with an invalid filter limit. ErrInvalidOwnerID is called when trying to create a task with out a valid ownerID ErrFluxParseError is returned when an error is thrown by Flux.Parse in the task executor ErrQueryError is returned when an error is thrown by Query service in the task executor ErrResultIteratorError is returned when an error is thrown by exhaustResultIterators in the executor ErrUnexpectedTaskBucketErr a generic error we can use when we rail to retrieve a bucket ErrTaskTimeParse an error for time parsing errors/Users/austinjaybecker/projects/abeck-go-testing/telegraf/Users/austinjaybecker/projects/abeck-go-testing/telegraf/index.gotelegrafbyorgindexv1"telegrafbyorgindexv1" ByOrganizationIndexMapping is the mapping definition for fetching telegrafs by organization ID./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/base.goCPUStatsDiskIODiskStatsDockerKernelKubernetesLogParserPluginMemStatsNetIOStatsNetResponseNginxProcessesProcstatPrometheusRedisSwapStatsSystemStatsTailbaseInputInputExe/Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/cpu.go"cpu"[[inputs.%s]]
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics.
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states.
  report_active = false
`[[inputs.%s]]
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics.
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states.
  report_active = false
` CPUStats is based on telegraf CPUStats. PluginName is based on telegraf plugin name. UnmarshalTOML decodes the parsed data to the object TOML encodes to toml string/Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/disk.go"disk"[[inputs.%s]]
  ## By default stats will be gathered for all mount points.
  ## Set mount_points will restrict the stats to only the specified mount points.
  # mount_points = ["/"]
  ## Ignore mount points by filesystem type.
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "overlay", "aufs", "squashfs"]
`[[inputs.%s]]
  ## By default stats will be gathered for all mount points.
  ## Set mount_points will restrict the stats to only the specified mount points.
  # mount_points = ["/"]
  ## Ignore mount points by filesystem type.
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "overlay", "aufs", "squashfs"]
` DiskStats is based on telegraf DiskStats./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/diskio.godiskio"diskio"[[inputs.%s]]
`[[inputs.%s]]
` DiskIO is based on telegraf DiskIO. TOML encodes to toml string./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/docker.godataOKjson:"endpoint"`json:"endpoint"`docker"docker"bad endpoint for docker input plugin"bad endpoint for docker input plugin"[[inputs.%s]]	
  ## Docker Endpoint
  ##   To use TCP, set endpoint = "tcp://[ip]:[port]"
  ##   To use environment variables (ie, docker-machine), set endpoint = "ENV"
  ##   exp: unix:///var/run/docker.sock
  endpoint = "%s"

  ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)
  gather_services = false

  ## Only collect metrics for these containers, collect all if empty
  container_names = []

  ## Containers to include and exclude. Globs accepted.
  ## Note that an empty array for both will include all containers
  container_name_include = []
  container_name_exclude = []

  ## Container states to include and exclude. Globs accepted.
  ## When empty only containers in the "running" state will be captured.
  # container_state_include = []
  # container_state_exclude = []

  ## Timeout for docker list, info, and stats commands
  timeout = "5s"

  ## Whether to report for each container per-device blkio (8:0, 8:1...) and
  ## network (eth0, eth1, ...) stats or not
  perdevice = true

  ## Whether to report for each container total blkio and network stats or not
  total = false
  
  ## Which environment variables should we use as a tag
  ##tag_env = ["JAVA_HOME", "HEAP_SIZE"]
  ## docker labels to include and exclude as tags.  Globs accepted.
  ## Note that an empty array for both will include all labels as tags
  docker_label_include = []
  docker_label_exclude = []
`[[inputs.%s]]	
  ## Docker Endpoint
  ##   To use TCP, set endpoint = "tcp://[ip]:[port]"
  ##   To use environment variables (ie, docker-machine), set endpoint = "ENV"
  ##   exp: unix:///var/run/docker.sock
  endpoint = "%s"

  ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)
  gather_services = false

  ## Only collect metrics for these containers, collect all if empty
  container_names = []

  ## Containers to include and exclude. Globs accepted.
  ## Note that an empty array for both will include all containers
  container_name_include = []
  container_name_exclude = []

  ## Container states to include and exclude. Globs accepted.
  ## When empty only containers in the "running" state will be captured.
  # container_state_include = []
  # container_state_exclude = []

  ## Timeout for docker list, info, and stats commands
  timeout = "5s"

  ## Whether to report for each container per-device blkio (8:0, 8:1...) and
  ## network (eth0, eth1, ...) stats or not
  perdevice = true

  ## Whether to report for each container total blkio and network stats or not
  total = false
  
  ## Which environment variables should we use as a tag
  ##tag_env = ["JAVA_HOME", "HEAP_SIZE"]
  ## docker labels to include and exclude as tags.  Globs accepted.
  ## Note that an empty array for both will include all labels as tags
  docker_label_include = []
  docker_label_exclude = []
` Docker is based on telegraf Docker plugin./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/file.gobad files for file input plugin"bad files for file input plugin""files"not an array for file input plugin"not an array for file input plugin"[[inputs.%s]]	
  ## Files to parse each interval.
  ## These accept standard unix glob matching rules, but with the addition of
  ## ** as a "super asterisk". ie:
  ##   /var/log/**.log     -> recursively find all .log files in /var/log
  ##   /var/log/*/*.log    -> find all .log files with a parent dir in /var/log
  ##   /var/log/apache.log -> only read the apache log file
  files = [%s]

  ## The dataformat to be read from files
  ## Each data format has its own unique set of configuration options, read
  ## more about them here:
  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
  data_format = "influx"
`[[inputs.%s]]	
  ## Files to parse each interval.
  ## These accept standard unix glob matching rules, but with the addition of
  ## ** as a "super asterisk". ie:
  ##   /var/log/**.log     -> recursively find all .log files in /var/log
  ##   /var/log/*/*.log    -> find all .log files with a parent dir in /var/log
  ##   /var/log/apache.log -> only read the apache log file
  files = [%s]

  ## The dataformat to be read from files
  ## Each data format has its own unique set of configuration options, read
  ## more about them here:
  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
  data_format = "influx"
` File is based on telegraf input File plugin./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/kernel.gokernel"kernel" Kernel is based on telegraf Kernel./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/kubernetes.gokubernetes"kubernetes"[[inputs.%s]]
  ## URL for the kubelet
  ## exp: http://1.1.1.1:10255
  url = "%s"	
`[[inputs.%s]]
  ## URL for the kubelet
  ## exp: http://1.1.1.1:10255
  url = "%s"	
`bad url for kubernetes input plugin"bad url for kubernetes input plugin" Kubernetes is based on telegraf Kubernetes plugin/Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/logparser.gologparser"logparser"[[inputs.%s]]	
  ## Log files to parse.
  ## These accept standard unix glob matching rules, but with the addition of
  ## ** as a "super asterisk". ie:
  ##   /var/log/**.log     -> recursively find all .log files in /var/log
  ##   /var/log/*/*.log    -> find all .log files with a parent dir in /var/log
  ##   /var/log/apache.log -> only tail the apache log file
  files = [%s]

  ## Read files that currently exist from the beginning. Files that are created
  ## while telegraf is running (and that match the "files" globs) will always
  ## be read from the beginning.
  from_beginning = false
  ## Method used to watch for file updates.  Can be either "inotify" or "poll".
  # watch_method = "inotify"
  ## Parse logstash-style "grok" patterns:
  [inputs.logparser.grok]
    ## This is a list of patterns to check the given log file(s) for.
    ## Note that adding patterns here increases processing time. The most
    ## efficient configuration is to have one pattern per logparser.
    ## Other common built-in patterns are:
    ##   %%{COMMON_LOG_FORMAT}   (plain apache & nginx access logs)
    ##   %%{COMBINED_LOG_FORMAT} (access logs + referrer & agent)
    patterns = ["%%{COMBINED_LOG_FORMAT}"]
    ## Name of the outputted measurement name.
    measurement = "apache_access_log"
`[[inputs.%s]]	
  ## Log files to parse.
  ## These accept standard unix glob matching rules, but with the addition of
  ## ** as a "super asterisk". ie:
  ##   /var/log/**.log     -> recursively find all .log files in /var/log
  ##   /var/log/*/*.log    -> find all .log files with a parent dir in /var/log
  ##   /var/log/apache.log -> only tail the apache log file
  files = [%s]

  ## Read files that currently exist from the beginning. Files that are created
  ## while telegraf is running (and that match the "files" globs) will always
  ## be read from the beginning.
  from_beginning = false
  ## Method used to watch for file updates.  Can be either "inotify" or "poll".
  # watch_method = "inotify"
  ## Parse logstash-style "grok" patterns:
  [inputs.logparser.grok]
    ## This is a list of patterns to check the given log file(s) for.
    ## Note that adding patterns here increases processing time. The most
    ## efficient configuration is to have one pattern per logparser.
    ## Other common built-in patterns are:
    ##   %%{COMMON_LOG_FORMAT}   (plain apache & nginx access logs)
    ##   %%{COMBINED_LOG_FORMAT} (access logs + referrer & agent)
    patterns = ["%%{COMBINED_LOG_FORMAT}"]
    ## Name of the outputted measurement name.
    measurement = "apache_access_log"
`bad files for logparser input plugin"bad files for logparser input plugin"files is not an array for logparser input plugin"files is not an array for logparser input plugin" LogParserPlugin is based on telegraf LogParserPlugin./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/mem.go"mem" MemStats is based on telegraf MemStats./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/net.go NetIOStats is based on telegraf NetIOStats./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/net_response.gonet_response"net_response"[[inputs.%s]]
  ## Protocol, must be "tcp" or "udp"
  ## NOTE: because the "udp" protocol does not respond to requests, it requires
  ## a send/expect string pair (see below).
  protocol = "tcp"
  ## Server address (default localhost)
  address = "localhost:80"
`[[inputs.%s]]
  ## Protocol, must be "tcp" or "udp"
  ## NOTE: because the "udp" protocol does not respond to requests, it requires
  ## a send/expect string pair (see below).
  protocol = "tcp"
  ## Server address (default localhost)
  address = "localhost:80"
` NetResponse is based on telegraf NetResponse./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/ngnix.gonginx"nginx"[[inputs.%s]]
  # An array of Nginx stub_status URI to gather stats.
  # exp http://localhost/server_status
  urls = [%s]
`[[inputs.%s]]
  # An array of Nginx stub_status URI to gather stats.
  # exp http://localhost/server_status
  urls = [%s]
`bad urls for nginx input plugin"bad urls for nginx input plugin"urls is not an array for nginx input plugin"urls is not an array for nginx input plugin" Nginx is based on telegraf nginx plugin./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/processes.goprocesses"processes" Processes is based on telegraf Processes./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/procstats.gojson:"exe"`json:"exe"`procstat"procstat"[[inputs.%s]]
  ## executable name (ie, pgrep <exe>)
  exe = "%s"
`[[inputs.%s]]
  ## executable name (ie, pgrep <exe>)
  exe = "%s"
`bad exe for procstat input plugin"bad exe for procstat input plugin"exe"exe" Procstat is based on telegraf procstat input plugin./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/prometheus.go[[inputs.%s]]	
  ## An array of urls to scrape metrics from.
  urls = [%s]
`[[inputs.%s]]	
  ## An array of urls to scrape metrics from.
  urls = [%s]
`bad urls for prometheus input plugin"bad urls for prometheus input plugin"urls is not an array for prometheus input plugin"urls is not an array for prometheus input plugin" Prometheus is based on telegraf Prometheus plugin./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/redis.gojson:"servers"`json:"servers"`redis"redis"  # password = ""`  # password = ""`  password = "%s"`  password = "%s"`[[inputs.%s]]
  ## specify servers via a url matching:
  ##  [protocol://][:password]@address[:port]
  ##  e.g.
  ##    tcp://localhost:6379
  ##    tcp://:password@192.168.99.100
  ##    unix:///var/run/redis.sock
  ##
  ## If no servers are specified, then localhost is used as the host.
  ## If no port is specified, 6379 is used
  servers = [%s]

  ## specify server password
%s
`[[inputs.%s]]
  ## specify servers via a url matching:
  ##  [protocol://][:password]@address[:port]
  ##  e.g.
  ##    tcp://localhost:6379
  ##    tcp://:password@192.168.99.100
  ##    unix:///var/run/redis.sock
  ##
  ## If no servers are specified, then localhost is used as the host.
  ## If no port is specified, 6379 is used
  servers = [%s]

  ## specify server password
%s
`bad servers for redis input plugin"bad servers for redis input plugin""servers"servers is not an array for redis input plugin"servers is not an array for redis input plugin" Redis is based on telegraf Redis plugin./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/swap.goswap"swap" SwapStats is based on telegraf SwapStats./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/syslog.gojson:"server"`json:"server"`syslog"syslog"[[inputs.%s]]
  ## Specify an ip or hostname with port - eg., tcp://localhost:6514, tcp://10.0.0.1:6514
  ## Protocol, address and port to host the syslog receiver.
  ## If no host is specified, then localhost is used.
  ## If no port is specified, 6514 is used (RFC5425#section-4.1).
  server = "%s"
`[[inputs.%s]]
  ## Specify an ip or hostname with port - eg., tcp://localhost:6514, tcp://10.0.0.1:6514
  ## Protocol, address and port to host the syslog receiver.
  ## If no host is specified, then localhost is used.
  ## If no port is specified, 6514 is used (RFC5425#section-4.1).
  server = "%s"
`bad server for syslog input plugin"bad server for syslog input plugin" Syslog is based on telegraf Syslog plugin./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/system.go SystemStats is based on telegraf SystemStats./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/inputs/tail.go"tail"[[inputs.%s]]	
  ## files to tail.
  ## These accept standard unix glob matching rules, but with the addition of
  ## ** as a "super asterisk". ie:
  ##   "/var/log/**.log"  -> recursively find all .log files in /var/log
  ##   "/var/log/*/*.log" -> find all .log files with a parent dir in /var/log
  ##   "/var/log/apache.log" -> just tail the apache log file
  ##
  ## See https://github.com/gobwas/glob for more examples
  ##
  files = [%s]

  ## Read file from beginning.
  from_beginning = false
  ## Whether file is a named pipe
  pipe = false
  ## Method used to watch for file updates.  Can be either "inotify" or "poll".
  # watch_method = "inotify"
  ## Data format to consume.
  ## Each data format has its own unique set of configuration options, read
  ## more about them here:
  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
  data_format = "influx"
`[[inputs.%s]]	
  ## files to tail.
  ## These accept standard unix glob matching rules, but with the addition of
  ## ** as a "super asterisk". ie:
  ##   "/var/log/**.log"  -> recursively find all .log files in /var/log
  ##   "/var/log/*/*.log" -> find all .log files with a parent dir in /var/log
  ##   "/var/log/apache.log" -> just tail the apache log file
  ##
  ## See https://github.com/gobwas/glob for more examples
  ##
  files = [%s]

  ## Read file from beginning.
  from_beginning = false
  ## Whether file is a named pipe
  pipe = false
  ## Method used to watch for file updates.  Can be either "inotify" or "poll".
  # watch_method = "inotify"
  ## Data format to consume.
  ## Each data format has its own unique set of configuration options, read
  ## more about them here:
  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
  data_format = "influx"
`bad files for tail input plugin"bad files for tail input plugin"not an array for tail input plugin"not an array for tail input plugin" Tail is based on telegraf Tail plugin./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/outputs/Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/outputs/base.goFileConfigInfluxDBV2baseOutput/Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/outputs/file.gojson:"path"`json:"path"`stdout"stdout"[[outputs.%s]]
  ## Files to write to, "stdout" is a specially handled file.
  files = [%s]
`[[outputs.%s]]
  ## Files to write to, "stdout" is a specially handled file.
  files = [%s]
`bad files for file output plugin"bad files for file output plugin"not an array for file output plugin"not an array for file output plugin" File is based on telegraf file output plugin. FileConfig is the config settings of output file plugin./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/outputs/influxdb_v2.goinfluxdb_v2"influxdb_v2"[[outputs.%s]]	
  ## The URLs of the InfluxDB cluster nodes.
  ##
  ## Multiple URLs can be specified for a single cluster, only ONE of the
  ## urls will be written to each interval.
  ## urls exp: http://127.0.0.1:8086
  urls = [%s]

  ## Token for authentication.
  token = "%s"

  ## Organization is the name of the organization you wish to write to; must exist.
  organization = "%s"

  ## Destination bucket to write into.
  bucket = "%s"
`[[outputs.%s]]	
  ## The URLs of the InfluxDB cluster nodes.
  ##
  ## Multiple URLs can be specified for a single cluster, only ONE of the
  ## urls will be written to each interval.
  ## urls exp: http://127.0.0.1:8086
  urls = [%s]

  ## Token for authentication.
  token = "%s"

  ## Organization is the name of the organization you wish to write to; must exist.
  organization = "%s"

  ## Destination bucket to write into.
  bucket = "%s"
`bad urls for influxdb_v2 output plugin"bad urls for influxdb_v2 output plugin"urls is not an array for influxdb_v2 output plugin"urls is not an array for influxdb_v2 output plugin"token is missing for influxdb_v2 output plugin"token is missing for influxdb_v2 output plugin"organization is missing for influxdb_v2 output plugin"organization is missing for influxdb_v2 output plugin"bucket is missing for influxdb_v2 output plugin"bucket is missing for influxdb_v2 output plugin" InfluxDBV2 is based on telegraf influxdb_v2 output plugin./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/plugins.goAgentConfigAvailableAggregatorsAvailableBundlesAvailableInputsAvailableOutputsAvailableProcessorsProcessoravailableAggregatorsavailableInputsavailableOutputsavailableProcessorssortPluginsjson:"config,omitempty"`json:"config,omitempty"`json:"os,omitempty"`json:"os,omitempty"`json:"plugins,omitempty"`json:"plugins,omitempty"`processor"processor""aggregator"bundle"bundle"unknown plugin type '%s'"unknown plugin type '%s'"1.13.0"1.13.0""unix"System Bundle"System Bundle"Collection of system related inputs"Collection of system related inputs"# Read metrics about cpu usage
[[inputs.cpu]]
  # alias="cpu"
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics.
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states.
  report_active = false
"# Read metrics about cpu usage\n[[inputs.cpu]]\n  # alias=\"cpu\"\n  ## Whether to report per-cpu stats or not\n  percpu = true\n  ## Whether to report total system cpu stats or not\n  totalcpu = true\n  ## If true, collect raw CPU time metrics.\n  collect_cpu_time = false\n  ## If true, compute and report the sum of all non-idle CPU states.\n  report_active = false\n"# Read metrics about swap memory usage
[[inputs.swap]]
  # alias="swap"
"# Read metrics about swap memory usage\n[[inputs.swap]]\n  # alias=\"swap\"\n"# Read metrics about cpu usage
[[inputs.cpu]]
  # alias="cpu"
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics.
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states.
  report_active = false
# Read metrics about swap memory usage
[[inputs.swap]]
  # alias="swap"
# Read metrics about disk usage by mount point
[[inputs.disk]]
  # alias="disk"
  ## By default stats will be gathered for all mount points.
  ## Set mount_points will restrict the stats to only the specified mount points.
  # mount_points = ["/"]

  ## Ignore mount points by filesystem type.
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs"]
"# Read metrics about disk usage by mount point\n[[inputs.disk]]\n  # alias=\"disk\"\n  ## By default stats will be gathered for all mount points.\n  ## Set mount_points will restrict the stats to only the specified mount points.\n  # mount_points = [\"/\"]\n\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n"# Read metrics about cpu usage
[[inputs.cpu]]
  # alias="cpu"
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics.
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states.
  report_active = false
# Read metrics about swap memory usage
[[inputs.swap]]
  # alias="swap"
# Read metrics about disk usage by mount point
[[inputs.disk]]
  # alias="disk"
  ## By default stats will be gathered for all mount points.
  ## Set mount_points will restrict the stats to only the specified mount points.
  # mount_points = ["/"]

  ## Ignore mount points by filesystem type.
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs"]
# Read metrics about memory usage
[[inputs.mem]]
  # alias="mem"
"# Read metrics about memory usage\n[[inputs.mem]]\n  # alias=\"mem\"\n"# Read metrics about cpu usage
[[inputs.cpu]]
  # alias="cpu"
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics.
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states.
  report_active = false
# Read metrics about swap memory usage
[[inputs.swap]]
  # alias="swap"
# Read metrics about disk usage by mount point
[[inputs.disk]]
  # alias="disk"
  ## By default stats will be gathered for all mount points.
  ## Set mount_points will restrict the stats to only the specified mount points.
  # mount_points = ["/"]

  ## Ignore mount points by filesystem type.
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs"]
# Read metrics about memory usage
[[inputs.mem]]
  # alias="mem"
# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
  ## output, and will flush this buffer on a successful write. Oldest metrics
  ## are dropped first when this buffer fills.
  ## This buffer only fills when writes fail to output plugin(s).
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Logging configuration:
  ## Run telegraf with debug log messages.
  debug = false
  ## Run telegraf in quiet mode (error log messages only).
  quiet = false
  ## Specify the log file name. The empty string means to log to stderr.
  logfile = ""

  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false
`# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
  ## output, and will flush this buffer on a successful write. Oldest metrics
  ## are dropped first when this buffer fills.
  ## This buffer only fills when writes fail to output plugin(s).
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Logging configuration:
  ## Run telegraf with debug log messages.
  debug = false
  ## Run telegraf in quiet mode (error log messages only).
  quiet = false
  ## Specify the log file name. The empty string means to log to stderr.
  logfile = ""

  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false
`{
  "version": "1.13.0",
  "os": "linux",
  "plugins": [
    {
      "type": "input",
      "name": "tcp_listener",
      "description": "Generic TCP listener",
      "config": "# Generic TCP listener\n[[inputs.tcp_listener]]\n  # alias=\"tcp_listener\"\n  # DEPRECATED: the TCP listener plugin has been deprecated in favor of the\n  # socket_listener plugin\n  # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/socket_listener\n\n"
    },
    {
      "type": "input",
      "name": "kernel",
      "description": "Get kernel statistics from /proc/stat",
      "config": "# Get kernel statistics from /proc/stat\n[[inputs.kernel]]\n  # alias=\"kernel\"\n"
    },
    {
      "type": "input",
      "name": "powerdns",
      "description": "Read metrics from one or many PowerDNS servers",
      "config": "# Read metrics from one or many PowerDNS servers\n[[inputs.powerdns]]\n  # alias=\"powerdns\"\n  ## An array of sockets to gather stats about.\n  ## Specify a path to unix socket.\n  unix_sockets = [\"/var/run/pdns.controlsocket\"]\n\n"
    },
    {
      "type": "input",
      "name": "processes",
      "description": "Get the number of processes and group them by status",
      "config": "# Get the number of processes and group them by status\n[[inputs.processes]]\n  # alias=\"processes\"\n"
    },
    {
      "type": "input",
      "name": "snmp_legacy",
      "description": "DEPRECATED! PLEASE USE inputs.snmp INSTEAD.",
      "config": "# DEPRECATED! PLEASE USE inputs.snmp INSTEAD.\n[[inputs.snmp_legacy]]\n  # alias=\"snmp_legacy\"\n  ## Use 'oids.txt' file to translate oids to names\n  ## To generate 'oids.txt' you need to run:\n  ##   snmptranslate -m all -Tz -On | sed -e 's/\"//g' \u003e /tmp/oids.txt\n  ## Or if you have an other MIB folder with custom MIBs\n  ##   snmptranslate -M /mycustommibfolder -Tz -On -m all | sed -e 's/\"//g' \u003e oids.txt\n  snmptranslate_file = \"/tmp/oids.txt\"\n  [[inputs.snmp.host]]\n    address = \"192.168.2.2:161\"\n    # SNMP community\n    community = \"public\" # default public\n    # SNMP version (1, 2 or 3)\n    # Version 3 not supported yet\n    version = 2 # default 2\n    # SNMP response timeout\n    timeout = 2.0 # default 2.0\n    # SNMP request retries\n    retries = 2 # default 2\n    # Which get/bulk do you want to collect for this host\n    collect = [\"mybulk\", \"sysservices\", \"sysdescr\"]\n    # Simple list of OIDs to get, in addition to \"collect\"\n    get_oids = []\n\n  [[inputs.snmp.host]]\n    address = \"192.168.2.3:161\"\n    community = \"public\"\n    version = 2\n    timeout = 2.0\n    retries = 2\n    collect = [\"mybulk\"]\n    get_oids = [\n        \"ifNumber\",\n        \".1.3.6.1.2.1.1.3.0\",\n    ]\n\n  [[inputs.snmp.get]]\n    name = \"ifnumber\"\n    oid = \"ifNumber\"\n\n  [[inputs.snmp.get]]\n    name = \"interface_speed\"\n    oid = \"ifSpeed\"\n    instance = \"0\"\n\n  [[inputs.snmp.get]]\n    name = \"sysuptime\"\n    oid = \".1.3.6.1.2.1.1.3.0\"\n    unit = \"second\"\n\n  [[inputs.snmp.bulk]]\n    name = \"mybulk\"\n    max_repetition = 127\n    oid = \".1.3.6.1.2.1.1\"\n\n  [[inputs.snmp.bulk]]\n    name = \"ifoutoctets\"\n    max_repetition = 127\n    oid = \"ifOutOctets\"\n\n  [[inputs.snmp.host]]\n    address = \"192.168.2.13:161\"\n    #address = \"127.0.0.1:161\"\n    community = \"public\"\n    version = 2\n    timeout = 2.0\n    retries = 2\n    #collect = [\"mybulk\", \"sysservices\", \"sysdescr\", \"systype\"]\n    collect = [\"sysuptime\" ]\n    [[inputs.snmp.host.table]]\n      name = \"iftable3\"\n      include_instances = [\"enp5s0\", \"eth1\"]\n\n  # SNMP TABLEs\n  # table without mapping neither subtables\n  [[inputs.snmp.table]]\n    name = \"iftable1\"\n    oid = \".1.3.6.1.2.1.31.1.1.1\"\n\n  # table without mapping but with subtables\n  [[inputs.snmp.table]]\n    name = \"iftable2\"\n    oid = \".1.3.6.1.2.1.31.1.1.1\"\n    sub_tables = [\".1.3.6.1.2.1.2.2.1.13\"]\n\n  # table with mapping but without subtables\n  [[inputs.snmp.table]]\n    name = \"iftable3\"\n    oid = \".1.3.6.1.2.1.31.1.1.1\"\n    # if empty. get all instances\n    mapping_table = \".1.3.6.1.2.1.31.1.1.1.1\"\n    # if empty, get all subtables\n\n  # table with both mapping and subtables\n  [[inputs.snmp.table]]\n    name = \"iftable4\"\n    oid = \".1.3.6.1.2.1.31.1.1.1\"\n    # if empty get all instances\n    mapping_table = \".1.3.6.1.2.1.31.1.1.1.1\"\n    # if empty get all subtables\n    # sub_tables could be not \"real subtables\"\n    sub_tables=[\".1.3.6.1.2.1.2.2.1.13\", \"bytes_recv\", \"bytes_send\"]\n\n"
    },
    {
      "type": "input",
      "name": "statsd",
      "description": "Statsd UDP/TCP Server",
      "config": "# Statsd UDP/TCP Server\n[[inputs.statsd]]\n  # alias=\"statsd\"\n  ## Protocol, must be \"tcp\", \"udp\", \"udp4\" or \"udp6\" (default=udp)\n  protocol = \"udp\"\n\n  ## MaxTCPConnection - applicable when protocol is set to tcp (default=250)\n  max_tcp_connections = 250\n\n  ## Enable TCP keep alive probes (default=false)\n  tcp_keep_alive = false\n\n  ## Specifies the keep-alive period for an active network connection.\n  ## Only applies to TCP sockets and will be ignored if tcp_keep_alive is false.\n  ## Defaults to the OS configuration.\n  # tcp_keep_alive_period = \"2h\"\n\n  ## Address and port to host UDP listener on\n  service_address = \":8125\"\n\n  ## The following configuration options control when telegraf clears it's cache\n  ## of previous values. If set to false, then telegraf will only clear it's\n  ## cache when the daemon is restarted.\n  ## Reset gauges every interval (default=true)\n  delete_gauges = true\n  ## Reset counters every interval (default=true)\n  delete_counters = true\n  ## Reset sets every interval (default=true)\n  delete_sets = true\n  ## Reset timings \u0026 histograms every interval (default=true)\n  delete_timings = true\n\n  ## Percentiles to calculate for timing \u0026 histogram stats\n  percentiles = [50.0, 90.0, 99.0, 99.9, 99.95, 100.0]\n\n  ## separator to use between elements of a statsd metric\n  metric_separator = \"_\"\n\n  ## Parses tags in the datadog statsd format\n  ## http://docs.datadoghq.com/guides/dogstatsd/\n  parse_data_dog_tags = false\n\n  ## Parses datadog extensions to the statsd format\n  datadog_extensions = false\n\n  ## Statsd data translation templates, more info can be read here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md\n  # templates = [\n  #     \"cpu.* measurement*\"\n  # ]\n\n  ## Number of UDP messages allowed to queue up, once filled,\n  ## the statsd server will start dropping packets\n  allowed_pending_messages = 10000\n\n  ## Number of timing/histogram values to track per-measurement in the\n  ## calculation of percentiles. Raising this limit increases the accuracy\n  ## of percentiles but also increases the memory usage and cpu time.\n  percentile_limit = 1000\n\n"
    },
    {
      "type": "input",
      "name": "bcache",
      "description": "Read metrics of bcache from stats_total and dirty_data",
      "config": "# Read metrics of bcache from stats_total and dirty_data\n[[inputs.bcache]]\n  # alias=\"bcache\"\n  ## Bcache sets path\n  ## If not specified, then default is:\n  bcachePath = \"/sys/fs/bcache\"\n\n  ## By default, telegraf gather stats for all bcache devices\n  ## Setting devices will restrict the stats to the specified\n  ## bcache devices.\n  bcacheDevs = [\"bcache0\"]\n\n"
    },
    {
      "type": "input",
      "name": "mesos",
      "description": "Telegraf plugin for gathering metrics from N Mesos masters",
      "config": "# Telegraf plugin for gathering metrics from N Mesos masters\n[[inputs.mesos]]\n  # alias=\"mesos\"\n  ## Timeout, in ms.\n  timeout = 100\n\n  ## A list of Mesos masters.\n  masters = [\"http://localhost:5050\"]\n\n  ## Master metrics groups to be collected, by default, all enabled.\n  master_collections = [\n    \"resources\",\n    \"master\",\n    \"system\",\n    \"agents\",\n    \"frameworks\",\n    \"framework_offers\",\n    \"tasks\",\n    \"messages\",\n    \"evqueue\",\n    \"registrar\",\n    \"allocator\",\n  ]\n\n  ## A list of Mesos slaves, default is []\n  # slaves = []\n\n  ## Slave metrics groups to be collected, by default, all enabled.\n  # slave_collections = [\n  #   \"resources\",\n  #   \"agent\",\n  #   \"system\",\n  #   \"executors\",\n  #   \"tasks\",\n  #   \"messages\",\n  # ]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "pf",
      "description": "Gather counters from PF",
      "config": "# Gather counters from PF\n[[inputs.pf]]\n  # alias=\"pf\"\n  ## PF require root access on most systems.\n  ## Setting 'use_sudo' to true will make use of sudo to run pfctl.\n  ## Users must configure sudo to allow telegraf user to run pfctl with no password.\n  ## pfctl can be restricted to only list command \"pfctl -s info\".\n  use_sudo = false\n\n"
    },
    {
      "type": "input",
      "name": "webhooks",
      "description": "A Webhooks Event collector",
      "config": "# A Webhooks Event collector\n[[inputs.webhooks]]\n  # alias=\"webhooks\"\n  ## Address and port to host Webhook listener on\n  service_address = \":1619\"\n\n  [inputs.webhooks.filestack]\n    path = \"/filestack\"\n\n  [inputs.webhooks.github]\n    path = \"/github\"\n    # secret = \"\"\n\n  [inputs.webhooks.mandrill]\n    path = \"/mandrill\"\n\n  [inputs.webhooks.rollbar]\n    path = \"/rollbar\"\n\n  [inputs.webhooks.papertrail]\n    path = \"/papertrail\"\n\n  [inputs.webhooks.particle]\n    path = \"/particle\"\n\n"
    },
    {
      "type": "input",
      "name": "http_listener_v2",
      "description": "Generic HTTP write listener",
      "config": "# Generic HTTP write listener\n[[inputs.http_listener_v2]]\n  # alias=\"http_listener_v2\"\n  ## Address and port to host HTTP listener on\n  service_address = \":8080\"\n\n  ## Path to listen to.\n  # path = \"/telegraf\"\n\n  ## HTTP methods to accept.\n  # methods = [\"POST\", \"PUT\"]\n\n  ## maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n  ## maximum duration before timing out write of the response\n  # write_timeout = \"10s\"\n\n  ## Maximum allowed http request body size in bytes.\n  ## 0 means to use the default of 524,288,00 bytes (500 mebibytes)\n  # max_body_size = \"500MB\"\n\n  ## Part of the request to consume.  Available options are \"body\" and\n  ## \"query\".\n  # data_source = \"body\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Optional username and password to accept for HTTP basic authentication.\n  ## You probably want to make sure you have TLS configured above for this.\n  # basic_username = \"foobar\"\n  # basic_password = \"barfoo\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "http_listener",
      "description": "Influx HTTP write listener",
      "config": "# Influx HTTP write listener\n[[inputs.http_listener]]\n  # alias=\"http_listener\"\n  ## Address and port to host HTTP listener on\n  service_address = \":8186\"\n\n  ## maximum duration before timing out read of the request\n  read_timeout = \"10s\"\n  ## maximum duration before timing out write of the response\n  write_timeout = \"10s\"\n\n  ## Maximum allowed http request body size in bytes.\n  ## 0 means to use the default of 524,288,000 bytes (500 mebibytes)\n  max_body_size = \"500MiB\"\n\n  ## Maximum line size allowed to be sent in bytes.\n  ## 0 means to use the default of 65536 bytes (64 kibibytes)\n  max_line_size = \"64KiB\"\n  \n\n  ## Optional tag name used to store the database. \n  ## If the write has a database in the query string then it will be kept in this tag name.\n  ## This tag can be used in downstream outputs.\n  ## The default value of nothing means it will be off and the database will not be recorded.\n  # database_tag = \"\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  tls_cert = \"/etc/telegraf/cert.pem\"\n  tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Optional username and password to accept for HTTP basic authentication.\n  ## You probably want to make sure you have TLS configured above for this.\n  # basic_username = \"foobar\"\n  # basic_password = \"barfoo\"\n\n"
    },
    {
      "type": "input",
      "name": "sysstat",
      "description": "Sysstat metrics collector",
      "config": "# Sysstat metrics collector\n[[inputs.sysstat]]\n  # alias=\"sysstat\"\n  ## Path to the sadc command.\n  #\n  ## Common Defaults:\n  ##   Debian/Ubuntu: /usr/lib/sysstat/sadc\n  ##   Arch:          /usr/lib/sa/sadc\n  ##   RHEL/CentOS:   /usr/lib64/sa/sadc\n  sadc_path = \"/usr/lib/sa/sadc\" # required\n\n  ## Path to the sadf command, if it is not in PATH\n  # sadf_path = \"/usr/bin/sadf\"\n\n  ## Activities is a list of activities, that are passed as argument to the\n  ## sadc collector utility (e.g: DISK, SNMP etc...)\n  ## The more activities that are added, the more data is collected.\n  # activities = [\"DISK\"]\n\n  ## Group metrics to measurements.\n  ##\n  ## If group is false each metric will be prefixed with a description\n  ## and represents itself a measurement.\n  ##\n  ## If Group is true, corresponding metrics are grouped to a single measurement.\n  # group = true\n\n  ## Options for the sadf command. The values on the left represent the sadf\n  ## options and the values on the right their description (which are used for\n  ## grouping and prefixing metrics).\n  ##\n  ## Run 'sar -h' or 'man sar' to find out the supported options for your\n  ## sysstat version.\n  [inputs.sysstat.options]\n    -C = \"cpu\"\n    -B = \"paging\"\n    -b = \"io\"\n    -d = \"disk\"             # requires DISK activity\n    \"-n ALL\" = \"network\"\n    \"-P ALL\" = \"per_cpu\"\n    -q = \"queue\"\n    -R = \"mem\"\n    -r = \"mem_util\"\n    -S = \"swap_util\"\n    -u = \"cpu_util\"\n    -v = \"inode\"\n    -W = \"swap\"\n    -w = \"task\"\n  #  -H = \"hugepages\"        # only available for newer linux distributions\n  #  \"-I ALL\" = \"interrupts\" # requires INT activity\n\n  ## Device tags can be used to add additional tags for devices.\n  ## For example the configuration below adds a tag vg with value rootvg for\n  ## all metrics with sda devices.\n  # [[inputs.sysstat.device_tags.sda]]\n  #  vg = \"rootvg\"\n\n"
    },
    {
      "type": "input",
      "name": "systemd_units",
      "description": "Gather systemd units state",
      "config": "# Gather systemd units state\n[[inputs.systemd_units]]\n  # alias=\"systemd_units\"\n  ## Set timeout for systemctl execution\n  # timeout = \"1s\"\n  #\n  ## Filter for a specific unit type, default is \"service\", other possible\n  ## values are \"socket\", \"target\", \"device\", \"mount\", \"automount\", \"swap\",\n  ## \"timer\", \"path\", \"slice\" and \"scope \":\n  # unittype = \"service\"\n\n"
    },
    {
      "type": "input",
      "name": "temp",
      "description": "Read metrics about temperature",
      "config": "# Read metrics about temperature\n[[inputs.temp]]\n  # alias=\"temp\"\n"
    },
    {
      "type": "input",
      "name": "cgroup",
      "description": "Read specific statistics per cgroup",
      "config": "# Read specific statistics per cgroup\n[[inputs.cgroup]]\n  # alias=\"cgroup\"\n  ## Directories in which to look for files, globs are supported.\n  ## Consider restricting paths to the set of cgroups you really\n  ## want to monitor if you have a large number of cgroups, to avoid\n  ## any cardinality issues.\n  # paths = [\n  #   \"/cgroup/memory\",\n  #   \"/cgroup/memory/child1\",\n  #   \"/cgroup/memory/child2/*\",\n  # ]\n  ## cgroup stat fields, as file names, globs are supported.\n  ## these file names are appended to each path from above.\n  # files = [\"memory.*usage*\", \"memory.limit_in_bytes\"]\n\n"
    },
    {
      "type": "input",
      "name": "mysql",
      "description": "Read metrics from one or many mysql servers",
      "config": "# Read metrics from one or many mysql servers\n[[inputs.mysql]]\n  # alias=\"mysql\"\n  ## specify servers via a url matching:\n  ##  [username[:password]@][protocol[(address)]]/[?tls=[true|false|skip-verify|custom]]\n  ##  see https://github.com/go-sql-driver/mysql#dsn-data-source-name\n  ##  e.g.\n  ##    servers = [\"user:passwd@tcp(127.0.0.1:3306)/?tls=false\"]\n  ##    servers = [\"user@tcp(127.0.0.1:3306)/?tls=false\"]\n  #\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"tcp(127.0.0.1:3306)/\"]\n\n  ## Selects the metric output format.\n  ##\n  ## This option exists to maintain backwards compatibility, if you have\n  ## existing metrics do not set or change this value until you are ready to\n  ## migrate to the new format.\n  ##\n  ## If you do not have existing metrics from this plugin set to the latest\n  ## version.\n  ##\n  ## Telegraf \u003e=1.6: metric_version = 2\n  ##           \u003c1.6: metric_version = 1 (or unset)\n  metric_version = 2\n\n  ## if the list is empty, then metrics are gathered from all databasee tables\n  # table_schema_databases = []\n\n  ## gather metrics from INFORMATION_SCHEMA.TABLES for databases provided above list\n  # gather_table_schema = false\n\n  ## gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST\n  # gather_process_list = false\n\n  ## gather user statistics from INFORMATION_SCHEMA.USER_STATISTICS\n  # gather_user_statistics = false\n\n  ## gather auto_increment columns and max values from information schema\n  # gather_info_schema_auto_inc = false\n\n  ## gather metrics from INFORMATION_SCHEMA.INNODB_METRICS\n  # gather_innodb_metrics = false\n\n  ## gather metrics from SHOW SLAVE STATUS command output\n  # gather_slave_status = false\n\n  ## gather metrics from SHOW BINARY LOGS command output\n  # gather_binary_logs = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE\n  # gather_table_io_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS\n  # gather_table_lock_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE\n  # gather_index_io_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS\n  # gather_event_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME\n  # gather_file_events_stats = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST\n  # gather_perf_events_statements = false\n\n  ## the limits for metrics form perf_events_statements\n  # perf_events_statements_digest_text_limit = 120\n  # perf_events_statements_limit = 250\n  # perf_events_statements_time_limit = 86400\n\n  ## Some queries we may want to run less often (such as SHOW GLOBAL VARIABLES)\n  ##   example: interval_slow = \"30m\"\n  # interval_slow = \"\"\n\n  ## Optional TLS Config (will be used if tls=custom parameter specified in server uri)\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "redis",
      "description": "Read metrics from one or many redis servers",
      "config": "# Read metrics from one or many redis servers\n[[inputs.redis]]\n  # alias=\"redis\"\n  ## specify servers via a url matching:\n  ##  [protocol://][:password]@address[:port]\n  ##  e.g.\n  ##    tcp://localhost:6379\n  ##    tcp://:password@192.168.99.100\n  ##    unix:///var/run/redis.sock\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no port is specified, 6379 is used\n  servers = [\"tcp://localhost:6379\"]\n\n  ## specify server password\n  # password = \"s#cr@t%\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = true\n\n"
    },
    {
      "type": "input",
      "name": "couchbase",
      "description": "Read metrics from one or many couchbase clusters",
      "config": "# Read metrics from one or many couchbase clusters\n[[inputs.couchbase]]\n  # alias=\"couchbase\"\n  ## specify servers via a url matching:\n  ##  [protocol://][:password]@address[:port]\n  ##  e.g.\n  ##    http://couchbase-0.example.com/\n  ##    http://admin:secret@couchbase-0.example.com:8091/\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no protocol is specified, HTTP is used.\n  ## If no port is specified, 8091 is used.\n  servers = [\"http://localhost:8091\"]\n\n"
    },
    {
      "type": "input",
      "name": "file",
      "description": "Reload and gather from file[s] on telegraf's interval.",
      "config": "# Reload and gather from file[s] on telegraf's interval.\n[[inputs.file]]\n  # alias=\"file\"\n  ## Files to parse each interval.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   /var/log/**.log     -\u003e recursively find all .log files in /var/log\n  ##   /var/log/*/*.log    -\u003e find all .log files with a parent dir in /var/log\n  ##   /var/log/apache.log -\u003e only read the apache log file\n  files = [\"/var/log/apache/access.log\"]\n\n  ## The dataformat to be read from files\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Name a tag containing the name of the file the data was parsed from.  Leave empty\n  ## to disable.\n  # file_tag = \"\"\n\n"
    },
    {
      "type": "input",
      "name": "kube_inventory",
      "description": "Read metrics from the Kubernetes api",
      "config": "# Read metrics from the Kubernetes api\n[[inputs.kube_inventory]]\n  # alias=\"kube_inventory\"\n  ## URL for the Kubernetes API\n  url = \"https://127.0.0.1\"\n\n  ## Namespace to use. Set to \"\" to use all namespaces.\n  # namespace = \"default\"\n\n  ## Use bearer token for authorization. ('bearer_token' takes priority)\n  ## If both of these are empty, we'll use the default serviceaccount:\n  ## at: /run/secrets/kubernetes.io/serviceaccount/token\n  # bearer_token = \"/path/to/bearer/token\"\n  ## OR\n  # bearer_token_string = \"abc_123\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## Optional Resources to exclude from gathering\n  ## Leave them with blank with try to gather everything available.\n  ## Values can be - \"daemonsets\", deployments\", \"endpoints\", \"ingress\", \"nodes\",\n  ## \"persistentvolumes\", \"persistentvolumeclaims\", \"pods\", \"services\", \"statefulsets\"\n  # resource_exclude = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n\n  ## Optional Resources to include when gathering\n  ## Overrides resource_exclude if both set.\n  # resource_include = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n\n  ## Optional TLS Config\n  # tls_ca = \"/path/to/cafile\"\n  # tls_cert = \"/path/to/certfile\"\n  # tls_key = \"/path/to/keyfile\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "neptune_apex",
      "description": "Neptune Apex data collector",
      "config": "# Neptune Apex data collector\n[[inputs.neptune_apex]]\n  # alias=\"neptune_apex\"\n  ## The Neptune Apex plugin reads the publicly available status.xml data from a local Apex.\n  ## Measurements will be logged under \"apex\".\n\n  ## The base URL of the local Apex(es). If you specify more than one server, they will\n  ## be differentiated by the \"source\" tag.\n  servers = [\n    \"http://apex.local\",\n  ]\n\n  ## The response_timeout specifies how long to wait for a reply from the Apex.\n  #response_timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "openntpd",
      "description": "Get standard NTP query metrics from OpenNTPD.",
      "config": "# Get standard NTP query metrics from OpenNTPD.\n[[inputs.openntpd]]\n  # alias=\"openntpd\"\n  ## Run ntpctl binary with sudo.\n  # use_sudo = false\n\n  ## Location of the ntpctl binary.\n  # binary = \"/usr/sbin/ntpctl\"\n\n  ## Maximum time the ntpctl binary is allowed to run.\n  # timeout = \"5ms\"\n  \n"
    },
    {
      "type": "input",
      "name": "ipset",
      "description": "Gather packets and bytes counters from Linux ipsets",
      "config": "# Gather packets and bytes counters from Linux ipsets\n[[inputs.ipset]]\n  # alias=\"ipset\"\n  ## By default, we only show sets which have already matched at least 1 packet.\n  ## set include_unmatched_sets = true to gather them all.\n  include_unmatched_sets = false\n  ## Adjust your sudo settings appropriately if using this option (\"sudo ipset save\")\n  use_sudo = false\n  ## The default timeout of 1s for ipset execution can be overridden here:\n  # timeout = \"1s\"\n\n"
    },
    {
      "type": "input",
      "name": "tengine",
      "description": "Read Tengine's basic status information (ngx_http_reqstat_module)",
      "config": "# Read Tengine's basic status information (ngx_http_reqstat_module)\n[[inputs.tengine]]\n  # alias=\"tengine\"\n  # An array of Tengine reqstat module URI to gather stats.\n  urls = [\"http://127.0.0.1/us\"]\n\n  # HTTP response timeout (default: 5s)\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.cer\"\n  # tls_key = \"/etc/telegraf/key.key\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "vsphere",
      "description": "Read metrics from VMware vCenter",
      "config": "# Read metrics from VMware vCenter\n[[inputs.vsphere]]\n  # alias=\"vsphere\"\n  ## List of vCenter URLs to be monitored. These three lines must be uncommented\n  ## and edited for the plugin to work.\n  vcenters = [ \"https://vcenter.local/sdk\" ]\n  username = \"user@corp.local\"\n  password = \"secret\"\n\n  ## VMs\n  ## Typical VM metrics (if omitted or empty, all metrics are collected)\n  vm_metric_include = [\n    \"cpu.demand.average\",\n    \"cpu.idle.summation\",\n    \"cpu.latency.average\",\n    \"cpu.readiness.average\",\n    \"cpu.ready.summation\",\n    \"cpu.run.summation\",\n    \"cpu.usagemhz.average\",\n    \"cpu.used.summation\",\n    \"cpu.wait.summation\",\n    \"mem.active.average\",\n    \"mem.granted.average\",\n    \"mem.latency.average\",\n    \"mem.swapin.average\",\n    \"mem.swapinRate.average\",\n    \"mem.swapout.average\",\n    \"mem.swapoutRate.average\",\n    \"mem.usage.average\",\n    \"mem.vmmemctl.average\",\n    \"net.bytesRx.average\",\n    \"net.bytesTx.average\",\n    \"net.droppedRx.summation\",\n    \"net.droppedTx.summation\",\n    \"net.usage.average\",\n    \"power.power.average\",\n    \"virtualDisk.numberReadAveraged.average\",\n    \"virtualDisk.numberWriteAveraged.average\",\n    \"virtualDisk.read.average\",\n    \"virtualDisk.readOIO.latest\",\n    \"virtualDisk.throughput.usage.average\",\n    \"virtualDisk.totalReadLatency.average\",\n    \"virtualDisk.totalWriteLatency.average\",\n    \"virtualDisk.write.average\",\n    \"virtualDisk.writeOIO.latest\",\n    \"sys.uptime.latest\",\n  ]\n  # vm_metric_exclude = [] ## Nothing is excluded by default\n  # vm_instances = true ## true by default\n\n  ## Hosts\n  ## Typical host metrics (if omitted or empty, all metrics are collected)\n  host_metric_include = [\n    \"cpu.coreUtilization.average\",\n    \"cpu.costop.summation\",\n    \"cpu.demand.average\",\n    \"cpu.idle.summation\",\n    \"cpu.latency.average\",\n    \"cpu.readiness.average\",\n    \"cpu.ready.summation\",\n    \"cpu.swapwait.summation\",\n    \"cpu.usage.average\",\n    \"cpu.usagemhz.average\",\n    \"cpu.used.summation\",\n    \"cpu.utilization.average\",\n    \"cpu.wait.summation\",\n    \"disk.deviceReadLatency.average\",\n    \"disk.deviceWriteLatency.average\",\n    \"disk.kernelReadLatency.average\",\n    \"disk.kernelWriteLatency.average\",\n    \"disk.numberReadAveraged.average\",\n    \"disk.numberWriteAveraged.average\",\n    \"disk.read.average\",\n    \"disk.totalReadLatency.average\",\n    \"disk.totalWriteLatency.average\",\n    \"disk.write.average\",\n    \"mem.active.average\",\n    \"mem.latency.average\",\n    \"mem.state.latest\",\n    \"mem.swapin.average\",\n    \"mem.swapinRate.average\",\n    \"mem.swapout.average\",\n    \"mem.swapoutRate.average\",\n    \"mem.totalCapacity.average\",\n    \"mem.usage.average\",\n    \"mem.vmmemctl.average\",\n    \"net.bytesRx.average\",\n    \"net.bytesTx.average\",\n    \"net.droppedRx.summation\",\n    \"net.droppedTx.summation\",\n    \"net.errorsRx.summation\",\n    \"net.errorsTx.summation\",\n    \"net.usage.average\",\n    \"power.power.average\",\n    \"storageAdapter.numberReadAveraged.average\",\n    \"storageAdapter.numberWriteAveraged.average\",\n    \"storageAdapter.read.average\",\n    \"storageAdapter.write.average\",\n    \"sys.uptime.latest\",\n  ]\n  ## Collect IP addresses? Valid values are \"ipv4\" and \"ipv6\"\n  # ip_addresses = [\"ipv6\", \"ipv4\" ]\n  # host_metric_exclude = [] ## Nothing excluded by default\n  # host_instances = true ## true by default\n\n  ## Clusters\n  # cluster_metric_include = [] ## if omitted or empty, all metrics are collected\n  # cluster_metric_exclude = [] ## Nothing excluded by default\n  # cluster_instances = false ## false by default\n\n  ## Datastores\n  # datastore_metric_include = [] ## if omitted or empty, all metrics are collected\n  # datastore_metric_exclude = [] ## Nothing excluded by default\n  # datastore_instances = false ## false by default for Datastores only\n\n  ## Datacenters\n  datacenter_metric_include = [] ## if omitted or empty, all metrics are collected\n  datacenter_metric_exclude = [ \"*\" ] ## Datacenters are not collected by default.\n  # datacenter_instances = false ## false by default for Datastores only\n\n  ## Plugin Settings  \n  ## separator character to use for measurement and field names (default: \"_\")\n  # separator = \"_\"\n\n  ## number of objects to retreive per query for realtime resources (vms and hosts)\n  ## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n  # max_query_objects = 256\n\n  ## number of metrics to retreive per query for non-realtime resources (clusters and datastores)\n  ## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n  # max_query_metrics = 256\n\n  ## number of go routines to use for collection and discovery of objects and metrics\n  # collect_concurrency = 1\n  # discover_concurrency = 1\n\n  ## whether or not to force discovery of new objects on initial gather call before collecting metrics\n  ## when true for large environments this may cause errors for time elapsed while collecting metrics\n  ## when false (default) the first collection cycle may result in no or limited metrics while objects are discovered\n  # force_discover_on_init = false\n\n  ## the interval before (re)discovering objects subject to metrics collection (default: 300s)\n  # object_discovery_interval = \"300s\"\n\n  ## timeout applies to any of the api request made to vcenter\n  # timeout = \"60s\"\n\n  ## When set to true, all samples are sent as integers. This makes the output\n  ## data types backwards compatible with Telegraf 1.9 or lower. Normally all\n  ## samples from vCenter, with the exception of percentages, are integer\n  ## values, but under some conditions, some averaging takes place internally in\n  ## the plugin. Setting this flag to \"false\" will send values as floats to\n  ## preserve the full precision when averaging takes place.\n  # use_int_samples = true\n\n  ## Custom attributes from vCenter can be very useful for queries in order to slice the\n  ## metrics along different dimension and for forming ad-hoc relationships. They are disabled\n  ## by default, since they can add a considerable amount of tags to the resulting metrics. To\n  ## enable, simply set custom_attribute_exlude to [] (empty set) and use custom_attribute_include\n  ## to select the attributes you want to include.\n  # custom_attribute_include = []\n  # custom_attribute_exclude = [\"*\"] \n\n  ## Optional SSL Config\n  # ssl_ca = \"/path/to/cafile\"\n  # ssl_cert = \"/path/to/certfile\"\n  # ssl_key = \"/path/to/keyfile\"\n  ## Use SSL but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "aurora",
      "description": "Gather metrics from Apache Aurora schedulers",
      "config": "# Gather metrics from Apache Aurora schedulers\n[[inputs.aurora]]\n  # alias=\"aurora\"\n  ## Schedulers are the base addresses of your Aurora Schedulers\n  schedulers = [\"http://127.0.0.1:8081\"]\n\n  ## Set of role types to collect metrics from.\n  ##\n  ## The scheduler roles are checked each interval by contacting the\n  ## scheduler nodes; zookeeper is not contacted.\n  # roles = [\"leader\", \"follower\"]\n\n  ## Timeout is the max time for total network operations.\n  # timeout = \"5s\"\n\n  ## Username and password are sent using HTTP Basic Auth.\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "burrow",
      "description": "Collect Kafka topics and consumers status from Burrow HTTP API.",
      "config": "# Collect Kafka topics and consumers status from Burrow HTTP API.\n[[inputs.burrow]]\n  # alias=\"burrow\"\n  ## Burrow API endpoints in format \"schema://host:port\".\n  ## Default is \"http://localhost:8000\".\n  servers = [\"http://localhost:8000\"]\n\n  ## Override Burrow API prefix.\n  ## Useful when Burrow is behind reverse-proxy.\n  # api_prefix = \"/v3/kafka\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Limit per-server concurrent connections.\n  ## Useful in case of large number of topics or consumer groups.\n  # concurrent_connections = 20\n\n  ## Filter clusters, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # clusters_include = []\n  # clusters_exclude = []\n\n  ## Filter consumer groups, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # groups_include = []\n  # groups_exclude = []\n\n  ## Filter topics, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # topics_include = []\n  # topics_exclude = []\n\n  ## Credentials for basic HTTP authentication.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional SSL config\n  # ssl_ca = \"/etc/telegraf/ca.pem\"\n  # ssl_cert = \"/etc/telegraf/cert.pem\"\n  # ssl_key = \"/etc/telegraf/key.pem\"\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "consul",
      "description": "Gather health check statuses from services registered in Consul",
      "config": "# Gather health check statuses from services registered in Consul\n[[inputs.consul]]\n  # alias=\"consul\"\n  ## Consul server address\n  # address = \"localhost\"\n\n  ## URI scheme for the Consul server, one of \"http\", \"https\"\n  # scheme = \"http\"\n\n  ## ACL token used in every request\n  # token = \"\"\n\n  ## HTTP Basic Authentication username and password.\n  # username = \"\"\n  # password = \"\"\n\n  ## Data center to query the health checks from\n  # datacenter = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = true\n\n  ## Consul checks' tag splitting\n  # When tags are formatted like \"key:value\" with \":\" as a delimiter then\n  # they will be splitted and reported as proper key:value in Telegraf\n  # tag_delimiter = \":\"\n\n"
    },
    {
      "type": "input",
      "name": "dovecot",
      "description": "Read statistics from one or many dovecot servers",
      "config": "# Read statistics from one or many dovecot servers\n[[inputs.dovecot]]\n  # alias=\"dovecot\"\n  ## specify dovecot servers via an address:port list\n  ##  e.g.\n  ##    localhost:24242\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"localhost:24242\"]\n\n  ## Type is one of \"user\", \"domain\", \"ip\", or \"global\"\n  type = \"global\"\n\n  ## Wildcard matches like \"*.com\". An empty string \"\" is same as \"*\"\n  ## If type = \"ip\" filters should be \u003cIP/network\u003e\n  filters = [\"\"]\n\n"
    },
    {
      "type": "input",
      "name": "fireboard",
      "description": "Read real time temps from fireboard.io servers",
      "config": "# Read real time temps from fireboard.io servers\n[[inputs.fireboard]]\n  # alias=\"fireboard\"\n  ## Specify auth token for your account\n  auth_token = \"invalidAuthToken\"\n  ## You can override the fireboard server URL if necessary\n  # url = https://fireboard.io/api/v1/devices.json\n  ## You can set a different http_timeout if you need to\n  ## You should set a string using an number and time indicator\n  ## for example \"12s\" for 12 seconds.\n  # http_timeout = \"4s\"\n\n"
    },
    {
      "type": "input",
      "name": "ecs",
      "description": "Read metrics about docker containers from Fargate/ECS v2 meta endpoints.",
      "config": "# Read metrics about docker containers from Fargate/ECS v2 meta endpoints.\n[[inputs.ecs]]\n  # alias=\"ecs\"\n  ## ECS metadata url\n  # endpoint_url = \"http://169.254.170.2\"\n\n  ## Containers to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all containers\n  # container_name_include = []\n  # container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"RUNNING\" state will be captured.\n  ## Possible values are \"NONE\", \"PULLED\", \"CREATED\", \"RUNNING\",\n  ## \"RESOURCES_PROVISIONED\", \"STOPPED\".\n  # container_status_include = []\n  # container_status_exclude = []\n\n  ## ecs labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  ecs_label_include = [ \"com.amazonaws.ecs.*\" ]\n  ecs_label_exclude = []\n\n  ## Timeout for queries.\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "icinga2",
      "description": "Gather Icinga2 status",
      "config": "# Gather Icinga2 status\n[[inputs.icinga2]]\n  # alias=\"icinga2\"\n  ## Required Icinga2 server address\n  # server = \"https://localhost:5665\"\n  \n  ## Required Icinga2 object type (\"services\" or \"hosts\")\n  # object_type = \"services\"\n\n  ## Credentials for basic HTTP authentication\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = true\n  \n"
    },
    {
      "type": "input",
      "name": "diskio",
      "description": "Read metrics about disk IO by device",
      "config": "# Read metrics about disk IO by device\n[[inputs.diskio]]\n  # alias=\"diskio\"\n  ## By default, telegraf will gather stats for all devices including\n  ## disk partitions.\n  ## Setting devices will restrict the stats to the specified devices.\n  # devices = [\"sda\", \"sdb\", \"vd*\"]\n  ## Uncomment the following line if you need disk serial numbers.\n  # skip_serial_number = false\n  #\n  ## On systems which support it, device metadata can be added in the form of\n  ## tags.\n  ## Currently only Linux is supported via udev properties. You can view\n  ## available properties for a device by running:\n  ## 'udevadm info -q property -n /dev/sda'\n  ## Note: Most, but not all, udev properties can be accessed this way. Properties\n  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.\n  # device_tags = [\"ID_FS_TYPE\", \"ID_FS_USAGE\"]\n  #\n  ## Using the same metadata source as device_tags, you can also customize the\n  ## name of the device via templates.\n  ## The 'name_templates' parameter is a list of templates to try and apply to\n  ## the device. The template may contain variables in the form of '$PROPERTY' or\n  ## '${PROPERTY}'. The first template which does not contain any variables not\n  ## present for the device is used as the device name tag.\n  ## The typical use case is for LVM volumes, to get the VG/LV name instead of\n  ## the near-meaningless DM-0 name.\n  # name_templates = [\"$ID_FS_LABEL\",\"$DM_VG_NAME/$DM_LV_NAME\"]\n\n"
    },
    {
      "type": "input",
      "name": "http",
      "description": "Read formatted metrics from one or more HTTP endpoints",
      "config": "# Read formatted metrics from one or more HTTP endpoints\n[[inputs.http]]\n  # alias=\"http\"\n  ## One or more URLs from which to read formatted metrics\n  urls = [\n    \"http://localhost/metrics\"\n  ]\n\n  ## HTTP method\n  # method = \"GET\"\n\n  ## Optional HTTP headers\n  # headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## Optional HTTP Basic Auth Credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## HTTP entity-body to send with POST/PUT requests.\n  # body = \"\"\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n\n  ## List of success status codes\n  # success_status_codes = [200]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "uwsgi",
      "description": "Read uWSGI metrics.",
      "config": "# Read uWSGI metrics.\n[[inputs.uwsgi]]\n  # alias=\"uwsgi\"\n  ## List with urls of uWSGI Stats servers. URL must match pattern:\n  ## scheme://address[:port]\n  ##\n  ## For example:\n  ## servers = [\"tcp://localhost:5050\", \"http://localhost:1717\", \"unix:///tmp/statsock\"]\n  servers = [\"tcp://127.0.0.1:1717\"]\n\n  ## General connection timout\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "chrony",
      "description": "Get standard chrony metrics, requires chronyc executable.",
      "config": "# Get standard chrony metrics, requires chronyc executable.\n[[inputs.chrony]]\n  # alias=\"chrony\"\n  ## If true, chronyc tries to perform a DNS lookup for the time server.\n  # dns_lookup = false\n  \n"
    },
    {
      "type": "input",
      "name": "elasticsearch",
      "description": "Read stats from one or more Elasticsearch servers or clusters",
      "config": "# Read stats from one or more Elasticsearch servers or clusters\n[[inputs.elasticsearch]]\n  # alias=\"elasticsearch\"\n  ## specify a list of one or more Elasticsearch servers\n  # you can add username and password to your url to use basic authentication:\n  # servers = [\"http://user:pass@localhost:9200\"]\n  servers = [\"http://localhost:9200\"]\n\n  ## Timeout for HTTP requests to the elastic search server(s)\n  http_timeout = \"5s\"\n\n  ## When local is true (the default), the node will read only its own stats.\n  ## Set local to false when you want to read the node stats from all nodes\n  ## of the cluster.\n  local = true\n\n  ## Set cluster_health to true when you want to also obtain cluster health stats\n  cluster_health = false\n\n  ## Adjust cluster_health_level when you want to also obtain detailed health stats\n  ## The options are\n  ##  - indices (default)\n  ##  - cluster\n  # cluster_health_level = \"indices\"\n\n  ## Set cluster_stats to true when you want to also obtain cluster stats.\n  cluster_stats = false\n\n  ## Only gather cluster_stats from the master node. To work this require local = true\n  cluster_stats_only_from_master = true\n\n  ## Indices to collect; can be one or more indices names or _all\n  indices_include = [\"_all\"]\n\n  ## One of \"shards\", \"cluster\", \"indices\"\n  indices_level = \"shards\"\n\n  ## node_stats is a list of sub-stats that you want to have gathered. Valid options\n  ## are \"indices\", \"os\", \"process\", \"jvm\", \"thread_pool\", \"fs\", \"transport\", \"http\",\n  ## \"breaker\". Per default, all stats are gathered.\n  # node_stats = [\"jvm\", \"http\"]\n\n  ## HTTP Basic Authentication username and password.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "kafka_consumer",
      "description": "Read metrics from Kafka topics",
      "config": "# Read metrics from Kafka topics\n[[inputs.kafka_consumer]]\n  # alias=\"kafka_consumer\"\n  ## Kafka brokers.\n  brokers = [\"localhost:9092\"]\n\n  ## Topics to consume.\n  topics = [\"telegraf\"]\n\n  ## When set this tag will be added to all metrics with the topic as the value.\n  # topic_tag = \"\"\n\n  ## Optional Client id\n  # client_id = \"Telegraf\"\n\n  ## Set the minimal supported Kafka version.  Setting this enables the use of new\n  ## Kafka features and APIs.  Must be 0.10.2.0 or greater.\n  ##   ex: version = \"1.1.0\"\n  # version = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional SASL Config\n  # sasl_username = \"kafka\"\n  # sasl_password = \"secret\"\n\n  ## Name of the consumer group.\n  # consumer_group = \"telegraf_metrics_consumers\"\n\n  ## Initial offset position; one of \"oldest\" or \"newest\".\n  # offset = \"oldest\"\n\n  ## Consumer group partition assignment strategy; one of \"range\", \"roundrobin\" or \"sticky\".\n  # balance_strategy = \"range\"\n\n  ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n  ## larger messages are dropped\n  max_message_len = 1000000\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "tail",
      "description": "Stream a log file, like the tail -f command",
      "config": "# Stream a log file, like the tail -f command\n[[inputs.tail]]\n  # alias=\"tail\"\n  ## files to tail.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   \"/var/log/**.log\"  -\u003e recursively find all .log files in /var/log\n  ##   \"/var/log/*/*.log\" -\u003e find all .log files with a parent dir in /var/log\n  ##   \"/var/log/apache.log\" -\u003e just tail the apache log file\n  ##\n  ## See https://github.com/gobwas/glob for more examples\n  ##\n  files = [\"/var/mymetrics.out\"]\n  ## Read file from beginning.\n  from_beginning = false\n  ## Whether file is a named pipe\n  pipe = false\n\n  ## Method used to watch for file updates.  Can be either \"inotify\" or \"poll\".\n  # watch_method = \"inotify\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "udp_listener",
      "description": "Generic UDP listener",
      "config": "# Generic UDP listener\n[[inputs.udp_listener]]\n  # alias=\"udp_listener\"\n  # DEPRECATED: the TCP listener plugin has been deprecated in favor of the\n  # socket_listener plugin\n  # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/socket_listener\n\n"
    },
    {
      "type": "input",
      "name": "beanstalkd",
      "description": "Collects Beanstalkd server and tubes stats",
      "config": "# Collects Beanstalkd server and tubes stats\n[[inputs.beanstalkd]]\n  # alias=\"beanstalkd\"\n  ## Server to collect data from\n  server = \"localhost:11300\"\n\n  ## List of tubes to gather stats about.\n  ## If no tubes specified then data gathered for each tube on server reported by list-tubes command\n  tubes = [\"notifications\"]\n\n"
    },
    {
      "type": "input",
      "name": "github",
      "description": "Gather repository information from GitHub hosted repositories.",
      "config": "# Gather repository information from GitHub hosted repositories.\n[[inputs.github]]\n  # alias=\"github\"\n  ## List of repositories to monitor.\n  repositories = [\n\t  \"influxdata/telegraf\",\n\t  \"influxdata/influxdb\"\n  ]\n\n  ## Github API access token.  Unauthenticated requests are limited to 60 per hour.\n  # access_token = \"\"\n\n  ## Github API enterprise url. Github Enterprise accounts must specify their base url.\n  # enterprise_base_url = \"\"\n\n  ## Timeout for HTTP requests.\n  # http_timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "logparser",
      "description": "Stream and parse log file(s).",
      "config": "# Stream and parse log file(s).\n[[inputs.logparser]]\n  # alias=\"logparser\"\n  ## Log files to parse.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   /var/log/**.log     -\u003e recursively find all .log files in /var/log\n  ##   /var/log/*/*.log    -\u003e find all .log files with a parent dir in /var/log\n  ##   /var/log/apache.log -\u003e only tail the apache log file\n  files = [\"/var/log/apache/access.log\"]\n\n  ## Read files that currently exist from the beginning. Files that are created\n  ## while telegraf is running (and that match the \"files\" globs) will always\n  ## be read from the beginning.\n  from_beginning = false\n\n  ## Method used to watch for file updates.  Can be either \"inotify\" or \"poll\".\n  # watch_method = \"inotify\"\n\n  ## Parse logstash-style \"grok\" patterns:\n  [inputs.logparser.grok]\n    ## This is a list of patterns to check the given log file(s) for.\n    ## Note that adding patterns here increases processing time. The most\n    ## efficient configuration is to have one pattern per logparser.\n    ## Other common built-in patterns are:\n    ##   %{COMMON_LOG_FORMAT}   (plain apache \u0026 nginx access logs)\n    ##   %{COMBINED_LOG_FORMAT} (access logs + referrer \u0026 agent)\n    patterns = [\"%{COMBINED_LOG_FORMAT}\"]\n\n    ## Name of the outputted measurement name.\n    measurement = \"apache_access_log\"\n\n    ## Full path(s) to custom pattern files.\n    custom_pattern_files = []\n\n    ## Custom patterns can also be defined here. Put one pattern per line.\n    custom_patterns = '''\n    '''\n\n    ## Timezone allows you to provide an override for timestamps that\n    ## don't already include an offset\n    ## e.g. 04/06/2016 12:41:45 data one two 5.43Âµs\n    ##\n    ## Default: \"\" which renders UTC\n    ## Options are as follows:\n    ##   1. Local             -- interpret based on machine localtime\n    ##   2. \"Canada/Eastern\"  -- Unix TZ values like those found in https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\n    ##   3. UTC               -- or blank/unspecified, will return timestamp in UTC\n    # timezone = \"Canada/Eastern\"\n\n\t## When set to \"disable\", timestamp will not incremented if there is a\n\t## duplicate.\n    # unique_timestamp = \"auto\"\n\n"
    },
    {
      "type": "input",
      "name": "tomcat",
      "description": "Gather metrics from the Tomcat server status page.",
      "config": "# Gather metrics from the Tomcat server status page.\n[[inputs.tomcat]]\n  # alias=\"tomcat\"\n  ## URL of the Tomcat server status\n  # url = \"http://127.0.0.1:8080/manager/status/all?XML=true\"\n\n  ## HTTP Basic Auth Credentials\n  # username = \"tomcat\"\n  # password = \"s3cret\"\n\n  ## Request timeout\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "twemproxy",
      "description": "Read Twemproxy stats data",
      "config": "# Read Twemproxy stats data\n[[inputs.twemproxy]]\n  # alias=\"twemproxy\"\n  ## Twemproxy stats address and port (no scheme)\n  addr = \"localhost:22222\"\n  ## Monitor pool name\n  pools = [\"redis_pool\", \"mc_pool\"]\n\n"
    },
    {
      "type": "input",
      "name": "influxdb_listener",
      "description": "Influx HTTP write listener",
      "config": "# Influx HTTP write listener\n[[inputs.influxdb_listener]]\n  # alias=\"influxdb_listener\"\n  ## Address and port to host HTTP listener on\n  service_address = \":8186\"\n\n  ## maximum duration before timing out read of the request\n  read_timeout = \"10s\"\n  ## maximum duration before timing out write of the response\n  write_timeout = \"10s\"\n\n  ## Maximum allowed http request body size in bytes.\n  ## 0 means to use the default of 524,288,000 bytes (500 mebibytes)\n  max_body_size = \"500MiB\"\n\n  ## Maximum line size allowed to be sent in bytes.\n  ## 0 means to use the default of 65536 bytes (64 kibibytes)\n  max_line_size = \"64KiB\"\n  \n\n  ## Optional tag name used to store the database. \n  ## If the write has a database in the query string then it will be kept in this tag name.\n  ## This tag can be used in downstream outputs.\n  ## The default value of nothing means it will be off and the database will not be recorded.\n  # database_tag = \"\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  tls_cert = \"/etc/telegraf/cert.pem\"\n  tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Optional username and password to accept for HTTP basic authentication.\n  ## You probably want to make sure you have TLS configured above for this.\n  # basic_username = \"foobar\"\n  # basic_password = \"barfoo\"\n\n"
    },
    {
      "type": "input",
      "name": "jti_openconfig_telemetry",
      "description": "Read JTI OpenConfig Telemetry from listed sensors",
      "config": "# Read JTI OpenConfig Telemetry from listed sensors\n[[inputs.jti_openconfig_telemetry]]\n  # alias=\"jti_openconfig_telemetry\"\n  ## List of device addresses to collect telemetry from\n  servers = [\"localhost:1883\"]\n\n  ## Authentication details. Username and password are must if device expects\n  ## authentication. Client ID must be unique when connecting from multiple instances\n  ## of telegraf to the same device\n  username = \"user\"\n  password = \"pass\"\n  client_id = \"telegraf\"\n\n  ## Frequency to get data\n  sample_frequency = \"1000ms\"\n\n  ## Sensors to subscribe for\n  ## A identifier for each sensor can be provided in path by separating with space\n  ## Else sensor path will be used as identifier\n  ## When identifier is used, we can provide a list of space separated sensors.\n  ## A single subscription will be created with all these sensors and data will\n  ## be saved to measurement with this identifier name\n  sensors = [\n   \"/interfaces/\",\n   \"collection /components/ /lldp\",\n  ]\n\n  ## We allow specifying sensor group level reporting rate. To do this, specify the\n  ## reporting rate in Duration at the beginning of sensor paths / collection\n  ## name. For entries without reporting rate, we use configured sample frequency\n  sensors = [\n   \"1000ms customReporting /interfaces /lldp\",\n   \"2000ms collection /components\",\n   \"/interfaces\",\n  ]\n\n  ## Optional TLS Config\n  # enable_tls = true\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Delay between retry attempts of failed RPC calls or streams. Defaults to 1000ms.\n  ## Failed streams/calls will not be retried if 0 is provided\n  retry_delay = \"1000ms\"\n\n  ## To treat all string values as tags, set this to true\n  str_as_tags = false\n\n"
    },
    {
      "type": "input",
      "name": "kinesis_consumer",
      "description": "Configuration for the AWS Kinesis input.",
      "config": "# Configuration for the AWS Kinesis input.\n[[inputs.kinesis_consumer]]\n  # alias=\"kinesis_consumer\"\n  ## Amazon REGION of kinesis endpoint.\n  region = \"ap-southeast-2\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Assumed credentials via STS if role_arn is specified\n  ## 2) explicit credentials from 'access_key' and 'secret_key'\n  ## 3) shared profile from 'profile'\n  ## 4) environment variables\n  ## 5) shared credentials file\n  ## 6) EC2 Instance Profile\n  # access_key = \"\"\n  # secret_key = \"\"\n  # token = \"\"\n  # role_arn = \"\"\n  # profile = \"\"\n  # shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Kinesis StreamName must exist prior to starting telegraf.\n  streamname = \"StreamName\"\n\n  ## Shard iterator type (only 'TRIM_HORIZON' and 'LATEST' currently supported)\n  # shard_iterator_type = \"TRIM_HORIZON\"\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Optional\n  ## Configuration for a dynamodb checkpoint\n  [inputs.kinesis_consumer.checkpoint_dynamodb]\n\t## unique name for this consumer\n\tapp_name = \"default\"\n\ttable_name = \"default\"\n\n"
    },
    {
      "type": "input",
      "name": "pgbouncer",
      "description": "Read metrics from one or many pgbouncer servers",
      "config": "# Read metrics from one or many pgbouncer servers\n[[inputs.pgbouncer]]\n  # alias=\"pgbouncer\"\n  ## specify address via a url matching:\n  ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n  ##       ?sslmode=[disable|verify-ca|verify-full]\n  ## or a simple string:\n  ##   host=localhost user=pqotest password=... sslmode=... dbname=app_production\n  ##\n  ## All connection parameters are optional.\n  ##\n  address = \"host=localhost user=pgbouncer sslmode=disable\"\n\n"
    },
    {
      "type": "input",
      "name": "internal",
      "description": "Collect statistics about itself",
      "config": "# Collect statistics about itself\n[[inputs.internal]]\n  # alias=\"internal\"\n  ## If true, collect telegraf memory stats.\n  # collect_memstats = true\n\n"
    },
    {
      "type": "input",
      "name": "mcrouter",
      "description": "Read metrics from one or many mcrouter servers",
      "config": "# Read metrics from one or many mcrouter servers\n[[inputs.mcrouter]]\n  # alias=\"mcrouter\"\n  ## An array of address to gather stats about. Specify an ip or hostname\n  ## with port. ie tcp://localhost:11211, tcp://10.0.0.1:11211, etc.\n\tservers = [\"tcp://localhost:11211\", \"unix:///var/run/mcrouter.sock\"]\n\n\t## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "postgresql_extensible",
      "description": "Read metrics from one or many postgresql servers",
      "config": "# Read metrics from one or many postgresql servers\n[[inputs.postgresql_extensible]]\n  # alias=\"postgresql_extensible\"\n  ## specify address via a url matching:\n  ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n  ##       ?sslmode=[disable|verify-ca|verify-full]\n  ## or a simple string:\n  ##   host=localhost user=pqotest password=... sslmode=... dbname=app_production\n  #\n  ## All connection parameters are optional.  #\n  ## Without the dbname parameter, the driver will default to a database\n  ## with the same name as the user. This dbname is just for instantiating a\n  ## connection with the server and doesn't restrict the databases we are trying\n  ## to grab metrics for.\n  #\n  address = \"host=localhost user=postgres sslmode=disable\"\n\n  ## connection configuration.\n  ## maxlifetime - specify the maximum lifetime of a connection.\n  ## default is forever (0s)\n  max_lifetime = \"0s\"\n\n  ## A list of databases to pull metrics about. If not specified, metrics for all\n  ## databases are gathered.\n  ## databases = [\"app_production\", \"testing\"]\n  #\n  ## A custom name for the database that will be used as the \"server\" tag in the\n  ## measurement output. If not specified, a default one generated from\n  ## the connection address is used.\n  # outputaddress = \"db01\"\n  #\n  ## Define the toml config where the sql queries are stored\n  ## New queries can be added, if the withdbname is set to true and there is no\n  ## databases defined in the 'databases field', the sql query is ended by a\n  ## 'is not null' in order to make the query succeed.\n  ## Example :\n  ## The sqlquery : \"SELECT * FROM pg_stat_database where datname\" become\n  ## \"SELECT * FROM pg_stat_database where datname IN ('postgres', 'pgbench')\"\n  ## because the databases variable was set to ['postgres', 'pgbench' ] and the\n  ## withdbname was true. Be careful that if the withdbname is set to false you\n  ## don't have to define the where clause (aka with the dbname) the tagvalue\n  ## field is used to define custom tags (separated by commas)\n  ## The optional \"measurement\" value can be used to override the default\n  ## output measurement name (\"postgresql\").\n  ##\n  ## The script option can be used to specify the .sql file path.\n  ## If script and sqlquery options specified at same time, sqlquery will be used \n  ##\n  ## Structure :\n  ## [[inputs.postgresql_extensible.query]]\n  ##   sqlquery string\n  ##   version string\n  ##   withdbname boolean\n  ##   tagvalue string (comma separated)\n  ##   measurement string\n  [[inputs.postgresql_extensible.query]]\n    sqlquery=\"SELECT * FROM pg_stat_database\"\n    version=901\n    withdbname=false\n    tagvalue=\"\"\n    measurement=\"\"\n  [[inputs.postgresql_extensible.query]]\n    sqlquery=\"SELECT * FROM pg_stat_bgwriter\"\n    version=901\n    withdbname=false\n    tagvalue=\"postgresql.stats\"\n\n"
    },
    {
      "type": "input",
      "name": "varnish",
      "description": "A plugin to collect stats from Varnish HTTP Cache",
      "config": "# A plugin to collect stats from Varnish HTTP Cache\n[[inputs.varnish]]\n  # alias=\"varnish\"\n  ## If running as a restricted user you can prepend sudo for additional access:\n  #use_sudo = false\n\n  ## The default location of the varnishstat binary can be overridden with:\n  binary = \"/usr/bin/varnishstat\"\n\n  ## By default, telegraf gather stats for 3 metric points.\n  ## Setting stats will override the defaults shown below.\n  ## Glob matching can be used, ie, stats = [\"MAIN.*\"]\n  ## stats may also be set to [\"*\"], which will collect all stats\n  stats = [\"MAIN.cache_hit\", \"MAIN.cache_miss\", \"MAIN.uptime\"]\n\n  ## Optional name for the varnish instance (or working directory) to query\n  ## Usually appened after -n in varnish cli\n  # instance_name = instanceName\n\n  ## Timeout for varnishstat command\n  # timeout = \"1s\"\n\n"
    },
    {
      "type": "input",
      "name": "wireless",
      "description": "Monitor wifi signal strength and quality",
      "config": "# Monitor wifi signal strength and quality\n[[inputs.wireless]]\n  # alias=\"wireless\"\n  ## Sets 'proc' directory path\n  ## If not specified, then default is /proc\n  # host_proc = \"/proc\"\n\n"
    },
    {
      "type": "input",
      "name": "rabbitmq",
      "description": "Reads metrics from RabbitMQ servers via the Management Plugin",
      "config": "# Reads metrics from RabbitMQ servers via the Management Plugin\n[[inputs.rabbitmq]]\n  # alias=\"rabbitmq\"\n  ## Management Plugin url. (default: http://localhost:15672)\n  # url = \"http://localhost:15672\"\n  ## Tag added to rabbitmq_overview series; deprecated: use tags\n  # name = \"rmq-server-1\"\n  ## Credentials\n  # username = \"guest\"\n  # password = \"guest\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional request timeouts\n  ##\n  ## ResponseHeaderTimeout, if non-zero, specifies the amount of time to wait\n  ## for a server's response headers after fully writing the request.\n  # header_timeout = \"3s\"\n  ##\n  ## client_timeout specifies a time limit for requests made by this client.\n  ## Includes connection time, any redirects, and reading the response body.\n  # client_timeout = \"4s\"\n\n  ## A list of nodes to gather as the rabbitmq_node measurement. If not\n  ## specified, metrics for all nodes are gathered.\n  # nodes = [\"rabbit@node1\", \"rabbit@node2\"]\n\n  ## A list of queues to gather as the rabbitmq_queue measurement. If not\n  ## specified, metrics for all queues are gathered.\n  # queues = [\"telegraf\"]\n\n  ## A list of exchanges to gather as the rabbitmq_exchange measurement. If not\n  ## specified, metrics for all exchanges are gathered.\n  # exchanges = [\"telegraf\"]\n\n  ## Queues to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all queues\n  queue_name_include = []\n  queue_name_exclude = []\n\n  ## Federation upstreams include and exclude when gathering the rabbitmq_federation measurement.\n  ## If neither are specified, metrics for all federation upstreams are gathered.\n  ## Federation link metrics will only be gathered for queues and exchanges\n  ## whose non-federation metrics will be collected (e.g a queue excluded\n  ## by the 'queue_name_exclude' option will also be excluded from federation).\n  ## Globs accepted.\n  # federation_upstream_include = [\"dataCentre-*\"]\n  # federation_upstream_exclude = []\n\n"
    },
    {
      "type": "input",
      "name": "x509_cert",
      "description": "Reads metrics from a SSL certificate",
      "config": "# Reads metrics from a SSL certificate\n[[inputs.x509_cert]]\n  # alias=\"x509_cert\"\n  ## List certificate sources\n  sources = [\"/etc/ssl/certs/ssl-cert-snakeoil.pem\", \"tcp://example.org:443\"]\n\n  ## Timeout for SSL connection\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n"
    },
    {
      "type": "input",
      "name": "cassandra",
      "description": "Read Cassandra metrics through Jolokia",
      "config": "# Read Cassandra metrics through Jolokia\n[[inputs.cassandra]]\n  # alias=\"cassandra\"\n  ## DEPRECATED: The cassandra plugin has been deprecated.  Please use the\n  ## jolokia2 plugin instead.\n  ##\n  ## see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\n\n  context = \"/jolokia/read\"\n  ## List of cassandra servers exposing jolokia read service\n  servers = [\"myuser:mypassword@10.10.10.1:8778\",\"10.10.10.2:8778\",\":8778\"]\n  ## List of metrics collected on above servers\n  ## Each metric consists of a jmx path.\n  ## This will collect all heap memory usage metrics from the jvm and\n  ## ReadLatency metrics for all keyspaces and tables.\n  ## \"type=Table\" in the query works with Cassandra3.0. Older versions might\n  ## need to use \"type=ColumnFamily\"\n  metrics  = [\n    \"/java.lang:type=Memory/HeapMemoryUsage\",\n    \"/org.apache.cassandra.metrics:type=Table,keyspace=*,scope=*,name=ReadLatency\"\n  ]\n\n"
    },
    {
      "type": "input",
      "name": "cloud_pubsub",
      "description": "Read metrics from Google PubSub",
      "config": "# Read metrics from Google PubSub\n[[inputs.cloud_pubsub]]\n  # alias=\"cloud_pubsub\"\n  ## Required. Name of Google Cloud Platform (GCP) Project that owns\n  ## the given PubSub subscription.\n  project = \"my-project\"\n\n  ## Required. Name of PubSub subscription to ingest metrics from.\n  subscription = \"my-subscription\"\n\n  ## Required. Data format to consume.\n  ## Each data format has its own unique set of configuration options.\n  ## Read more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n  ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n  ## Application Default Credentials, which is preferred.\n  # credentials_file = \"path/to/my/creds.json\"\n\n  ## Optional. Number of seconds to wait before attempting to restart the \n  ## PubSub subscription receiver after an unexpected error. \n  ## If the streaming pull for a PubSub Subscription fails (receiver),\n  ## the agent attempts to restart receiving messages after this many seconds.\n  # retry_delay_seconds = 5\n\n  ## Optional. Maximum byte length of a message to consume.\n  ## Larger messages are dropped with an error. If less than 0 or unspecified,\n  ## treated as no limit.\n  # max_message_len = 1000000\n\n  ## Optional. Maximum messages to read from PubSub that have not been written\n  ## to an output. Defaults to 1000.\n  ## For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message contains 10 metrics and the output\n  ## metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## The following are optional Subscription ReceiveSettings in PubSub.\n  ## Read more about these values:\n  ## https://godoc.org/cloud.google.com/go/pubsub#ReceiveSettings\n\n  ## Optional. Maximum number of seconds for which a PubSub subscription\n  ## should auto-extend the PubSub ACK deadline for each message. If less than\n  ## 0, auto-extension is disabled.\n  # max_extension = 0\n\n  ## Optional. Maximum number of unprocessed messages in PubSub\n  ## (unacknowledged but not yet expired in PubSub).\n  ## A value of 0 is treated as the default PubSub value.\n  ## Negative values will be treated as unlimited.\n  # max_outstanding_messages = 0\n\n  ## Optional. Maximum size in bytes of unprocessed messages in PubSub\n  ## (unacknowledged but not yet expired in PubSub).\n  ## A value of 0 is treated as the default PubSub value.\n  ## Negative values will be treated as unlimited.\n  # max_outstanding_bytes = 0\n\n  ## Optional. Max number of goroutines a PubSub Subscription receiver can spawn\n  ## to pull messages from PubSub concurrently. This limit applies to each\n  ## subscription separately and is treated as the PubSub default if less than\n  ## 1. Note this setting does not limit the number of messages that can be\n  ## processed concurrently (use \"max_outstanding_messages\" instead).\n  # max_receiver_go_routines = 0\n\n  ## Optional. If true, Telegraf will attempt to base64 decode the \n  ## PubSub message data before parsing\n  # base64_data = false\n\n"
    },
    {
      "type": "input",
      "name": "ipmi_sensor",
      "description": "Read metrics from the bare metal servers via IPMI",
      "config": "# Read metrics from the bare metal servers via IPMI\n[[inputs.ipmi_sensor]]\n  # alias=\"ipmi_sensor\"\n  ## optionally specify the path to the ipmitool executable\n  # path = \"/usr/bin/ipmitool\"\n  ##\n  ## optionally force session privilege level. Can be CALLBACK, USER, OPERATOR, ADMINISTRATOR\n  # privilege = \"ADMINISTRATOR\"\n  ##\n  ## optionally specify one or more servers via a url matching\n  ##  [username[:password]@][protocol[(address)]]\n  ##  e.g.\n  ##    root:passwd@lan(127.0.0.1)\n  ##\n  ## if no servers are specified, local machine sensor stats will be queried\n  ##\n  # servers = [\"USERID:PASSW0RD@lan(192.168.1.1)\"]\n\n  ## Recommended: use metric 'interval' that is a multiple of 'timeout' to avoid\n  ## gaps or overlap in pulled data\n  interval = \"30s\"\n\n  ## Timeout for the ipmitool command to complete\n  timeout = \"20s\"\n\n  ## Schema Version: (Optional, defaults to version 1)\n  metric_version = 2\n\n"
    },
    {
      "type": "input",
      "name": "jolokia",
      "description": "Read JMX metrics through Jolokia",
      "config": "# Read JMX metrics through Jolokia\n[[inputs.jolokia]]\n  # alias=\"jolokia\"\n  # DEPRECATED: the jolokia plugin has been deprecated in favor of the\n  # jolokia2 plugin\n  # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\n\n  ## This is the context root used to compose the jolokia url\n  ## NOTE that Jolokia requires a trailing slash at the end of the context root\n  ## NOTE that your jolokia security policy must allow for POST requests.\n  context = \"/jolokia/\"\n\n  ## This specifies the mode used\n  # mode = \"proxy\"\n  #\n  ## When in proxy mode this section is used to specify further\n  ## proxy address configurations.\n  ## Remember to change host address to fit your environment.\n  # [inputs.jolokia.proxy]\n  #   host = \"127.0.0.1\"\n  #   port = \"8080\"\n\n  ## Optional http timeouts\n  ##\n  ## response_header_timeout, if non-zero, specifies the amount of time to wait\n  ## for a server's response headers after fully writing the request.\n  # response_header_timeout = \"3s\"\n  ##\n  ## client_timeout specifies a time limit for requests made by this client.\n  ## Includes connection time, any redirects, and reading the response body.\n  # client_timeout = \"4s\"\n\n  ## Attribute delimiter\n  ##\n  ## When multiple attributes are returned for a single\n  ## [inputs.jolokia.metrics], the field name is a concatenation of the metric\n  ## name, and the attribute name, separated by the given delimiter.\n  # delimiter = \"_\"\n\n  ## List of servers exposing jolokia read service\n  [[inputs.jolokia.servers]]\n    name = \"as-server-01\"\n    host = \"127.0.0.1\"\n    port = \"8080\"\n    # username = \"myuser\"\n    # password = \"mypassword\"\n\n  ## List of metrics collected on above servers\n  ## Each metric consists in a name, a jmx path and either\n  ## a pass or drop slice attribute.\n  ## This collect all heap memory usage metrics.\n  [[inputs.jolokia.metrics]]\n    name = \"heap_memory_usage\"\n    mbean  = \"java.lang:type=Memory\"\n    attribute = \"HeapMemoryUsage\"\n\n  ## This collect thread counts metrics.\n  [[inputs.jolokia.metrics]]\n    name = \"thread_count\"\n    mbean  = \"java.lang:type=Threading\"\n    attribute = \"TotalStartedThreadCount,ThreadCount,DaemonThreadCount,PeakThreadCount\"\n\n  ## This collect number of class loaded/unloaded counts metrics.\n  [[inputs.jolokia.metrics]]\n    name = \"class_count\"\n    mbean  = \"java.lang:type=ClassLoading\"\n    attribute = \"LoadedClassCount,UnloadedClassCount,TotalLoadedClassCount\"\n\n"
    },
    {
      "type": "input",
      "name": "mem",
      "description": "Read metrics about memory usage",
      "config": "# Read metrics about memory usage\n[[inputs.mem]]\n  # alias=\"mem\"\n"
    },
    {
      "type": "input",
      "name": "filecount",
      "description": "Count files in a directory",
      "config": "# Count files in a directory\n[[inputs.filecount]]\n  # alias=\"filecount\"\n  ## Directory to gather stats about.\n  ##   deprecated in 1.9; use the directories option\n  # directory = \"/var/cache/apt/archives\"\n\n  ## Directories to gather stats about.\n  ## This accept standard unit glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   /var/log/**    -\u003e recursively find all directories in /var/log and count files in each directories\n  ##   /var/log/*/*   -\u003e find all directories with a parent dir in /var/log and count files in each directories\n  ##   /var/log       -\u003e count all files in /var/log and all of its subdirectories\n  directories = [\"/var/cache/apt/archives\"]\n\n  ## Only count files that match the name pattern. Defaults to \"*\".\n  name = \"*.deb\"\n\n  ## Count files in subdirectories. Defaults to true.\n  recursive = false\n\n  ## Only count regular files. Defaults to true.\n  regular_only = true\n\n  ## Follow all symlinks while walking the directory tree. Defaults to false.\n  follow_symlinks = false\n\n  ## Only count files that are at least this size. If size is\n  ## a negative number, only count files that are smaller than the\n  ## absolute value of size. Acceptable units are B, KiB, MiB, KB, ...\n  ## Without quotes and units, interpreted as size in bytes.\n  size = \"0B\"\n\n  ## Only count files that have not been touched for at least this\n  ## duration. If mtime is negative, only count files that have been\n  ## touched in this duration. Defaults to \"0s\".\n  mtime = \"0s\"\n\n"
    },
    {
      "type": "input",
      "name": "kafka_consumer_legacy",
      "description": "Read metrics from Kafka topic(s)",
      "config": "# Read metrics from Kafka topic(s)\n[[inputs.kafka_consumer_legacy]]\n  # alias=\"kafka_consumer_legacy\"\n  ## topic(s) to consume\n  topics = [\"telegraf\"]\n\n  ## an array of Zookeeper connection strings\n  zookeeper_peers = [\"localhost:2181\"]\n\n  ## Zookeeper Chroot\n  zookeeper_chroot = \"\"\n\n  ## the name of the consumer group\n  consumer_group = \"telegraf_metrics_consumers\"\n\n  ## Offset (must be either \"oldest\" or \"newest\")\n  offset = \"oldest\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n  ## larger messages are dropped\n  max_message_len = 65536\n\n"
    },
    {
      "type": "input",
      "name": "net",
      "description": "Read metrics about network interface usage",
      "config": "# Read metrics about network interface usage\n[[inputs.net]]\n  # alias=\"net\"\n  ## By default, telegraf gathers stats from any up interface (excluding loopback)\n  ## Setting interfaces will tell it to gather these explicit interfaces,\n  ## regardless of status.\n  ##\n  # interfaces = [\"eth0\"]\n  ##\n  ## On linux systems telegraf also collects protocol stats.\n  ## Setting ignore_protocol_stats to true will skip reporting of protocol metrics.\n  ##\n  # ignore_protocol_stats = false\n  ##\n\n"
    },
    {
      "type": "input",
      "name": "nsq",
      "description": "Read NSQ topic and channel statistics.",
      "config": "# Read NSQ topic and channel statistics.\n[[inputs.nsq]]\n  # alias=\"nsq\"\n  ## An array of NSQD HTTP API endpoints\n  endpoints  = [\"http://localhost:4151\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "conntrack",
      "description": "Collects conntrack stats from the configured directories and files.",
      "config": "# Collects conntrack stats from the configured directories and files.\n[[inputs.conntrack]]\n  # alias=\"conntrack\"\n   ## The following defaults would work with multiple versions of conntrack.\n   ## Note the nf_ and ip_ filename prefixes are mutually exclusive across\n   ## kernel versions, as are the directory locations.\n\n   ## Superset of filenames to look for within the conntrack dirs.\n   ## Missing files will be ignored.\n   files = [\"ip_conntrack_count\",\"ip_conntrack_max\",\n            \"nf_conntrack_count\",\"nf_conntrack_max\"]\n\n   ## Directories to search within for the conntrack files above.\n   ## Missing directrories will be ignored.\n   dirs = [\"/proc/sys/net/ipv4/netfilter\",\"/proc/sys/net/netfilter\"]\n\n"
    },
    {
      "type": "input",
      "name": "iptables",
      "description": "Gather packets and bytes throughput from iptables",
      "config": "# Gather packets and bytes throughput from iptables\n[[inputs.iptables]]\n  # alias=\"iptables\"\n  ## iptables require root access on most systems.\n  ## Setting 'use_sudo' to true will make use of sudo to run iptables.\n  ## Users must configure sudo to allow telegraf user to run iptables with no password.\n  ## iptables can be restricted to only list command \"iptables -nvL\".\n  use_sudo = false\n  ## Setting 'use_lock' to true runs iptables with the \"-w\" option.\n  ## Adjust your sudo settings appropriately if using this option (\"iptables -w 5 -nvl\")\n  use_lock = false\n  ## Define an alternate executable, such as \"ip6tables\". Default is \"iptables\".\n  # binary = \"ip6tables\"\n  ## defines the table to monitor:\n  table = \"filter\"\n  ## defines the chains to monitor.\n  ## NOTE: iptables rules without a comment will not be monitored.\n  ## Read the plugin documentation for more information.\n  chains = [ \"INPUT\" ]\n\n"
    },
    {
      "type": "input",
      "name": "memcached",
      "description": "Read metrics from one or many memcached servers",
      "config": "# Read metrics from one or many memcached servers\n[[inputs.memcached]]\n  # alias=\"memcached\"\n  ## An array of address to gather stats about. Specify an ip on hostname\n  ## with optional port. ie localhost, 10.0.0.1:11211, etc.\n  servers = [\"localhost:11211\"]\n  # unix_sockets = [\"/var/run/memcached.sock\"]\n\n"
    },
    {
      "type": "input",
      "name": "snmp_trap",
      "description": "Receive SNMP traps",
      "config": "# Receive SNMP traps\n[[inputs.snmp_trap]]\n  # alias=\"snmp_trap\"\n  ## Transport, local address, and port to listen on.  Transport must\n  ## be \"udp://\".  Omit local address to listen on all interfaces.\n  ##   example: \"udp://127.0.0.1:1234\"\n  # service_address = udp://:162\n  ## Timeout running snmptranslate command\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "dns_query",
      "description": "Query given DNS server and gives statistics",
      "config": "# Query given DNS server and gives statistics\n[[inputs.dns_query]]\n  # alias=\"dns_query\"\n  ## servers to query\n  servers = [\"8.8.8.8\"]\n\n  ## Network is the network protocol name.\n  # network = \"udp\"\n\n  ## Domains or subdomains to query.\n  # domains = [\".\"]\n\n  ## Query record type.\n  ## Posible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\n  # record_type = \"A\"\n\n  ## Dns server port.\n  # port = 53\n\n  ## Query timeout in seconds.\n  # timeout = 2\n\n"
    },
    {
      "type": "input",
      "name": "linux_sysctl_fs",
      "description": "Provides Linux sysctl fs metrics",
      "config": "# Provides Linux sysctl fs metrics\n[[inputs.linux_sysctl_fs]]\n  # alias=\"linux_sysctl_fs\"\n"
    },
    {
      "type": "input",
      "name": "netstat",
      "description": "Read TCP metrics such as established, time wait and sockets counts.",
      "config": "# Read TCP metrics such as established, time wait and sockets counts.\n[[inputs.netstat]]\n  # alias=\"netstat\"\n"
    },
    {
      "type": "input",
      "name": "postfix",
      "description": "Measure postfix queue statistics",
      "config": "# Measure postfix queue statistics\n[[inputs.postfix]]\n  # alias=\"postfix\"\n  ## Postfix queue directory. If not provided, telegraf will try to use\n  ## 'postconf -h queue_directory' to determine it.\n  # queue_directory = \"/var/spool/postfix\"\n\n"
    },
    {
      "type": "input",
      "name": "rethinkdb",
      "description": "Read metrics from one or many RethinkDB servers",
      "config": "# Read metrics from one or many RethinkDB servers\n[[inputs.rethinkdb]]\n  # alias=\"rethinkdb\"\n  ## An array of URI to gather stats about. Specify an ip or hostname\n  ## with optional port add password. ie,\n  ##   rethinkdb://user:auth_key@10.10.3.30:28105,\n  ##   rethinkdb://10.10.3.33:18832,\n  ##   10.0.0.1:10000, etc.\n  servers = [\"127.0.0.1:28015\"]\n  ##\n  ## If you use actual rethinkdb of \u003e 2.3.0 with username/password authorization,\n  ## protocol have to be named \"rethinkdb2\" - it will use 1_0 H.\n  # servers = [\"rethinkdb2://username:password@127.0.0.1:28015\"]\n  ##\n  ## If you use older versions of rethinkdb (\u003c2.2) with auth_key, protocol\n  ## have to be named \"rethinkdb\".\n  # servers = [\"rethinkdb://username:auth_key@127.0.0.1:28015\"]\n\n"
    },
    {
      "type": "input",
      "name": "bond",
      "description": "Collect bond interface status, slaves statuses and failures count",
      "config": "# Collect bond interface status, slaves statuses and failures count\n[[inputs.bond]]\n  # alias=\"bond\"\n  ## Sets 'proc' directory path\n  ## If not specified, then default is /proc\n  # host_proc = \"/proc\"\n\n  ## By default, telegraf gather stats for all bond interfaces\n  ## Setting interfaces will restrict the stats to the specified\n  ## bond interfaces.\n  # bond_interfaces = [\"bond0\"]\n\n"
    },
    {
      "type": "input",
      "name": "couchdb",
      "description": "Read CouchDB Stats from one or more servers",
      "config": "# Read CouchDB Stats from one or more servers\n[[inputs.couchdb]]\n  # alias=\"couchdb\"\n  ## Works with CouchDB stats endpoints out of the box\n  ## Multiple Hosts from which to read CouchDB stats:\n  hosts = [\"http://localhost:8086/_stats\"]\n\n  ## Use HTTP Basic Authentication.\n  # basic_username = \"telegraf\"\n  # basic_password = \"p@ssw0rd\"\n\n"
    },
    {
      "type": "input",
      "name": "kibana",
      "description": "Read status information from one or more Kibana servers",
      "config": "# Read status information from one or more Kibana servers\n[[inputs.kibana]]\n  # alias=\"kibana\"\n  ## specify a list of one or more Kibana servers\n  servers = [\"http://localhost:5601\"]\n\n  ## Timeout for HTTP requests\n  timeout = \"5s\"\n\n  ## HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "sensors",
      "description": "Monitor sensors, requires lm-sensors package",
      "config": "# Monitor sensors, requires lm-sensors package\n[[inputs.sensors]]\n  # alias=\"sensors\"\n  ## Remove numbers from field names.\n  ## If true, a field name like 'temp1_input' will be changed to 'temp_input'.\n  # remove_numbers = true\n\n  ## Timeout is the maximum amount of time that the sensors command can run.\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "synproxy",
      "description": "Get synproxy counter statistics from procfs",
      "config": "# Get synproxy counter statistics from procfs\n[[inputs.synproxy]]\n  # alias=\"synproxy\"\n"
    },
    {
      "type": "input",
      "name": "prometheus",
      "description": "Read metrics from one or many prometheus clients",
      "config": "# Read metrics from one or many prometheus clients\n[[inputs.prometheus]]\n  # alias=\"prometheus\"\n  ## An array of urls to scrape metrics from.\n  urls = [\"http://localhost:9100/metrics\"]\n\n  ## Metric version controls the mapping from Prometheus metrics into\n  ## Telegraf metrics.  When using the prometheus_client output, use the same\n  ## value in both plugins to ensure metrics are round-tripped without\n  ## modification.\n  ##\n  ##   example: metric_version = 1; deprecated in 1.13\n  ##            metric_version = 2; recommended version\n  # metric_version = 1\n\n  ## Url tag name (tag containing scrapped url. optional, default is \"url\")\n  # url_tag = \"scrapeUrl\"\n\n  ## An array of Kubernetes services to scrape metrics from.\n  # kubernetes_services = [\"http://my-service-dns.my-namespace:9100/metrics\"]\n\n  ## Kubernetes config file to create client from.\n  # kube_config = \"/path/to/kubernetes.config\"\n\n  ## Scrape Kubernetes pods for the following prometheus annotations:\n  ## - prometheus.io/scrape: Enable scraping for this pod\n  ## - prometheus.io/scheme: If the metrics endpoint is secured then you will need to\n  ##     set this to 'https' \u0026 most likely set the tls config.\n  ## - prometheus.io/path: If the metrics path is not /metrics, define it with this annotation.\n  ## - prometheus.io/port: If port is not 9102 use this annotation\n  # monitor_kubernetes_pods = true\n  ## Restricts Kubernetes monitoring to a single namespace\n  ##   ex: monitor_kubernetes_pods_namespace = \"default\"\n  # monitor_kubernetes_pods_namespace = \"\"\n\n  ## Use bearer token for authorization. ('bearer_token' takes priority)\n  # bearer_token = \"/path/to/bearer/token\"\n  ## OR\n  # bearer_token_string = \"abc_123\"\n\n  ## HTTP Basic Authentication username and password. ('bearer_token' and\n  ## 'bearer_token_string' take priority)\n  # username = \"\"\n  # password = \"\"\n\n  ## Specify timeout duration for slower prometheus clients (default is 3s)\n  # response_timeout = \"3s\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "cisco_telemetry_mdt",
      "description": "Cisco model-driven telemetry (MDT) input plugin for IOS XR, IOS XE and NX-OS platforms",
      "config": "# Cisco model-driven telemetry (MDT) input plugin for IOS XR, IOS XE and NX-OS platforms\n[[inputs.cisco_telemetry_mdt]]\n  # alias=\"cisco_telemetry_mdt\"\n ## Telemetry transport can be \"tcp\" or \"grpc\".  TLS is only supported when\n ## using the grpc transport.\n transport = \"grpc\"\n\n ## Address and port to host telemetry listener\n service_address = \":57000\"\n\n ## Enable TLS; grpc transport only.\n # tls_cert = \"/etc/telegraf/cert.pem\"\n # tls_key = \"/etc/telegraf/key.pem\"\n\n ## Enable TLS client authentication and define allowed CA certificates; grpc\n ##  transport only.\n # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n ## Define (for certain nested telemetry measurements with embedded tags) which fields are tags\n # embedded_tags = [\"Cisco-IOS-XR-qos-ma-oper:qos/interface-table/interface/input/service-policy-names/service-policy-instance/statistics/class-stats/class-name\"]\n\n ## Define aliases to map telemetry encoding paths to simple measurement names\n [inputs.cisco_telemetry_mdt.aliases]\n   ifstats = \"ietf-interfaces:interfaces-state/interface/statistics\"\n\n"
    },
    {
      "type": "input",
      "name": "fail2ban",
      "description": "Read metrics from fail2ban.",
      "config": "# Read metrics from fail2ban.\n[[inputs.fail2ban]]\n  # alias=\"fail2ban\"\n  ## Use sudo to run fail2ban-client\n  use_sudo = false\n\n"
    },
    {
      "type": "input",
      "name": "nsq_consumer",
      "description": "Read NSQ topic for metrics.",
      "config": "# Read NSQ topic for metrics.\n[[inputs.nsq_consumer]]\n  # alias=\"nsq_consumer\"\n  ## Server option still works but is deprecated, we just prepend it to the nsqd array.\n  # server = \"localhost:4150\"\n\n  ## An array representing the NSQD TCP HTTP Endpoints\n  nsqd = [\"localhost:4150\"]\n\n  ## An array representing the NSQLookupd HTTP Endpoints\n  nsqlookupd = [\"localhost:4161\"]\n  topic = \"telegraf\"\n  channel = \"consumer\"\n  max_in_flight = 100\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "opensmtpd",
      "description": "A plugin to collect stats from Opensmtpd - a validating, recursive, and caching DNS resolver ",
      "config": "# A plugin to collect stats from Opensmtpd - a validating, recursive, and caching DNS resolver \n[[inputs.opensmtpd]]\n  # alias=\"opensmtpd\"\n  ## If running as a restricted user you can prepend sudo for additional access:\n  #use_sudo = false\n\n  ## The default location of the smtpctl binary can be overridden with:\n  binary = \"/usr/sbin/smtpctl\"\n\n  ## The default timeout of 1000ms can be overriden with (in milliseconds):\n  timeout = 1000\n\n"
    },
    {
      "type": "input",
      "name": "postgresql",
      "description": "Read metrics from one or many postgresql servers",
      "config": "# Read metrics from one or many postgresql servers\n[[inputs.postgresql]]\n  # alias=\"postgresql\"\n  ## specify address via a url matching:\n  ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n  ##       ?sslmode=[disable|verify-ca|verify-full]\n  ## or a simple string:\n  ##   host=localhost user=pqotest password=... sslmode=... dbname=app_production\n  ##\n  ## All connection parameters are optional.\n  ##\n  ## Without the dbname parameter, the driver will default to a database\n  ## with the same name as the user. This dbname is just for instantiating a\n  ## connection with the server and doesn't restrict the databases we are trying\n  ## to grab metrics for.\n  ##\n  address = \"host=localhost user=postgres sslmode=disable\"\n  ## A custom name for the database that will be used as the \"server\" tag in the\n  ## measurement output. If not specified, a default one generated from\n  ## the connection address is used.\n  # outputaddress = \"db01\"\n\n  ## connection configuration.\n  ## maxlifetime - specify the maximum lifetime of a connection.\n  ## default is forever (0s)\n  max_lifetime = \"0s\"\n\n  ## A  list of databases to explicitly ignore.  If not specified, metrics for all\n  ## databases are gathered.  Do NOT use with the 'databases' option.\n  # ignored_databases = [\"postgres\", \"template0\", \"template1\"]\n\n  ## A list of databases to pull metrics about. If not specified, metrics for all\n  ## databases are gathered.  Do NOT use with the 'ignored_databases' option.\n  # databases = [\"app_production\", \"testing\"]\n\n"
    },
    {
      "type": "input",
      "name": "apcupsd",
      "description": "Monitor APC UPSes connected to apcupsd",
      "config": "# Monitor APC UPSes connected to apcupsd\n[[inputs.apcupsd]]\n  # alias=\"apcupsd\"\n  # A list of running apcupsd server to connect to.\n  # If not provided will default to tcp://127.0.0.1:3551\n  servers = [\"tcp://127.0.0.1:3551\"]\n\n  ## Timeout for dialing server.\n  timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "phpfpm",
      "description": "Read metrics of phpfpm, via HTTP status page or socket",
      "config": "# Read metrics of phpfpm, via HTTP status page or socket\n[[inputs.phpfpm]]\n  # alias=\"phpfpm\"\n  ## An array of addresses to gather stats about. Specify an ip or hostname\n  ## with optional port and path\n  ##\n  ## Plugin can be configured in three modes (either can be used):\n  ##   - http: the URL must start with http:// or https://, ie:\n  ##       \"http://localhost/status\"\n  ##       \"http://192.168.130.1/status?full\"\n  ##\n  ##   - unixsocket: path to fpm socket, ie:\n  ##       \"/var/run/php5-fpm.sock\"\n  ##      or using a custom fpm status path:\n  ##       \"/var/run/php5-fpm.sock:fpm-custom-status-path\"\n  ##\n  ##   - fcgi: the URL must start with fcgi:// or cgi://, and port must be present, ie:\n  ##       \"fcgi://10.0.0.12:9000/status\"\n  ##       \"cgi://10.0.10.12:9001/status\"\n  ##\n  ## Example of multiple gathering from local socket and remote host\n  ## urls = [\"http://192.168.1.20/status\", \"/tmp/fpm.sock\"]\n  urls = [\"http://localhost/status\"]\n\n  ## Duration allowed to complete HTTP requests.\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "smart",
      "description": "Read metrics from storage devices supporting S.M.A.R.T.",
      "config": "# Read metrics from storage devices supporting S.M.A.R.T.\n[[inputs.smart]]\n  # alias=\"smart\"\n  ## Optionally specify the path to the smartctl executable\n  # path = \"/usr/bin/smartctl\"\n\n  ## On most platforms smartctl requires root access.\n  ## Setting 'use_sudo' to true will make use of sudo to run smartctl.\n  ## Sudo must be configured to to allow the telegraf user to run smartctl\n  ## without a password.\n  # use_sudo = false\n\n  ## Skip checking disks in this power mode. Defaults to\n  ## \"standby\" to not wake up disks that have stoped rotating.\n  ## See --nocheck in the man pages for smartctl.\n  ## smartctl version 5.41 and 5.42 have faulty detection of\n  ## power mode and might require changing this value to\n  ## \"never\" depending on your disks.\n  # nocheck = \"standby\"\n\n  ## Gather all returned S.M.A.R.T. attribute metrics and the detailed\n  ## information from each drive into the 'smart_attribute' measurement.\n  # attributes = false\n\n  ## Optionally specify devices to exclude from reporting.\n  # excludes = [ \"/dev/pass6\" ]\n\n  ## Optionally specify devices and device type, if unset\n  ## a scan (smartctl --scan) for S.M.A.R.T. devices will\n  ## done and all found will be included except for the\n  ## excluded in excludes.\n  # devices = [ \"/dev/ada0 -d atacam\" ]\n\n  ## Timeout for the smartctl command to complete.\n  # timeout = \"30s\"\n\n"
    },
    {
      "type": "input",
      "name": "swap",
      "description": "Read metrics about swap memory usage",
      "config": "# Read metrics about swap memory usage\n[[inputs.swap]]\n  # alias=\"swap\"\n"
    },
    {
      "type": "input",
      "name": "zookeeper",
      "description": "Reads 'mntr' stats from one or many zookeeper servers",
      "config": "# Reads 'mntr' stats from one or many zookeeper servers\n[[inputs.zookeeper]]\n  # alias=\"zookeeper\"\n  ## An array of address to gather stats about. Specify an ip or hostname\n  ## with port. ie localhost:2181, 10.0.0.1:2181, etc.\n\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no port is specified, 2181 is used\n  servers = [\":2181\"]\n\n  ## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # enable_tls = true\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## If false, skip chain \u0026 host verification\n  # insecure_skip_verify = true\n\n"
    },
    {
      "type": "input",
      "name": "disque",
      "description": "Read metrics from one or many disque servers",
      "config": "# Read metrics from one or many disque servers\n[[inputs.disque]]\n  # alias=\"disque\"\n  ## An array of URI to gather stats about. Specify an ip or hostname\n  ## with optional port and password.\n  ## ie disque://localhost, disque://10.10.3.33:18832, 10.0.0.1:10000, etc.\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"localhost\"]\n\n"
    },
    {
      "type": "input",
      "name": "hddtemp",
      "description": "Monitor disks' temperatures using hddtemp",
      "config": "# Monitor disks' temperatures using hddtemp\n[[inputs.hddtemp]]\n  # alias=\"hddtemp\"\n  ## By default, telegraf gathers temps data from all disks detected by the\n  ## hddtemp.\n  ##\n  ## Only collect temps from the selected disks.\n  ##\n  ## A * as the device name will return the temperature values of all disks.\n  ##\n  # address = \"127.0.0.1:7634\"\n  # devices = [\"sda\", \"*\"]\n\n"
    },
    {
      "type": "input",
      "name": "interrupts",
      "description": "This plugin gathers interrupts data from /proc/interrupts and /proc/softirqs.",
      "config": "# This plugin gathers interrupts data from /proc/interrupts and /proc/softirqs.\n[[inputs.interrupts]]\n  # alias=\"interrupts\"\n  ## When set to true, cpu metrics are tagged with the cpu.  Otherwise cpu is\n  ## stored as a field.\n  ##\n  ## The default is false for backwards compatibility, and will be changed to\n  ## true in a future version.  It is recommended to set to true on new\n  ## deployments.\n  # cpu_as_tag = false\n\n  ## To filter which IRQs to collect, make use of tagpass / tagdrop, i.e.\n  # [inputs.interrupts.tagdrop]\n  #   irq = [ \"NET_RX\", \"TASKLET\" ]\n\n"
    },
    {
      "type": "input",
      "name": "jenkins",
      "description": "Read jobs and cluster metrics from Jenkins instances",
      "config": "# Read jobs and cluster metrics from Jenkins instances\n[[inputs.jenkins]]\n  # alias=\"jenkins\"\n  ## The Jenkins URL in the format \"schema://host:port\"\n  url = \"http://my-jenkins-instance:8080\"\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Set response_timeout\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use SSL but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional Max Job Build Age filter\n  ## Default 1 hour, ignore builds older than max_build_age\n  # max_build_age = \"1h\"\n\n  ## Optional Sub Job Depth filter\n  ## Jenkins can have unlimited layer of sub jobs\n  ## This config will limit the layers of pulling, default value 0 means\n  ## unlimited pulling until no more sub jobs\n  # max_subjob_depth = 0\n\n  ## Optional Sub Job Per Layer\n  ## In workflow-multibranch-plugin, each branch will be created as a sub job.\n  ## This config will limit to call only the lasted branches in each layer, \n  ## empty will use default value 10\n  # max_subjob_per_layer = 10\n\n  ## Jobs to exclude from gathering\n  # job_exclude = [ \"job1\", \"job2/subjob1/subjob2\", \"job3/*\"]\n\n  ## Nodes to exclude from gathering\n  # node_exclude = [ \"node1\", \"node2\" ]\n\n  ## Worker pool for jenkins plugin only\n  ## Empty this field will use default value 5\n  # max_connections = 5\n\n"
    },
    {
      "type": "input",
      "name": "nvidia_smi",
      "description": "Pulls statistics from nvidia GPUs attached to the host",
      "config": "# Pulls statistics from nvidia GPUs attached to the host\n[[inputs.nvidia_smi]]\n  # alias=\"nvidia_smi\"\n  ## Optional: path to nvidia-smi binary, defaults to $PATH via exec.LookPath\n  # bin_path = \"/usr/bin/nvidia-smi\"\n\n  ## Optional: timeout for GPU polling\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "ceph",
      "description": "Collects performance metrics from the MON and OSD nodes in a Ceph storage cluster.",
      "config": "# Collects performance metrics from the MON and OSD nodes in a Ceph storage cluster.\n[[inputs.ceph]]\n  # alias=\"ceph\"\n  ## This is the recommended interval to poll.  Too frequent and you will lose\n  ## data points due to timeouts during rebalancing and recovery\n  interval = '1m'\n\n  ## All configuration values are optional, defaults are shown below\n\n  ## location of ceph binary\n  ceph_binary = \"/usr/bin/ceph\"\n\n  ## directory in which to look for socket files\n  socket_dir = \"/var/run/ceph\"\n\n  ## prefix of MON and OSD socket files, used to determine socket type\n  mon_prefix = \"ceph-mon\"\n  osd_prefix = \"ceph-osd\"\n\n  ## suffix used to identify socket files\n  socket_suffix = \"asok\"\n\n  ## Ceph user to authenticate as\n  ceph_user = \"client.admin\"\n\n  ## Ceph configuration to use to locate the cluster\n  ceph_config = \"/etc/ceph/ceph.conf\"\n\n  ## Whether to gather statistics via the admin socket\n  gather_admin_socket_stats = true\n\n  ## Whether to gather statistics via ceph commands\n  gather_cluster_stats = false\n\n"
    },
    {
      "type": "input",
      "name": "dmcache",
      "description": "Provide a native collection for dmsetup based statistics for dm-cache",
      "config": "# Provide a native collection for dmsetup based statistics for dm-cache\n[[inputs.dmcache]]\n  # alias=\"dmcache\"\n  ## Whether to report per-device stats or not\n  per_device = true\n\n"
    },
    {
      "type": "input",
      "name": "net_response",
      "description": "Collect response time of a TCP or UDP connection",
      "config": "# Collect response time of a TCP or UDP connection\n[[inputs.net_response]]\n  # alias=\"net_response\"\n  ## Protocol, must be \"tcp\" or \"udp\"\n  ## NOTE: because the \"udp\" protocol does not respond to requests, it requires\n  ## a send/expect string pair (see below).\n  protocol = \"tcp\"\n  ## Server address (default localhost)\n  address = \"localhost:80\"\n\n  ## Set timeout\n  # timeout = \"1s\"\n\n  ## Set read timeout (only used if expecting a response)\n  # read_timeout = \"1s\"\n\n  ## The following options are required for UDP checks. For TCP, they are\n  ## optional. The plugin will send the given string to the server and then\n  ## expect to receive the given 'expect' string back.\n  ## string sent to the server\n  # send = \"ssh\"\n  ## expected string in answer\n  # expect = \"ssh\"\n\n  ## Uncomment to remove deprecated fields\n  # fielddrop = [\"result_type\", \"string_found\"]\n\n"
    },
    {
      "type": "input",
      "name": "puppetagent",
      "description": "Reads last_run_summary.yaml file and converts to measurements",
      "config": "# Reads last_run_summary.yaml file and converts to measurements\n[[inputs.puppetagent]]\n  # alias=\"puppetagent\"\n  ## Location of puppet last run summary file\n  location = \"/var/lib/puppet/state/last_run_summary.yaml\"\n\n"
    },
    {
      "type": "input",
      "name": "zfs",
      "description": "Read metrics of ZFS from arcstats, zfetchstats, vdev_cache_stats, and pools",
      "config": "# Read metrics of ZFS from arcstats, zfetchstats, vdev_cache_stats, and pools\n[[inputs.zfs]]\n  # alias=\"zfs\"\n  ## ZFS kstat path. Ignored on FreeBSD\n  ## If not specified, then default is:\n  # kstatPath = \"/proc/spl/kstat/zfs\"\n\n  ## By default, telegraf gather all zfs stats\n  ## If not specified, then default is:\n  # kstatMetrics = [\"arcstats\", \"zfetchstats\", \"vdev_cache_stats\"]\n  ## For Linux, the default is:\n  # kstatMetrics = [\"abdstats\", \"arcstats\", \"dnodestats\", \"dbufcachestats\",\n  #   \"dmu_tx\", \"fm\", \"vdev_mirror_stats\", \"zfetchstats\", \"zil\"]\n  ## By default, don't gather zpool stats\n  # poolMetrics = false\n\n"
    },
    {
      "type": "input",
      "name": "aerospike",
      "description": "Read stats from aerospike server(s)",
      "config": "# Read stats from aerospike server(s)\n[[inputs.aerospike]]\n  # alias=\"aerospike\"\n  ## Aerospike servers to connect to (with port)\n  ## This plugin will query all namespaces the aerospike\n  ## server has configured and get stats for them.\n  servers = [\"localhost:3000\"]\n\n  # username = \"telegraf\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # enable_tls = false\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## If false, skip chain \u0026 host verification\n  # insecure_skip_verify = true\n \n"
    },
    {
      "type": "input",
      "name": "exec",
      "description": "Read metrics from one or more commands that can output to stdout",
      "config": "# Read metrics from one or more commands that can output to stdout\n[[inputs.exec]]\n  # alias=\"exec\"\n  ## Commands array\n  commands = [\n    \"/tmp/test.sh\",\n    \"/usr/bin/mycollector --foo=bar\",\n    \"/tmp/collect_*.sh\"\n  ]\n\n  ## Timeout for each command to complete.\n  timeout = \"5s\"\n\n  ## measurement name suffix (for separating different commands)\n  name_suffix = \"_mycollector\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "influxdb",
      "description": "Read InfluxDB-formatted JSON metrics from one or more HTTP endpoints",
      "config": "# Read InfluxDB-formatted JSON metrics from one or more HTTP endpoints\n[[inputs.influxdb]]\n  # alias=\"influxdb\"\n  ## Works with InfluxDB debug endpoints out of the box,\n  ## but other services can use this format too.\n  ## See the influxdb plugin's README for more details.\n\n  ## Multiple URLs from which to read InfluxDB-formatted JSON\n  ## Default is \"http://localhost:8086/debug/vars\".\n  urls = [\n    \"http://localhost:8086/debug/vars\"\n  ]\n\n  ## Username and password to send using HTTP Basic Authentication.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## http request \u0026 header timeout\n  timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "nginx",
      "description": "Read Nginx's basic status information (ngx_http_stub_status_module)",
      "config": "# Read Nginx's basic status information (ngx_http_stub_status_module)\n[[inputs.nginx]]\n  # alias=\"nginx\"\n  # An array of Nginx stub_status URI to gather stats.\n  urls = [\"http://localhost/server_status\"]\n\n  ## Optional TLS Config\n  tls_ca = \"/etc/telegraf/ca.pem\"\n  tls_cert = \"/etc/telegraf/cert.cer\"\n  tls_key = \"/etc/telegraf/key.key\"\n  ## Use TLS but skip chain \u0026 host verification\n  insecure_skip_verify = false\n\n  # HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "ping",
      "description": "Ping given url(s) and return statistics",
      "config": "# Ping given url(s) and return statistics\n[[inputs.ping]]\n  # alias=\"ping\"\n  ## Hosts to send ping packets to.\n  urls = [\"example.org\"]\n\n  ## Method used for sending pings, can be either \"exec\" or \"native\".  When set\n  ## to \"exec\" the systems ping command will be executed.  When set to \"native\"\n  ## the plugin will send pings directly.\n  ##\n  ## While the default is \"exec\" for backwards compatibility, new deployments\n  ## are encouraged to use the \"native\" method for improved compatibility and\n  ## performance.\n  # method = \"exec\"\n\n  ## Number of ping packets to send per interval.  Corresponds to the \"-c\"\n  ## option of the ping command.\n  # count = 1\n\n  ## Time to wait between sending ping packets in seconds.  Operates like the\n  ## \"-i\" option of the ping command.\n  # ping_interval = 1.0\n\n  ## If set, the time to wait for a ping response in seconds.  Operates like\n  ## the \"-W\" option of the ping command.\n  # timeout = 1.0\n\n  ## If set, the total ping deadline, in seconds.  Operates like the -w option\n  ## of the ping command.\n  # deadline = 10\n\n  ## Interface or source address to send ping from.  Operates like the -I or -S\n  ## option of the ping command.\n  # interface = \"\"\n\n  ## Specify the ping executable binary.\n  # binary = \"ping\"\n\n  ## Arguments for ping command. When arguments is not empty, the command from\n  ## the binary option will be used and other options (ping_interval, timeout,\n  ## etc) will be ignored.\n  # arguments = [\"-c\", \"3\"]\n\n  ## Use only IPv6 addresses when resolving a hostname.\n  # ipv6 = false\n\n"
    },
    {
      "type": "input",
      "name": "stackdriver",
      "description": "Gather timeseries from Google Cloud Platform v3 monitoring API",
      "config": "# Gather timeseries from Google Cloud Platform v3 monitoring API\n[[inputs.stackdriver]]\n  # alias=\"stackdriver\"\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Include timeseries that start with the given metric type.\n  metric_type_prefix_include = [\n    \"compute.googleapis.com/\",\n  ]\n\n  ## Exclude timeseries that start with the given metric type.\n  # metric_type_prefix_exclude = []\n\n  ## Many metrics are updated once per minute; it is recommended to override\n  ## the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  # \t\"ALIGN_PERCENTILE_99\",\n  # \t\"ALIGN_PERCENTILE_95\",\n  # \t\"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= \u003cresource_labels\u003e {AND \u003cmetric_labels\u003e}\n  ##   resource_labels ::= \u003cresource_labels\u003e {OR \u003cresource_label\u003e}\n  ##   metric_labels ::= \u003cmetric_labels\u003e {OR \u003cmetric_label\u003e}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.\u003ckey\u003e = \u003cvalue\u003e\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.\u003ckey\u003e = \u003cvalue\u003e\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #  \t key = \"device_name\"\n  #  \t value = 'one_of(\"sda\", \"sdb\")'\n\n"
    },
    {
      "type": "input",
      "name": "syslog",
      "description": "Accepts syslog messages following RFC5424 format with transports as per RFC5426, RFC5425, or RFC6587",
      "config": "# Accepts syslog messages following RFC5424 format with transports as per RFC5426, RFC5425, or RFC6587\n[[inputs.syslog]]\n  # alias=\"syslog\"\n  ## Specify an ip or hostname with port - eg., tcp://localhost:6514, tcp://10.0.0.1:6514\n  ## Protocol, address and port to host the syslog receiver.\n  ## If no host is specified, then localhost is used.\n  ## If no port is specified, 6514 is used (RFC5425#section-4.1).\n  server = \"tcp://:6514\"\n\n  ## TLS Config\n  # tls_allowed_cacerts = [\"/etc/telegraf/ca.pem\"]\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Period between keep alive probes.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  ## Only applies to stream sockets (e.g. TCP).\n  # keep_alive_period = \"5m\"\n\n  ## Maximum number of concurrent connections (default = 0).\n  ## 0 means unlimited.\n  ## Only applies to stream sockets (e.g. TCP).\n  # max_connections = 1024\n\n  ## Read timeout is the maximum time allowed for reading a single message (default = 5s).\n  ## 0 means unlimited.\n  # read_timeout = \"5s\"\n\n  ## The framing technique with which it is expected that messages are transported (default = \"octet-counting\").\n  ## Whether the messages come using the octect-counting (RFC5425#section-4.3.1, RFC6587#section-3.4.1),\n  ## or the non-transparent framing technique (RFC6587#section-3.4.2).\n  ## Must be one of \"octet-counting\", \"non-transparent\".\n  # framing = \"octet-counting\"\n\n  ## The trailer to be expected in case of non-trasparent framing (default = \"LF\").\n  ## Must be one of \"LF\", or \"NUL\".\n  # trailer = \"LF\"\n\n  ## Whether to parse in best effort mode or not (default = false).\n  ## By default best effort parsing is off.\n  # best_effort = false\n\n  ## Character to prepend to SD-PARAMs (default = \"_\").\n  ## A syslog message can contain multiple parameters and multiple identifiers within structured data section.\n  ## Eg., [id1 name1=\"val1\" name2=\"val2\"][id2 name1=\"val1\" nameA=\"valA\"]\n  ## For each combination a field is created.\n  ## Its name is created concatenating identifier, sdparam_separator, and parameter name.\n  # sdparam_separator = \"_\"\n\n"
    },
    {
      "type": "input",
      "name": "activemq",
      "description": "Gather ActiveMQ metrics",
      "config": "# Gather ActiveMQ metrics\n[[inputs.activemq]]\n  # alias=\"activemq\"\n  ## ActiveMQ WebConsole URL\n  url = \"http://127.0.0.1:8161\"\n\n  ## Required ActiveMQ Endpoint\n  ##   deprecated in 1.11; use the url option\n  # server = \"127.0.0.1\"\n  # port = 8161\n\n  ## Credentials for basic HTTP authentication\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Required ActiveMQ webadmin root path\n  # webadmin = \"admin\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n  \n"
    },
    {
      "type": "input",
      "name": "bind",
      "description": "Read BIND nameserver XML statistics",
      "config": "# Read BIND nameserver XML statistics\n[[inputs.bind]]\n  # alias=\"bind\"\n  ## An array of BIND XML statistics URI to gather stats.\n  ## Default is \"http://localhost:8053/xml/v3\".\n  # urls = [\"http://localhost:8053/xml/v3\"]\n  # gather_memory_contexts = false\n  # gather_views = false\n\n"
    },
    {
      "type": "input",
      "name": "httpjson",
      "description": "Read flattened metrics from one or more JSON HTTP endpoints",
      "config": "# Read flattened metrics from one or more JSON HTTP endpoints\n[[inputs.httpjson]]\n  # alias=\"httpjson\"\n  ## NOTE This plugin only reads numerical measurements, strings and booleans\n  ## will be ignored.\n\n  ## Name for the service being polled.  Will be appended to the name of the\n  ## measurement e.g. httpjson_webserver_stats\n  ##\n  ## Deprecated (1.3.0): Use name_override, name_suffix, name_prefix instead.\n  name = \"webserver_stats\"\n\n  ## URL of each server in the service's cluster\n  servers = [\n    \"http://localhost:8086/stats/\",\n    \"http://localhost:9998/stats/\",\n  ]\n  ## Set response_timeout (default 5 seconds)\n  response_timeout = \"5s\"\n\n  ## HTTP method to use: GET or POST (case-sensitive)\n  method = \"GET\"\n\n  ## List of tag names to extract from top-level of JSON server response\n  # tag_keys = [\n  #   \"my_tag_1\",\n  #   \"my_tag_2\"\n  # ]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## HTTP parameters (all values must be strings).  For \"GET\" requests, data\n  ## will be included in the query.  For \"POST\" requests, data will be included\n  ## in the request body as \"x-www-form-urlencoded\".\n  # [inputs.httpjson.parameters]\n  #   event_type = \"cpu_spike\"\n  #   threshold = \"0.75\"\n\n  ## HTTP Headers (all values must be strings)\n  # [inputs.httpjson.headers]\n  #   X-Auth-Token = \"my-xauth-token\"\n  #   apiVersion = \"v1\"\n\n"
    },
    {
      "type": "input",
      "name": "kapacitor",
      "description": "Read Kapacitor-formatted JSON metrics from one or more HTTP endpoints",
      "config": "# Read Kapacitor-formatted JSON metrics from one or more HTTP endpoints\n[[inputs.kapacitor]]\n  # alias=\"kapacitor\"\n  ## Multiple URLs from which to read Kapacitor-formatted JSON\n  ## Default is \"http://localhost:9092/kapacitor/v1/debug/vars\".\n  urls = [\n    \"http://localhost:9092/kapacitor/v1/debug/vars\"\n  ]\n\n  ## Time limit for http requests\n  timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "multifile",
      "description": "Aggregates the contents of multiple files into a single point",
      "config": "# Aggregates the contents of multiple files into a single point\n[[inputs.multifile]]\n  # alias=\"multifile\"\n  ## Base directory where telegraf will look for files.\n  ## Omit this option to use absolute paths.\n  base_dir = \"/sys/bus/i2c/devices/1-0076/iio:device0\"\n\n  ## If true, Telegraf discard all data when a single file can't be read.\n  ## Else, Telegraf omits the field generated from this file.\n  # fail_early = true\n\n  ## Files to parse each interval.\n  [[inputs.multifile.file]]\n    file = \"in_pressure_input\"\n    dest = \"pressure\"\n    conversion = \"float\"\n  [[inputs.multifile.file]]\n    file = \"in_temp_input\"\n    dest = \"temperature\"\n    conversion = \"float(3)\"\n  [[inputs.multifile.file]]\n    file = \"in_humidityrelative_input\"\n    dest = \"humidityrelative\"\n    conversion = \"float(3)\"\n\n"
    },
    {
      "type": "input",
      "name": "raindrops",
      "description": "Read raindrops stats (raindrops - real-time stats for preforking Rack servers)",
      "config": "# Read raindrops stats (raindrops - real-time stats for preforking Rack servers)\n[[inputs.raindrops]]\n  # alias=\"raindrops\"\n  ## An array of raindrops middleware URI to gather stats.\n  urls = [\"http://localhost:8080/_raindrops\"]\n\n"
    },
    {
      "type": "input",
      "name": "riak",
      "description": "Read metrics one or many Riak servers",
      "config": "# Read metrics one or many Riak servers\n[[inputs.riak]]\n  # alias=\"riak\"\n  # Specify a list of one or more riak http servers\n  servers = [\"http://localhost:8098\"]\n\n"
    },
    {
      "type": "input",
      "name": "socket_listener",
      "description": "Generic socket listener capable of handling multiple socket types.",
      "config": "# Generic socket listener capable of handling multiple socket types.\n[[inputs.socket_listener]]\n  # alias=\"socket_listener\"\n  ## URL to listen on\n  # service_address = \"tcp://:8094\"\n  # service_address = \"tcp://127.0.0.1:http\"\n  # service_address = \"tcp4://:8094\"\n  # service_address = \"tcp6://:8094\"\n  # service_address = \"tcp6://[2001:db8::1]:8094\"\n  # service_address = \"udp://:8094\"\n  # service_address = \"udp4://:8094\"\n  # service_address = \"udp6://:8094\"\n  # service_address = \"unix:///tmp/telegraf.sock\"\n  # service_address = \"unixgram:///tmp/telegraf.sock\"\n\n  ## Change the file mode bits on unix sockets.  These permissions may not be\n  ## respected by some platforms, to safely restrict write permissions it is best\n  ## to place the socket into a directory that has previously been created\n  ## with the desired permissions.\n  ##   ex: socket_mode = \"777\"\n  # socket_mode = \"\"\n\n  ## Maximum number of concurrent connections.\n  ## Only applies to stream sockets (e.g. TCP).\n  ## 0 (default) is unlimited.\n  # max_connections = 1024\n\n  ## Read timeout.\n  ## Only applies to stream sockets (e.g. TCP).\n  ## 0 (default) is unlimited.\n  # read_timeout = \"30s\"\n\n  ## Optional TLS configuration.\n  ## Only applies to stream sockets (e.g. TCP).\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key  = \"/etc/telegraf/key.pem\"\n  ## Enables client authentication if set.\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Maximum socket buffer size (in bytes when no unit specified).\n  ## For stream sockets, once the buffer fills up, the sender will start backing up.\n  ## For datagram sockets, once the buffer fills up, metrics will start dropping.\n  ## Defaults to the OS default.\n  # read_buffer_size = \"64KiB\"\n\n  ## Period between keep alive probes.\n  ## Only applies to TCP sockets.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  # keep_alive_period = \"5m\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"influx\"\n\n  ## Content encoding for message payloads, can be set to \"gzip\" to or\n  ## \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n"
    },
    {
      "type": "input",
      "name": "cisco_telemetry_gnmi",
      "description": "Cisco GNMI telemetry input plugin based on GNMI telemetry data produced in IOS XR",
      "config": "# Cisco GNMI telemetry input plugin based on GNMI telemetry data produced in IOS XR\n[[inputs.cisco_telemetry_gnmi]]\n  # alias=\"cisco_telemetry_gnmi\"\n ## Address and port of the GNMI GRPC server\n addresses = [\"10.49.234.114:57777\"]\n\n ## define credentials\n username = \"cisco\"\n password = \"cisco\"\n\n ## GNMI encoding requested (one of: \"proto\", \"json\", \"json_ietf\")\n # encoding = \"proto\"\n\n ## redial in case of failures after\n redial = \"10s\"\n\n ## enable client-side TLS and define CA to authenticate the device\n # enable_tls = true\n # tls_ca = \"/etc/telegraf/ca.pem\"\n # insecure_skip_verify = true\n\n ## define client-side TLS certificate \u0026 key to authenticate to the device\n # tls_cert = \"/etc/telegraf/cert.pem\"\n # tls_key = \"/etc/telegraf/key.pem\"\n\n ## GNMI subscription prefix (optional, can usually be left empty)\n ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n # origin = \"\"\n # prefix = \"\"\n # target = \"\"\n\n ## Define additional aliases to map telemetry encoding paths to simple measurement names\n #[inputs.cisco_telemetry_gnmi.aliases]\n #  ifcounters = \"openconfig:/interfaces/interface/state/counters\"\n\n [[inputs.cisco_telemetry_gnmi.subscription]]\n  ## Name of the measurement that will be emitted\n  name = \"ifcounters\"\n\n  ## Origin and path of the subscription\n  ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n  ##\n  ## origin usually refers to a (YANG) data model implemented by the device\n  ## and path to a specific substructe inside it that should be subscribed to (similar to an XPath)\n  ## YANG models can be found e.g. here: https://github.com/YangModels/yang/tree/master/vendor/cisco/xr\n  origin = \"openconfig-interfaces\"\n  path = \"/interfaces/interface/state/counters\"\n\n  # Subscription mode (one of: \"target_defined\", \"sample\", \"on_change\") and interval\n  subscription_mode = \"sample\"\n  sample_interval = \"10s\"\n\n  ## Suppress redundant transmissions when measured values are unchanged\n  # suppress_redundant = false\n\n  ## If suppression is enabled, send updates at least every X seconds anyway\n  # heartbeat_interval = \"60s\"\n\n"
    },
    {
      "type": "input",
      "name": "haproxy",
      "description": "Read metrics of haproxy, via socket or csv stats page",
      "config": "# Read metrics of haproxy, via socket or csv stats page\n[[inputs.haproxy]]\n  # alias=\"haproxy\"\n  ## An array of address to gather stats about. Specify an ip on hostname\n  ## with optional port. ie localhost, 10.10.3.33:1936, etc.\n  ## Make sure you specify the complete path to the stats endpoint\n  ## including the protocol, ie http://10.10.3.33:1936/haproxy?stats\n\n  ## If no servers are specified, then default to 127.0.0.1:1936/haproxy?stats\n  servers = [\"http://myhaproxy.com:1936/haproxy?stats\"]\n\n  ## Credentials for basic HTTP authentication\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## You can also use local socket with standard wildcard globbing.\n  ## Server address not starting with 'http' will be treated as a possible\n  ## socket, so both examples below are valid.\n  # servers = [\"socket:/run/haproxy/admin.sock\", \"/run/haproxy/*.sock\"]\n\n  ## By default, some of the fields are renamed from what haproxy calls them.\n  ## Setting this option to true results in the plugin keeping the original\n  ## field names.\n  # keep_field_names = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "kubernetes",
      "description": "Read metrics from the kubernetes kubelet api",
      "config": "# Read metrics from the kubernetes kubelet api\n[[inputs.kubernetes]]\n  # alias=\"kubernetes\"\n  ## URL for the kubelet\n  url = \"http://127.0.0.1:10255\"\n\n  ## Use bearer token for authorization. ('bearer_token' takes priority)\n  ## If both of these are empty, we'll use the default serviceaccount:\n  ## at: /run/secrets/kubernetes.io/serviceaccount/token\n  # bearer_token = \"/path/to/bearer/token\"\n  ## OR\n  # bearer_token_string = \"abc_123\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "logstash",
      "description": "Read metrics exposed by Logstash",
      "config": "# Read metrics exposed by Logstash\n[[inputs.logstash]]\n  # alias=\"logstash\"\n  ## The URL of the exposed Logstash API endpoint.\n  url = \"http://127.0.0.1:9600\"\n\n  ## Use Logstash 5 single pipeline API, set to true when monitoring\n  ## Logstash 5.\n  # single_pipeline = false\n\n  ## Enable optional collection components.  Can contain\n  ## \"pipelines\", \"process\", and \"jvm\".\n  # collect = [\"pipelines\", \"process\", \"jvm\"]\n\n  ## Timeout for HTTP requests.\n  # timeout = \"5s\"\n\n  ## Optional HTTP Basic Auth credentials.\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Use TLS but skip chain \u0026 host verification.\n  # insecure_skip_verify = false\n\n  ## Optional HTTP headers.\n  # [inputs.logstash.headers]\n  #   \"X-Special-Header\" = \"Special-Value\"\n\n"
    },
    {
      "type": "input",
      "name": "nats_consumer",
      "description": "Read metrics from NATS subject(s)",
      "config": "# Read metrics from NATS subject(s)\n[[inputs.nats_consumer]]\n  # alias=\"nats_consumer\"\n  ## urls of NATS servers\n  servers = [\"nats://localhost:4222\"]\n\n  ## subject(s) to consume\n  subjects = [\"telegraf\"]\n\n  ## name a queue group\n  queue_group = \"telegraf_consumers\"\n\n  ## Optional credentials\n  # username = \"\"\n  # password = \"\"\n\n  ## Use Transport Layer Security\n  # secure = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Sets the limits for pending msgs and bytes for each subscription\n  ## These shouldn't need to be adjusted except in very high throughput scenarios\n  # pending_message_limit = 65536\n  # pending_bytes_limit = 67108864\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "trig",
      "description": "Inserts sine and cosine waves for demonstration purposes",
      "config": "# Inserts sine and cosine waves for demonstration purposes\n[[inputs.trig]]\n  # alias=\"trig\"\n  ## Set the amplitude\n  amplitude = 10.0\n\n"
    },
    {
      "type": "input",
      "name": "mqtt_consumer",
      "description": "Read metrics from MQTT topic(s)",
      "config": "# Read metrics from MQTT topic(s)\n[[inputs.mqtt_consumer]]\n  # alias=\"mqtt_consumer\"\n  ## MQTT broker URLs to be used. The format should be scheme://host:port,\n  ## schema can be tcp, ssl, or ws.\n  servers = [\"tcp://127.0.0.1:1883\"]\n\n  ## Topics that will be subscribed to.\n  topics = [\n    \"telegraf/host01/cpu\",\n    \"telegraf/+/mem\",\n    \"sensors/#\",\n  ]\n\n  ## The message topic will be stored in a tag specified by this value.  If set\n  ## to the empty string no topic tag will be created.\n  # topic_tag = \"topic\"\n\n  ## QoS policy for messages\n  ##   0 = at most once\n  ##   1 = at least once\n  ##   2 = exactly once\n  ##\n  ## When using a QoS of 1 or 2, you should enable persistent_session to allow\n  ## resuming unacknowledged messages.\n  # qos = 0\n\n  ## Connection timeout for initial connection in seconds\n  # connection_timeout = \"30s\"\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Persistent session disables clearing of the client session on connection.\n  ## In order for this option to work you must also set client_id to identify\n  ## the client.  To receive messages that arrived while the client is offline,\n  ## also set the qos option to 1 or 2 and don't forget to also set the QoS when\n  ## publishing.\n  # persistent_session = false\n\n  ## If unset, a random client ID will be generated.\n  # client_id = \"\"\n\n  ## Username and password to connect MQTT server.\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "snmp",
      "description": "Retrieves SNMP values from remote agents",
      "config": "# Retrieves SNMP values from remote agents\n[[inputs.snmp]]\n  # alias=\"snmp\"\n  agents = [ \"127.0.0.1:161\" ]\n  ## Timeout for each SNMP query.\n  timeout = \"5s\"\n  ## Number of retries to attempt within timeout.\n  retries = 3\n  ## SNMP version, values can be 1, 2, or 3\n  version = 2\n\n  ## SNMP community string.\n  community = \"public\"\n\n  ## The GETBULK max-repetitions parameter\n  max_repetitions = 10\n\n  ## SNMPv3 auth parameters\n  #sec_name = \"myuser\"\n  #auth_protocol = \"md5\"      # Values: \"MD5\", \"SHA\", \"\"\n  #auth_password = \"pass\"\n  #sec_level = \"authNoPriv\"   # Values: \"noAuthNoPriv\", \"authNoPriv\", \"authPriv\"\n  #context_name = \"\"\n  #priv_protocol = \"\"         # Values: \"DES\", \"AES\", \"\"\n  #priv_password = \"\"\n\n  ## measurement name\n  name = \"system\"\n  [[inputs.snmp.field]]\n    name = \"hostname\"\n    oid = \".1.0.0.1.1\"\n  [[inputs.snmp.field]]\n    name = \"uptime\"\n    oid = \".1.0.0.1.2\"\n  [[inputs.snmp.field]]\n    name = \"load\"\n    oid = \".1.0.0.1.3\"\n  [[inputs.snmp.field]]\n    oid = \"HOST-RESOURCES-MIB::hrMemorySize\"\n\n  [[inputs.snmp.table]]\n    ## measurement name\n    name = \"remote_servers\"\n    inherit_tags = [ \"hostname\" ]\n    [[inputs.snmp.table.field]]\n      name = \"server\"\n      oid = \".1.0.0.0.1.0\"\n      is_tag = true\n    [[inputs.snmp.table.field]]\n      name = \"connections\"\n      oid = \".1.0.0.0.1.1\"\n    [[inputs.snmp.table.field]]\n      name = \"latency\"\n      oid = \".1.0.0.0.1.2\"\n\n  [[inputs.snmp.table]]\n    ## auto populate table's fields using the MIB\n    oid = \"HOST-RESOURCES-MIB::hrNetworkTable\"\n\n"
    },
    {
      "type": "input",
      "name": "teamspeak",
      "description": "Reads metrics from a Teamspeak 3 Server via ServerQuery",
      "config": "# Reads metrics from a Teamspeak 3 Server via ServerQuery\n[[inputs.teamspeak]]\n  # alias=\"teamspeak\"\n  ## Server address for Teamspeak 3 ServerQuery\n  # server = \"127.0.0.1:10011\"\n  ## Username for ServerQuery\n  username = \"serverqueryuser\"\n  ## Password for ServerQuery\n  password = \"secret\"\n  ## Array of virtual servers\n  # virtual_servers = [1]\n\n"
    },
    {
      "type": "input",
      "name": "azure_storage_queue",
      "description": "Gather Azure Storage Queue metrics",
      "config": "# Gather Azure Storage Queue metrics\n[[inputs.azure_storage_queue]]\n  # alias=\"azure_storage_queue\"\n  ## Required Azure Storage Account name\n  account_name = \"mystorageaccount\"\n\n  ## Required Azure Storage Account access key\n  account_key = \"storageaccountaccesskey\"\n\n  ## Set to false to disable peeking age of oldest message (executes faster)\n  # peek_oldest_message_age = true\n  \n"
    },
    {
      "type": "input",
      "name": "cpu",
      "description": "Read metrics about cpu usage",
      "config": "# Read metrics about cpu usage\n[[inputs.cpu]]\n  # alias=\"cpu\"\n  ## Whether to report per-cpu stats or not\n  percpu = true\n  ## Whether to report total system cpu stats or not\n  totalcpu = true\n  ## If true, collect raw CPU time metrics.\n  collect_cpu_time = false\n  ## If true, compute and report the sum of all non-idle CPU states.\n  report_active = false\n\n"
    },
    {
      "type": "input",
      "name": "dcos",
      "description": "Input plugin for DC/OS metrics",
      "config": "# Input plugin for DC/OS metrics\n[[inputs.dcos]]\n  # alias=\"dcos\"\n  ## The DC/OS cluster URL.\n  cluster_url = \"https://dcos-ee-master-1\"\n\n  ## The ID of the service account.\n  service_account_id = \"telegraf\"\n  ## The private key file for the service account.\n  service_account_private_key = \"/etc/telegraf/telegraf-sa-key.pem\"\n\n  ## Path containing login token.  If set, will read on every gather.\n  # token_file = \"/home/dcos/.dcos/token\"\n\n  ## In all filter options if both include and exclude are empty all items\n  ## will be collected.  Arrays may contain glob patterns.\n  ##\n  ## Node IDs to collect metrics from.  If a node is excluded, no metrics will\n  ## be collected for its containers or apps.\n  # node_include = []\n  # node_exclude = []\n  ## Container IDs to collect container metrics from.\n  # container_include = []\n  # container_exclude = []\n  ## Container IDs to collect app metrics from.\n  # app_include = []\n  # app_exclude = []\n\n  ## Maximum concurrent connections to the cluster.\n  # max_connections = 10\n  ## Maximum time to receive a response from cluster.\n  # response_timeout = \"20s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## If false, skip chain \u0026 host verification\n  # insecure_skip_verify = true\n\n  ## Recommended filtering to reduce series cardinality.\n  # [inputs.dcos.tagdrop]\n  #   path = [\"/var/lib/mesos/slave/slaves/*\"]\n\n"
    },
    {
      "type": "input",
      "name": "http_response",
      "description": "HTTP/HTTPS request given an address a method and a timeout",
      "config": "# HTTP/HTTPS request given an address a method and a timeout\n[[inputs.http_response]]\n  # alias=\"http_response\"\n  ## Deprecated in 1.12, use 'urls'\n  ## Server address (default http://localhost)\n  # address = \"http://localhost\"\n\n  ## List of urls to query.\n  # urls = [\"http://localhost\"]\n\n  ## Set http_proxy (telegraf uses the system wide proxy settings if it's is not set)\n  # http_proxy = \"http://localhost:8888\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## HTTP Request Method\n  # method = \"GET\"\n\n  ## Whether to follow redirects from the server (defaults to false)\n  # follow_redirects = false\n\n  ## Optional HTTP Request Body\n  # body = '''\n  # {'fake':'data'}\n  # '''\n\n  ## Optional substring or regex match in body of the response\n  # response_string_match = \"\\\"service_status\\\": \\\"up\\\"\"\n  # response_string_match = \"ok\"\n  # response_string_match = \"\\\".*_status\\\".?:.?\\\"up\\\"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## HTTP Request Headers (all values must be strings)\n  # [inputs.http_response.headers]\n  #   Host = \"github.com\"\n\n  ## Interface to use when dialing an address\n  # interface = \"eth0\"\n\n"
    },
    {
      "type": "input",
      "name": "mongodb",
      "description": "Read metrics from one or many MongoDB servers",
      "config": "# Read metrics from one or many MongoDB servers\n[[inputs.mongodb]]\n  # alias=\"mongodb\"\n  ## An array of URLs of the form:\n  ##   \"mongodb://\" [user \":\" pass \"@\"] host [ \":\" port]\n  ## For example:\n  ##   mongodb://user:auth_key@10.10.3.30:27017,\n  ##   mongodb://10.10.3.33:18832,\n  servers = [\"mongodb://127.0.0.1:27017\"]\n\n  ## When true, collect per database stats\n  # gather_perdb_stats = false\n\n  ## When true, collect per collection stats\n  # gather_col_stats = false\n\n  ## List of db where collections stats are collected\n  ## If empty, all db are concerned\n  # col_stats_dbs = [\"local\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "unbound",
      "description": "A plugin to collect stats from the Unbound DNS resolver",
      "config": "# A plugin to collect stats from the Unbound DNS resolver\n[[inputs.unbound]]\n  # alias=\"unbound\"\n  ## Address of server to connect to, read from unbound conf default, optionally ':port'\n  ## Will lookup IP if given a hostname\n  server = \"127.0.0.1:8953\"\n\n  ## If running as a restricted user you can prepend sudo for additional access:\n  # use_sudo = false\n\n  ## The default location of the unbound-control binary can be overridden with:\n  # binary = \"/usr/sbin/unbound-control\"\n\n  ## The default timeout of 1s can be overriden with:\n  # timeout = \"1s\"\n\n  ## When set to true, thread metrics are tagged with the thread id.\n  ##\n  ## The default is false for backwards compatibility, and will be changed to\n  ## true in a future version.  It is recommended to set to true on new\n  ## deployments.\n  thread_as_tag = false\n\n"
    },
    {
      "type": "input",
      "name": "jolokia2_agent",
      "description": "Read JMX metrics from a Jolokia REST agent endpoint",
      "config": "# Read JMX metrics from a Jolokia REST agent endpoint\n[[inputs.jolokia2_agent]]\n  # alias=\"jolokia2_agent\"\n  # default_tag_prefix      = \"\"\n  # default_field_prefix    = \"\"\n  # default_field_separator = \".\"\n\n  # Add agents URLs to query\n  urls = [\"http://localhost:8080/jolokia\"]\n  # username = \"\"\n  # password = \"\"\n  # response_timeout = \"5s\"\n\n  ## Optional TLS config\n  # tls_ca   = \"/var/private/ca.pem\"\n  # tls_cert = \"/var/private/client.pem\"\n  # tls_key  = \"/var/private/client-key.pem\"\n  # insecure_skip_verify = false\n\n  ## Add metrics to read\n  [[inputs.jolokia2_agent.metric]]\n    name  = \"java_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n\n"
    },
    {
      "type": "input",
      "name": "jolokia2_proxy",
      "description": "Read JMX metrics from a Jolokia REST proxy endpoint",
      "config": "# Read JMX metrics from a Jolokia REST proxy endpoint\n[[inputs.jolokia2_proxy]]\n  # alias=\"jolokia2_proxy\"\n  # default_tag_prefix      = \"\"\n  # default_field_prefix    = \"\"\n  # default_field_separator = \".\"\n\n  ## Proxy agent\n  url = \"http://localhost:8080/jolokia\"\n  # username = \"\"\n  # password = \"\"\n  # response_timeout = \"5s\"\n\n  ## Optional TLS config\n  # tls_ca   = \"/var/private/ca.pem\"\n  # tls_cert = \"/var/private/client.pem\"\n  # tls_key  = \"/var/private/client-key.pem\"\n  # insecure_skip_verify = false\n\n  ## Add proxy targets to query\n  # default_target_username = \"\"\n  # default_target_password = \"\"\n  [[inputs.jolokia2_proxy.target]]\n    url = \"service:jmx:rmi:///jndi/rmi://targethost:8086/jmxrmi\"\n    # username = \"\"\n    # password = \"\"\n\n  ## Add metrics to read\n  [[inputs.jolokia2_proxy.metric]]\n    name  = \"java_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n\n"
    },
    {
      "type": "input",
      "name": "mailchimp",
      "description": "Gathers metrics from the /3.0/reports MailChimp API",
      "config": "# Gathers metrics from the /3.0/reports MailChimp API\n[[inputs.mailchimp]]\n  # alias=\"mailchimp\"\n  ## MailChimp API key\n  ## get from https://admin.mailchimp.com/account/api/\n  api_key = \"\" # required\n  ## Reports for campaigns sent more than days_old ago will not be collected.\n  ## 0 means collect all.\n  days_old = 0\n  ## Campaign ID to get, if empty gets all campaigns, this option overrides days_old\n  # campaign_id = \"\"\n\n"
    },
    {
      "type": "input",
      "name": "minecraft",
      "description": "Collects scores from a Minecraft server's scoreboard using the RCON protocol",
      "config": "# Collects scores from a Minecraft server's scoreboard using the RCON protocol\n[[inputs.minecraft]]\n  # alias=\"minecraft\"\n  ## Address of the Minecraft server.\n  # server = \"localhost\"\n\n  ## Server RCON Port.\n  # port = \"25575\"\n\n  ## Server RCON Password.\n  password = \"\"\n\n  ## Uncomment to remove deprecated metric components.\n  # tagdrop = [\"server\"]\n\n"
    },
    {
      "type": "input",
      "name": "solr",
      "description": "Read stats from one or more Solr servers or cores",
      "config": "# Read stats from one or more Solr servers or cores\n[[inputs.solr]]\n  # alias=\"solr\"\n  ## specify a list of one or more Solr servers\n  servers = [\"http://localhost:8983\"]\n\n  ## specify a list of one or more Solr cores (default - all)\n  # cores = [\"main\"]\n\n  ## Optional HTTP Basic Auth Credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n"
    },
    {
      "type": "input",
      "name": "nginx_plus_api",
      "description": "Read Nginx Plus Api documentation",
      "config": "# Read Nginx Plus Api documentation\n[[inputs.nginx_plus_api]]\n  # alias=\"nginx_plus_api\"\n  ## An array of API URI to gather stats.\n  urls = [\"http://localhost/api\"]\n\n  # Nginx API version, default: 3\n  # api_version = 3\n\n  # HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "nstat",
      "description": "Collect kernel snmp counters and network interface statistics",
      "config": "# Collect kernel snmp counters and network interface statistics\n[[inputs.nstat]]\n  # alias=\"nstat\"\n  ## file paths for proc files. If empty default paths will be used:\n  ##    /proc/net/netstat, /proc/net/snmp, /proc/net/snmp6\n  ## These can also be overridden with env variables, see README.\n  proc_net_netstat = \"/proc/net/netstat\"\n  proc_net_snmp = \"/proc/net/snmp\"\n  proc_net_snmp6 = \"/proc/net/snmp6\"\n  ## dump metrics with 0 values too\n  dump_zeros       = true\n\n"
    },
    {
      "type": "input",
      "name": "openweathermap",
      "description": "Read current weather and forecasts data from openweathermap.org",
      "config": "# Read current weather and forecasts data from openweathermap.org\n[[inputs.openweathermap]]\n  # alias=\"openweathermap\"\n  ## OpenWeatherMap API key.\n  app_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\n  ## City ID's to collect weather data from.\n  city_id = [\"5391959\"]\n\n  ## Language of the description field. Can be one of \"ar\", \"bg\",\n  ## \"ca\", \"cz\", \"de\", \"el\", \"en\", \"fa\", \"fi\", \"fr\", \"gl\", \"hr\", \"hu\",\n  ## \"it\", \"ja\", \"kr\", \"la\", \"lt\", \"mk\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\",\n  ## \"se\", \"sk\", \"sl\", \"es\", \"tr\", \"ua\", \"vi\", \"zh_cn\", \"zh_tw\"\n  # lang = \"en\"\n\n  ## APIs to fetch; can contain \"weather\" or \"forecast\".\n  fetch = [\"weather\", \"forecast\"]\n\n  ## OpenWeatherMap base URL\n  # base_url = \"https://api.openweathermap.org/\"\n\n  ## Timeout for HTTP response.\n  # response_timeout = \"5s\"\n\n  ## Preferred unit system for temperature and wind speed. Can be one of\n  ## \"metric\", \"imperial\", or \"standard\".\n  # units = \"metric\"\n\n  ## Query interval; OpenWeatherMap updates their weather data every 10\n  ## minutes.\n  interval = \"10m\"\n\n"
    },
    {
      "type": "input",
      "name": "amqp_consumer",
      "description": "AMQP consumer plugin",
      "config": "# AMQP consumer plugin\n[[inputs.amqp_consumer]]\n  # alias=\"amqp_consumer\"\n  ## Broker to consume from.\n  ##   deprecated in 1.7; use the brokers option\n  # url = \"amqp://localhost:5672/influxdb\"\n\n  ## Brokers to consume from.  If multiple brokers are specified a random broker\n  ## will be selected anytime a connection is established.  This can be\n  ## helpful for load balancing when not using a dedicated load balancer.\n  brokers = [\"amqp://localhost:5672/influxdb\"]\n\n  ## Authentication credentials for the PLAIN auth_method.\n  # username = \"\"\n  # password = \"\"\n\n  ## Name of the exchange to declare.  If unset, no exchange will be declared.\n  exchange = \"telegraf\"\n\n  ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n  # exchange_type = \"topic\"\n\n  ## If true, exchange will be passively declared.\n  # exchange_passive = false\n\n  ## Exchange durability can be either \"transient\" or \"durable\".\n  # exchange_durability = \"durable\"\n\n  ## Additional exchange arguments.\n  # exchange_arguments = { }\n  # exchange_arguments = {\"hash_propery\" = \"timestamp\"}\n\n  ## AMQP queue name.\n  queue = \"telegraf\"\n\n  ## AMQP queue durability can be \"transient\" or \"durable\".\n  queue_durability = \"durable\"\n\n  ## If true, queue will be passively declared.\n  # queue_passive = false\n\n  ## A binding between the exchange and queue using this binding key is\n  ## created.  If unset, no binding is created.\n  binding_key = \"#\"\n\n  ## Maximum number of messages server should give to the worker.\n  # prefetch_count = 50\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Auth method. PLAIN and EXTERNAL are supported\n  ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n  ## described here: https://www.rabbitmq.com/plugins.html\n  # auth_method = \"PLAIN\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Content encoding for message payloads, can be set to \"gzip\" to or\n  ## \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "ethtool",
      "description": "Returns ethtool statistics for given interfaces",
      "config": "# Returns ethtool statistics for given interfaces\n[[inputs.ethtool]]\n  # alias=\"ethtool\"\n  ## List of interfaces to pull metrics for\n  # interface_include = [\"eth0\"]\n\n  ## List of interfaces to ignore when pulling metrics.\n  # interface_exclude = [\"eth1\"]\n\n"
    },
    {
      "type": "input",
      "name": "filestat",
      "description": "Read stats about given file(s)",
      "config": "# Read stats about given file(s)\n[[inputs.filestat]]\n  # alias=\"filestat\"\n  ## Files to gather stats about.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   \"/var/log/**.log\"  -\u003e recursively find all .log files in /var/log\n  ##   \"/var/log/*/*.log\" -\u003e find all .log files with a parent dir in /var/log\n  ##   \"/var/log/apache.log\" -\u003e just tail the apache log file\n  ##\n  ## See https://github.com/gobwas/glob for more examples\n  ##\n  files = [\"/var/log/**.log\"]\n\n  ## If true, read the entire file and calculate an md5 checksum.\n  md5 = false\n\n"
    },
    {
      "type": "input",
      "name": "kernel_vmstat",
      "description": "Get kernel statistics from /proc/vmstat",
      "config": "# Get kernel statistics from /proc/vmstat\n[[inputs.kernel_vmstat]]\n  # alias=\"kernel_vmstat\"\n"
    },
    {
      "type": "input",
      "name": "nginx_plus",
      "description": "Read Nginx Plus' full status information (ngx_http_status_module)",
      "config": "# Read Nginx Plus' full status information (ngx_http_status_module)\n[[inputs.nginx_plus]]\n  # alias=\"nginx_plus\"\n  ## An array of ngx_http_status_module or status URI to gather stats.\n  urls = [\"http://localhost/status\"]\n\n  # HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "powerdns_recursor",
      "description": "Read metrics from one or many PowerDNS Recursor servers",
      "config": "# Read metrics from one or many PowerDNS Recursor servers\n[[inputs.powerdns_recursor]]\n  # alias=\"powerdns_recursor\"\n  ## Path to the Recursor control socket.\n  unix_sockets = [\"/var/run/pdns_recursor.controlsocket\"]\n\n  ## Directory to create receive socket.  This default is likely not writable,\n  ## please reference the full plugin documentation for a recommended setup.\n  # socket_dir = \"/var/run/\"\n  ## Socket permissions for the receive socket.\n  # socket_mode = \"0666\"\n\n"
    },
    {
      "type": "input",
      "name": "sqlserver",
      "description": "Read metrics from Microsoft SQL Server",
      "config": "# Read metrics from Microsoft SQL Server\n[[inputs.sqlserver]]\n  # alias=\"sqlserver\"\n  ## Specify instances to monitor with a list of connection strings.\n  ## All connection parameters are optional.\n  ## By default, the host is localhost, listening on default port, TCP 1433.\n  ##   for Windows, the user is the currently running AD user (SSO).\n  ##   See https://github.com/denisenkom/go-mssqldb for detailed connection\n  ##   parameters, in particular, tls connections can be created like so:\n  ##   \"encrypt=true;certificate=\u003ccert\u003e;hostNameInCertificate=\u003cSqlServer host fqdn\u003e\"\n  # servers = [\n  #  \"Server=192.168.1.10;Port=1433;User Id=\u003cuser\u003e;Password=\u003cpw\u003e;app name=telegraf;log=1;\",\n  # ]\n\n  ## Optional parameter, setting this to 2 will use a new version\n  ## of the collection queries that break compatibility with the original\n  ## dashboards.\n  query_version = 2\n\n  ## If you are using AzureDB, setting this to true will gather resource utilization metrics\n  # azuredb = false\n\n  ## If you would like to exclude some of the metrics queries, list them here\n  ## Possible choices:\n  ## - PerformanceCounters\n  ## - WaitStatsCategorized\n  ## - DatabaseIO\n  ## - DatabaseProperties\n  ## - CPUHistory\n  ## - DatabaseSize\n  ## - DatabaseStats\n  ## - MemoryClerk\n  ## - VolumeSpace\n  ## - PerformanceMetrics\n  ## - Schedulers\n  ## - AzureDBResourceStats\n  ## - AzureDBResourceGovernance\n  ## - SqlRequests\n  ## - ServerProperties\n  exclude_query = [ 'Schedulers' ]\n\n"
    },
    {
      "type": "input",
      "name": "disk",
      "description": "Read metrics about disk usage by mount point",
      "config": "# Read metrics about disk usage by mount point\n[[inputs.disk]]\n  # alias=\"disk\"\n  ## By default stats will be gathered for all mount points.\n  ## Set mount_points will restrict the stats to only the specified mount points.\n  # mount_points = [\"/\"]\n\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n\n"
    },
    {
      "type": "input",
      "name": "fibaro",
      "description": "Read devices value(s) from a Fibaro controller",
      "config": "# Read devices value(s) from a Fibaro controller\n[[inputs.fibaro]]\n  # alias=\"fibaro\"\n  ## Required Fibaro controller address/hostname.\n  ## Note: at the time of writing this plugin, Fibaro only implemented http - no https available\n  url = \"http://\u003ccontroller\u003e:80\"\n\n  ## Required credentials to access the API (http://\u003ccontroller/api/\u003ccomponent\u003e)\n  username = \"\u003cusername\u003e\"\n  password = \"\u003cpassword\u003e\"\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "graylog",
      "description": "Read flattened metrics from one or more GrayLog HTTP endpoints",
      "config": "# Read flattened metrics from one or more GrayLog HTTP endpoints\n[[inputs.graylog]]\n  # alias=\"graylog\"\n  ## API endpoint, currently supported API:\n  ##\n  ##   - multiple  (Ex http://\u003chost\u003e:12900/system/metrics/multiple)\n  ##   - namespace (Ex http://\u003chost\u003e:12900/system/metrics/namespace/{namespace})\n  ##\n  ## For namespace endpoint, the metrics array will be ignored for that call.\n  ## Endpoint can contain namespace and multiple type calls.\n  ##\n  ## Please check http://[graylog-server-ip]:12900/api-browser for full list\n  ## of endpoints\n  servers = [\n    \"http://[graylog-server-ip]:12900/system/metrics/multiple\",\n  ]\n\n  ## Metrics list\n  ## List of metrics can be found on Graylog webservice documentation.\n  ## Or by hitting the the web service api at:\n  ##   http://[graylog-host]:12900/system/metrics\n  metrics = [\n    \"jvm.cl.loaded\",\n    \"jvm.memory.pools.Metaspace.committed\"\n  ]\n\n  ## Username and password\n  username = \"\"\n  password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "lustre2",
      "description": "Read metrics from local Lustre service on OST, MDS",
      "config": "# Read metrics from local Lustre service on OST, MDS\n[[inputs.lustre2]]\n  # alias=\"lustre2\"\n  ## An array of /proc globs to search for Lustre stats\n  ## If not specified, the default will work on Lustre 2.5.x\n  ##\n  # ost_procfiles = [\n  #   \"/proc/fs/lustre/obdfilter/*/stats\",\n  #   \"/proc/fs/lustre/osd-ldiskfs/*/stats\",\n  #   \"/proc/fs/lustre/obdfilter/*/job_stats\",\n  # ]\n  # mds_procfiles = [\n  #   \"/proc/fs/lustre/mdt/*/md_stats\",\n  #   \"/proc/fs/lustre/mdt/*/job_stats\",\n  # ]\n\n"
    },
    {
      "type": "input",
      "name": "nginx_upstream_check",
      "description": "Read nginx_upstream_check module status information (https://github.com/yaoweibin/nginx_upstream_check_module)",
      "config": "# Read nginx_upstream_check module status information (https://github.com/yaoweibin/nginx_upstream_check_module)\n[[inputs.nginx_upstream_check]]\n  # alias=\"nginx_upstream_check\"\n  ## An URL where Nginx Upstream check module is enabled\n  ## It should be set to return a JSON formatted response\n  url = \"http://127.0.0.1/status?format=json\"\n\n  ## HTTP method\n  # method = \"GET\"\n\n  ## Optional HTTP headers\n  # headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## Override HTTP \"Host\" header\n  # host_header = \"check.example.com\"\n\n  ## Timeout for HTTP requests\n  timeout = \"5s\"\n\n  ## Optional HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "apache",
      "description": "Read Apache status information (mod_status)",
      "config": "# Read Apache status information (mod_status)\n[[inputs.apache]]\n  # alias=\"apache\"\n  ## An array of URLs to gather from, must be directed at the machine\n  ## readable version of the mod_status page including the auto query string.\n  ## Default is \"http://localhost/server-status?auto\".\n  urls = [\"http://localhost/server-status?auto\"]\n\n  ## Credentials for basic HTTP authentication.\n  # username = \"myuser\"\n  # password = \"mypassword\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "passenger",
      "description": "Read metrics of passenger using passenger-status",
      "config": "# Read metrics of passenger using passenger-status\n[[inputs.passenger]]\n  # alias=\"passenger\"\n  ## Path of passenger-status.\n  ##\n  ## Plugin gather metric via parsing XML output of passenger-status\n  ## More information about the tool:\n  ##   https://www.phusionpassenger.com/library/admin/apache/overall_status_report.html\n  ##\n  ## If no path is specified, then the plugin simply execute passenger-status\n  ## hopefully it can be found in your PATH\n  command = \"passenger-status -v --show=xml\"\n\n"
    },
    {
      "type": "input",
      "name": "suricata",
      "description": "Suricata stats plugin",
      "config": "# Suricata stats plugin\n[[inputs.suricata]]\n  # alias=\"suricata\"\n  ## Data sink for Suricata stats log\n  # This is expected to be a filename of a\n  # unix socket to be created for listening.\n  source = \"/var/run/suricata-stats.sock\"\n\n  # Delimiter for flattening field keys, e.g. subitem \"alert\" of \"detect\"\n  # becomes \"detect_alert\" when delimiter is \"_\".\n  delimiter = \"_\"\n\n"
    },
    {
      "type": "input",
      "name": "zipkin",
      "description": "This plugin implements the Zipkin http server to gather trace and timing data needed to troubleshoot latency problems in microservice architectures.",
      "config": "# This plugin implements the Zipkin http server to gather trace and timing data needed to troubleshoot latency problems in microservice architectures.\n[[inputs.zipkin]]\n  # alias=\"zipkin\"\n  # path = \"/api/v1/spans\" # URL path for span data\n  # port = 9411            # Port on which Telegraf listens\n\n"
    },
    {
      "type": "input",
      "name": "marklogic",
      "description": "Retrieves information on a specific host in a MarkLogic Cluster",
      "config": "# Retrieves information on a specific host in a MarkLogic Cluster\n[[inputs.marklogic]]\n  # alias=\"marklogic\"\n  ## Base URL of the MarkLogic HTTP Server.\n  url = \"http://localhost:8002\"\n\n  ## List of specific hostnames to retrieve information. At least (1) required.\n  # hosts = [\"hostname1\", \"hostname2\"]\n\n  ## Using HTTP Basic Authentication. Management API requires 'manage-user' role privileges\n  # username = \"myuser\"\n  # password = \"mypassword\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "cloudwatch",
      "description": "Pull Metric Statistics from Amazon CloudWatch",
      "config": "# Pull Metric Statistics from Amazon CloudWatch\n[[inputs.cloudwatch]]\n  # alias=\"cloudwatch\"\n  ## Amazon Region\n  region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Assumed credentials via STS if role_arn is specified\n  ## 2) explicit credentials from 'access_key' and 'secret_key'\n  ## 3) shared profile from 'profile'\n  ## 4) environment variables\n  ## 5) shared credentials file\n  ## 6) EC2 Instance Profile\n  # access_key = \"\"\n  # secret_key = \"\"\n  # token = \"\"\n  # role_arn = \"\"\n  # profile = \"\"\n  # shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  # The minimum period for Cloudwatch metrics is 1 minute (60s). However not all\n  # metrics are made available to the 1 minute period. Some are collected at\n  # 3 minute, 5 minute, or larger intervals. See https://aws.amazon.com/cloudwatch/faqs/#monitoring.\n  # Note that if a period is configured that is smaller than the minimum for a\n  # particular metric, that metric will not be returned by the Cloudwatch API\n  # and will not be collected by Telegraf.\n  #\n  ## Requested CloudWatch aggregation Period (required - must be a multiple of 60s)\n  period = \"5m\"\n\n  ## Collection Delay (required - must account for metrics availability via CloudWatch API)\n  delay = \"5m\"\n\n  ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid\n  ## gaps or overlap in pulled data\n  interval = \"5m\"\n\n  ## Configure the TTL for the internal cache of metrics.\n  # cache_ttl = \"1h\"\n\n  ## Metric Statistic Namespace (required)\n  namespace = \"AWS/ELB\"\n\n  ## Maximum requests per second. Note that the global default AWS rate limit is\n  ## 50 reqs/sec, so if you define multiple namespaces, these should add up to a\n  ## maximum of 50.\n  ## See http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_limits.html\n  # ratelimit = 25\n\n  ## Timeout for http requests made by the cloudwatch client.\n  # timeout = \"5s\"\n\n  ## Namespace-wide statistic filters. These allow fewer queries to be made to\n  ## cloudwatch.\n  # statistic_include = [ \"average\", \"sum\", \"minimum\", \"maximum\", sample_count\" ]\n  # statistic_exclude = []\n\n  ## Metrics to Pull\n  ## Defaults to all Metrics in Namespace if nothing is provided\n  ## Refreshes Namespace available metrics every 1h\n  #[[inputs.cloudwatch.metrics]]\n  #  names = [\"Latency\", \"RequestCount\"]\n  #\n  #  ## Statistic filters for Metric.  These allow for retrieving specific\n  #  ## statistics for an individual metric.\n  #  # statistic_include = [ \"average\", \"sum\", \"minimum\", \"maximum\", sample_count\" ]\n  #  # statistic_exclude = []\n  #\n  #  ## Dimension filters for Metric.  All dimensions defined for the metric names\n  #  ## must be specified in order to retrieve the metric statistics.\n  #  [[inputs.cloudwatch.metrics.dimensions]]\n  #    name = \"LoadBalancerName\"\n  #    value = \"p-example\"\n\n"
    },
    {
      "type": "input",
      "name": "system",
      "description": "Read metrics about system load \u0026 uptime",
      "config": "# Read metrics about system load \u0026 uptime\n[[inputs.system]]\n  # alias=\"system\"\n  ## Uncomment to remove deprecated metrics.\n  # fielddrop = [\"uptime_format\"]\n\n"
    },
    {
      "type": "input",
      "name": "docker",
      "description": "Read metrics about docker containers",
      "config": "# Read metrics about docker containers\n[[inputs.docker]]\n  # alias=\"docker\"\n  ## Docker Endpoint\n  ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n  ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n  endpoint = \"unix:///var/run/docker.sock\"\n\n  ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)\n  gather_services = false\n\n  ## Only collect metrics for these containers, collect all if empty\n  container_names = []\n\n  ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n  source_tag = false\n\n  ## Containers to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all containers\n  container_name_include = []\n  container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"running\" state will be captured.\n  ## example: container_state_include = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n  ## example: container_state_exclude = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n  # container_state_include = []\n  # container_state_exclude = []\n\n  ## Timeout for docker list, info, and stats commands\n  timeout = \"5s\"\n\n  ## Whether to report for each container per-device blkio (8:0, 8:1...) and\n  ## network (eth0, eth1, ...) stats or not\n  perdevice = true\n\n  ## Whether to report for each container total blkio and network stats or not\n  total = false\n\n  ## Which environment variables should we use as a tag\n  ##tag_env = [\"JAVA_HOME\", \"HEAP_SIZE\"]\n\n  ## docker labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  docker_label_include = []\n  docker_label_exclude = []\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "docker_log",
      "description": "Read logging output from the Docker engine",
      "config": "# Read logging output from the Docker engine\n[[inputs.docker_log]]\n  # alias=\"docker_log\"\n  ## Docker Endpoint\n  ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n  ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n  # endpoint = \"unix:///var/run/docker.sock\"\n\n  ## When true, container logs are read from the beginning; otherwise\n  ## reading begins at the end of the log.\n  # from_beginning = false\n\n  ## Timeout for Docker API calls.\n  # timeout = \"5s\"\n\n  ## Containers to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all containers\n  # container_name_include = []\n  # container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"running\" state will be captured.\n  # container_state_include = []\n  # container_state_exclude = []\n\n  ## docker labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  # docker_label_include = []\n  # docker_label_exclude = []\n\n  ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n  source_tag = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "leofs",
      "description": "Read metrics from a LeoFS Server via SNMP",
      "config": "# Read metrics from a LeoFS Server via SNMP\n[[inputs.leofs]]\n  # alias=\"leofs\"\n  ## An array of URLs of the form:\n  ##   host [ \":\" port]\n  servers = [\"127.0.0.1:4020\"]\n\n"
    },
    {
      "type": "input",
      "name": "procstat",
      "description": "Monitor process cpu and memory usage",
      "config": "# Monitor process cpu and memory usage\n[[inputs.procstat]]\n  # alias=\"procstat\"\n  ## PID file to monitor process\n  pid_file = \"/var/run/nginx.pid\"\n  ## executable name (ie, pgrep \u003cexe\u003e)\n  # exe = \"nginx\"\n  ## pattern as argument for pgrep (ie, pgrep -f \u003cpattern\u003e)\n  # pattern = \"nginx\"\n  ## user as argument for pgrep (ie, pgrep -u \u003cuser\u003e)\n  # user = \"nginx\"\n  ## Systemd unit name\n  # systemd_unit = \"nginx.service\"\n  ## CGroup name or path\n  # cgroup = \"systemd/system.slice/nginx.service\"\n\n  ## Windows service name\n  # win_service = \"\"\n\n  ## override for process_name\n  ## This is optional; default is sourced from /proc/\u003cpid\u003e/status\n  # process_name = \"bar\"\n\n  ## Field name prefix\n  # prefix = \"\"\n\n  ## When true add the full cmdline as a tag.\n  # cmdline_tag = false\n\n  ## Add PID as a tag instead of a field; useful to differentiate between\n  ## processes whose tags are otherwise the same.  Can create a large number\n  ## of series, use judiciously.\n  # pid_tag = false\n\n  ## Method to use when finding process IDs.  Can be one of 'pgrep', or\n  ## 'native'.  The pgrep finder calls the pgrep executable in the PATH while\n  ## the native finder performs the search directly in a manor dependent on the\n  ## platform.  Default is 'pgrep'\n  # pid_finder = \"pgrep\"\n\n"
    },
    {
      "type": "input",
      "name": "salesforce",
      "description": "Read API usage and limits for a Salesforce organisation",
      "config": "# Read API usage and limits for a Salesforce organisation\n[[inputs.salesforce]]\n  # alias=\"salesforce\"\n  ## specify your credentials\n  ##\n  username = \"your_username\"\n  password = \"your_password\"\n  ##\n  ## (optional) security token\n  # security_token = \"your_security_token\"\n  ##\n  ## (optional) environment type (sandbox or production)\n  ## default is: production\n  ##\n  # environment = \"production\"\n  ##\n  ## (optional) API version (default: \"39.0\")\n  ##\n  # version = \"39.0\"\n\n"
    },
    {
      "type": "input",
      "name": "cloud_pubsub_push",
      "description": "Google Cloud Pub/Sub Push HTTP listener",
      "config": "# Google Cloud Pub/Sub Push HTTP listener\n[[inputs.cloud_pubsub_push]]\n  # alias=\"cloud_pubsub_push\"\n  ## Address and port to host HTTP listener on\n  service_address = \":8080\"\n\n  ## Application secret to verify messages originate from Cloud Pub/Sub\n  # token = \"\"\n\n  ## Path to listen to.\n  # path = \"/\"\n\n  ## Maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n  ## Maximum duration before timing out write of the response. This should be set to a value\n  ## large enough that you can send at least 'metric_batch_size' number of messages within the\n  ## duration.\n  # write_timeout = \"10s\"\n\n  ## Maximum allowed http request body size in bytes.\n  ## 0 means to use the default of 524,288,00 bytes (500 mebibytes)\n  # max_body_size = \"500MB\"\n\n  ## Whether to add the pubsub metadata, such as message attributes and subscription as a tag.\n  # add_meta = false\n\n  ## Optional. Maximum messages to read from PubSub that have not been written\n  ## to an output. Defaults to 1000.\n  ## For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message contains 10 metrics and the output\n  ## metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "ipvs",
      "description": "Collect virtual and real server stats from Linux IPVS",
      "config": "# Collect virtual and real server stats from Linux IPVS\n[[inputs.ipvs]]\n  # alias=\"ipvs\"\n"
    },
    {
      "type": "input",
      "name": "nginx_vts",
      "description": "Read Nginx virtual host traffic status module information (nginx-module-vts)",
      "config": "# Read Nginx virtual host traffic status module information (nginx-module-vts)\n[[inputs.nginx_vts]]\n  # alias=\"nginx_vts\"\n  ## An array of ngx_http_status_module or status URI to gather stats.\n  urls = [\"http://localhost/status\"]\n\n  ## HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "ntpq",
      "description": "Get standard NTP query metrics, requires ntpq executable.",
      "config": "# Get standard NTP query metrics, requires ntpq executable.\n[[inputs.ntpq]]\n  # alias=\"ntpq\"\n  ## If false, set the -n ntpq flag. Can reduce metric gather time.\n  dns_lookup = true\n\n"
    },
    {
      "type": "input",
      "name": "openldap",
      "description": "OpenLDAP cn=Monitor plugin",
      "config": "# OpenLDAP cn=Monitor plugin\n[[inputs.openldap]]\n  # alias=\"openldap\"\n  host = \"localhost\"\n  port = 389\n\n  # ldaps, starttls, or no encryption. default is an empty string, disabling all encryption.\n  # note that port will likely need to be changed to 636 for ldaps\n  # valid options: \"\" | \"starttls\" | \"ldaps\"\n  tls = \"\"\n\n  # skip peer certificate verification. Default is false.\n  insecure_skip_verify = false\n\n  # Path to PEM-encoded Root certificate to use to verify server certificate\n  tls_ca = \"/etc/ssl/certs.pem\"\n\n  # dn/password to bind with. If bind_dn is empty, an anonymous bind is performed.\n  bind_dn = \"\"\n  bind_password = \"\"\n\n  # Reverse metric names so they sort more naturally. Recommended.\n  # This defaults to false if unset, but is set to true when generating a new config\n  reverse_metric_names = true\n\n"
    },
    {
      "type": "input",
      "name": "fluentd",
      "description": "Read metrics exposed by fluentd in_monitor plugin",
      "config": "# Read metrics exposed by fluentd in_monitor plugin\n[[inputs.fluentd]]\n  # alias=\"fluentd\"\n  ## This plugin reads information exposed by fluentd (using /api/plugins.json endpoint).\n  ##\n  ## Endpoint:\n  ## - only one URI is allowed\n  ## - https is not supported\n  endpoint = \"http://localhost:24220/api/plugins.json\"\n\n  ## Define which plugins have to be excluded (based on \"type\" field - e.g. monitor_agent)\n  exclude = [\n\t  \"monitor_agent\",\n\t  \"dummy\",\n  ]\n\n"
    },
    {
      "type": "input",
      "name": "nats",
      "description": "Provides metrics about the state of a NATS server",
      "config": "# Provides metrics about the state of a NATS server\n[[inputs.nats]]\n  # alias=\"nats\"\n  ## The address of the monitoring endpoint of the NATS server\n  server = \"http://localhost:8222\"\n\n  ## Maximum time to receive response\n  # response_timeout = \"5s\"\n\n"
    }
  ]
}
`{
  "version": "1.13.0",
  "os": "linux",
  "plugins": [
    {
      "type": "input",
      "name": "tcp_listener",
      "description": "Generic TCP listener",
      "config": "# Generic TCP listener\n[[inputs.tcp_listener]]\n  # alias=\"tcp_listener\"\n  # DEPRECATED: the TCP listener plugin has been deprecated in favor of the\n  # socket_listener plugin\n  # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/socket_listener\n\n"
    },
    {
      "type": "input",
      "name": "kernel",
      "description": "Get kernel statistics from /proc/stat",
      "config": "# Get kernel statistics from /proc/stat\n[[inputs.kernel]]\n  # alias=\"kernel\"\n"
    },
    {
      "type": "input",
      "name": "powerdns",
      "description": "Read metrics from one or many PowerDNS servers",
      "config": "# Read metrics from one or many PowerDNS servers\n[[inputs.powerdns]]\n  # alias=\"powerdns\"\n  ## An array of sockets to gather stats about.\n  ## Specify a path to unix socket.\n  unix_sockets = [\"/var/run/pdns.controlsocket\"]\n\n"
    },
    {
      "type": "input",
      "name": "processes",
      "description": "Get the number of processes and group them by status",
      "config": "# Get the number of processes and group them by status\n[[inputs.processes]]\n  # alias=\"processes\"\n"
    },
    {
      "type": "input",
      "name": "snmp_legacy",
      "description": "DEPRECATED! PLEASE USE inputs.snmp INSTEAD.",
      "config": "# DEPRECATED! PLEASE USE inputs.snmp INSTEAD.\n[[inputs.snmp_legacy]]\n  # alias=\"snmp_legacy\"\n  ## Use 'oids.txt' file to translate oids to names\n  ## To generate 'oids.txt' you need to run:\n  ##   snmptranslate -m all -Tz -On | sed -e 's/\"//g' \u003e /tmp/oids.txt\n  ## Or if you have an other MIB folder with custom MIBs\n  ##   snmptranslate -M /mycustommibfolder -Tz -On -m all | sed -e 's/\"//g' \u003e oids.txt\n  snmptranslate_file = \"/tmp/oids.txt\"\n  [[inputs.snmp.host]]\n    address = \"192.168.2.2:161\"\n    # SNMP community\n    community = \"public\" # default public\n    # SNMP version (1, 2 or 3)\n    # Version 3 not supported yet\n    version = 2 # default 2\n    # SNMP response timeout\n    timeout = 2.0 # default 2.0\n    # SNMP request retries\n    retries = 2 # default 2\n    # Which get/bulk do you want to collect for this host\n    collect = [\"mybulk\", \"sysservices\", \"sysdescr\"]\n    # Simple list of OIDs to get, in addition to \"collect\"\n    get_oids = []\n\n  [[inputs.snmp.host]]\n    address = \"192.168.2.3:161\"\n    community = \"public\"\n    version = 2\n    timeout = 2.0\n    retries = 2\n    collect = [\"mybulk\"]\n    get_oids = [\n        \"ifNumber\",\n        \".1.3.6.1.2.1.1.3.0\",\n    ]\n\n  [[inputs.snmp.get]]\n    name = \"ifnumber\"\n    oid = \"ifNumber\"\n\n  [[inputs.snmp.get]]\n    name = \"interface_speed\"\n    oid = \"ifSpeed\"\n    instance = \"0\"\n\n  [[inputs.snmp.get]]\n    name = \"sysuptime\"\n    oid = \".1.3.6.1.2.1.1.3.0\"\n    unit = \"second\"\n\n  [[inputs.snmp.bulk]]\n    name = \"mybulk\"\n    max_repetition = 127\n    oid = \".1.3.6.1.2.1.1\"\n\n  [[inputs.snmp.bulk]]\n    name = \"ifoutoctets\"\n    max_repetition = 127\n    oid = \"ifOutOctets\"\n\n  [[inputs.snmp.host]]\n    address = \"192.168.2.13:161\"\n    #address = \"127.0.0.1:161\"\n    community = \"public\"\n    version = 2\n    timeout = 2.0\n    retries = 2\n    #collect = [\"mybulk\", \"sysservices\", \"sysdescr\", \"systype\"]\n    collect = [\"sysuptime\" ]\n    [[inputs.snmp.host.table]]\n      name = \"iftable3\"\n      include_instances = [\"enp5s0\", \"eth1\"]\n\n  # SNMP TABLEs\n  # table without mapping neither subtables\n  [[inputs.snmp.table]]\n    name = \"iftable1\"\n    oid = \".1.3.6.1.2.1.31.1.1.1\"\n\n  # table without mapping but with subtables\n  [[inputs.snmp.table]]\n    name = \"iftable2\"\n    oid = \".1.3.6.1.2.1.31.1.1.1\"\n    sub_tables = [\".1.3.6.1.2.1.2.2.1.13\"]\n\n  # table with mapping but without subtables\n  [[inputs.snmp.table]]\n    name = \"iftable3\"\n    oid = \".1.3.6.1.2.1.31.1.1.1\"\n    # if empty. get all instances\n    mapping_table = \".1.3.6.1.2.1.31.1.1.1.1\"\n    # if empty, get all subtables\n\n  # table with both mapping and subtables\n  [[inputs.snmp.table]]\n    name = \"iftable4\"\n    oid = \".1.3.6.1.2.1.31.1.1.1\"\n    # if empty get all instances\n    mapping_table = \".1.3.6.1.2.1.31.1.1.1.1\"\n    # if empty get all subtables\n    # sub_tables could be not \"real subtables\"\n    sub_tables=[\".1.3.6.1.2.1.2.2.1.13\", \"bytes_recv\", \"bytes_send\"]\n\n"
    },
    {
      "type": "input",
      "name": "statsd",
      "description": "Statsd UDP/TCP Server",
      "config": "# Statsd UDP/TCP Server\n[[inputs.statsd]]\n  # alias=\"statsd\"\n  ## Protocol, must be \"tcp\", \"udp\", \"udp4\" or \"udp6\" (default=udp)\n  protocol = \"udp\"\n\n  ## MaxTCPConnection - applicable when protocol is set to tcp (default=250)\n  max_tcp_connections = 250\n\n  ## Enable TCP keep alive probes (default=false)\n  tcp_keep_alive = false\n\n  ## Specifies the keep-alive period for an active network connection.\n  ## Only applies to TCP sockets and will be ignored if tcp_keep_alive is false.\n  ## Defaults to the OS configuration.\n  # tcp_keep_alive_period = \"2h\"\n\n  ## Address and port to host UDP listener on\n  service_address = \":8125\"\n\n  ## The following configuration options control when telegraf clears it's cache\n  ## of previous values. If set to false, then telegraf will only clear it's\n  ## cache when the daemon is restarted.\n  ## Reset gauges every interval (default=true)\n  delete_gauges = true\n  ## Reset counters every interval (default=true)\n  delete_counters = true\n  ## Reset sets every interval (default=true)\n  delete_sets = true\n  ## Reset timings \u0026 histograms every interval (default=true)\n  delete_timings = true\n\n  ## Percentiles to calculate for timing \u0026 histogram stats\n  percentiles = [50.0, 90.0, 99.0, 99.9, 99.95, 100.0]\n\n  ## separator to use between elements of a statsd metric\n  metric_separator = \"_\"\n\n  ## Parses tags in the datadog statsd format\n  ## http://docs.datadoghq.com/guides/dogstatsd/\n  parse_data_dog_tags = false\n\n  ## Parses datadog extensions to the statsd format\n  datadog_extensions = false\n\n  ## Statsd data translation templates, more info can be read here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md\n  # templates = [\n  #     \"cpu.* measurement*\"\n  # ]\n\n  ## Number of UDP messages allowed to queue up, once filled,\n  ## the statsd server will start dropping packets\n  allowed_pending_messages = 10000\n\n  ## Number of timing/histogram values to track per-measurement in the\n  ## calculation of percentiles. Raising this limit increases the accuracy\n  ## of percentiles but also increases the memory usage and cpu time.\n  percentile_limit = 1000\n\n"
    },
    {
      "type": "input",
      "name": "bcache",
      "description": "Read metrics of bcache from stats_total and dirty_data",
      "config": "# Read metrics of bcache from stats_total and dirty_data\n[[inputs.bcache]]\n  # alias=\"bcache\"\n  ## Bcache sets path\n  ## If not specified, then default is:\n  bcachePath = \"/sys/fs/bcache\"\n\n  ## By default, telegraf gather stats for all bcache devices\n  ## Setting devices will restrict the stats to the specified\n  ## bcache devices.\n  bcacheDevs = [\"bcache0\"]\n\n"
    },
    {
      "type": "input",
      "name": "mesos",
      "description": "Telegraf plugin for gathering metrics from N Mesos masters",
      "config": "# Telegraf plugin for gathering metrics from N Mesos masters\n[[inputs.mesos]]\n  # alias=\"mesos\"\n  ## Timeout, in ms.\n  timeout = 100\n\n  ## A list of Mesos masters.\n  masters = [\"http://localhost:5050\"]\n\n  ## Master metrics groups to be collected, by default, all enabled.\n  master_collections = [\n    \"resources\",\n    \"master\",\n    \"system\",\n    \"agents\",\n    \"frameworks\",\n    \"framework_offers\",\n    \"tasks\",\n    \"messages\",\n    \"evqueue\",\n    \"registrar\",\n    \"allocator\",\n  ]\n\n  ## A list of Mesos slaves, default is []\n  # slaves = []\n\n  ## Slave metrics groups to be collected, by default, all enabled.\n  # slave_collections = [\n  #   \"resources\",\n  #   \"agent\",\n  #   \"system\",\n  #   \"executors\",\n  #   \"tasks\",\n  #   \"messages\",\n  # ]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "pf",
      "description": "Gather counters from PF",
      "config": "# Gather counters from PF\n[[inputs.pf]]\n  # alias=\"pf\"\n  ## PF require root access on most systems.\n  ## Setting 'use_sudo' to true will make use of sudo to run pfctl.\n  ## Users must configure sudo to allow telegraf user to run pfctl with no password.\n  ## pfctl can be restricted to only list command \"pfctl -s info\".\n  use_sudo = false\n\n"
    },
    {
      "type": "input",
      "name": "webhooks",
      "description": "A Webhooks Event collector",
      "config": "# A Webhooks Event collector\n[[inputs.webhooks]]\n  # alias=\"webhooks\"\n  ## Address and port to host Webhook listener on\n  service_address = \":1619\"\n\n  [inputs.webhooks.filestack]\n    path = \"/filestack\"\n\n  [inputs.webhooks.github]\n    path = \"/github\"\n    # secret = \"\"\n\n  [inputs.webhooks.mandrill]\n    path = \"/mandrill\"\n\n  [inputs.webhooks.rollbar]\n    path = \"/rollbar\"\n\n  [inputs.webhooks.papertrail]\n    path = \"/papertrail\"\n\n  [inputs.webhooks.particle]\n    path = \"/particle\"\n\n"
    },
    {
      "type": "input",
      "name": "http_listener_v2",
      "description": "Generic HTTP write listener",
      "config": "# Generic HTTP write listener\n[[inputs.http_listener_v2]]\n  # alias=\"http_listener_v2\"\n  ## Address and port to host HTTP listener on\n  service_address = \":8080\"\n\n  ## Path to listen to.\n  # path = \"/telegraf\"\n\n  ## HTTP methods to accept.\n  # methods = [\"POST\", \"PUT\"]\n\n  ## maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n  ## maximum duration before timing out write of the response\n  # write_timeout = \"10s\"\n\n  ## Maximum allowed http request body size in bytes.\n  ## 0 means to use the default of 524,288,00 bytes (500 mebibytes)\n  # max_body_size = \"500MB\"\n\n  ## Part of the request to consume.  Available options are \"body\" and\n  ## \"query\".\n  # data_source = \"body\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Optional username and password to accept for HTTP basic authentication.\n  ## You probably want to make sure you have TLS configured above for this.\n  # basic_username = \"foobar\"\n  # basic_password = \"barfoo\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "http_listener",
      "description": "Influx HTTP write listener",
      "config": "# Influx HTTP write listener\n[[inputs.http_listener]]\n  # alias=\"http_listener\"\n  ## Address and port to host HTTP listener on\n  service_address = \":8186\"\n\n  ## maximum duration before timing out read of the request\n  read_timeout = \"10s\"\n  ## maximum duration before timing out write of the response\n  write_timeout = \"10s\"\n\n  ## Maximum allowed http request body size in bytes.\n  ## 0 means to use the default of 524,288,000 bytes (500 mebibytes)\n  max_body_size = \"500MiB\"\n\n  ## Maximum line size allowed to be sent in bytes.\n  ## 0 means to use the default of 65536 bytes (64 kibibytes)\n  max_line_size = \"64KiB\"\n  \n\n  ## Optional tag name used to store the database. \n  ## If the write has a database in the query string then it will be kept in this tag name.\n  ## This tag can be used in downstream outputs.\n  ## The default value of nothing means it will be off and the database will not be recorded.\n  # database_tag = \"\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  tls_cert = \"/etc/telegraf/cert.pem\"\n  tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Optional username and password to accept for HTTP basic authentication.\n  ## You probably want to make sure you have TLS configured above for this.\n  # basic_username = \"foobar\"\n  # basic_password = \"barfoo\"\n\n"
    },
    {
      "type": "input",
      "name": "sysstat",
      "description": "Sysstat metrics collector",
      "config": "# Sysstat metrics collector\n[[inputs.sysstat]]\n  # alias=\"sysstat\"\n  ## Path to the sadc command.\n  #\n  ## Common Defaults:\n  ##   Debian/Ubuntu: /usr/lib/sysstat/sadc\n  ##   Arch:          /usr/lib/sa/sadc\n  ##   RHEL/CentOS:   /usr/lib64/sa/sadc\n  sadc_path = \"/usr/lib/sa/sadc\" # required\n\n  ## Path to the sadf command, if it is not in PATH\n  # sadf_path = \"/usr/bin/sadf\"\n\n  ## Activities is a list of activities, that are passed as argument to the\n  ## sadc collector utility (e.g: DISK, SNMP etc...)\n  ## The more activities that are added, the more data is collected.\n  # activities = [\"DISK\"]\n\n  ## Group metrics to measurements.\n  ##\n  ## If group is false each metric will be prefixed with a description\n  ## and represents itself a measurement.\n  ##\n  ## If Group is true, corresponding metrics are grouped to a single measurement.\n  # group = true\n\n  ## Options for the sadf command. The values on the left represent the sadf\n  ## options and the values on the right their description (which are used for\n  ## grouping and prefixing metrics).\n  ##\n  ## Run 'sar -h' or 'man sar' to find out the supported options for your\n  ## sysstat version.\n  [inputs.sysstat.options]\n    -C = \"cpu\"\n    -B = \"paging\"\n    -b = \"io\"\n    -d = \"disk\"             # requires DISK activity\n    \"-n ALL\" = \"network\"\n    \"-P ALL\" = \"per_cpu\"\n    -q = \"queue\"\n    -R = \"mem\"\n    -r = \"mem_util\"\n    -S = \"swap_util\"\n    -u = \"cpu_util\"\n    -v = \"inode\"\n    -W = \"swap\"\n    -w = \"task\"\n  #  -H = \"hugepages\"        # only available for newer linux distributions\n  #  \"-I ALL\" = \"interrupts\" # requires INT activity\n\n  ## Device tags can be used to add additional tags for devices.\n  ## For example the configuration below adds a tag vg with value rootvg for\n  ## all metrics with sda devices.\n  # [[inputs.sysstat.device_tags.sda]]\n  #  vg = \"rootvg\"\n\n"
    },
    {
      "type": "input",
      "name": "systemd_units",
      "description": "Gather systemd units state",
      "config": "# Gather systemd units state\n[[inputs.systemd_units]]\n  # alias=\"systemd_units\"\n  ## Set timeout for systemctl execution\n  # timeout = \"1s\"\n  #\n  ## Filter for a specific unit type, default is \"service\", other possible\n  ## values are \"socket\", \"target\", \"device\", \"mount\", \"automount\", \"swap\",\n  ## \"timer\", \"path\", \"slice\" and \"scope \":\n  # unittype = \"service\"\n\n"
    },
    {
      "type": "input",
      "name": "temp",
      "description": "Read metrics about temperature",
      "config": "# Read metrics about temperature\n[[inputs.temp]]\n  # alias=\"temp\"\n"
    },
    {
      "type": "input",
      "name": "cgroup",
      "description": "Read specific statistics per cgroup",
      "config": "# Read specific statistics per cgroup\n[[inputs.cgroup]]\n  # alias=\"cgroup\"\n  ## Directories in which to look for files, globs are supported.\n  ## Consider restricting paths to the set of cgroups you really\n  ## want to monitor if you have a large number of cgroups, to avoid\n  ## any cardinality issues.\n  # paths = [\n  #   \"/cgroup/memory\",\n  #   \"/cgroup/memory/child1\",\n  #   \"/cgroup/memory/child2/*\",\n  # ]\n  ## cgroup stat fields, as file names, globs are supported.\n  ## these file names are appended to each path from above.\n  # files = [\"memory.*usage*\", \"memory.limit_in_bytes\"]\n\n"
    },
    {
      "type": "input",
      "name": "mysql",
      "description": "Read metrics from one or many mysql servers",
      "config": "# Read metrics from one or many mysql servers\n[[inputs.mysql]]\n  # alias=\"mysql\"\n  ## specify servers via a url matching:\n  ##  [username[:password]@][protocol[(address)]]/[?tls=[true|false|skip-verify|custom]]\n  ##  see https://github.com/go-sql-driver/mysql#dsn-data-source-name\n  ##  e.g.\n  ##    servers = [\"user:passwd@tcp(127.0.0.1:3306)/?tls=false\"]\n  ##    servers = [\"user@tcp(127.0.0.1:3306)/?tls=false\"]\n  #\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"tcp(127.0.0.1:3306)/\"]\n\n  ## Selects the metric output format.\n  ##\n  ## This option exists to maintain backwards compatibility, if you have\n  ## existing metrics do not set or change this value until you are ready to\n  ## migrate to the new format.\n  ##\n  ## If you do not have existing metrics from this plugin set to the latest\n  ## version.\n  ##\n  ## Telegraf \u003e=1.6: metric_version = 2\n  ##           \u003c1.6: metric_version = 1 (or unset)\n  metric_version = 2\n\n  ## if the list is empty, then metrics are gathered from all databasee tables\n  # table_schema_databases = []\n\n  ## gather metrics from INFORMATION_SCHEMA.TABLES for databases provided above list\n  # gather_table_schema = false\n\n  ## gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST\n  # gather_process_list = false\n\n  ## gather user statistics from INFORMATION_SCHEMA.USER_STATISTICS\n  # gather_user_statistics = false\n\n  ## gather auto_increment columns and max values from information schema\n  # gather_info_schema_auto_inc = false\n\n  ## gather metrics from INFORMATION_SCHEMA.INNODB_METRICS\n  # gather_innodb_metrics = false\n\n  ## gather metrics from SHOW SLAVE STATUS command output\n  # gather_slave_status = false\n\n  ## gather metrics from SHOW BINARY LOGS command output\n  # gather_binary_logs = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE\n  # gather_table_io_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS\n  # gather_table_lock_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE\n  # gather_index_io_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS\n  # gather_event_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME\n  # gather_file_events_stats = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST\n  # gather_perf_events_statements = false\n\n  ## the limits for metrics form perf_events_statements\n  # perf_events_statements_digest_text_limit = 120\n  # perf_events_statements_limit = 250\n  # perf_events_statements_time_limit = 86400\n\n  ## Some queries we may want to run less often (such as SHOW GLOBAL VARIABLES)\n  ##   example: interval_slow = \"30m\"\n  # interval_slow = \"\"\n\n  ## Optional TLS Config (will be used if tls=custom parameter specified in server uri)\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "redis",
      "description": "Read metrics from one or many redis servers",
      "config": "# Read metrics from one or many redis servers\n[[inputs.redis]]\n  # alias=\"redis\"\n  ## specify servers via a url matching:\n  ##  [protocol://][:password]@address[:port]\n  ##  e.g.\n  ##    tcp://localhost:6379\n  ##    tcp://:password@192.168.99.100\n  ##    unix:///var/run/redis.sock\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no port is specified, 6379 is used\n  servers = [\"tcp://localhost:6379\"]\n\n  ## specify server password\n  # password = \"s#cr@t%\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = true\n\n"
    },
    {
      "type": "input",
      "name": "couchbase",
      "description": "Read metrics from one or many couchbase clusters",
      "config": "# Read metrics from one or many couchbase clusters\n[[inputs.couchbase]]\n  # alias=\"couchbase\"\n  ## specify servers via a url matching:\n  ##  [protocol://][:password]@address[:port]\n  ##  e.g.\n  ##    http://couchbase-0.example.com/\n  ##    http://admin:secret@couchbase-0.example.com:8091/\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no protocol is specified, HTTP is used.\n  ## If no port is specified, 8091 is used.\n  servers = [\"http://localhost:8091\"]\n\n"
    },
    {
      "type": "input",
      "name": "file",
      "description": "Reload and gather from file[s] on telegraf's interval.",
      "config": "# Reload and gather from file[s] on telegraf's interval.\n[[inputs.file]]\n  # alias=\"file\"\n  ## Files to parse each interval.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   /var/log/**.log     -\u003e recursively find all .log files in /var/log\n  ##   /var/log/*/*.log    -\u003e find all .log files with a parent dir in /var/log\n  ##   /var/log/apache.log -\u003e only read the apache log file\n  files = [\"/var/log/apache/access.log\"]\n\n  ## The dataformat to be read from files\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Name a tag containing the name of the file the data was parsed from.  Leave empty\n  ## to disable.\n  # file_tag = \"\"\n\n"
    },
    {
      "type": "input",
      "name": "kube_inventory",
      "description": "Read metrics from the Kubernetes api",
      "config": "# Read metrics from the Kubernetes api\n[[inputs.kube_inventory]]\n  # alias=\"kube_inventory\"\n  ## URL for the Kubernetes API\n  url = \"https://127.0.0.1\"\n\n  ## Namespace to use. Set to \"\" to use all namespaces.\n  # namespace = \"default\"\n\n  ## Use bearer token for authorization. ('bearer_token' takes priority)\n  ## If both of these are empty, we'll use the default serviceaccount:\n  ## at: /run/secrets/kubernetes.io/serviceaccount/token\n  # bearer_token = \"/path/to/bearer/token\"\n  ## OR\n  # bearer_token_string = \"abc_123\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## Optional Resources to exclude from gathering\n  ## Leave them with blank with try to gather everything available.\n  ## Values can be - \"daemonsets\", deployments\", \"endpoints\", \"ingress\", \"nodes\",\n  ## \"persistentvolumes\", \"persistentvolumeclaims\", \"pods\", \"services\", \"statefulsets\"\n  # resource_exclude = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n\n  ## Optional Resources to include when gathering\n  ## Overrides resource_exclude if both set.\n  # resource_include = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n\n  ## Optional TLS Config\n  # tls_ca = \"/path/to/cafile\"\n  # tls_cert = \"/path/to/certfile\"\n  # tls_key = \"/path/to/keyfile\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "neptune_apex",
      "description": "Neptune Apex data collector",
      "config": "# Neptune Apex data collector\n[[inputs.neptune_apex]]\n  # alias=\"neptune_apex\"\n  ## The Neptune Apex plugin reads the publicly available status.xml data from a local Apex.\n  ## Measurements will be logged under \"apex\".\n\n  ## The base URL of the local Apex(es). If you specify more than one server, they will\n  ## be differentiated by the \"source\" tag.\n  servers = [\n    \"http://apex.local\",\n  ]\n\n  ## The response_timeout specifies how long to wait for a reply from the Apex.\n  #response_timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "openntpd",
      "description": "Get standard NTP query metrics from OpenNTPD.",
      "config": "# Get standard NTP query metrics from OpenNTPD.\n[[inputs.openntpd]]\n  # alias=\"openntpd\"\n  ## Run ntpctl binary with sudo.\n  # use_sudo = false\n\n  ## Location of the ntpctl binary.\n  # binary = \"/usr/sbin/ntpctl\"\n\n  ## Maximum time the ntpctl binary is allowed to run.\n  # timeout = \"5ms\"\n  \n"
    },
    {
      "type": "input",
      "name": "ipset",
      "description": "Gather packets and bytes counters from Linux ipsets",
      "config": "# Gather packets and bytes counters from Linux ipsets\n[[inputs.ipset]]\n  # alias=\"ipset\"\n  ## By default, we only show sets which have already matched at least 1 packet.\n  ## set include_unmatched_sets = true to gather them all.\n  include_unmatched_sets = false\n  ## Adjust your sudo settings appropriately if using this option (\"sudo ipset save\")\n  use_sudo = false\n  ## The default timeout of 1s for ipset execution can be overridden here:\n  # timeout = \"1s\"\n\n"
    },
    {
      "type": "input",
      "name": "tengine",
      "description": "Read Tengine's basic status information (ngx_http_reqstat_module)",
      "config": "# Read Tengine's basic status information (ngx_http_reqstat_module)\n[[inputs.tengine]]\n  # alias=\"tengine\"\n  # An array of Tengine reqstat module URI to gather stats.\n  urls = [\"http://127.0.0.1/us\"]\n\n  # HTTP response timeout (default: 5s)\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.cer\"\n  # tls_key = \"/etc/telegraf/key.key\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "vsphere",
      "description": "Read metrics from VMware vCenter",
      "config": "# Read metrics from VMware vCenter\n[[inputs.vsphere]]\n  # alias=\"vsphere\"\n  ## List of vCenter URLs to be monitored. These three lines must be uncommented\n  ## and edited for the plugin to work.\n  vcenters = [ \"https://vcenter.local/sdk\" ]\n  username = \"user@corp.local\"\n  password = \"secret\"\n\n  ## VMs\n  ## Typical VM metrics (if omitted or empty, all metrics are collected)\n  vm_metric_include = [\n    \"cpu.demand.average\",\n    \"cpu.idle.summation\",\n    \"cpu.latency.average\",\n    \"cpu.readiness.average\",\n    \"cpu.ready.summation\",\n    \"cpu.run.summation\",\n    \"cpu.usagemhz.average\",\n    \"cpu.used.summation\",\n    \"cpu.wait.summation\",\n    \"mem.active.average\",\n    \"mem.granted.average\",\n    \"mem.latency.average\",\n    \"mem.swapin.average\",\n    \"mem.swapinRate.average\",\n    \"mem.swapout.average\",\n    \"mem.swapoutRate.average\",\n    \"mem.usage.average\",\n    \"mem.vmmemctl.average\",\n    \"net.bytesRx.average\",\n    \"net.bytesTx.average\",\n    \"net.droppedRx.summation\",\n    \"net.droppedTx.summation\",\n    \"net.usage.average\",\n    \"power.power.average\",\n    \"virtualDisk.numberReadAveraged.average\",\n    \"virtualDisk.numberWriteAveraged.average\",\n    \"virtualDisk.read.average\",\n    \"virtualDisk.readOIO.latest\",\n    \"virtualDisk.throughput.usage.average\",\n    \"virtualDisk.totalReadLatency.average\",\n    \"virtualDisk.totalWriteLatency.average\",\n    \"virtualDisk.write.average\",\n    \"virtualDisk.writeOIO.latest\",\n    \"sys.uptime.latest\",\n  ]\n  # vm_metric_exclude = [] ## Nothing is excluded by default\n  # vm_instances = true ## true by default\n\n  ## Hosts\n  ## Typical host metrics (if omitted or empty, all metrics are collected)\n  host_metric_include = [\n    \"cpu.coreUtilization.average\",\n    \"cpu.costop.summation\",\n    \"cpu.demand.average\",\n    \"cpu.idle.summation\",\n    \"cpu.latency.average\",\n    \"cpu.readiness.average\",\n    \"cpu.ready.summation\",\n    \"cpu.swapwait.summation\",\n    \"cpu.usage.average\",\n    \"cpu.usagemhz.average\",\n    \"cpu.used.summation\",\n    \"cpu.utilization.average\",\n    \"cpu.wait.summation\",\n    \"disk.deviceReadLatency.average\",\n    \"disk.deviceWriteLatency.average\",\n    \"disk.kernelReadLatency.average\",\n    \"disk.kernelWriteLatency.average\",\n    \"disk.numberReadAveraged.average\",\n    \"disk.numberWriteAveraged.average\",\n    \"disk.read.average\",\n    \"disk.totalReadLatency.average\",\n    \"disk.totalWriteLatency.average\",\n    \"disk.write.average\",\n    \"mem.active.average\",\n    \"mem.latency.average\",\n    \"mem.state.latest\",\n    \"mem.swapin.average\",\n    \"mem.swapinRate.average\",\n    \"mem.swapout.average\",\n    \"mem.swapoutRate.average\",\n    \"mem.totalCapacity.average\",\n    \"mem.usage.average\",\n    \"mem.vmmemctl.average\",\n    \"net.bytesRx.average\",\n    \"net.bytesTx.average\",\n    \"net.droppedRx.summation\",\n    \"net.droppedTx.summation\",\n    \"net.errorsRx.summation\",\n    \"net.errorsTx.summation\",\n    \"net.usage.average\",\n    \"power.power.average\",\n    \"storageAdapter.numberReadAveraged.average\",\n    \"storageAdapter.numberWriteAveraged.average\",\n    \"storageAdapter.read.average\",\n    \"storageAdapter.write.average\",\n    \"sys.uptime.latest\",\n  ]\n  ## Collect IP addresses? Valid values are \"ipv4\" and \"ipv6\"\n  # ip_addresses = [\"ipv6\", \"ipv4\" ]\n  # host_metric_exclude = [] ## Nothing excluded by default\n  # host_instances = true ## true by default\n\n  ## Clusters\n  # cluster_metric_include = [] ## if omitted or empty, all metrics are collected\n  # cluster_metric_exclude = [] ## Nothing excluded by default\n  # cluster_instances = false ## false by default\n\n  ## Datastores\n  # datastore_metric_include = [] ## if omitted or empty, all metrics are collected\n  # datastore_metric_exclude = [] ## Nothing excluded by default\n  # datastore_instances = false ## false by default for Datastores only\n\n  ## Datacenters\n  datacenter_metric_include = [] ## if omitted or empty, all metrics are collected\n  datacenter_metric_exclude = [ \"*\" ] ## Datacenters are not collected by default.\n  # datacenter_instances = false ## false by default for Datastores only\n\n  ## Plugin Settings  \n  ## separator character to use for measurement and field names (default: \"_\")\n  # separator = \"_\"\n\n  ## number of objects to retreive per query for realtime resources (vms and hosts)\n  ## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n  # max_query_objects = 256\n\n  ## number of metrics to retreive per query for non-realtime resources (clusters and datastores)\n  ## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n  # max_query_metrics = 256\n\n  ## number of go routines to use for collection and discovery of objects and metrics\n  # collect_concurrency = 1\n  # discover_concurrency = 1\n\n  ## whether or not to force discovery of new objects on initial gather call before collecting metrics\n  ## when true for large environments this may cause errors for time elapsed while collecting metrics\n  ## when false (default) the first collection cycle may result in no or limited metrics while objects are discovered\n  # force_discover_on_init = false\n\n  ## the interval before (re)discovering objects subject to metrics collection (default: 300s)\n  # object_discovery_interval = \"300s\"\n\n  ## timeout applies to any of the api request made to vcenter\n  # timeout = \"60s\"\n\n  ## When set to true, all samples are sent as integers. This makes the output\n  ## data types backwards compatible with Telegraf 1.9 or lower. Normally all\n  ## samples from vCenter, with the exception of percentages, are integer\n  ## values, but under some conditions, some averaging takes place internally in\n  ## the plugin. Setting this flag to \"false\" will send values as floats to\n  ## preserve the full precision when averaging takes place.\n  # use_int_samples = true\n\n  ## Custom attributes from vCenter can be very useful for queries in order to slice the\n  ## metrics along different dimension and for forming ad-hoc relationships. They are disabled\n  ## by default, since they can add a considerable amount of tags to the resulting metrics. To\n  ## enable, simply set custom_attribute_exlude to [] (empty set) and use custom_attribute_include\n  ## to select the attributes you want to include.\n  # custom_attribute_include = []\n  # custom_attribute_exclude = [\"*\"] \n\n  ## Optional SSL Config\n  # ssl_ca = \"/path/to/cafile\"\n  # ssl_cert = \"/path/to/certfile\"\n  # ssl_key = \"/path/to/keyfile\"\n  ## Use SSL but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "aurora",
      "description": "Gather metrics from Apache Aurora schedulers",
      "config": "# Gather metrics from Apache Aurora schedulers\n[[inputs.aurora]]\n  # alias=\"aurora\"\n  ## Schedulers are the base addresses of your Aurora Schedulers\n  schedulers = [\"http://127.0.0.1:8081\"]\n\n  ## Set of role types to collect metrics from.\n  ##\n  ## The scheduler roles are checked each interval by contacting the\n  ## scheduler nodes; zookeeper is not contacted.\n  # roles = [\"leader\", \"follower\"]\n\n  ## Timeout is the max time for total network operations.\n  # timeout = \"5s\"\n\n  ## Username and password are sent using HTTP Basic Auth.\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "burrow",
      "description": "Collect Kafka topics and consumers status from Burrow HTTP API.",
      "config": "# Collect Kafka topics and consumers status from Burrow HTTP API.\n[[inputs.burrow]]\n  # alias=\"burrow\"\n  ## Burrow API endpoints in format \"schema://host:port\".\n  ## Default is \"http://localhost:8000\".\n  servers = [\"http://localhost:8000\"]\n\n  ## Override Burrow API prefix.\n  ## Useful when Burrow is behind reverse-proxy.\n  # api_prefix = \"/v3/kafka\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Limit per-server concurrent connections.\n  ## Useful in case of large number of topics or consumer groups.\n  # concurrent_connections = 20\n\n  ## Filter clusters, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # clusters_include = []\n  # clusters_exclude = []\n\n  ## Filter consumer groups, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # groups_include = []\n  # groups_exclude = []\n\n  ## Filter topics, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # topics_include = []\n  # topics_exclude = []\n\n  ## Credentials for basic HTTP authentication.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional SSL config\n  # ssl_ca = \"/etc/telegraf/ca.pem\"\n  # ssl_cert = \"/etc/telegraf/cert.pem\"\n  # ssl_key = \"/etc/telegraf/key.pem\"\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "consul",
      "description": "Gather health check statuses from services registered in Consul",
      "config": "# Gather health check statuses from services registered in Consul\n[[inputs.consul]]\n  # alias=\"consul\"\n  ## Consul server address\n  # address = \"localhost\"\n\n  ## URI scheme for the Consul server, one of \"http\", \"https\"\n  # scheme = \"http\"\n\n  ## ACL token used in every request\n  # token = \"\"\n\n  ## HTTP Basic Authentication username and password.\n  # username = \"\"\n  # password = \"\"\n\n  ## Data center to query the health checks from\n  # datacenter = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = true\n\n  ## Consul checks' tag splitting\n  # When tags are formatted like \"key:value\" with \":\" as a delimiter then\n  # they will be splitted and reported as proper key:value in Telegraf\n  # tag_delimiter = \":\"\n\n"
    },
    {
      "type": "input",
      "name": "dovecot",
      "description": "Read statistics from one or many dovecot servers",
      "config": "# Read statistics from one or many dovecot servers\n[[inputs.dovecot]]\n  # alias=\"dovecot\"\n  ## specify dovecot servers via an address:port list\n  ##  e.g.\n  ##    localhost:24242\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"localhost:24242\"]\n\n  ## Type is one of \"user\", \"domain\", \"ip\", or \"global\"\n  type = \"global\"\n\n  ## Wildcard matches like \"*.com\". An empty string \"\" is same as \"*\"\n  ## If type = \"ip\" filters should be \u003cIP/network\u003e\n  filters = [\"\"]\n\n"
    },
    {
      "type": "input",
      "name": "fireboard",
      "description": "Read real time temps from fireboard.io servers",
      "config": "# Read real time temps from fireboard.io servers\n[[inputs.fireboard]]\n  # alias=\"fireboard\"\n  ## Specify auth token for your account\n  auth_token = \"invalidAuthToken\"\n  ## You can override the fireboard server URL if necessary\n  # url = https://fireboard.io/api/v1/devices.json\n  ## You can set a different http_timeout if you need to\n  ## You should set a string using an number and time indicator\n  ## for example \"12s\" for 12 seconds.\n  # http_timeout = \"4s\"\n\n"
    },
    {
      "type": "input",
      "name": "ecs",
      "description": "Read metrics about docker containers from Fargate/ECS v2 meta endpoints.",
      "config": "# Read metrics about docker containers from Fargate/ECS v2 meta endpoints.\n[[inputs.ecs]]\n  # alias=\"ecs\"\n  ## ECS metadata url\n  # endpoint_url = \"http://169.254.170.2\"\n\n  ## Containers to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all containers\n  # container_name_include = []\n  # container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"RUNNING\" state will be captured.\n  ## Possible values are \"NONE\", \"PULLED\", \"CREATED\", \"RUNNING\",\n  ## \"RESOURCES_PROVISIONED\", \"STOPPED\".\n  # container_status_include = []\n  # container_status_exclude = []\n\n  ## ecs labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  ecs_label_include = [ \"com.amazonaws.ecs.*\" ]\n  ecs_label_exclude = []\n\n  ## Timeout for queries.\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "icinga2",
      "description": "Gather Icinga2 status",
      "config": "# Gather Icinga2 status\n[[inputs.icinga2]]\n  # alias=\"icinga2\"\n  ## Required Icinga2 server address\n  # server = \"https://localhost:5665\"\n  \n  ## Required Icinga2 object type (\"services\" or \"hosts\")\n  # object_type = \"services\"\n\n  ## Credentials for basic HTTP authentication\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = true\n  \n"
    },
    {
      "type": "input",
      "name": "diskio",
      "description": "Read metrics about disk IO by device",
      "config": "# Read metrics about disk IO by device\n[[inputs.diskio]]\n  # alias=\"diskio\"\n  ## By default, telegraf will gather stats for all devices including\n  ## disk partitions.\n  ## Setting devices will restrict the stats to the specified devices.\n  # devices = [\"sda\", \"sdb\", \"vd*\"]\n  ## Uncomment the following line if you need disk serial numbers.\n  # skip_serial_number = false\n  #\n  ## On systems which support it, device metadata can be added in the form of\n  ## tags.\n  ## Currently only Linux is supported via udev properties. You can view\n  ## available properties for a device by running:\n  ## 'udevadm info -q property -n /dev/sda'\n  ## Note: Most, but not all, udev properties can be accessed this way. Properties\n  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.\n  # device_tags = [\"ID_FS_TYPE\", \"ID_FS_USAGE\"]\n  #\n  ## Using the same metadata source as device_tags, you can also customize the\n  ## name of the device via templates.\n  ## The 'name_templates' parameter is a list of templates to try and apply to\n  ## the device. The template may contain variables in the form of '$PROPERTY' or\n  ## '${PROPERTY}'. The first template which does not contain any variables not\n  ## present for the device is used as the device name tag.\n  ## The typical use case is for LVM volumes, to get the VG/LV name instead of\n  ## the near-meaningless DM-0 name.\n  # name_templates = [\"$ID_FS_LABEL\",\"$DM_VG_NAME/$DM_LV_NAME\"]\n\n"
    },
    {
      "type": "input",
      "name": "http",
      "description": "Read formatted metrics from one or more HTTP endpoints",
      "config": "# Read formatted metrics from one or more HTTP endpoints\n[[inputs.http]]\n  # alias=\"http\"\n  ## One or more URLs from which to read formatted metrics\n  urls = [\n    \"http://localhost/metrics\"\n  ]\n\n  ## HTTP method\n  # method = \"GET\"\n\n  ## Optional HTTP headers\n  # headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## Optional HTTP Basic Auth Credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## HTTP entity-body to send with POST/PUT requests.\n  # body = \"\"\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n\n  ## List of success status codes\n  # success_status_codes = [200]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "uwsgi",
      "description": "Read uWSGI metrics.",
      "config": "# Read uWSGI metrics.\n[[inputs.uwsgi]]\n  # alias=\"uwsgi\"\n  ## List with urls of uWSGI Stats servers. URL must match pattern:\n  ## scheme://address[:port]\n  ##\n  ## For example:\n  ## servers = [\"tcp://localhost:5050\", \"http://localhost:1717\", \"unix:///tmp/statsock\"]\n  servers = [\"tcp://127.0.0.1:1717\"]\n\n  ## General connection timout\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "chrony",
      "description": "Get standard chrony metrics, requires chronyc executable.",
      "config": "# Get standard chrony metrics, requires chronyc executable.\n[[inputs.chrony]]\n  # alias=\"chrony\"\n  ## If true, chronyc tries to perform a DNS lookup for the time server.\n  # dns_lookup = false\n  \n"
    },
    {
      "type": "input",
      "name": "elasticsearch",
      "description": "Read stats from one or more Elasticsearch servers or clusters",
      "config": "# Read stats from one or more Elasticsearch servers or clusters\n[[inputs.elasticsearch]]\n  # alias=\"elasticsearch\"\n  ## specify a list of one or more Elasticsearch servers\n  # you can add username and password to your url to use basic authentication:\n  # servers = [\"http://user:pass@localhost:9200\"]\n  servers = [\"http://localhost:9200\"]\n\n  ## Timeout for HTTP requests to the elastic search server(s)\n  http_timeout = \"5s\"\n\n  ## When local is true (the default), the node will read only its own stats.\n  ## Set local to false when you want to read the node stats from all nodes\n  ## of the cluster.\n  local = true\n\n  ## Set cluster_health to true when you want to also obtain cluster health stats\n  cluster_health = false\n\n  ## Adjust cluster_health_level when you want to also obtain detailed health stats\n  ## The options are\n  ##  - indices (default)\n  ##  - cluster\n  # cluster_health_level = \"indices\"\n\n  ## Set cluster_stats to true when you want to also obtain cluster stats.\n  cluster_stats = false\n\n  ## Only gather cluster_stats from the master node. To work this require local = true\n  cluster_stats_only_from_master = true\n\n  ## Indices to collect; can be one or more indices names or _all\n  indices_include = [\"_all\"]\n\n  ## One of \"shards\", \"cluster\", \"indices\"\n  indices_level = \"shards\"\n\n  ## node_stats is a list of sub-stats that you want to have gathered. Valid options\n  ## are \"indices\", \"os\", \"process\", \"jvm\", \"thread_pool\", \"fs\", \"transport\", \"http\",\n  ## \"breaker\". Per default, all stats are gathered.\n  # node_stats = [\"jvm\", \"http\"]\n\n  ## HTTP Basic Authentication username and password.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "kafka_consumer",
      "description": "Read metrics from Kafka topics",
      "config": "# Read metrics from Kafka topics\n[[inputs.kafka_consumer]]\n  # alias=\"kafka_consumer\"\n  ## Kafka brokers.\n  brokers = [\"localhost:9092\"]\n\n  ## Topics to consume.\n  topics = [\"telegraf\"]\n\n  ## When set this tag will be added to all metrics with the topic as the value.\n  # topic_tag = \"\"\n\n  ## Optional Client id\n  # client_id = \"Telegraf\"\n\n  ## Set the minimal supported Kafka version.  Setting this enables the use of new\n  ## Kafka features and APIs.  Must be 0.10.2.0 or greater.\n  ##   ex: version = \"1.1.0\"\n  # version = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional SASL Config\n  # sasl_username = \"kafka\"\n  # sasl_password = \"secret\"\n\n  ## Name of the consumer group.\n  # consumer_group = \"telegraf_metrics_consumers\"\n\n  ## Initial offset position; one of \"oldest\" or \"newest\".\n  # offset = \"oldest\"\n\n  ## Consumer group partition assignment strategy; one of \"range\", \"roundrobin\" or \"sticky\".\n  # balance_strategy = \"range\"\n\n  ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n  ## larger messages are dropped\n  max_message_len = 1000000\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "tail",
      "description": "Stream a log file, like the tail -f command",
      "config": "# Stream a log file, like the tail -f command\n[[inputs.tail]]\n  # alias=\"tail\"\n  ## files to tail.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   \"/var/log/**.log\"  -\u003e recursively find all .log files in /var/log\n  ##   \"/var/log/*/*.log\" -\u003e find all .log files with a parent dir in /var/log\n  ##   \"/var/log/apache.log\" -\u003e just tail the apache log file\n  ##\n  ## See https://github.com/gobwas/glob for more examples\n  ##\n  files = [\"/var/mymetrics.out\"]\n  ## Read file from beginning.\n  from_beginning = false\n  ## Whether file is a named pipe\n  pipe = false\n\n  ## Method used to watch for file updates.  Can be either \"inotify\" or \"poll\".\n  # watch_method = \"inotify\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "udp_listener",
      "description": "Generic UDP listener",
      "config": "# Generic UDP listener\n[[inputs.udp_listener]]\n  # alias=\"udp_listener\"\n  # DEPRECATED: the TCP listener plugin has been deprecated in favor of the\n  # socket_listener plugin\n  # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/socket_listener\n\n"
    },
    {
      "type": "input",
      "name": "beanstalkd",
      "description": "Collects Beanstalkd server and tubes stats",
      "config": "# Collects Beanstalkd server and tubes stats\n[[inputs.beanstalkd]]\n  # alias=\"beanstalkd\"\n  ## Server to collect data from\n  server = \"localhost:11300\"\n\n  ## List of tubes to gather stats about.\n  ## If no tubes specified then data gathered for each tube on server reported by list-tubes command\n  tubes = [\"notifications\"]\n\n"
    },
    {
      "type": "input",
      "name": "github",
      "description": "Gather repository information from GitHub hosted repositories.",
      "config": "# Gather repository information from GitHub hosted repositories.\n[[inputs.github]]\n  # alias=\"github\"\n  ## List of repositories to monitor.\n  repositories = [\n\t  \"influxdata/telegraf\",\n\t  \"influxdata/influxdb\"\n  ]\n\n  ## Github API access token.  Unauthenticated requests are limited to 60 per hour.\n  # access_token = \"\"\n\n  ## Github API enterprise url. Github Enterprise accounts must specify their base url.\n  # enterprise_base_url = \"\"\n\n  ## Timeout for HTTP requests.\n  # http_timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "logparser",
      "description": "Stream and parse log file(s).",
      "config": "# Stream and parse log file(s).\n[[inputs.logparser]]\n  # alias=\"logparser\"\n  ## Log files to parse.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   /var/log/**.log     -\u003e recursively find all .log files in /var/log\n  ##   /var/log/*/*.log    -\u003e find all .log files with a parent dir in /var/log\n  ##   /var/log/apache.log -\u003e only tail the apache log file\n  files = [\"/var/log/apache/access.log\"]\n\n  ## Read files that currently exist from the beginning. Files that are created\n  ## while telegraf is running (and that match the \"files\" globs) will always\n  ## be read from the beginning.\n  from_beginning = false\n\n  ## Method used to watch for file updates.  Can be either \"inotify\" or \"poll\".\n  # watch_method = \"inotify\"\n\n  ## Parse logstash-style \"grok\" patterns:\n  [inputs.logparser.grok]\n    ## This is a list of patterns to check the given log file(s) for.\n    ## Note that adding patterns here increases processing time. The most\n    ## efficient configuration is to have one pattern per logparser.\n    ## Other common built-in patterns are:\n    ##   %{COMMON_LOG_FORMAT}   (plain apache \u0026 nginx access logs)\n    ##   %{COMBINED_LOG_FORMAT} (access logs + referrer \u0026 agent)\n    patterns = [\"%{COMBINED_LOG_FORMAT}\"]\n\n    ## Name of the outputted measurement name.\n    measurement = \"apache_access_log\"\n\n    ## Full path(s) to custom pattern files.\n    custom_pattern_files = []\n\n    ## Custom patterns can also be defined here. Put one pattern per line.\n    custom_patterns = '''\n    '''\n\n    ## Timezone allows you to provide an override for timestamps that\n    ## don't already include an offset\n    ## e.g. 04/06/2016 12:41:45 data one two 5.43Âµs\n    ##\n    ## Default: \"\" which renders UTC\n    ## Options are as follows:\n    ##   1. Local             -- interpret based on machine localtime\n    ##   2. \"Canada/Eastern\"  -- Unix TZ values like those found in https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\n    ##   3. UTC               -- or blank/unspecified, will return timestamp in UTC\n    # timezone = \"Canada/Eastern\"\n\n\t## When set to \"disable\", timestamp will not incremented if there is a\n\t## duplicate.\n    # unique_timestamp = \"auto\"\n\n"
    },
    {
      "type": "input",
      "name": "tomcat",
      "description": "Gather metrics from the Tomcat server status page.",
      "config": "# Gather metrics from the Tomcat server status page.\n[[inputs.tomcat]]\n  # alias=\"tomcat\"\n  ## URL of the Tomcat server status\n  # url = \"http://127.0.0.1:8080/manager/status/all?XML=true\"\n\n  ## HTTP Basic Auth Credentials\n  # username = \"tomcat\"\n  # password = \"s3cret\"\n\n  ## Request timeout\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "twemproxy",
      "description": "Read Twemproxy stats data",
      "config": "# Read Twemproxy stats data\n[[inputs.twemproxy]]\n  # alias=\"twemproxy\"\n  ## Twemproxy stats address and port (no scheme)\n  addr = \"localhost:22222\"\n  ## Monitor pool name\n  pools = [\"redis_pool\", \"mc_pool\"]\n\n"
    },
    {
      "type": "input",
      "name": "influxdb_listener",
      "description": "Influx HTTP write listener",
      "config": "# Influx HTTP write listener\n[[inputs.influxdb_listener]]\n  # alias=\"influxdb_listener\"\n  ## Address and port to host HTTP listener on\n  service_address = \":8186\"\n\n  ## maximum duration before timing out read of the request\n  read_timeout = \"10s\"\n  ## maximum duration before timing out write of the response\n  write_timeout = \"10s\"\n\n  ## Maximum allowed http request body size in bytes.\n  ## 0 means to use the default of 524,288,000 bytes (500 mebibytes)\n  max_body_size = \"500MiB\"\n\n  ## Maximum line size allowed to be sent in bytes.\n  ## 0 means to use the default of 65536 bytes (64 kibibytes)\n  max_line_size = \"64KiB\"\n  \n\n  ## Optional tag name used to store the database. \n  ## If the write has a database in the query string then it will be kept in this tag name.\n  ## This tag can be used in downstream outputs.\n  ## The default value of nothing means it will be off and the database will not be recorded.\n  # database_tag = \"\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  tls_cert = \"/etc/telegraf/cert.pem\"\n  tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Optional username and password to accept for HTTP basic authentication.\n  ## You probably want to make sure you have TLS configured above for this.\n  # basic_username = \"foobar\"\n  # basic_password = \"barfoo\"\n\n"
    },
    {
      "type": "input",
      "name": "jti_openconfig_telemetry",
      "description": "Read JTI OpenConfig Telemetry from listed sensors",
      "config": "# Read JTI OpenConfig Telemetry from listed sensors\n[[inputs.jti_openconfig_telemetry]]\n  # alias=\"jti_openconfig_telemetry\"\n  ## List of device addresses to collect telemetry from\n  servers = [\"localhost:1883\"]\n\n  ## Authentication details. Username and password are must if device expects\n  ## authentication. Client ID must be unique when connecting from multiple instances\n  ## of telegraf to the same device\n  username = \"user\"\n  password = \"pass\"\n  client_id = \"telegraf\"\n\n  ## Frequency to get data\n  sample_frequency = \"1000ms\"\n\n  ## Sensors to subscribe for\n  ## A identifier for each sensor can be provided in path by separating with space\n  ## Else sensor path will be used as identifier\n  ## When identifier is used, we can provide a list of space separated sensors.\n  ## A single subscription will be created with all these sensors and data will\n  ## be saved to measurement with this identifier name\n  sensors = [\n   \"/interfaces/\",\n   \"collection /components/ /lldp\",\n  ]\n\n  ## We allow specifying sensor group level reporting rate. To do this, specify the\n  ## reporting rate in Duration at the beginning of sensor paths / collection\n  ## name. For entries without reporting rate, we use configured sample frequency\n  sensors = [\n   \"1000ms customReporting /interfaces /lldp\",\n   \"2000ms collection /components\",\n   \"/interfaces\",\n  ]\n\n  ## Optional TLS Config\n  # enable_tls = true\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Delay between retry attempts of failed RPC calls or streams. Defaults to 1000ms.\n  ## Failed streams/calls will not be retried if 0 is provided\n  retry_delay = \"1000ms\"\n\n  ## To treat all string values as tags, set this to true\n  str_as_tags = false\n\n"
    },
    {
      "type": "input",
      "name": "kinesis_consumer",
      "description": "Configuration for the AWS Kinesis input.",
      "config": "# Configuration for the AWS Kinesis input.\n[[inputs.kinesis_consumer]]\n  # alias=\"kinesis_consumer\"\n  ## Amazon REGION of kinesis endpoint.\n  region = \"ap-southeast-2\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Assumed credentials via STS if role_arn is specified\n  ## 2) explicit credentials from 'access_key' and 'secret_key'\n  ## 3) shared profile from 'profile'\n  ## 4) environment variables\n  ## 5) shared credentials file\n  ## 6) EC2 Instance Profile\n  # access_key = \"\"\n  # secret_key = \"\"\n  # token = \"\"\n  # role_arn = \"\"\n  # profile = \"\"\n  # shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Kinesis StreamName must exist prior to starting telegraf.\n  streamname = \"StreamName\"\n\n  ## Shard iterator type (only 'TRIM_HORIZON' and 'LATEST' currently supported)\n  # shard_iterator_type = \"TRIM_HORIZON\"\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Optional\n  ## Configuration for a dynamodb checkpoint\n  [inputs.kinesis_consumer.checkpoint_dynamodb]\n\t## unique name for this consumer\n\tapp_name = \"default\"\n\ttable_name = \"default\"\n\n"
    },
    {
      "type": "input",
      "name": "pgbouncer",
      "description": "Read metrics from one or many pgbouncer servers",
      "config": "# Read metrics from one or many pgbouncer servers\n[[inputs.pgbouncer]]\n  # alias=\"pgbouncer\"\n  ## specify address via a url matching:\n  ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n  ##       ?sslmode=[disable|verify-ca|verify-full]\n  ## or a simple string:\n  ##   host=localhost user=pqotest password=... sslmode=... dbname=app_production\n  ##\n  ## All connection parameters are optional.\n  ##\n  address = \"host=localhost user=pgbouncer sslmode=disable\"\n\n"
    },
    {
      "type": "input",
      "name": "internal",
      "description": "Collect statistics about itself",
      "config": "# Collect statistics about itself\n[[inputs.internal]]\n  # alias=\"internal\"\n  ## If true, collect telegraf memory stats.\n  # collect_memstats = true\n\n"
    },
    {
      "type": "input",
      "name": "mcrouter",
      "description": "Read metrics from one or many mcrouter servers",
      "config": "# Read metrics from one or many mcrouter servers\n[[inputs.mcrouter]]\n  # alias=\"mcrouter\"\n  ## An array of address to gather stats about. Specify an ip or hostname\n  ## with port. ie tcp://localhost:11211, tcp://10.0.0.1:11211, etc.\n\tservers = [\"tcp://localhost:11211\", \"unix:///var/run/mcrouter.sock\"]\n\n\t## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "postgresql_extensible",
      "description": "Read metrics from one or many postgresql servers",
      "config": "# Read metrics from one or many postgresql servers\n[[inputs.postgresql_extensible]]\n  # alias=\"postgresql_extensible\"\n  ## specify address via a url matching:\n  ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n  ##       ?sslmode=[disable|verify-ca|verify-full]\n  ## or a simple string:\n  ##   host=localhost user=pqotest password=... sslmode=... dbname=app_production\n  #\n  ## All connection parameters are optional.  #\n  ## Without the dbname parameter, the driver will default to a database\n  ## with the same name as the user. This dbname is just for instantiating a\n  ## connection with the server and doesn't restrict the databases we are trying\n  ## to grab metrics for.\n  #\n  address = \"host=localhost user=postgres sslmode=disable\"\n\n  ## connection configuration.\n  ## maxlifetime - specify the maximum lifetime of a connection.\n  ## default is forever (0s)\n  max_lifetime = \"0s\"\n\n  ## A list of databases to pull metrics about. If not specified, metrics for all\n  ## databases are gathered.\n  ## databases = [\"app_production\", \"testing\"]\n  #\n  ## A custom name for the database that will be used as the \"server\" tag in the\n  ## measurement output. If not specified, a default one generated from\n  ## the connection address is used.\n  # outputaddress = \"db01\"\n  #\n  ## Define the toml config where the sql queries are stored\n  ## New queries can be added, if the withdbname is set to true and there is no\n  ## databases defined in the 'databases field', the sql query is ended by a\n  ## 'is not null' in order to make the query succeed.\n  ## Example :\n  ## The sqlquery : \"SELECT * FROM pg_stat_database where datname\" become\n  ## \"SELECT * FROM pg_stat_database where datname IN ('postgres', 'pgbench')\"\n  ## because the databases variable was set to ['postgres', 'pgbench' ] and the\n  ## withdbname was true. Be careful that if the withdbname is set to false you\n  ## don't have to define the where clause (aka with the dbname) the tagvalue\n  ## field is used to define custom tags (separated by commas)\n  ## The optional \"measurement\" value can be used to override the default\n  ## output measurement name (\"postgresql\").\n  ##\n  ## The script option can be used to specify the .sql file path.\n  ## If script and sqlquery options specified at same time, sqlquery will be used \n  ##\n  ## Structure :\n  ## [[inputs.postgresql_extensible.query]]\n  ##   sqlquery string\n  ##   version string\n  ##   withdbname boolean\n  ##   tagvalue string (comma separated)\n  ##   measurement string\n  [[inputs.postgresql_extensible.query]]\n    sqlquery=\"SELECT * FROM pg_stat_database\"\n    version=901\n    withdbname=false\n    tagvalue=\"\"\n    measurement=\"\"\n  [[inputs.postgresql_extensible.query]]\n    sqlquery=\"SELECT * FROM pg_stat_bgwriter\"\n    version=901\n    withdbname=false\n    tagvalue=\"postgresql.stats\"\n\n"
    },
    {
      "type": "input",
      "name": "varnish",
      "description": "A plugin to collect stats from Varnish HTTP Cache",
      "config": "# A plugin to collect stats from Varnish HTTP Cache\n[[inputs.varnish]]\n  # alias=\"varnish\"\n  ## If running as a restricted user you can prepend sudo for additional access:\n  #use_sudo = false\n\n  ## The default location of the varnishstat binary can be overridden with:\n  binary = \"/usr/bin/varnishstat\"\n\n  ## By default, telegraf gather stats for 3 metric points.\n  ## Setting stats will override the defaults shown below.\n  ## Glob matching can be used, ie, stats = [\"MAIN.*\"]\n  ## stats may also be set to [\"*\"], which will collect all stats\n  stats = [\"MAIN.cache_hit\", \"MAIN.cache_miss\", \"MAIN.uptime\"]\n\n  ## Optional name for the varnish instance (or working directory) to query\n  ## Usually appened after -n in varnish cli\n  # instance_name = instanceName\n\n  ## Timeout for varnishstat command\n  # timeout = \"1s\"\n\n"
    },
    {
      "type": "input",
      "name": "wireless",
      "description": "Monitor wifi signal strength and quality",
      "config": "# Monitor wifi signal strength and quality\n[[inputs.wireless]]\n  # alias=\"wireless\"\n  ## Sets 'proc' directory path\n  ## If not specified, then default is /proc\n  # host_proc = \"/proc\"\n\n"
    },
    {
      "type": "input",
      "name": "rabbitmq",
      "description": "Reads metrics from RabbitMQ servers via the Management Plugin",
      "config": "# Reads metrics from RabbitMQ servers via the Management Plugin\n[[inputs.rabbitmq]]\n  # alias=\"rabbitmq\"\n  ## Management Plugin url. (default: http://localhost:15672)\n  # url = \"http://localhost:15672\"\n  ## Tag added to rabbitmq_overview series; deprecated: use tags\n  # name = \"rmq-server-1\"\n  ## Credentials\n  # username = \"guest\"\n  # password = \"guest\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional request timeouts\n  ##\n  ## ResponseHeaderTimeout, if non-zero, specifies the amount of time to wait\n  ## for a server's response headers after fully writing the request.\n  # header_timeout = \"3s\"\n  ##\n  ## client_timeout specifies a time limit for requests made by this client.\n  ## Includes connection time, any redirects, and reading the response body.\n  # client_timeout = \"4s\"\n\n  ## A list of nodes to gather as the rabbitmq_node measurement. If not\n  ## specified, metrics for all nodes are gathered.\n  # nodes = [\"rabbit@node1\", \"rabbit@node2\"]\n\n  ## A list of queues to gather as the rabbitmq_queue measurement. If not\n  ## specified, metrics for all queues are gathered.\n  # queues = [\"telegraf\"]\n\n  ## A list of exchanges to gather as the rabbitmq_exchange measurement. If not\n  ## specified, metrics for all exchanges are gathered.\n  # exchanges = [\"telegraf\"]\n\n  ## Queues to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all queues\n  queue_name_include = []\n  queue_name_exclude = []\n\n  ## Federation upstreams include and exclude when gathering the rabbitmq_federation measurement.\n  ## If neither are specified, metrics for all federation upstreams are gathered.\n  ## Federation link metrics will only be gathered for queues and exchanges\n  ## whose non-federation metrics will be collected (e.g a queue excluded\n  ## by the 'queue_name_exclude' option will also be excluded from federation).\n  ## Globs accepted.\n  # federation_upstream_include = [\"dataCentre-*\"]\n  # federation_upstream_exclude = []\n\n"
    },
    {
      "type": "input",
      "name": "x509_cert",
      "description": "Reads metrics from a SSL certificate",
      "config": "# Reads metrics from a SSL certificate\n[[inputs.x509_cert]]\n  # alias=\"x509_cert\"\n  ## List certificate sources\n  sources = [\"/etc/ssl/certs/ssl-cert-snakeoil.pem\", \"tcp://example.org:443\"]\n\n  ## Timeout for SSL connection\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n"
    },
    {
      "type": "input",
      "name": "cassandra",
      "description": "Read Cassandra metrics through Jolokia",
      "config": "# Read Cassandra metrics through Jolokia\n[[inputs.cassandra]]\n  # alias=\"cassandra\"\n  ## DEPRECATED: The cassandra plugin has been deprecated.  Please use the\n  ## jolokia2 plugin instead.\n  ##\n  ## see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\n\n  context = \"/jolokia/read\"\n  ## List of cassandra servers exposing jolokia read service\n  servers = [\"myuser:mypassword@10.10.10.1:8778\",\"10.10.10.2:8778\",\":8778\"]\n  ## List of metrics collected on above servers\n  ## Each metric consists of a jmx path.\n  ## This will collect all heap memory usage metrics from the jvm and\n  ## ReadLatency metrics for all keyspaces and tables.\n  ## \"type=Table\" in the query works with Cassandra3.0. Older versions might\n  ## need to use \"type=ColumnFamily\"\n  metrics  = [\n    \"/java.lang:type=Memory/HeapMemoryUsage\",\n    \"/org.apache.cassandra.metrics:type=Table,keyspace=*,scope=*,name=ReadLatency\"\n  ]\n\n"
    },
    {
      "type": "input",
      "name": "cloud_pubsub",
      "description": "Read metrics from Google PubSub",
      "config": "# Read metrics from Google PubSub\n[[inputs.cloud_pubsub]]\n  # alias=\"cloud_pubsub\"\n  ## Required. Name of Google Cloud Platform (GCP) Project that owns\n  ## the given PubSub subscription.\n  project = \"my-project\"\n\n  ## Required. Name of PubSub subscription to ingest metrics from.\n  subscription = \"my-subscription\"\n\n  ## Required. Data format to consume.\n  ## Each data format has its own unique set of configuration options.\n  ## Read more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n  ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n  ## Application Default Credentials, which is preferred.\n  # credentials_file = \"path/to/my/creds.json\"\n\n  ## Optional. Number of seconds to wait before attempting to restart the \n  ## PubSub subscription receiver after an unexpected error. \n  ## If the streaming pull for a PubSub Subscription fails (receiver),\n  ## the agent attempts to restart receiving messages after this many seconds.\n  # retry_delay_seconds = 5\n\n  ## Optional. Maximum byte length of a message to consume.\n  ## Larger messages are dropped with an error. If less than 0 or unspecified,\n  ## treated as no limit.\n  # max_message_len = 1000000\n\n  ## Optional. Maximum messages to read from PubSub that have not been written\n  ## to an output. Defaults to 1000.\n  ## For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message contains 10 metrics and the output\n  ## metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## The following are optional Subscription ReceiveSettings in PubSub.\n  ## Read more about these values:\n  ## https://godoc.org/cloud.google.com/go/pubsub#ReceiveSettings\n\n  ## Optional. Maximum number of seconds for which a PubSub subscription\n  ## should auto-extend the PubSub ACK deadline for each message. If less than\n  ## 0, auto-extension is disabled.\n  # max_extension = 0\n\n  ## Optional. Maximum number of unprocessed messages in PubSub\n  ## (unacknowledged but not yet expired in PubSub).\n  ## A value of 0 is treated as the default PubSub value.\n  ## Negative values will be treated as unlimited.\n  # max_outstanding_messages = 0\n\n  ## Optional. Maximum size in bytes of unprocessed messages in PubSub\n  ## (unacknowledged but not yet expired in PubSub).\n  ## A value of 0 is treated as the default PubSub value.\n  ## Negative values will be treated as unlimited.\n  # max_outstanding_bytes = 0\n\n  ## Optional. Max number of goroutines a PubSub Subscription receiver can spawn\n  ## to pull messages from PubSub concurrently. This limit applies to each\n  ## subscription separately and is treated as the PubSub default if less than\n  ## 1. Note this setting does not limit the number of messages that can be\n  ## processed concurrently (use \"max_outstanding_messages\" instead).\n  # max_receiver_go_routines = 0\n\n  ## Optional. If true, Telegraf will attempt to base64 decode the \n  ## PubSub message data before parsing\n  # base64_data = false\n\n"
    },
    {
      "type": "input",
      "name": "ipmi_sensor",
      "description": "Read metrics from the bare metal servers via IPMI",
      "config": "# Read metrics from the bare metal servers via IPMI\n[[inputs.ipmi_sensor]]\n  # alias=\"ipmi_sensor\"\n  ## optionally specify the path to the ipmitool executable\n  # path = \"/usr/bin/ipmitool\"\n  ##\n  ## optionally force session privilege level. Can be CALLBACK, USER, OPERATOR, ADMINISTRATOR\n  # privilege = \"ADMINISTRATOR\"\n  ##\n  ## optionally specify one or more servers via a url matching\n  ##  [username[:password]@][protocol[(address)]]\n  ##  e.g.\n  ##    root:passwd@lan(127.0.0.1)\n  ##\n  ## if no servers are specified, local machine sensor stats will be queried\n  ##\n  # servers = [\"USERID:PASSW0RD@lan(192.168.1.1)\"]\n\n  ## Recommended: use metric 'interval' that is a multiple of 'timeout' to avoid\n  ## gaps or overlap in pulled data\n  interval = \"30s\"\n\n  ## Timeout for the ipmitool command to complete\n  timeout = \"20s\"\n\n  ## Schema Version: (Optional, defaults to version 1)\n  metric_version = 2\n\n"
    },
    {
      "type": "input",
      "name": "jolokia",
      "description": "Read JMX metrics through Jolokia",
      "config": "# Read JMX metrics through Jolokia\n[[inputs.jolokia]]\n  # alias=\"jolokia\"\n  # DEPRECATED: the jolokia plugin has been deprecated in favor of the\n  # jolokia2 plugin\n  # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\n\n  ## This is the context root used to compose the jolokia url\n  ## NOTE that Jolokia requires a trailing slash at the end of the context root\n  ## NOTE that your jolokia security policy must allow for POST requests.\n  context = \"/jolokia/\"\n\n  ## This specifies the mode used\n  # mode = \"proxy\"\n  #\n  ## When in proxy mode this section is used to specify further\n  ## proxy address configurations.\n  ## Remember to change host address to fit your environment.\n  # [inputs.jolokia.proxy]\n  #   host = \"127.0.0.1\"\n  #   port = \"8080\"\n\n  ## Optional http timeouts\n  ##\n  ## response_header_timeout, if non-zero, specifies the amount of time to wait\n  ## for a server's response headers after fully writing the request.\n  # response_header_timeout = \"3s\"\n  ##\n  ## client_timeout specifies a time limit for requests made by this client.\n  ## Includes connection time, any redirects, and reading the response body.\n  # client_timeout = \"4s\"\n\n  ## Attribute delimiter\n  ##\n  ## When multiple attributes are returned for a single\n  ## [inputs.jolokia.metrics], the field name is a concatenation of the metric\n  ## name, and the attribute name, separated by the given delimiter.\n  # delimiter = \"_\"\n\n  ## List of servers exposing jolokia read service\n  [[inputs.jolokia.servers]]\n    name = \"as-server-01\"\n    host = \"127.0.0.1\"\n    port = \"8080\"\n    # username = \"myuser\"\n    # password = \"mypassword\"\n\n  ## List of metrics collected on above servers\n  ## Each metric consists in a name, a jmx path and either\n  ## a pass or drop slice attribute.\n  ## This collect all heap memory usage metrics.\n  [[inputs.jolokia.metrics]]\n    name = \"heap_memory_usage\"\n    mbean  = \"java.lang:type=Memory\"\n    attribute = \"HeapMemoryUsage\"\n\n  ## This collect thread counts metrics.\n  [[inputs.jolokia.metrics]]\n    name = \"thread_count\"\n    mbean  = \"java.lang:type=Threading\"\n    attribute = \"TotalStartedThreadCount,ThreadCount,DaemonThreadCount,PeakThreadCount\"\n\n  ## This collect number of class loaded/unloaded counts metrics.\n  [[inputs.jolokia.metrics]]\n    name = \"class_count\"\n    mbean  = \"java.lang:type=ClassLoading\"\n    attribute = \"LoadedClassCount,UnloadedClassCount,TotalLoadedClassCount\"\n\n"
    },
    {
      "type": "input",
      "name": "mem",
      "description": "Read metrics about memory usage",
      "config": "# Read metrics about memory usage\n[[inputs.mem]]\n  # alias=\"mem\"\n"
    },
    {
      "type": "input",
      "name": "filecount",
      "description": "Count files in a directory",
      "config": "# Count files in a directory\n[[inputs.filecount]]\n  # alias=\"filecount\"\n  ## Directory to gather stats about.\n  ##   deprecated in 1.9; use the directories option\n  # directory = \"/var/cache/apt/archives\"\n\n  ## Directories to gather stats about.\n  ## This accept standard unit glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   /var/log/**    -\u003e recursively find all directories in /var/log and count files in each directories\n  ##   /var/log/*/*   -\u003e find all directories with a parent dir in /var/log and count files in each directories\n  ##   /var/log       -\u003e count all files in /var/log and all of its subdirectories\n  directories = [\"/var/cache/apt/archives\"]\n\n  ## Only count files that match the name pattern. Defaults to \"*\".\n  name = \"*.deb\"\n\n  ## Count files in subdirectories. Defaults to true.\n  recursive = false\n\n  ## Only count regular files. Defaults to true.\n  regular_only = true\n\n  ## Follow all symlinks while walking the directory tree. Defaults to false.\n  follow_symlinks = false\n\n  ## Only count files that are at least this size. If size is\n  ## a negative number, only count files that are smaller than the\n  ## absolute value of size. Acceptable units are B, KiB, MiB, KB, ...\n  ## Without quotes and units, interpreted as size in bytes.\n  size = \"0B\"\n\n  ## Only count files that have not been touched for at least this\n  ## duration. If mtime is negative, only count files that have been\n  ## touched in this duration. Defaults to \"0s\".\n  mtime = \"0s\"\n\n"
    },
    {
      "type": "input",
      "name": "kafka_consumer_legacy",
      "description": "Read metrics from Kafka topic(s)",
      "config": "# Read metrics from Kafka topic(s)\n[[inputs.kafka_consumer_legacy]]\n  # alias=\"kafka_consumer_legacy\"\n  ## topic(s) to consume\n  topics = [\"telegraf\"]\n\n  ## an array of Zookeeper connection strings\n  zookeeper_peers = [\"localhost:2181\"]\n\n  ## Zookeeper Chroot\n  zookeeper_chroot = \"\"\n\n  ## the name of the consumer group\n  consumer_group = \"telegraf_metrics_consumers\"\n\n  ## Offset (must be either \"oldest\" or \"newest\")\n  offset = \"oldest\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n  ## larger messages are dropped\n  max_message_len = 65536\n\n"
    },
    {
      "type": "input",
      "name": "net",
      "description": "Read metrics about network interface usage",
      "config": "# Read metrics about network interface usage\n[[inputs.net]]\n  # alias=\"net\"\n  ## By default, telegraf gathers stats from any up interface (excluding loopback)\n  ## Setting interfaces will tell it to gather these explicit interfaces,\n  ## regardless of status.\n  ##\n  # interfaces = [\"eth0\"]\n  ##\n  ## On linux systems telegraf also collects protocol stats.\n  ## Setting ignore_protocol_stats to true will skip reporting of protocol metrics.\n  ##\n  # ignore_protocol_stats = false\n  ##\n\n"
    },
    {
      "type": "input",
      "name": "nsq",
      "description": "Read NSQ topic and channel statistics.",
      "config": "# Read NSQ topic and channel statistics.\n[[inputs.nsq]]\n  # alias=\"nsq\"\n  ## An array of NSQD HTTP API endpoints\n  endpoints  = [\"http://localhost:4151\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "conntrack",
      "description": "Collects conntrack stats from the configured directories and files.",
      "config": "# Collects conntrack stats from the configured directories and files.\n[[inputs.conntrack]]\n  # alias=\"conntrack\"\n   ## The following defaults would work with multiple versions of conntrack.\n   ## Note the nf_ and ip_ filename prefixes are mutually exclusive across\n   ## kernel versions, as are the directory locations.\n\n   ## Superset of filenames to look for within the conntrack dirs.\n   ## Missing files will be ignored.\n   files = [\"ip_conntrack_count\",\"ip_conntrack_max\",\n            \"nf_conntrack_count\",\"nf_conntrack_max\"]\n\n   ## Directories to search within for the conntrack files above.\n   ## Missing directrories will be ignored.\n   dirs = [\"/proc/sys/net/ipv4/netfilter\",\"/proc/sys/net/netfilter\"]\n\n"
    },
    {
      "type": "input",
      "name": "iptables",
      "description": "Gather packets and bytes throughput from iptables",
      "config": "# Gather packets and bytes throughput from iptables\n[[inputs.iptables]]\n  # alias=\"iptables\"\n  ## iptables require root access on most systems.\n  ## Setting 'use_sudo' to true will make use of sudo to run iptables.\n  ## Users must configure sudo to allow telegraf user to run iptables with no password.\n  ## iptables can be restricted to only list command \"iptables -nvL\".\n  use_sudo = false\n  ## Setting 'use_lock' to true runs iptables with the \"-w\" option.\n  ## Adjust your sudo settings appropriately if using this option (\"iptables -w 5 -nvl\")\n  use_lock = false\n  ## Define an alternate executable, such as \"ip6tables\". Default is \"iptables\".\n  # binary = \"ip6tables\"\n  ## defines the table to monitor:\n  table = \"filter\"\n  ## defines the chains to monitor.\n  ## NOTE: iptables rules without a comment will not be monitored.\n  ## Read the plugin documentation for more information.\n  chains = [ \"INPUT\" ]\n\n"
    },
    {
      "type": "input",
      "name": "memcached",
      "description": "Read metrics from one or many memcached servers",
      "config": "# Read metrics from one or many memcached servers\n[[inputs.memcached]]\n  # alias=\"memcached\"\n  ## An array of address to gather stats about. Specify an ip on hostname\n  ## with optional port. ie localhost, 10.0.0.1:11211, etc.\n  servers = [\"localhost:11211\"]\n  # unix_sockets = [\"/var/run/memcached.sock\"]\n\n"
    },
    {
      "type": "input",
      "name": "snmp_trap",
      "description": "Receive SNMP traps",
      "config": "# Receive SNMP traps\n[[inputs.snmp_trap]]\n  # alias=\"snmp_trap\"\n  ## Transport, local address, and port to listen on.  Transport must\n  ## be \"udp://\".  Omit local address to listen on all interfaces.\n  ##   example: \"udp://127.0.0.1:1234\"\n  # service_address = udp://:162\n  ## Timeout running snmptranslate command\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "dns_query",
      "description": "Query given DNS server and gives statistics",
      "config": "# Query given DNS server and gives statistics\n[[inputs.dns_query]]\n  # alias=\"dns_query\"\n  ## servers to query\n  servers = [\"8.8.8.8\"]\n\n  ## Network is the network protocol name.\n  # network = \"udp\"\n\n  ## Domains or subdomains to query.\n  # domains = [\".\"]\n\n  ## Query record type.\n  ## Posible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\n  # record_type = \"A\"\n\n  ## Dns server port.\n  # port = 53\n\n  ## Query timeout in seconds.\n  # timeout = 2\n\n"
    },
    {
      "type": "input",
      "name": "linux_sysctl_fs",
      "description": "Provides Linux sysctl fs metrics",
      "config": "# Provides Linux sysctl fs metrics\n[[inputs.linux_sysctl_fs]]\n  # alias=\"linux_sysctl_fs\"\n"
    },
    {
      "type": "input",
      "name": "netstat",
      "description": "Read TCP metrics such as established, time wait and sockets counts.",
      "config": "# Read TCP metrics such as established, time wait and sockets counts.\n[[inputs.netstat]]\n  # alias=\"netstat\"\n"
    },
    {
      "type": "input",
      "name": "postfix",
      "description": "Measure postfix queue statistics",
      "config": "# Measure postfix queue statistics\n[[inputs.postfix]]\n  # alias=\"postfix\"\n  ## Postfix queue directory. If not provided, telegraf will try to use\n  ## 'postconf -h queue_directory' to determine it.\n  # queue_directory = \"/var/spool/postfix\"\n\n"
    },
    {
      "type": "input",
      "name": "rethinkdb",
      "description": "Read metrics from one or many RethinkDB servers",
      "config": "# Read metrics from one or many RethinkDB servers\n[[inputs.rethinkdb]]\n  # alias=\"rethinkdb\"\n  ## An array of URI to gather stats about. Specify an ip or hostname\n  ## with optional port add password. ie,\n  ##   rethinkdb://user:auth_key@10.10.3.30:28105,\n  ##   rethinkdb://10.10.3.33:18832,\n  ##   10.0.0.1:10000, etc.\n  servers = [\"127.0.0.1:28015\"]\n  ##\n  ## If you use actual rethinkdb of \u003e 2.3.0 with username/password authorization,\n  ## protocol have to be named \"rethinkdb2\" - it will use 1_0 H.\n  # servers = [\"rethinkdb2://username:password@127.0.0.1:28015\"]\n  ##\n  ## If you use older versions of rethinkdb (\u003c2.2) with auth_key, protocol\n  ## have to be named \"rethinkdb\".\n  # servers = [\"rethinkdb://username:auth_key@127.0.0.1:28015\"]\n\n"
    },
    {
      "type": "input",
      "name": "bond",
      "description": "Collect bond interface status, slaves statuses and failures count",
      "config": "# Collect bond interface status, slaves statuses and failures count\n[[inputs.bond]]\n  # alias=\"bond\"\n  ## Sets 'proc' directory path\n  ## If not specified, then default is /proc\n  # host_proc = \"/proc\"\n\n  ## By default, telegraf gather stats for all bond interfaces\n  ## Setting interfaces will restrict the stats to the specified\n  ## bond interfaces.\n  # bond_interfaces = [\"bond0\"]\n\n"
    },
    {
      "type": "input",
      "name": "couchdb",
      "description": "Read CouchDB Stats from one or more servers",
      "config": "# Read CouchDB Stats from one or more servers\n[[inputs.couchdb]]\n  # alias=\"couchdb\"\n  ## Works with CouchDB stats endpoints out of the box\n  ## Multiple Hosts from which to read CouchDB stats:\n  hosts = [\"http://localhost:8086/_stats\"]\n\n  ## Use HTTP Basic Authentication.\n  # basic_username = \"telegraf\"\n  # basic_password = \"p@ssw0rd\"\n\n"
    },
    {
      "type": "input",
      "name": "kibana",
      "description": "Read status information from one or more Kibana servers",
      "config": "# Read status information from one or more Kibana servers\n[[inputs.kibana]]\n  # alias=\"kibana\"\n  ## specify a list of one or more Kibana servers\n  servers = [\"http://localhost:5601\"]\n\n  ## Timeout for HTTP requests\n  timeout = \"5s\"\n\n  ## HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "sensors",
      "description": "Monitor sensors, requires lm-sensors package",
      "config": "# Monitor sensors, requires lm-sensors package\n[[inputs.sensors]]\n  # alias=\"sensors\"\n  ## Remove numbers from field names.\n  ## If true, a field name like 'temp1_input' will be changed to 'temp_input'.\n  # remove_numbers = true\n\n  ## Timeout is the maximum amount of time that the sensors command can run.\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "synproxy",
      "description": "Get synproxy counter statistics from procfs",
      "config": "# Get synproxy counter statistics from procfs\n[[inputs.synproxy]]\n  # alias=\"synproxy\"\n"
    },
    {
      "type": "input",
      "name": "prometheus",
      "description": "Read metrics from one or many prometheus clients",
      "config": "# Read metrics from one or many prometheus clients\n[[inputs.prometheus]]\n  # alias=\"prometheus\"\n  ## An array of urls to scrape metrics from.\n  urls = [\"http://localhost:9100/metrics\"]\n\n  ## Metric version controls the mapping from Prometheus metrics into\n  ## Telegraf metrics.  When using the prometheus_client output, use the same\n  ## value in both plugins to ensure metrics are round-tripped without\n  ## modification.\n  ##\n  ##   example: metric_version = 1; deprecated in 1.13\n  ##            metric_version = 2; recommended version\n  # metric_version = 1\n\n  ## Url tag name (tag containing scrapped url. optional, default is \"url\")\n  # url_tag = \"scrapeUrl\"\n\n  ## An array of Kubernetes services to scrape metrics from.\n  # kubernetes_services = [\"http://my-service-dns.my-namespace:9100/metrics\"]\n\n  ## Kubernetes config file to create client from.\n  # kube_config = \"/path/to/kubernetes.config\"\n\n  ## Scrape Kubernetes pods for the following prometheus annotations:\n  ## - prometheus.io/scrape: Enable scraping for this pod\n  ## - prometheus.io/scheme: If the metrics endpoint is secured then you will need to\n  ##     set this to 'https' \u0026 most likely set the tls config.\n  ## - prometheus.io/path: If the metrics path is not /metrics, define it with this annotation.\n  ## - prometheus.io/port: If port is not 9102 use this annotation\n  # monitor_kubernetes_pods = true\n  ## Restricts Kubernetes monitoring to a single namespace\n  ##   ex: monitor_kubernetes_pods_namespace = \"default\"\n  # monitor_kubernetes_pods_namespace = \"\"\n\n  ## Use bearer token for authorization. ('bearer_token' takes priority)\n  # bearer_token = \"/path/to/bearer/token\"\n  ## OR\n  # bearer_token_string = \"abc_123\"\n\n  ## HTTP Basic Authentication username and password. ('bearer_token' and\n  ## 'bearer_token_string' take priority)\n  # username = \"\"\n  # password = \"\"\n\n  ## Specify timeout duration for slower prometheus clients (default is 3s)\n  # response_timeout = \"3s\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "cisco_telemetry_mdt",
      "description": "Cisco model-driven telemetry (MDT) input plugin for IOS XR, IOS XE and NX-OS platforms",
      "config": "# Cisco model-driven telemetry (MDT) input plugin for IOS XR, IOS XE and NX-OS platforms\n[[inputs.cisco_telemetry_mdt]]\n  # alias=\"cisco_telemetry_mdt\"\n ## Telemetry transport can be \"tcp\" or \"grpc\".  TLS is only supported when\n ## using the grpc transport.\n transport = \"grpc\"\n\n ## Address and port to host telemetry listener\n service_address = \":57000\"\n\n ## Enable TLS; grpc transport only.\n # tls_cert = \"/etc/telegraf/cert.pem\"\n # tls_key = \"/etc/telegraf/key.pem\"\n\n ## Enable TLS client authentication and define allowed CA certificates; grpc\n ##  transport only.\n # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n ## Define (for certain nested telemetry measurements with embedded tags) which fields are tags\n # embedded_tags = [\"Cisco-IOS-XR-qos-ma-oper:qos/interface-table/interface/input/service-policy-names/service-policy-instance/statistics/class-stats/class-name\"]\n\n ## Define aliases to map telemetry encoding paths to simple measurement names\n [inputs.cisco_telemetry_mdt.aliases]\n   ifstats = \"ietf-interfaces:interfaces-state/interface/statistics\"\n\n"
    },
    {
      "type": "input",
      "name": "fail2ban",
      "description": "Read metrics from fail2ban.",
      "config": "# Read metrics from fail2ban.\n[[inputs.fail2ban]]\n  # alias=\"fail2ban\"\n  ## Use sudo to run fail2ban-client\n  use_sudo = false\n\n"
    },
    {
      "type": "input",
      "name": "nsq_consumer",
      "description": "Read NSQ topic for metrics.",
      "config": "# Read NSQ topic for metrics.\n[[inputs.nsq_consumer]]\n  # alias=\"nsq_consumer\"\n  ## Server option still works but is deprecated, we just prepend it to the nsqd array.\n  # server = \"localhost:4150\"\n\n  ## An array representing the NSQD TCP HTTP Endpoints\n  nsqd = [\"localhost:4150\"]\n\n  ## An array representing the NSQLookupd HTTP Endpoints\n  nsqlookupd = [\"localhost:4161\"]\n  topic = \"telegraf\"\n  channel = \"consumer\"\n  max_in_flight = 100\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "opensmtpd",
      "description": "A plugin to collect stats from Opensmtpd - a validating, recursive, and caching DNS resolver ",
      "config": "# A plugin to collect stats from Opensmtpd - a validating, recursive, and caching DNS resolver \n[[inputs.opensmtpd]]\n  # alias=\"opensmtpd\"\n  ## If running as a restricted user you can prepend sudo for additional access:\n  #use_sudo = false\n\n  ## The default location of the smtpctl binary can be overridden with:\n  binary = \"/usr/sbin/smtpctl\"\n\n  ## The default timeout of 1000ms can be overriden with (in milliseconds):\n  timeout = 1000\n\n"
    },
    {
      "type": "input",
      "name": "postgresql",
      "description": "Read metrics from one or many postgresql servers",
      "config": "# Read metrics from one or many postgresql servers\n[[inputs.postgresql]]\n  # alias=\"postgresql\"\n  ## specify address via a url matching:\n  ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n  ##       ?sslmode=[disable|verify-ca|verify-full]\n  ## or a simple string:\n  ##   host=localhost user=pqotest password=... sslmode=... dbname=app_production\n  ##\n  ## All connection parameters are optional.\n  ##\n  ## Without the dbname parameter, the driver will default to a database\n  ## with the same name as the user. This dbname is just for instantiating a\n  ## connection with the server and doesn't restrict the databases we are trying\n  ## to grab metrics for.\n  ##\n  address = \"host=localhost user=postgres sslmode=disable\"\n  ## A custom name for the database that will be used as the \"server\" tag in the\n  ## measurement output. If not specified, a default one generated from\n  ## the connection address is used.\n  # outputaddress = \"db01\"\n\n  ## connection configuration.\n  ## maxlifetime - specify the maximum lifetime of a connection.\n  ## default is forever (0s)\n  max_lifetime = \"0s\"\n\n  ## A  list of databases to explicitly ignore.  If not specified, metrics for all\n  ## databases are gathered.  Do NOT use with the 'databases' option.\n  # ignored_databases = [\"postgres\", \"template0\", \"template1\"]\n\n  ## A list of databases to pull metrics about. If not specified, metrics for all\n  ## databases are gathered.  Do NOT use with the 'ignored_databases' option.\n  # databases = [\"app_production\", \"testing\"]\n\n"
    },
    {
      "type": "input",
      "name": "apcupsd",
      "description": "Monitor APC UPSes connected to apcupsd",
      "config": "# Monitor APC UPSes connected to apcupsd\n[[inputs.apcupsd]]\n  # alias=\"apcupsd\"\n  # A list of running apcupsd server to connect to.\n  # If not provided will default to tcp://127.0.0.1:3551\n  servers = [\"tcp://127.0.0.1:3551\"]\n\n  ## Timeout for dialing server.\n  timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "phpfpm",
      "description": "Read metrics of phpfpm, via HTTP status page or socket",
      "config": "# Read metrics of phpfpm, via HTTP status page or socket\n[[inputs.phpfpm]]\n  # alias=\"phpfpm\"\n  ## An array of addresses to gather stats about. Specify an ip or hostname\n  ## with optional port and path\n  ##\n  ## Plugin can be configured in three modes (either can be used):\n  ##   - http: the URL must start with http:// or https://, ie:\n  ##       \"http://localhost/status\"\n  ##       \"http://192.168.130.1/status?full\"\n  ##\n  ##   - unixsocket: path to fpm socket, ie:\n  ##       \"/var/run/php5-fpm.sock\"\n  ##      or using a custom fpm status path:\n  ##       \"/var/run/php5-fpm.sock:fpm-custom-status-path\"\n  ##\n  ##   - fcgi: the URL must start with fcgi:// or cgi://, and port must be present, ie:\n  ##       \"fcgi://10.0.0.12:9000/status\"\n  ##       \"cgi://10.0.10.12:9001/status\"\n  ##\n  ## Example of multiple gathering from local socket and remote host\n  ## urls = [\"http://192.168.1.20/status\", \"/tmp/fpm.sock\"]\n  urls = [\"http://localhost/status\"]\n\n  ## Duration allowed to complete HTTP requests.\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "smart",
      "description": "Read metrics from storage devices supporting S.M.A.R.T.",
      "config": "# Read metrics from storage devices supporting S.M.A.R.T.\n[[inputs.smart]]\n  # alias=\"smart\"\n  ## Optionally specify the path to the smartctl executable\n  # path = \"/usr/bin/smartctl\"\n\n  ## On most platforms smartctl requires root access.\n  ## Setting 'use_sudo' to true will make use of sudo to run smartctl.\n  ## Sudo must be configured to to allow the telegraf user to run smartctl\n  ## without a password.\n  # use_sudo = false\n\n  ## Skip checking disks in this power mode. Defaults to\n  ## \"standby\" to not wake up disks that have stoped rotating.\n  ## See --nocheck in the man pages for smartctl.\n  ## smartctl version 5.41 and 5.42 have faulty detection of\n  ## power mode and might require changing this value to\n  ## \"never\" depending on your disks.\n  # nocheck = \"standby\"\n\n  ## Gather all returned S.M.A.R.T. attribute metrics and the detailed\n  ## information from each drive into the 'smart_attribute' measurement.\n  # attributes = false\n\n  ## Optionally specify devices to exclude from reporting.\n  # excludes = [ \"/dev/pass6\" ]\n\n  ## Optionally specify devices and device type, if unset\n  ## a scan (smartctl --scan) for S.M.A.R.T. devices will\n  ## done and all found will be included except for the\n  ## excluded in excludes.\n  # devices = [ \"/dev/ada0 -d atacam\" ]\n\n  ## Timeout for the smartctl command to complete.\n  # timeout = \"30s\"\n\n"
    },
    {
      "type": "input",
      "name": "swap",
      "description": "Read metrics about swap memory usage",
      "config": "# Read metrics about swap memory usage\n[[inputs.swap]]\n  # alias=\"swap\"\n"
    },
    {
      "type": "input",
      "name": "zookeeper",
      "description": "Reads 'mntr' stats from one or many zookeeper servers",
      "config": "# Reads 'mntr' stats from one or many zookeeper servers\n[[inputs.zookeeper]]\n  # alias=\"zookeeper\"\n  ## An array of address to gather stats about. Specify an ip or hostname\n  ## with port. ie localhost:2181, 10.0.0.1:2181, etc.\n\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no port is specified, 2181 is used\n  servers = [\":2181\"]\n\n  ## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # enable_tls = true\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## If false, skip chain \u0026 host verification\n  # insecure_skip_verify = true\n\n"
    },
    {
      "type": "input",
      "name": "disque",
      "description": "Read metrics from one or many disque servers",
      "config": "# Read metrics from one or many disque servers\n[[inputs.disque]]\n  # alias=\"disque\"\n  ## An array of URI to gather stats about. Specify an ip or hostname\n  ## with optional port and password.\n  ## ie disque://localhost, disque://10.10.3.33:18832, 10.0.0.1:10000, etc.\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"localhost\"]\n\n"
    },
    {
      "type": "input",
      "name": "hddtemp",
      "description": "Monitor disks' temperatures using hddtemp",
      "config": "# Monitor disks' temperatures using hddtemp\n[[inputs.hddtemp]]\n  # alias=\"hddtemp\"\n  ## By default, telegraf gathers temps data from all disks detected by the\n  ## hddtemp.\n  ##\n  ## Only collect temps from the selected disks.\n  ##\n  ## A * as the device name will return the temperature values of all disks.\n  ##\n  # address = \"127.0.0.1:7634\"\n  # devices = [\"sda\", \"*\"]\n\n"
    },
    {
      "type": "input",
      "name": "interrupts",
      "description": "This plugin gathers interrupts data from /proc/interrupts and /proc/softirqs.",
      "config": "# This plugin gathers interrupts data from /proc/interrupts and /proc/softirqs.\n[[inputs.interrupts]]\n  # alias=\"interrupts\"\n  ## When set to true, cpu metrics are tagged with the cpu.  Otherwise cpu is\n  ## stored as a field.\n  ##\n  ## The default is false for backwards compatibility, and will be changed to\n  ## true in a future version.  It is recommended to set to true on new\n  ## deployments.\n  # cpu_as_tag = false\n\n  ## To filter which IRQs to collect, make use of tagpass / tagdrop, i.e.\n  # [inputs.interrupts.tagdrop]\n  #   irq = [ \"NET_RX\", \"TASKLET\" ]\n\n"
    },
    {
      "type": "input",
      "name": "jenkins",
      "description": "Read jobs and cluster metrics from Jenkins instances",
      "config": "# Read jobs and cluster metrics from Jenkins instances\n[[inputs.jenkins]]\n  # alias=\"jenkins\"\n  ## The Jenkins URL in the format \"schema://host:port\"\n  url = \"http://my-jenkins-instance:8080\"\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Set response_timeout\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use SSL but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional Max Job Build Age filter\n  ## Default 1 hour, ignore builds older than max_build_age\n  # max_build_age = \"1h\"\n\n  ## Optional Sub Job Depth filter\n  ## Jenkins can have unlimited layer of sub jobs\n  ## This config will limit the layers of pulling, default value 0 means\n  ## unlimited pulling until no more sub jobs\n  # max_subjob_depth = 0\n\n  ## Optional Sub Job Per Layer\n  ## In workflow-multibranch-plugin, each branch will be created as a sub job.\n  ## This config will limit to call only the lasted branches in each layer, \n  ## empty will use default value 10\n  # max_subjob_per_layer = 10\n\n  ## Jobs to exclude from gathering\n  # job_exclude = [ \"job1\", \"job2/subjob1/subjob2\", \"job3/*\"]\n\n  ## Nodes to exclude from gathering\n  # node_exclude = [ \"node1\", \"node2\" ]\n\n  ## Worker pool for jenkins plugin only\n  ## Empty this field will use default value 5\n  # max_connections = 5\n\n"
    },
    {
      "type": "input",
      "name": "nvidia_smi",
      "description": "Pulls statistics from nvidia GPUs attached to the host",
      "config": "# Pulls statistics from nvidia GPUs attached to the host\n[[inputs.nvidia_smi]]\n  # alias=\"nvidia_smi\"\n  ## Optional: path to nvidia-smi binary, defaults to $PATH via exec.LookPath\n  # bin_path = \"/usr/bin/nvidia-smi\"\n\n  ## Optional: timeout for GPU polling\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "ceph",
      "description": "Collects performance metrics from the MON and OSD nodes in a Ceph storage cluster.",
      "config": "# Collects performance metrics from the MON and OSD nodes in a Ceph storage cluster.\n[[inputs.ceph]]\n  # alias=\"ceph\"\n  ## This is the recommended interval to poll.  Too frequent and you will lose\n  ## data points due to timeouts during rebalancing and recovery\n  interval = '1m'\n\n  ## All configuration values are optional, defaults are shown below\n\n  ## location of ceph binary\n  ceph_binary = \"/usr/bin/ceph\"\n\n  ## directory in which to look for socket files\n  socket_dir = \"/var/run/ceph\"\n\n  ## prefix of MON and OSD socket files, used to determine socket type\n  mon_prefix = \"ceph-mon\"\n  osd_prefix = \"ceph-osd\"\n\n  ## suffix used to identify socket files\n  socket_suffix = \"asok\"\n\n  ## Ceph user to authenticate as\n  ceph_user = \"client.admin\"\n\n  ## Ceph configuration to use to locate the cluster\n  ceph_config = \"/etc/ceph/ceph.conf\"\n\n  ## Whether to gather statistics via the admin socket\n  gather_admin_socket_stats = true\n\n  ## Whether to gather statistics via ceph commands\n  gather_cluster_stats = false\n\n"
    },
    {
      "type": "input",
      "name": "dmcache",
      "description": "Provide a native collection for dmsetup based statistics for dm-cache",
      "config": "# Provide a native collection for dmsetup based statistics for dm-cache\n[[inputs.dmcache]]\n  # alias=\"dmcache\"\n  ## Whether to report per-device stats or not\n  per_device = true\n\n"
    },
    {
      "type": "input",
      "name": "net_response",
      "description": "Collect response time of a TCP or UDP connection",
      "config": "# Collect response time of a TCP or UDP connection\n[[inputs.net_response]]\n  # alias=\"net_response\"\n  ## Protocol, must be \"tcp\" or \"udp\"\n  ## NOTE: because the \"udp\" protocol does not respond to requests, it requires\n  ## a send/expect string pair (see below).\n  protocol = \"tcp\"\n  ## Server address (default localhost)\n  address = \"localhost:80\"\n\n  ## Set timeout\n  # timeout = \"1s\"\n\n  ## Set read timeout (only used if expecting a response)\n  # read_timeout = \"1s\"\n\n  ## The following options are required for UDP checks. For TCP, they are\n  ## optional. The plugin will send the given string to the server and then\n  ## expect to receive the given 'expect' string back.\n  ## string sent to the server\n  # send = \"ssh\"\n  ## expected string in answer\n  # expect = \"ssh\"\n\n  ## Uncomment to remove deprecated fields\n  # fielddrop = [\"result_type\", \"string_found\"]\n\n"
    },
    {
      "type": "input",
      "name": "puppetagent",
      "description": "Reads last_run_summary.yaml file and converts to measurements",
      "config": "# Reads last_run_summary.yaml file and converts to measurements\n[[inputs.puppetagent]]\n  # alias=\"puppetagent\"\n  ## Location of puppet last run summary file\n  location = \"/var/lib/puppet/state/last_run_summary.yaml\"\n\n"
    },
    {
      "type": "input",
      "name": "zfs",
      "description": "Read metrics of ZFS from arcstats, zfetchstats, vdev_cache_stats, and pools",
      "config": "# Read metrics of ZFS from arcstats, zfetchstats, vdev_cache_stats, and pools\n[[inputs.zfs]]\n  # alias=\"zfs\"\n  ## ZFS kstat path. Ignored on FreeBSD\n  ## If not specified, then default is:\n  # kstatPath = \"/proc/spl/kstat/zfs\"\n\n  ## By default, telegraf gather all zfs stats\n  ## If not specified, then default is:\n  # kstatMetrics = [\"arcstats\", \"zfetchstats\", \"vdev_cache_stats\"]\n  ## For Linux, the default is:\n  # kstatMetrics = [\"abdstats\", \"arcstats\", \"dnodestats\", \"dbufcachestats\",\n  #   \"dmu_tx\", \"fm\", \"vdev_mirror_stats\", \"zfetchstats\", \"zil\"]\n  ## By default, don't gather zpool stats\n  # poolMetrics = false\n\n"
    },
    {
      "type": "input",
      "name": "aerospike",
      "description": "Read stats from aerospike server(s)",
      "config": "# Read stats from aerospike server(s)\n[[inputs.aerospike]]\n  # alias=\"aerospike\"\n  ## Aerospike servers to connect to (with port)\n  ## This plugin will query all namespaces the aerospike\n  ## server has configured and get stats for them.\n  servers = [\"localhost:3000\"]\n\n  # username = \"telegraf\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # enable_tls = false\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## If false, skip chain \u0026 host verification\n  # insecure_skip_verify = true\n \n"
    },
    {
      "type": "input",
      "name": "exec",
      "description": "Read metrics from one or more commands that can output to stdout",
      "config": "# Read metrics from one or more commands that can output to stdout\n[[inputs.exec]]\n  # alias=\"exec\"\n  ## Commands array\n  commands = [\n    \"/tmp/test.sh\",\n    \"/usr/bin/mycollector --foo=bar\",\n    \"/tmp/collect_*.sh\"\n  ]\n\n  ## Timeout for each command to complete.\n  timeout = \"5s\"\n\n  ## measurement name suffix (for separating different commands)\n  name_suffix = \"_mycollector\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "influxdb",
      "description": "Read InfluxDB-formatted JSON metrics from one or more HTTP endpoints",
      "config": "# Read InfluxDB-formatted JSON metrics from one or more HTTP endpoints\n[[inputs.influxdb]]\n  # alias=\"influxdb\"\n  ## Works with InfluxDB debug endpoints out of the box,\n  ## but other services can use this format too.\n  ## See the influxdb plugin's README for more details.\n\n  ## Multiple URLs from which to read InfluxDB-formatted JSON\n  ## Default is \"http://localhost:8086/debug/vars\".\n  urls = [\n    \"http://localhost:8086/debug/vars\"\n  ]\n\n  ## Username and password to send using HTTP Basic Authentication.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## http request \u0026 header timeout\n  timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "nginx",
      "description": "Read Nginx's basic status information (ngx_http_stub_status_module)",
      "config": "# Read Nginx's basic status information (ngx_http_stub_status_module)\n[[inputs.nginx]]\n  # alias=\"nginx\"\n  # An array of Nginx stub_status URI to gather stats.\n  urls = [\"http://localhost/server_status\"]\n\n  ## Optional TLS Config\n  tls_ca = \"/etc/telegraf/ca.pem\"\n  tls_cert = \"/etc/telegraf/cert.cer\"\n  tls_key = \"/etc/telegraf/key.key\"\n  ## Use TLS but skip chain \u0026 host verification\n  insecure_skip_verify = false\n\n  # HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "ping",
      "description": "Ping given url(s) and return statistics",
      "config": "# Ping given url(s) and return statistics\n[[inputs.ping]]\n  # alias=\"ping\"\n  ## Hosts to send ping packets to.\n  urls = [\"example.org\"]\n\n  ## Method used for sending pings, can be either \"exec\" or \"native\".  When set\n  ## to \"exec\" the systems ping command will be executed.  When set to \"native\"\n  ## the plugin will send pings directly.\n  ##\n  ## While the default is \"exec\" for backwards compatibility, new deployments\n  ## are encouraged to use the \"native\" method for improved compatibility and\n  ## performance.\n  # method = \"exec\"\n\n  ## Number of ping packets to send per interval.  Corresponds to the \"-c\"\n  ## option of the ping command.\n  # count = 1\n\n  ## Time to wait between sending ping packets in seconds.  Operates like the\n  ## \"-i\" option of the ping command.\n  # ping_interval = 1.0\n\n  ## If set, the time to wait for a ping response in seconds.  Operates like\n  ## the \"-W\" option of the ping command.\n  # timeout = 1.0\n\n  ## If set, the total ping deadline, in seconds.  Operates like the -w option\n  ## of the ping command.\n  # deadline = 10\n\n  ## Interface or source address to send ping from.  Operates like the -I or -S\n  ## option of the ping command.\n  # interface = \"\"\n\n  ## Specify the ping executable binary.\n  # binary = \"ping\"\n\n  ## Arguments for ping command. When arguments is not empty, the command from\n  ## the binary option will be used and other options (ping_interval, timeout,\n  ## etc) will be ignored.\n  # arguments = [\"-c\", \"3\"]\n\n  ## Use only IPv6 addresses when resolving a hostname.\n  # ipv6 = false\n\n"
    },
    {
      "type": "input",
      "name": "stackdriver",
      "description": "Gather timeseries from Google Cloud Platform v3 monitoring API",
      "config": "# Gather timeseries from Google Cloud Platform v3 monitoring API\n[[inputs.stackdriver]]\n  # alias=\"stackdriver\"\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Include timeseries that start with the given metric type.\n  metric_type_prefix_include = [\n    \"compute.googleapis.com/\",\n  ]\n\n  ## Exclude timeseries that start with the given metric type.\n  # metric_type_prefix_exclude = []\n\n  ## Many metrics are updated once per minute; it is recommended to override\n  ## the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  # \t\"ALIGN_PERCENTILE_99\",\n  # \t\"ALIGN_PERCENTILE_95\",\n  # \t\"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= \u003cresource_labels\u003e {AND \u003cmetric_labels\u003e}\n  ##   resource_labels ::= \u003cresource_labels\u003e {OR \u003cresource_label\u003e}\n  ##   metric_labels ::= \u003cmetric_labels\u003e {OR \u003cmetric_label\u003e}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.\u003ckey\u003e = \u003cvalue\u003e\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.\u003ckey\u003e = \u003cvalue\u003e\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #  \t key = \"device_name\"\n  #  \t value = 'one_of(\"sda\", \"sdb\")'\n\n"
    },
    {
      "type": "input",
      "name": "syslog",
      "description": "Accepts syslog messages following RFC5424 format with transports as per RFC5426, RFC5425, or RFC6587",
      "config": "# Accepts syslog messages following RFC5424 format with transports as per RFC5426, RFC5425, or RFC6587\n[[inputs.syslog]]\n  # alias=\"syslog\"\n  ## Specify an ip or hostname with port - eg., tcp://localhost:6514, tcp://10.0.0.1:6514\n  ## Protocol, address and port to host the syslog receiver.\n  ## If no host is specified, then localhost is used.\n  ## If no port is specified, 6514 is used (RFC5425#section-4.1).\n  server = \"tcp://:6514\"\n\n  ## TLS Config\n  # tls_allowed_cacerts = [\"/etc/telegraf/ca.pem\"]\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Period between keep alive probes.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  ## Only applies to stream sockets (e.g. TCP).\n  # keep_alive_period = \"5m\"\n\n  ## Maximum number of concurrent connections (default = 0).\n  ## 0 means unlimited.\n  ## Only applies to stream sockets (e.g. TCP).\n  # max_connections = 1024\n\n  ## Read timeout is the maximum time allowed for reading a single message (default = 5s).\n  ## 0 means unlimited.\n  # read_timeout = \"5s\"\n\n  ## The framing technique with which it is expected that messages are transported (default = \"octet-counting\").\n  ## Whether the messages come using the octect-counting (RFC5425#section-4.3.1, RFC6587#section-3.4.1),\n  ## or the non-transparent framing technique (RFC6587#section-3.4.2).\n  ## Must be one of \"octet-counting\", \"non-transparent\".\n  # framing = \"octet-counting\"\n\n  ## The trailer to be expected in case of non-trasparent framing (default = \"LF\").\n  ## Must be one of \"LF\", or \"NUL\".\n  # trailer = \"LF\"\n\n  ## Whether to parse in best effort mode or not (default = false).\n  ## By default best effort parsing is off.\n  # best_effort = false\n\n  ## Character to prepend to SD-PARAMs (default = \"_\").\n  ## A syslog message can contain multiple parameters and multiple identifiers within structured data section.\n  ## Eg., [id1 name1=\"val1\" name2=\"val2\"][id2 name1=\"val1\" nameA=\"valA\"]\n  ## For each combination a field is created.\n  ## Its name is created concatenating identifier, sdparam_separator, and parameter name.\n  # sdparam_separator = \"_\"\n\n"
    },
    {
      "type": "input",
      "name": "activemq",
      "description": "Gather ActiveMQ metrics",
      "config": "# Gather ActiveMQ metrics\n[[inputs.activemq]]\n  # alias=\"activemq\"\n  ## ActiveMQ WebConsole URL\n  url = \"http://127.0.0.1:8161\"\n\n  ## Required ActiveMQ Endpoint\n  ##   deprecated in 1.11; use the url option\n  # server = \"127.0.0.1\"\n  # port = 8161\n\n  ## Credentials for basic HTTP authentication\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Required ActiveMQ webadmin root path\n  # webadmin = \"admin\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n  \n"
    },
    {
      "type": "input",
      "name": "bind",
      "description": "Read BIND nameserver XML statistics",
      "config": "# Read BIND nameserver XML statistics\n[[inputs.bind]]\n  # alias=\"bind\"\n  ## An array of BIND XML statistics URI to gather stats.\n  ## Default is \"http://localhost:8053/xml/v3\".\n  # urls = [\"http://localhost:8053/xml/v3\"]\n  # gather_memory_contexts = false\n  # gather_views = false\n\n"
    },
    {
      "type": "input",
      "name": "httpjson",
      "description": "Read flattened metrics from one or more JSON HTTP endpoints",
      "config": "# Read flattened metrics from one or more JSON HTTP endpoints\n[[inputs.httpjson]]\n  # alias=\"httpjson\"\n  ## NOTE This plugin only reads numerical measurements, strings and booleans\n  ## will be ignored.\n\n  ## Name for the service being polled.  Will be appended to the name of the\n  ## measurement e.g. httpjson_webserver_stats\n  ##\n  ## Deprecated (1.3.0): Use name_override, name_suffix, name_prefix instead.\n  name = \"webserver_stats\"\n\n  ## URL of each server in the service's cluster\n  servers = [\n    \"http://localhost:8086/stats/\",\n    \"http://localhost:9998/stats/\",\n  ]\n  ## Set response_timeout (default 5 seconds)\n  response_timeout = \"5s\"\n\n  ## HTTP method to use: GET or POST (case-sensitive)\n  method = \"GET\"\n\n  ## List of tag names to extract from top-level of JSON server response\n  # tag_keys = [\n  #   \"my_tag_1\",\n  #   \"my_tag_2\"\n  # ]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## HTTP parameters (all values must be strings).  For \"GET\" requests, data\n  ## will be included in the query.  For \"POST\" requests, data will be included\n  ## in the request body as \"x-www-form-urlencoded\".\n  # [inputs.httpjson.parameters]\n  #   event_type = \"cpu_spike\"\n  #   threshold = \"0.75\"\n\n  ## HTTP Headers (all values must be strings)\n  # [inputs.httpjson.headers]\n  #   X-Auth-Token = \"my-xauth-token\"\n  #   apiVersion = \"v1\"\n\n"
    },
    {
      "type": "input",
      "name": "kapacitor",
      "description": "Read Kapacitor-formatted JSON metrics from one or more HTTP endpoints",
      "config": "# Read Kapacitor-formatted JSON metrics from one or more HTTP endpoints\n[[inputs.kapacitor]]\n  # alias=\"kapacitor\"\n  ## Multiple URLs from which to read Kapacitor-formatted JSON\n  ## Default is \"http://localhost:9092/kapacitor/v1/debug/vars\".\n  urls = [\n    \"http://localhost:9092/kapacitor/v1/debug/vars\"\n  ]\n\n  ## Time limit for http requests\n  timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "multifile",
      "description": "Aggregates the contents of multiple files into a single point",
      "config": "# Aggregates the contents of multiple files into a single point\n[[inputs.multifile]]\n  # alias=\"multifile\"\n  ## Base directory where telegraf will look for files.\n  ## Omit this option to use absolute paths.\n  base_dir = \"/sys/bus/i2c/devices/1-0076/iio:device0\"\n\n  ## If true, Telegraf discard all data when a single file can't be read.\n  ## Else, Telegraf omits the field generated from this file.\n  # fail_early = true\n\n  ## Files to parse each interval.\n  [[inputs.multifile.file]]\n    file = \"in_pressure_input\"\n    dest = \"pressure\"\n    conversion = \"float\"\n  [[inputs.multifile.file]]\n    file = \"in_temp_input\"\n    dest = \"temperature\"\n    conversion = \"float(3)\"\n  [[inputs.multifile.file]]\n    file = \"in_humidityrelative_input\"\n    dest = \"humidityrelative\"\n    conversion = \"float(3)\"\n\n"
    },
    {
      "type": "input",
      "name": "raindrops",
      "description": "Read raindrops stats (raindrops - real-time stats for preforking Rack servers)",
      "config": "# Read raindrops stats (raindrops - real-time stats for preforking Rack servers)\n[[inputs.raindrops]]\n  # alias=\"raindrops\"\n  ## An array of raindrops middleware URI to gather stats.\n  urls = [\"http://localhost:8080/_raindrops\"]\n\n"
    },
    {
      "type": "input",
      "name": "riak",
      "description": "Read metrics one or many Riak servers",
      "config": "# Read metrics one or many Riak servers\n[[inputs.riak]]\n  # alias=\"riak\"\n  # Specify a list of one or more riak http servers\n  servers = [\"http://localhost:8098\"]\n\n"
    },
    {
      "type": "input",
      "name": "socket_listener",
      "description": "Generic socket listener capable of handling multiple socket types.",
      "config": "# Generic socket listener capable of handling multiple socket types.\n[[inputs.socket_listener]]\n  # alias=\"socket_listener\"\n  ## URL to listen on\n  # service_address = \"tcp://:8094\"\n  # service_address = \"tcp://127.0.0.1:http\"\n  # service_address = \"tcp4://:8094\"\n  # service_address = \"tcp6://:8094\"\n  # service_address = \"tcp6://[2001:db8::1]:8094\"\n  # service_address = \"udp://:8094\"\n  # service_address = \"udp4://:8094\"\n  # service_address = \"udp6://:8094\"\n  # service_address = \"unix:///tmp/telegraf.sock\"\n  # service_address = \"unixgram:///tmp/telegraf.sock\"\n\n  ## Change the file mode bits on unix sockets.  These permissions may not be\n  ## respected by some platforms, to safely restrict write permissions it is best\n  ## to place the socket into a directory that has previously been created\n  ## with the desired permissions.\n  ##   ex: socket_mode = \"777\"\n  # socket_mode = \"\"\n\n  ## Maximum number of concurrent connections.\n  ## Only applies to stream sockets (e.g. TCP).\n  ## 0 (default) is unlimited.\n  # max_connections = 1024\n\n  ## Read timeout.\n  ## Only applies to stream sockets (e.g. TCP).\n  ## 0 (default) is unlimited.\n  # read_timeout = \"30s\"\n\n  ## Optional TLS configuration.\n  ## Only applies to stream sockets (e.g. TCP).\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key  = \"/etc/telegraf/key.pem\"\n  ## Enables client authentication if set.\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Maximum socket buffer size (in bytes when no unit specified).\n  ## For stream sockets, once the buffer fills up, the sender will start backing up.\n  ## For datagram sockets, once the buffer fills up, metrics will start dropping.\n  ## Defaults to the OS default.\n  # read_buffer_size = \"64KiB\"\n\n  ## Period between keep alive probes.\n  ## Only applies to TCP sockets.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  # keep_alive_period = \"5m\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"influx\"\n\n  ## Content encoding for message payloads, can be set to \"gzip\" to or\n  ## \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n"
    },
    {
      "type": "input",
      "name": "cisco_telemetry_gnmi",
      "description": "Cisco GNMI telemetry input plugin based on GNMI telemetry data produced in IOS XR",
      "config": "# Cisco GNMI telemetry input plugin based on GNMI telemetry data produced in IOS XR\n[[inputs.cisco_telemetry_gnmi]]\n  # alias=\"cisco_telemetry_gnmi\"\n ## Address and port of the GNMI GRPC server\n addresses = [\"10.49.234.114:57777\"]\n\n ## define credentials\n username = \"cisco\"\n password = \"cisco\"\n\n ## GNMI encoding requested (one of: \"proto\", \"json\", \"json_ietf\")\n # encoding = \"proto\"\n\n ## redial in case of failures after\n redial = \"10s\"\n\n ## enable client-side TLS and define CA to authenticate the device\n # enable_tls = true\n # tls_ca = \"/etc/telegraf/ca.pem\"\n # insecure_skip_verify = true\n\n ## define client-side TLS certificate \u0026 key to authenticate to the device\n # tls_cert = \"/etc/telegraf/cert.pem\"\n # tls_key = \"/etc/telegraf/key.pem\"\n\n ## GNMI subscription prefix (optional, can usually be left empty)\n ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n # origin = \"\"\n # prefix = \"\"\n # target = \"\"\n\n ## Define additional aliases to map telemetry encoding paths to simple measurement names\n #[inputs.cisco_telemetry_gnmi.aliases]\n #  ifcounters = \"openconfig:/interfaces/interface/state/counters\"\n\n [[inputs.cisco_telemetry_gnmi.subscription]]\n  ## Name of the measurement that will be emitted\n  name = \"ifcounters\"\n\n  ## Origin and path of the subscription\n  ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n  ##\n  ## origin usually refers to a (YANG) data model implemented by the device\n  ## and path to a specific substructe inside it that should be subscribed to (similar to an XPath)\n  ## YANG models can be found e.g. here: https://github.com/YangModels/yang/tree/master/vendor/cisco/xr\n  origin = \"openconfig-interfaces\"\n  path = \"/interfaces/interface/state/counters\"\n\n  # Subscription mode (one of: \"target_defined\", \"sample\", \"on_change\") and interval\n  subscription_mode = \"sample\"\n  sample_interval = \"10s\"\n\n  ## Suppress redundant transmissions when measured values are unchanged\n  # suppress_redundant = false\n\n  ## If suppression is enabled, send updates at least every X seconds anyway\n  # heartbeat_interval = \"60s\"\n\n"
    },
    {
      "type": "input",
      "name": "haproxy",
      "description": "Read metrics of haproxy, via socket or csv stats page",
      "config": "# Read metrics of haproxy, via socket or csv stats page\n[[inputs.haproxy]]\n  # alias=\"haproxy\"\n  ## An array of address to gather stats about. Specify an ip on hostname\n  ## with optional port. ie localhost, 10.10.3.33:1936, etc.\n  ## Make sure you specify the complete path to the stats endpoint\n  ## including the protocol, ie http://10.10.3.33:1936/haproxy?stats\n\n  ## If no servers are specified, then default to 127.0.0.1:1936/haproxy?stats\n  servers = [\"http://myhaproxy.com:1936/haproxy?stats\"]\n\n  ## Credentials for basic HTTP authentication\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## You can also use local socket with standard wildcard globbing.\n  ## Server address not starting with 'http' will be treated as a possible\n  ## socket, so both examples below are valid.\n  # servers = [\"socket:/run/haproxy/admin.sock\", \"/run/haproxy/*.sock\"]\n\n  ## By default, some of the fields are renamed from what haproxy calls them.\n  ## Setting this option to true results in the plugin keeping the original\n  ## field names.\n  # keep_field_names = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "kubernetes",
      "description": "Read metrics from the kubernetes kubelet api",
      "config": "# Read metrics from the kubernetes kubelet api\n[[inputs.kubernetes]]\n  # alias=\"kubernetes\"\n  ## URL for the kubelet\n  url = \"http://127.0.0.1:10255\"\n\n  ## Use bearer token for authorization. ('bearer_token' takes priority)\n  ## If both of these are empty, we'll use the default serviceaccount:\n  ## at: /run/secrets/kubernetes.io/serviceaccount/token\n  # bearer_token = \"/path/to/bearer/token\"\n  ## OR\n  # bearer_token_string = \"abc_123\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "logstash",
      "description": "Read metrics exposed by Logstash",
      "config": "# Read metrics exposed by Logstash\n[[inputs.logstash]]\n  # alias=\"logstash\"\n  ## The URL of the exposed Logstash API endpoint.\n  url = \"http://127.0.0.1:9600\"\n\n  ## Use Logstash 5 single pipeline API, set to true when monitoring\n  ## Logstash 5.\n  # single_pipeline = false\n\n  ## Enable optional collection components.  Can contain\n  ## \"pipelines\", \"process\", and \"jvm\".\n  # collect = [\"pipelines\", \"process\", \"jvm\"]\n\n  ## Timeout for HTTP requests.\n  # timeout = \"5s\"\n\n  ## Optional HTTP Basic Auth credentials.\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Use TLS but skip chain \u0026 host verification.\n  # insecure_skip_verify = false\n\n  ## Optional HTTP headers.\n  # [inputs.logstash.headers]\n  #   \"X-Special-Header\" = \"Special-Value\"\n\n"
    },
    {
      "type": "input",
      "name": "nats_consumer",
      "description": "Read metrics from NATS subject(s)",
      "config": "# Read metrics from NATS subject(s)\n[[inputs.nats_consumer]]\n  # alias=\"nats_consumer\"\n  ## urls of NATS servers\n  servers = [\"nats://localhost:4222\"]\n\n  ## subject(s) to consume\n  subjects = [\"telegraf\"]\n\n  ## name a queue group\n  queue_group = \"telegraf_consumers\"\n\n  ## Optional credentials\n  # username = \"\"\n  # password = \"\"\n\n  ## Use Transport Layer Security\n  # secure = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Sets the limits for pending msgs and bytes for each subscription\n  ## These shouldn't need to be adjusted except in very high throughput scenarios\n  # pending_message_limit = 65536\n  # pending_bytes_limit = 67108864\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "trig",
      "description": "Inserts sine and cosine waves for demonstration purposes",
      "config": "# Inserts sine and cosine waves for demonstration purposes\n[[inputs.trig]]\n  # alias=\"trig\"\n  ## Set the amplitude\n  amplitude = 10.0\n\n"
    },
    {
      "type": "input",
      "name": "mqtt_consumer",
      "description": "Read metrics from MQTT topic(s)",
      "config": "# Read metrics from MQTT topic(s)\n[[inputs.mqtt_consumer]]\n  # alias=\"mqtt_consumer\"\n  ## MQTT broker URLs to be used. The format should be scheme://host:port,\n  ## schema can be tcp, ssl, or ws.\n  servers = [\"tcp://127.0.0.1:1883\"]\n\n  ## Topics that will be subscribed to.\n  topics = [\n    \"telegraf/host01/cpu\",\n    \"telegraf/+/mem\",\n    \"sensors/#\",\n  ]\n\n  ## The message topic will be stored in a tag specified by this value.  If set\n  ## to the empty string no topic tag will be created.\n  # topic_tag = \"topic\"\n\n  ## QoS policy for messages\n  ##   0 = at most once\n  ##   1 = at least once\n  ##   2 = exactly once\n  ##\n  ## When using a QoS of 1 or 2, you should enable persistent_session to allow\n  ## resuming unacknowledged messages.\n  # qos = 0\n\n  ## Connection timeout for initial connection in seconds\n  # connection_timeout = \"30s\"\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Persistent session disables clearing of the client session on connection.\n  ## In order for this option to work you must also set client_id to identify\n  ## the client.  To receive messages that arrived while the client is offline,\n  ## also set the qos option to 1 or 2 and don't forget to also set the QoS when\n  ## publishing.\n  # persistent_session = false\n\n  ## If unset, a random client ID will be generated.\n  # client_id = \"\"\n\n  ## Username and password to connect MQTT server.\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "snmp",
      "description": "Retrieves SNMP values from remote agents",
      "config": "# Retrieves SNMP values from remote agents\n[[inputs.snmp]]\n  # alias=\"snmp\"\n  agents = [ \"127.0.0.1:161\" ]\n  ## Timeout for each SNMP query.\n  timeout = \"5s\"\n  ## Number of retries to attempt within timeout.\n  retries = 3\n  ## SNMP version, values can be 1, 2, or 3\n  version = 2\n\n  ## SNMP community string.\n  community = \"public\"\n\n  ## The GETBULK max-repetitions parameter\n  max_repetitions = 10\n\n  ## SNMPv3 auth parameters\n  #sec_name = \"myuser\"\n  #auth_protocol = \"md5\"      # Values: \"MD5\", \"SHA\", \"\"\n  #auth_password = \"pass\"\n  #sec_level = \"authNoPriv\"   # Values: \"noAuthNoPriv\", \"authNoPriv\", \"authPriv\"\n  #context_name = \"\"\n  #priv_protocol = \"\"         # Values: \"DES\", \"AES\", \"\"\n  #priv_password = \"\"\n\n  ## measurement name\n  name = \"system\"\n  [[inputs.snmp.field]]\n    name = \"hostname\"\n    oid = \".1.0.0.1.1\"\n  [[inputs.snmp.field]]\n    name = \"uptime\"\n    oid = \".1.0.0.1.2\"\n  [[inputs.snmp.field]]\n    name = \"load\"\n    oid = \".1.0.0.1.3\"\n  [[inputs.snmp.field]]\n    oid = \"HOST-RESOURCES-MIB::hrMemorySize\"\n\n  [[inputs.snmp.table]]\n    ## measurement name\n    name = \"remote_servers\"\n    inherit_tags = [ \"hostname\" ]\n    [[inputs.snmp.table.field]]\n      name = \"server\"\n      oid = \".1.0.0.0.1.0\"\n      is_tag = true\n    [[inputs.snmp.table.field]]\n      name = \"connections\"\n      oid = \".1.0.0.0.1.1\"\n    [[inputs.snmp.table.field]]\n      name = \"latency\"\n      oid = \".1.0.0.0.1.2\"\n\n  [[inputs.snmp.table]]\n    ## auto populate table's fields using the MIB\n    oid = \"HOST-RESOURCES-MIB::hrNetworkTable\"\n\n"
    },
    {
      "type": "input",
      "name": "teamspeak",
      "description": "Reads metrics from a Teamspeak 3 Server via ServerQuery",
      "config": "# Reads metrics from a Teamspeak 3 Server via ServerQuery\n[[inputs.teamspeak]]\n  # alias=\"teamspeak\"\n  ## Server address for Teamspeak 3 ServerQuery\n  # server = \"127.0.0.1:10011\"\n  ## Username for ServerQuery\n  username = \"serverqueryuser\"\n  ## Password for ServerQuery\n  password = \"secret\"\n  ## Array of virtual servers\n  # virtual_servers = [1]\n\n"
    },
    {
      "type": "input",
      "name": "azure_storage_queue",
      "description": "Gather Azure Storage Queue metrics",
      "config": "# Gather Azure Storage Queue metrics\n[[inputs.azure_storage_queue]]\n  # alias=\"azure_storage_queue\"\n  ## Required Azure Storage Account name\n  account_name = \"mystorageaccount\"\n\n  ## Required Azure Storage Account access key\n  account_key = \"storageaccountaccesskey\"\n\n  ## Set to false to disable peeking age of oldest message (executes faster)\n  # peek_oldest_message_age = true\n  \n"
    },
    {
      "type": "input",
      "name": "cpu",
      "description": "Read metrics about cpu usage",
      "config": "# Read metrics about cpu usage\n[[inputs.cpu]]\n  # alias=\"cpu\"\n  ## Whether to report per-cpu stats or not\n  percpu = true\n  ## Whether to report total system cpu stats or not\n  totalcpu = true\n  ## If true, collect raw CPU time metrics.\n  collect_cpu_time = false\n  ## If true, compute and report the sum of all non-idle CPU states.\n  report_active = false\n\n"
    },
    {
      "type": "input",
      "name": "dcos",
      "description": "Input plugin for DC/OS metrics",
      "config": "# Input plugin for DC/OS metrics\n[[inputs.dcos]]\n  # alias=\"dcos\"\n  ## The DC/OS cluster URL.\n  cluster_url = \"https://dcos-ee-master-1\"\n\n  ## The ID of the service account.\n  service_account_id = \"telegraf\"\n  ## The private key file for the service account.\n  service_account_private_key = \"/etc/telegraf/telegraf-sa-key.pem\"\n\n  ## Path containing login token.  If set, will read on every gather.\n  # token_file = \"/home/dcos/.dcos/token\"\n\n  ## In all filter options if both include and exclude are empty all items\n  ## will be collected.  Arrays may contain glob patterns.\n  ##\n  ## Node IDs to collect metrics from.  If a node is excluded, no metrics will\n  ## be collected for its containers or apps.\n  # node_include = []\n  # node_exclude = []\n  ## Container IDs to collect container metrics from.\n  # container_include = []\n  # container_exclude = []\n  ## Container IDs to collect app metrics from.\n  # app_include = []\n  # app_exclude = []\n\n  ## Maximum concurrent connections to the cluster.\n  # max_connections = 10\n  ## Maximum time to receive a response from cluster.\n  # response_timeout = \"20s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## If false, skip chain \u0026 host verification\n  # insecure_skip_verify = true\n\n  ## Recommended filtering to reduce series cardinality.\n  # [inputs.dcos.tagdrop]\n  #   path = [\"/var/lib/mesos/slave/slaves/*\"]\n\n"
    },
    {
      "type": "input",
      "name": "http_response",
      "description": "HTTP/HTTPS request given an address a method and a timeout",
      "config": "# HTTP/HTTPS request given an address a method and a timeout\n[[inputs.http_response]]\n  # alias=\"http_response\"\n  ## Deprecated in 1.12, use 'urls'\n  ## Server address (default http://localhost)\n  # address = \"http://localhost\"\n\n  ## List of urls to query.\n  # urls = [\"http://localhost\"]\n\n  ## Set http_proxy (telegraf uses the system wide proxy settings if it's is not set)\n  # http_proxy = \"http://localhost:8888\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## HTTP Request Method\n  # method = \"GET\"\n\n  ## Whether to follow redirects from the server (defaults to false)\n  # follow_redirects = false\n\n  ## Optional HTTP Request Body\n  # body = '''\n  # {'fake':'data'}\n  # '''\n\n  ## Optional substring or regex match in body of the response\n  # response_string_match = \"\\\"service_status\\\": \\\"up\\\"\"\n  # response_string_match = \"ok\"\n  # response_string_match = \"\\\".*_status\\\".?:.?\\\"up\\\"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## HTTP Request Headers (all values must be strings)\n  # [inputs.http_response.headers]\n  #   Host = \"github.com\"\n\n  ## Interface to use when dialing an address\n  # interface = \"eth0\"\n\n"
    },
    {
      "type": "input",
      "name": "mongodb",
      "description": "Read metrics from one or many MongoDB servers",
      "config": "# Read metrics from one or many MongoDB servers\n[[inputs.mongodb]]\n  # alias=\"mongodb\"\n  ## An array of URLs of the form:\n  ##   \"mongodb://\" [user \":\" pass \"@\"] host [ \":\" port]\n  ## For example:\n  ##   mongodb://user:auth_key@10.10.3.30:27017,\n  ##   mongodb://10.10.3.33:18832,\n  servers = [\"mongodb://127.0.0.1:27017\"]\n\n  ## When true, collect per database stats\n  # gather_perdb_stats = false\n\n  ## When true, collect per collection stats\n  # gather_col_stats = false\n\n  ## List of db where collections stats are collected\n  ## If empty, all db are concerned\n  # col_stats_dbs = [\"local\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "unbound",
      "description": "A plugin to collect stats from the Unbound DNS resolver",
      "config": "# A plugin to collect stats from the Unbound DNS resolver\n[[inputs.unbound]]\n  # alias=\"unbound\"\n  ## Address of server to connect to, read from unbound conf default, optionally ':port'\n  ## Will lookup IP if given a hostname\n  server = \"127.0.0.1:8953\"\n\n  ## If running as a restricted user you can prepend sudo for additional access:\n  # use_sudo = false\n\n  ## The default location of the unbound-control binary can be overridden with:\n  # binary = \"/usr/sbin/unbound-control\"\n\n  ## The default timeout of 1s can be overriden with:\n  # timeout = \"1s\"\n\n  ## When set to true, thread metrics are tagged with the thread id.\n  ##\n  ## The default is false for backwards compatibility, and will be changed to\n  ## true in a future version.  It is recommended to set to true on new\n  ## deployments.\n  thread_as_tag = false\n\n"
    },
    {
      "type": "input",
      "name": "jolokia2_agent",
      "description": "Read JMX metrics from a Jolokia REST agent endpoint",
      "config": "# Read JMX metrics from a Jolokia REST agent endpoint\n[[inputs.jolokia2_agent]]\n  # alias=\"jolokia2_agent\"\n  # default_tag_prefix      = \"\"\n  # default_field_prefix    = \"\"\n  # default_field_separator = \".\"\n\n  # Add agents URLs to query\n  urls = [\"http://localhost:8080/jolokia\"]\n  # username = \"\"\n  # password = \"\"\n  # response_timeout = \"5s\"\n\n  ## Optional TLS config\n  # tls_ca   = \"/var/private/ca.pem\"\n  # tls_cert = \"/var/private/client.pem\"\n  # tls_key  = \"/var/private/client-key.pem\"\n  # insecure_skip_verify = false\n\n  ## Add metrics to read\n  [[inputs.jolokia2_agent.metric]]\n    name  = \"java_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n\n"
    },
    {
      "type": "input",
      "name": "jolokia2_proxy",
      "description": "Read JMX metrics from a Jolokia REST proxy endpoint",
      "config": "# Read JMX metrics from a Jolokia REST proxy endpoint\n[[inputs.jolokia2_proxy]]\n  # alias=\"jolokia2_proxy\"\n  # default_tag_prefix      = \"\"\n  # default_field_prefix    = \"\"\n  # default_field_separator = \".\"\n\n  ## Proxy agent\n  url = \"http://localhost:8080/jolokia\"\n  # username = \"\"\n  # password = \"\"\n  # response_timeout = \"5s\"\n\n  ## Optional TLS config\n  # tls_ca   = \"/var/private/ca.pem\"\n  # tls_cert = \"/var/private/client.pem\"\n  # tls_key  = \"/var/private/client-key.pem\"\n  # insecure_skip_verify = false\n\n  ## Add proxy targets to query\n  # default_target_username = \"\"\n  # default_target_password = \"\"\n  [[inputs.jolokia2_proxy.target]]\n    url = \"service:jmx:rmi:///jndi/rmi://targethost:8086/jmxrmi\"\n    # username = \"\"\n    # password = \"\"\n\n  ## Add metrics to read\n  [[inputs.jolokia2_proxy.metric]]\n    name  = \"java_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n\n"
    },
    {
      "type": "input",
      "name": "mailchimp",
      "description": "Gathers metrics from the /3.0/reports MailChimp API",
      "config": "# Gathers metrics from the /3.0/reports MailChimp API\n[[inputs.mailchimp]]\n  # alias=\"mailchimp\"\n  ## MailChimp API key\n  ## get from https://admin.mailchimp.com/account/api/\n  api_key = \"\" # required\n  ## Reports for campaigns sent more than days_old ago will not be collected.\n  ## 0 means collect all.\n  days_old = 0\n  ## Campaign ID to get, if empty gets all campaigns, this option overrides days_old\n  # campaign_id = \"\"\n\n"
    },
    {
      "type": "input",
      "name": "minecraft",
      "description": "Collects scores from a Minecraft server's scoreboard using the RCON protocol",
      "config": "# Collects scores from a Minecraft server's scoreboard using the RCON protocol\n[[inputs.minecraft]]\n  # alias=\"minecraft\"\n  ## Address of the Minecraft server.\n  # server = \"localhost\"\n\n  ## Server RCON Port.\n  # port = \"25575\"\n\n  ## Server RCON Password.\n  password = \"\"\n\n  ## Uncomment to remove deprecated metric components.\n  # tagdrop = [\"server\"]\n\n"
    },
    {
      "type": "input",
      "name": "solr",
      "description": "Read stats from one or more Solr servers or cores",
      "config": "# Read stats from one or more Solr servers or cores\n[[inputs.solr]]\n  # alias=\"solr\"\n  ## specify a list of one or more Solr servers\n  servers = [\"http://localhost:8983\"]\n\n  ## specify a list of one or more Solr cores (default - all)\n  # cores = [\"main\"]\n\n  ## Optional HTTP Basic Auth Credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n"
    },
    {
      "type": "input",
      "name": "nginx_plus_api",
      "description": "Read Nginx Plus Api documentation",
      "config": "# Read Nginx Plus Api documentation\n[[inputs.nginx_plus_api]]\n  # alias=\"nginx_plus_api\"\n  ## An array of API URI to gather stats.\n  urls = [\"http://localhost/api\"]\n\n  # Nginx API version, default: 3\n  # api_version = 3\n\n  # HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "nstat",
      "description": "Collect kernel snmp counters and network interface statistics",
      "config": "# Collect kernel snmp counters and network interface statistics\n[[inputs.nstat]]\n  # alias=\"nstat\"\n  ## file paths for proc files. If empty default paths will be used:\n  ##    /proc/net/netstat, /proc/net/snmp, /proc/net/snmp6\n  ## These can also be overridden with env variables, see README.\n  proc_net_netstat = \"/proc/net/netstat\"\n  proc_net_snmp = \"/proc/net/snmp\"\n  proc_net_snmp6 = \"/proc/net/snmp6\"\n  ## dump metrics with 0 values too\n  dump_zeros       = true\n\n"
    },
    {
      "type": "input",
      "name": "openweathermap",
      "description": "Read current weather and forecasts data from openweathermap.org",
      "config": "# Read current weather and forecasts data from openweathermap.org\n[[inputs.openweathermap]]\n  # alias=\"openweathermap\"\n  ## OpenWeatherMap API key.\n  app_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\n  ## City ID's to collect weather data from.\n  city_id = [\"5391959\"]\n\n  ## Language of the description field. Can be one of \"ar\", \"bg\",\n  ## \"ca\", \"cz\", \"de\", \"el\", \"en\", \"fa\", \"fi\", \"fr\", \"gl\", \"hr\", \"hu\",\n  ## \"it\", \"ja\", \"kr\", \"la\", \"lt\", \"mk\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\",\n  ## \"se\", \"sk\", \"sl\", \"es\", \"tr\", \"ua\", \"vi\", \"zh_cn\", \"zh_tw\"\n  # lang = \"en\"\n\n  ## APIs to fetch; can contain \"weather\" or \"forecast\".\n  fetch = [\"weather\", \"forecast\"]\n\n  ## OpenWeatherMap base URL\n  # base_url = \"https://api.openweathermap.org/\"\n\n  ## Timeout for HTTP response.\n  # response_timeout = \"5s\"\n\n  ## Preferred unit system for temperature and wind speed. Can be one of\n  ## \"metric\", \"imperial\", or \"standard\".\n  # units = \"metric\"\n\n  ## Query interval; OpenWeatherMap updates their weather data every 10\n  ## minutes.\n  interval = \"10m\"\n\n"
    },
    {
      "type": "input",
      "name": "amqp_consumer",
      "description": "AMQP consumer plugin",
      "config": "# AMQP consumer plugin\n[[inputs.amqp_consumer]]\n  # alias=\"amqp_consumer\"\n  ## Broker to consume from.\n  ##   deprecated in 1.7; use the brokers option\n  # url = \"amqp://localhost:5672/influxdb\"\n\n  ## Brokers to consume from.  If multiple brokers are specified a random broker\n  ## will be selected anytime a connection is established.  This can be\n  ## helpful for load balancing when not using a dedicated load balancer.\n  brokers = [\"amqp://localhost:5672/influxdb\"]\n\n  ## Authentication credentials for the PLAIN auth_method.\n  # username = \"\"\n  # password = \"\"\n\n  ## Name of the exchange to declare.  If unset, no exchange will be declared.\n  exchange = \"telegraf\"\n\n  ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n  # exchange_type = \"topic\"\n\n  ## If true, exchange will be passively declared.\n  # exchange_passive = false\n\n  ## Exchange durability can be either \"transient\" or \"durable\".\n  # exchange_durability = \"durable\"\n\n  ## Additional exchange arguments.\n  # exchange_arguments = { }\n  # exchange_arguments = {\"hash_propery\" = \"timestamp\"}\n\n  ## AMQP queue name.\n  queue = \"telegraf\"\n\n  ## AMQP queue durability can be \"transient\" or \"durable\".\n  queue_durability = \"durable\"\n\n  ## If true, queue will be passively declared.\n  # queue_passive = false\n\n  ## A binding between the exchange and queue using this binding key is\n  ## created.  If unset, no binding is created.\n  binding_key = \"#\"\n\n  ## Maximum number of messages server should give to the worker.\n  # prefetch_count = 50\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Auth method. PLAIN and EXTERNAL are supported\n  ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n  ## described here: https://www.rabbitmq.com/plugins.html\n  # auth_method = \"PLAIN\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Content encoding for message payloads, can be set to \"gzip\" to or\n  ## \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "ethtool",
      "description": "Returns ethtool statistics for given interfaces",
      "config": "# Returns ethtool statistics for given interfaces\n[[inputs.ethtool]]\n  # alias=\"ethtool\"\n  ## List of interfaces to pull metrics for\n  # interface_include = [\"eth0\"]\n\n  ## List of interfaces to ignore when pulling metrics.\n  # interface_exclude = [\"eth1\"]\n\n"
    },
    {
      "type": "input",
      "name": "filestat",
      "description": "Read stats about given file(s)",
      "config": "# Read stats about given file(s)\n[[inputs.filestat]]\n  # alias=\"filestat\"\n  ## Files to gather stats about.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   \"/var/log/**.log\"  -\u003e recursively find all .log files in /var/log\n  ##   \"/var/log/*/*.log\" -\u003e find all .log files with a parent dir in /var/log\n  ##   \"/var/log/apache.log\" -\u003e just tail the apache log file\n  ##\n  ## See https://github.com/gobwas/glob for more examples\n  ##\n  files = [\"/var/log/**.log\"]\n\n  ## If true, read the entire file and calculate an md5 checksum.\n  md5 = false\n\n"
    },
    {
      "type": "input",
      "name": "kernel_vmstat",
      "description": "Get kernel statistics from /proc/vmstat",
      "config": "# Get kernel statistics from /proc/vmstat\n[[inputs.kernel_vmstat]]\n  # alias=\"kernel_vmstat\"\n"
    },
    {
      "type": "input",
      "name": "nginx_plus",
      "description": "Read Nginx Plus' full status information (ngx_http_status_module)",
      "config": "# Read Nginx Plus' full status information (ngx_http_status_module)\n[[inputs.nginx_plus]]\n  # alias=\"nginx_plus\"\n  ## An array of ngx_http_status_module or status URI to gather stats.\n  urls = [\"http://localhost/status\"]\n\n  # HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "powerdns_recursor",
      "description": "Read metrics from one or many PowerDNS Recursor servers",
      "config": "# Read metrics from one or many PowerDNS Recursor servers\n[[inputs.powerdns_recursor]]\n  # alias=\"powerdns_recursor\"\n  ## Path to the Recursor control socket.\n  unix_sockets = [\"/var/run/pdns_recursor.controlsocket\"]\n\n  ## Directory to create receive socket.  This default is likely not writable,\n  ## please reference the full plugin documentation for a recommended setup.\n  # socket_dir = \"/var/run/\"\n  ## Socket permissions for the receive socket.\n  # socket_mode = \"0666\"\n\n"
    },
    {
      "type": "input",
      "name": "sqlserver",
      "description": "Read metrics from Microsoft SQL Server",
      "config": "# Read metrics from Microsoft SQL Server\n[[inputs.sqlserver]]\n  # alias=\"sqlserver\"\n  ## Specify instances to monitor with a list of connection strings.\n  ## All connection parameters are optional.\n  ## By default, the host is localhost, listening on default port, TCP 1433.\n  ##   for Windows, the user is the currently running AD user (SSO).\n  ##   See https://github.com/denisenkom/go-mssqldb for detailed connection\n  ##   parameters, in particular, tls connections can be created like so:\n  ##   \"encrypt=true;certificate=\u003ccert\u003e;hostNameInCertificate=\u003cSqlServer host fqdn\u003e\"\n  # servers = [\n  #  \"Server=192.168.1.10;Port=1433;User Id=\u003cuser\u003e;Password=\u003cpw\u003e;app name=telegraf;log=1;\",\n  # ]\n\n  ## Optional parameter, setting this to 2 will use a new version\n  ## of the collection queries that break compatibility with the original\n  ## dashboards.\n  query_version = 2\n\n  ## If you are using AzureDB, setting this to true will gather resource utilization metrics\n  # azuredb = false\n\n  ## If you would like to exclude some of the metrics queries, list them here\n  ## Possible choices:\n  ## - PerformanceCounters\n  ## - WaitStatsCategorized\n  ## - DatabaseIO\n  ## - DatabaseProperties\n  ## - CPUHistory\n  ## - DatabaseSize\n  ## - DatabaseStats\n  ## - MemoryClerk\n  ## - VolumeSpace\n  ## - PerformanceMetrics\n  ## - Schedulers\n  ## - AzureDBResourceStats\n  ## - AzureDBResourceGovernance\n  ## - SqlRequests\n  ## - ServerProperties\n  exclude_query = [ 'Schedulers' ]\n\n"
    },
    {
      "type": "input",
      "name": "disk",
      "description": "Read metrics about disk usage by mount point",
      "config": "# Read metrics about disk usage by mount point\n[[inputs.disk]]\n  # alias=\"disk\"\n  ## By default stats will be gathered for all mount points.\n  ## Set mount_points will restrict the stats to only the specified mount points.\n  # mount_points = [\"/\"]\n\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n\n"
    },
    {
      "type": "input",
      "name": "fibaro",
      "description": "Read devices value(s) from a Fibaro controller",
      "config": "# Read devices value(s) from a Fibaro controller\n[[inputs.fibaro]]\n  # alias=\"fibaro\"\n  ## Required Fibaro controller address/hostname.\n  ## Note: at the time of writing this plugin, Fibaro only implemented http - no https available\n  url = \"http://\u003ccontroller\u003e:80\"\n\n  ## Required credentials to access the API (http://\u003ccontroller/api/\u003ccomponent\u003e)\n  username = \"\u003cusername\u003e\"\n  password = \"\u003cpassword\u003e\"\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "input",
      "name": "graylog",
      "description": "Read flattened metrics from one or more GrayLog HTTP endpoints",
      "config": "# Read flattened metrics from one or more GrayLog HTTP endpoints\n[[inputs.graylog]]\n  # alias=\"graylog\"\n  ## API endpoint, currently supported API:\n  ##\n  ##   - multiple  (Ex http://\u003chost\u003e:12900/system/metrics/multiple)\n  ##   - namespace (Ex http://\u003chost\u003e:12900/system/metrics/namespace/{namespace})\n  ##\n  ## For namespace endpoint, the metrics array will be ignored for that call.\n  ## Endpoint can contain namespace and multiple type calls.\n  ##\n  ## Please check http://[graylog-server-ip]:12900/api-browser for full list\n  ## of endpoints\n  servers = [\n    \"http://[graylog-server-ip]:12900/system/metrics/multiple\",\n  ]\n\n  ## Metrics list\n  ## List of metrics can be found on Graylog webservice documentation.\n  ## Or by hitting the the web service api at:\n  ##   http://[graylog-host]:12900/system/metrics\n  metrics = [\n    \"jvm.cl.loaded\",\n    \"jvm.memory.pools.Metaspace.committed\"\n  ]\n\n  ## Username and password\n  username = \"\"\n  password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "lustre2",
      "description": "Read metrics from local Lustre service on OST, MDS",
      "config": "# Read metrics from local Lustre service on OST, MDS\n[[inputs.lustre2]]\n  # alias=\"lustre2\"\n  ## An array of /proc globs to search for Lustre stats\n  ## If not specified, the default will work on Lustre 2.5.x\n  ##\n  # ost_procfiles = [\n  #   \"/proc/fs/lustre/obdfilter/*/stats\",\n  #   \"/proc/fs/lustre/osd-ldiskfs/*/stats\",\n  #   \"/proc/fs/lustre/obdfilter/*/job_stats\",\n  # ]\n  # mds_procfiles = [\n  #   \"/proc/fs/lustre/mdt/*/md_stats\",\n  #   \"/proc/fs/lustre/mdt/*/job_stats\",\n  # ]\n\n"
    },
    {
      "type": "input",
      "name": "nginx_upstream_check",
      "description": "Read nginx_upstream_check module status information (https://github.com/yaoweibin/nginx_upstream_check_module)",
      "config": "# Read nginx_upstream_check module status information (https://github.com/yaoweibin/nginx_upstream_check_module)\n[[inputs.nginx_upstream_check]]\n  # alias=\"nginx_upstream_check\"\n  ## An URL where Nginx Upstream check module is enabled\n  ## It should be set to return a JSON formatted response\n  url = \"http://127.0.0.1/status?format=json\"\n\n  ## HTTP method\n  # method = \"GET\"\n\n  ## Optional HTTP headers\n  # headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## Override HTTP \"Host\" header\n  # host_header = \"check.example.com\"\n\n  ## Timeout for HTTP requests\n  timeout = \"5s\"\n\n  ## Optional HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "apache",
      "description": "Read Apache status information (mod_status)",
      "config": "# Read Apache status information (mod_status)\n[[inputs.apache]]\n  # alias=\"apache\"\n  ## An array of URLs to gather from, must be directed at the machine\n  ## readable version of the mod_status page including the auto query string.\n  ## Default is \"http://localhost/server-status?auto\".\n  urls = [\"http://localhost/server-status?auto\"]\n\n  ## Credentials for basic HTTP authentication.\n  # username = \"myuser\"\n  # password = \"mypassword\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "passenger",
      "description": "Read metrics of passenger using passenger-status",
      "config": "# Read metrics of passenger using passenger-status\n[[inputs.passenger]]\n  # alias=\"passenger\"\n  ## Path of passenger-status.\n  ##\n  ## Plugin gather metric via parsing XML output of passenger-status\n  ## More information about the tool:\n  ##   https://www.phusionpassenger.com/library/admin/apache/overall_status_report.html\n  ##\n  ## If no path is specified, then the plugin simply execute passenger-status\n  ## hopefully it can be found in your PATH\n  command = \"passenger-status -v --show=xml\"\n\n"
    },
    {
      "type": "input",
      "name": "suricata",
      "description": "Suricata stats plugin",
      "config": "# Suricata stats plugin\n[[inputs.suricata]]\n  # alias=\"suricata\"\n  ## Data sink for Suricata stats log\n  # This is expected to be a filename of a\n  # unix socket to be created for listening.\n  source = \"/var/run/suricata-stats.sock\"\n\n  # Delimiter for flattening field keys, e.g. subitem \"alert\" of \"detect\"\n  # becomes \"detect_alert\" when delimiter is \"_\".\n  delimiter = \"_\"\n\n"
    },
    {
      "type": "input",
      "name": "zipkin",
      "description": "This plugin implements the Zipkin http server to gather trace and timing data needed to troubleshoot latency problems in microservice architectures.",
      "config": "# This plugin implements the Zipkin http server to gather trace and timing data needed to troubleshoot latency problems in microservice architectures.\n[[inputs.zipkin]]\n  # alias=\"zipkin\"\n  # path = \"/api/v1/spans\" # URL path for span data\n  # port = 9411            # Port on which Telegraf listens\n\n"
    },
    {
      "type": "input",
      "name": "marklogic",
      "description": "Retrieves information on a specific host in a MarkLogic Cluster",
      "config": "# Retrieves information on a specific host in a MarkLogic Cluster\n[[inputs.marklogic]]\n  # alias=\"marklogic\"\n  ## Base URL of the MarkLogic HTTP Server.\n  url = \"http://localhost:8002\"\n\n  ## List of specific hostnames to retrieve information. At least (1) required.\n  # hosts = [\"hostname1\", \"hostname2\"]\n\n  ## Using HTTP Basic Authentication. Management API requires 'manage-user' role privileges\n  # username = \"myuser\"\n  # password = \"mypassword\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "cloudwatch",
      "description": "Pull Metric Statistics from Amazon CloudWatch",
      "config": "# Pull Metric Statistics from Amazon CloudWatch\n[[inputs.cloudwatch]]\n  # alias=\"cloudwatch\"\n  ## Amazon Region\n  region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Assumed credentials via STS if role_arn is specified\n  ## 2) explicit credentials from 'access_key' and 'secret_key'\n  ## 3) shared profile from 'profile'\n  ## 4) environment variables\n  ## 5) shared credentials file\n  ## 6) EC2 Instance Profile\n  # access_key = \"\"\n  # secret_key = \"\"\n  # token = \"\"\n  # role_arn = \"\"\n  # profile = \"\"\n  # shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  # The minimum period for Cloudwatch metrics is 1 minute (60s). However not all\n  # metrics are made available to the 1 minute period. Some are collected at\n  # 3 minute, 5 minute, or larger intervals. See https://aws.amazon.com/cloudwatch/faqs/#monitoring.\n  # Note that if a period is configured that is smaller than the minimum for a\n  # particular metric, that metric will not be returned by the Cloudwatch API\n  # and will not be collected by Telegraf.\n  #\n  ## Requested CloudWatch aggregation Period (required - must be a multiple of 60s)\n  period = \"5m\"\n\n  ## Collection Delay (required - must account for metrics availability via CloudWatch API)\n  delay = \"5m\"\n\n  ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid\n  ## gaps or overlap in pulled data\n  interval = \"5m\"\n\n  ## Configure the TTL for the internal cache of metrics.\n  # cache_ttl = \"1h\"\n\n  ## Metric Statistic Namespace (required)\n  namespace = \"AWS/ELB\"\n\n  ## Maximum requests per second. Note that the global default AWS rate limit is\n  ## 50 reqs/sec, so if you define multiple namespaces, these should add up to a\n  ## maximum of 50.\n  ## See http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_limits.html\n  # ratelimit = 25\n\n  ## Timeout for http requests made by the cloudwatch client.\n  # timeout = \"5s\"\n\n  ## Namespace-wide statistic filters. These allow fewer queries to be made to\n  ## cloudwatch.\n  # statistic_include = [ \"average\", \"sum\", \"minimum\", \"maximum\", sample_count\" ]\n  # statistic_exclude = []\n\n  ## Metrics to Pull\n  ## Defaults to all Metrics in Namespace if nothing is provided\n  ## Refreshes Namespace available metrics every 1h\n  #[[inputs.cloudwatch.metrics]]\n  #  names = [\"Latency\", \"RequestCount\"]\n  #\n  #  ## Statistic filters for Metric.  These allow for retrieving specific\n  #  ## statistics for an individual metric.\n  #  # statistic_include = [ \"average\", \"sum\", \"minimum\", \"maximum\", sample_count\" ]\n  #  # statistic_exclude = []\n  #\n  #  ## Dimension filters for Metric.  All dimensions defined for the metric names\n  #  ## must be specified in order to retrieve the metric statistics.\n  #  [[inputs.cloudwatch.metrics.dimensions]]\n  #    name = \"LoadBalancerName\"\n  #    value = \"p-example\"\n\n"
    },
    {
      "type": "input",
      "name": "system",
      "description": "Read metrics about system load \u0026 uptime",
      "config": "# Read metrics about system load \u0026 uptime\n[[inputs.system]]\n  # alias=\"system\"\n  ## Uncomment to remove deprecated metrics.\n  # fielddrop = [\"uptime_format\"]\n\n"
    },
    {
      "type": "input",
      "name": "docker",
      "description": "Read metrics about docker containers",
      "config": "# Read metrics about docker containers\n[[inputs.docker]]\n  # alias=\"docker\"\n  ## Docker Endpoint\n  ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n  ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n  endpoint = \"unix:///var/run/docker.sock\"\n\n  ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)\n  gather_services = false\n\n  ## Only collect metrics for these containers, collect all if empty\n  container_names = []\n\n  ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n  source_tag = false\n\n  ## Containers to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all containers\n  container_name_include = []\n  container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"running\" state will be captured.\n  ## example: container_state_include = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n  ## example: container_state_exclude = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n  # container_state_include = []\n  # container_state_exclude = []\n\n  ## Timeout for docker list, info, and stats commands\n  timeout = \"5s\"\n\n  ## Whether to report for each container per-device blkio (8:0, 8:1...) and\n  ## network (eth0, eth1, ...) stats or not\n  perdevice = true\n\n  ## Whether to report for each container total blkio and network stats or not\n  total = false\n\n  ## Which environment variables should we use as a tag\n  ##tag_env = [\"JAVA_HOME\", \"HEAP_SIZE\"]\n\n  ## docker labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  docker_label_include = []\n  docker_label_exclude = []\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "docker_log",
      "description": "Read logging output from the Docker engine",
      "config": "# Read logging output from the Docker engine\n[[inputs.docker_log]]\n  # alias=\"docker_log\"\n  ## Docker Endpoint\n  ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n  ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n  # endpoint = \"unix:///var/run/docker.sock\"\n\n  ## When true, container logs are read from the beginning; otherwise\n  ## reading begins at the end of the log.\n  # from_beginning = false\n\n  ## Timeout for Docker API calls.\n  # timeout = \"5s\"\n\n  ## Containers to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all containers\n  # container_name_include = []\n  # container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"running\" state will be captured.\n  # container_state_include = []\n  # container_state_exclude = []\n\n  ## docker labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  # docker_label_include = []\n  # docker_label_exclude = []\n\n  ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n  source_tag = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "leofs",
      "description": "Read metrics from a LeoFS Server via SNMP",
      "config": "# Read metrics from a LeoFS Server via SNMP\n[[inputs.leofs]]\n  # alias=\"leofs\"\n  ## An array of URLs of the form:\n  ##   host [ \":\" port]\n  servers = [\"127.0.0.1:4020\"]\n\n"
    },
    {
      "type": "input",
      "name": "procstat",
      "description": "Monitor process cpu and memory usage",
      "config": "# Monitor process cpu and memory usage\n[[inputs.procstat]]\n  # alias=\"procstat\"\n  ## PID file to monitor process\n  pid_file = \"/var/run/nginx.pid\"\n  ## executable name (ie, pgrep \u003cexe\u003e)\n  # exe = \"nginx\"\n  ## pattern as argument for pgrep (ie, pgrep -f \u003cpattern\u003e)\n  # pattern = \"nginx\"\n  ## user as argument for pgrep (ie, pgrep -u \u003cuser\u003e)\n  # user = \"nginx\"\n  ## Systemd unit name\n  # systemd_unit = \"nginx.service\"\n  ## CGroup name or path\n  # cgroup = \"systemd/system.slice/nginx.service\"\n\n  ## Windows service name\n  # win_service = \"\"\n\n  ## override for process_name\n  ## This is optional; default is sourced from /proc/\u003cpid\u003e/status\n  # process_name = \"bar\"\n\n  ## Field name prefix\n  # prefix = \"\"\n\n  ## When true add the full cmdline as a tag.\n  # cmdline_tag = false\n\n  ## Add PID as a tag instead of a field; useful to differentiate between\n  ## processes whose tags are otherwise the same.  Can create a large number\n  ## of series, use judiciously.\n  # pid_tag = false\n\n  ## Method to use when finding process IDs.  Can be one of 'pgrep', or\n  ## 'native'.  The pgrep finder calls the pgrep executable in the PATH while\n  ## the native finder performs the search directly in a manor dependent on the\n  ## platform.  Default is 'pgrep'\n  # pid_finder = \"pgrep\"\n\n"
    },
    {
      "type": "input",
      "name": "salesforce",
      "description": "Read API usage and limits for a Salesforce organisation",
      "config": "# Read API usage and limits for a Salesforce organisation\n[[inputs.salesforce]]\n  # alias=\"salesforce\"\n  ## specify your credentials\n  ##\n  username = \"your_username\"\n  password = \"your_password\"\n  ##\n  ## (optional) security token\n  # security_token = \"your_security_token\"\n  ##\n  ## (optional) environment type (sandbox or production)\n  ## default is: production\n  ##\n  # environment = \"production\"\n  ##\n  ## (optional) API version (default: \"39.0\")\n  ##\n  # version = \"39.0\"\n\n"
    },
    {
      "type": "input",
      "name": "cloud_pubsub_push",
      "description": "Google Cloud Pub/Sub Push HTTP listener",
      "config": "# Google Cloud Pub/Sub Push HTTP listener\n[[inputs.cloud_pubsub_push]]\n  # alias=\"cloud_pubsub_push\"\n  ## Address and port to host HTTP listener on\n  service_address = \":8080\"\n\n  ## Application secret to verify messages originate from Cloud Pub/Sub\n  # token = \"\"\n\n  ## Path to listen to.\n  # path = \"/\"\n\n  ## Maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n  ## Maximum duration before timing out write of the response. This should be set to a value\n  ## large enough that you can send at least 'metric_batch_size' number of messages within the\n  ## duration.\n  # write_timeout = \"10s\"\n\n  ## Maximum allowed http request body size in bytes.\n  ## 0 means to use the default of 524,288,00 bytes (500 mebibytes)\n  # max_body_size = \"500MB\"\n\n  ## Whether to add the pubsub metadata, such as message attributes and subscription as a tag.\n  # add_meta = false\n\n  ## Optional. Maximum messages to read from PubSub that have not been written\n  ## to an output. Defaults to 1000.\n  ## For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message contains 10 metrics and the output\n  ## metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "input",
      "name": "ipvs",
      "description": "Collect virtual and real server stats from Linux IPVS",
      "config": "# Collect virtual and real server stats from Linux IPVS\n[[inputs.ipvs]]\n  # alias=\"ipvs\"\n"
    },
    {
      "type": "input",
      "name": "nginx_vts",
      "description": "Read Nginx virtual host traffic status module information (nginx-module-vts)",
      "config": "# Read Nginx virtual host traffic status module information (nginx-module-vts)\n[[inputs.nginx_vts]]\n  # alias=\"nginx_vts\"\n  ## An array of ngx_http_status_module or status URI to gather stats.\n  urls = [\"http://localhost/status\"]\n\n  ## HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "input",
      "name": "ntpq",
      "description": "Get standard NTP query metrics, requires ntpq executable.",
      "config": "# Get standard NTP query metrics, requires ntpq executable.\n[[inputs.ntpq]]\n  # alias=\"ntpq\"\n  ## If false, set the -n ntpq flag. Can reduce metric gather time.\n  dns_lookup = true\n\n"
    },
    {
      "type": "input",
      "name": "openldap",
      "description": "OpenLDAP cn=Monitor plugin",
      "config": "# OpenLDAP cn=Monitor plugin\n[[inputs.openldap]]\n  # alias=\"openldap\"\n  host = \"localhost\"\n  port = 389\n\n  # ldaps, starttls, or no encryption. default is an empty string, disabling all encryption.\n  # note that port will likely need to be changed to 636 for ldaps\n  # valid options: \"\" | \"starttls\" | \"ldaps\"\n  tls = \"\"\n\n  # skip peer certificate verification. Default is false.\n  insecure_skip_verify = false\n\n  # Path to PEM-encoded Root certificate to use to verify server certificate\n  tls_ca = \"/etc/ssl/certs.pem\"\n\n  # dn/password to bind with. If bind_dn is empty, an anonymous bind is performed.\n  bind_dn = \"\"\n  bind_password = \"\"\n\n  # Reverse metric names so they sort more naturally. Recommended.\n  # This defaults to false if unset, but is set to true when generating a new config\n  reverse_metric_names = true\n\n"
    },
    {
      "type": "input",
      "name": "fluentd",
      "description": "Read metrics exposed by fluentd in_monitor plugin",
      "config": "# Read metrics exposed by fluentd in_monitor plugin\n[[inputs.fluentd]]\n  # alias=\"fluentd\"\n  ## This plugin reads information exposed by fluentd (using /api/plugins.json endpoint).\n  ##\n  ## Endpoint:\n  ## - only one URI is allowed\n  ## - https is not supported\n  endpoint = \"http://localhost:24220/api/plugins.json\"\n\n  ## Define which plugins have to be excluded (based on \"type\" field - e.g. monitor_agent)\n  exclude = [\n\t  \"monitor_agent\",\n\t  \"dummy\",\n  ]\n\n"
    },
    {
      "type": "input",
      "name": "nats",
      "description": "Provides metrics about the state of a NATS server",
      "config": "# Provides metrics about the state of a NATS server\n[[inputs.nats]]\n  # alias=\"nats\"\n  ## The address of the monitoring endpoint of the NATS server\n  server = \"http://localhost:8222\"\n\n  ## Maximum time to receive response\n  # response_timeout = \"5s\"\n\n"
    }
  ]
}
`{
  "version": "1.13.0",
  "os": "linux",
  "plugins": [
    {
      "type": "output",
      "name": "http",
      "description": "A plugin that can transmit metrics over HTTP",
      "config": "# A plugin that can transmit metrics over HTTP\n[[outputs.http]]\n  # alias=\"http\"\n  ## URL is the address to send metrics to\n  url = \"http://127.0.0.1:8080/telegraf\"\n\n  ## Timeout for HTTP message\n  # timeout = \"5s\"\n\n  ## HTTP method, one of: \"POST\" or \"PUT\"\n  # method = \"POST\"\n\n  ## HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## OAuth2 Client Credentials Grant\n  # client_id = \"clientid\"\n  # client_secret = \"secret\"\n  # token_url = \"https://indentityprovider/oauth2/v1/token\"\n  # scopes = [\"urn:opc:idm:__myscopes__\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Data format to output.\n  ## Each data format has it's own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Additional HTTP headers\n  # [outputs.http.headers]\n  #   # Should be set manually to \"application/json\" for json data_format\n  #   Content-Type = \"text/plain; charset=utf-8\"\n\n"
    },
    {
      "type": "output",
      "name": "influxdb",
      "description": "Configuration for sending metrics to InfluxDB",
      "config": "# Configuration for sending metrics to InfluxDB\n[[outputs.influxdb]]\n  # alias=\"influxdb\"\n  ## The full HTTP or UDP URL for your InfluxDB instance.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  # urls = [\"unix:///var/run/influxdb.sock\"]\n  # urls = [\"udp://127.0.0.1:8089\"]\n  # urls = [\"http://127.0.0.1:8086\"]\n\n  ## The target database for metrics; will be created as needed.\n  ## For UDP url endpoint database needs to be configured on server side.\n  # database = \"telegraf\"\n\n  ## The value of this tag will be used to determine the database.  If this\n  ## tag is not set the 'database' option is used as the default.\n  # database_tag = \"\"\n\n  ## If true, the database tag will not be added to the metric.\n  # exclude_database_tag = false\n\n  ## If true, no CREATE DATABASE queries will be sent.  Set to true when using\n  ## Telegraf with a user without permissions to create databases or when the\n  ## database already exists.\n  # skip_database_creation = false\n\n  ## Name of existing retention policy to write to.  Empty string writes to\n  ## the default retention policy.  Only takes effect when using HTTP.\n  # retention_policy = \"\"\n\n  ## Write consistency (clusters only), can be: \"any\", \"one\", \"quorum\", \"all\".\n  ## Only takes effect when using HTTP.\n  # write_consistency = \"any\"\n\n  ## Timeout for HTTP messages.\n  # timeout = \"5s\"\n\n  ## HTTP Basic Auth\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## HTTP User-Agent\n  # user_agent = \"telegraf\"\n\n  ## UDP payload size is the maximum packet size to send.\n  # udp_payload = \"512B\"\n\n  ## Optional TLS Config for use on HTTP connections.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## HTTP Proxy override, if unset values the standard proxy environment\n  ## variables are consulted to determine which proxy, if any, should be used.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## When true, Telegraf will output unsigned integers as unsigned values,\n  ## i.e.: \"42u\".  You will need a version of InfluxDB supporting unsigned\n  ## integer values.  Enabling this option will result in field type errors if\n  ## existing data has been written.\n  # influx_uint_support = false\n\n"
    },
    {
      "type": "output",
      "name": "exec",
      "description": "Send metrics to command as input over stdin",
      "config": "# Send metrics to command as input over stdin\n[[outputs.exec]]\n  # alias=\"exec\"\n  ## Command to injest metrics via stdin.\n  command = [\"tee\", \"-a\", \"/dev/null\"]\n\n  ## Timeout for command to complete.\n  # timeout = \"5s\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "graphite",
      "description": "Configuration for Graphite server to send metrics to",
      "config": "# Configuration for Graphite server to send metrics to\n[[outputs.graphite]]\n  # alias=\"graphite\"\n  ## TCP endpoint for your graphite instance.\n  ## If multiple endpoints are configured, output will be load balanced.\n  ## Only one of the endpoints will be written to with each iteration.\n  servers = [\"localhost:2003\"]\n  ## Prefix metrics name\n  prefix = \"\"\n  ## Graphite output template\n  ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  template = \"host.tags.measurement.field\"\n\n  ## Enable Graphite tags support\n  # graphite_tag_support = false\n\n  ## timeout in seconds for the write connection to graphite\n  timeout = 2\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "output",
      "name": "graylog",
      "description": "Send telegraf metrics to graylog(s)",
      "config": "# Send telegraf metrics to graylog(s)\n[[outputs.graylog]]\n  # alias=\"graylog\"\n  ## UDP endpoint for your graylog instance.\n  servers = [\"127.0.0.1:12201\", \"192.168.1.1:12201\"]\n\n"
    },
    {
      "type": "output",
      "name": "nats",
      "description": "Send telegraf measurements to NATS",
      "config": "# Send telegraf measurements to NATS\n[[outputs.nats]]\n  # alias=\"nats\"\n  ## URLs of NATS servers\n  servers = [\"nats://localhost:4222\"]\n  ## Optional credentials\n  # username = \"\"\n  # password = \"\"\n  ## NATS subject for producer messages\n  subject = \"telegraf\"\n\n  ## Use Transport Layer Security\n  # secure = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "prometheus_client",
      "description": "Configuration for the Prometheus client to spawn",
      "config": "# Configuration for the Prometheus client to spawn\n[[outputs.prometheus_client]]\n  # alias=\"prometheus_client\"\n  ## Address to listen on\n  listen = \":9273\"\n\n  ## Metric version controls the mapping from Telegraf metrics into\n  ## Prometheus format.  When using the prometheus input, use the same value in\n  ## both plugins to ensure metrics are round-tripped without modification.\n  ##\n  ##   example: metric_version = 1; deprecated in 1.13\n  ##            metric_version = 2; recommended version\n  # metric_version = 1\n\n  ## Use HTTP Basic Authentication.\n  # basic_username = \"Foo\"\n  # basic_password = \"Bar\"\n\n  ## If set, the IP Ranges which are allowed to access metrics.\n  ##   ex: ip_range = [\"192.168.0.0/24\", \"192.168.1.0/30\"]\n  # ip_range = []\n\n  ## Path to publish the metrics on.\n  # path = \"/metrics\"\n\n  ## Expiration interval for each metric. 0 == no expiration\n  # expiration_interval = \"60s\"\n\n  ## Collectors to enable, valid entries are \"gocollector\" and \"process\".\n  ## If unset, both are enabled.\n  # collectors_exclude = [\"gocollector\", \"process\"]\n\n  ## Send string metrics as Prometheus labels.\n  ## Unless set to false all string metrics will be sent as labels.\n  # string_as_label = true\n\n  ## If set, enable TLS with the given certificate.\n  # tls_cert = \"/etc/ssl/telegraf.crt\"\n  # tls_key = \"/etc/ssl/telegraf.key\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Export metric collection time.\n  # export_timestamp = false\n\n"
    },
    {
      "type": "output",
      "name": "riemann",
      "description": "Configuration for the Riemann server to send metrics to",
      "config": "# Configuration for the Riemann server to send metrics to\n[[outputs.riemann]]\n  # alias=\"riemann\"\n  ## The full TCP or UDP URL of the Riemann server\n  url = \"tcp://localhost:5555\"\n\n  ## Riemann event TTL, floating-point time in seconds.\n  ## Defines how long that an event is considered valid for in Riemann\n  # ttl = 30.0\n\n  ## Separator to use between measurement and field name in Riemann service name\n  ## This does not have any effect if 'measurement_as_attribute' is set to 'true'\n  separator = \"/\"\n\n  ## Set measurement name as Riemann attribute 'measurement', instead of prepending it to the Riemann service name\n  # measurement_as_attribute = false\n\n  ## Send string metrics as Riemann event states.\n  ## Unless enabled all string metrics will be ignored\n  # string_as_state = false\n\n  ## A list of tag keys whose values get sent as Riemann tags.\n  ## If empty, all Telegraf tag values will be sent as tags\n  # tag_keys = [\"telegraf\",\"custom_tag\"]\n\n  ## Additional Riemann tags to send.\n  # tags = [\"telegraf-output\"]\n\n  ## Description for Riemann event\n  # description_text = \"metrics collected from telegraf\"\n\n  ## Riemann client write timeout, defaults to \"5s\" if not set.\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "output",
      "name": "wavefront",
      "description": "Configuration for Wavefront server to send metrics to",
      "config": "# Configuration for Wavefront server to send metrics to\n[[outputs.wavefront]]\n  # alias=\"wavefront\"\n  ## Url for Wavefront Direct Ingestion or using HTTP with Wavefront Proxy\n  ## If using Wavefront Proxy, also specify port. example: http://proxyserver:2878\n  url = \"https://metrics.wavefront.com\"\n\n  ## Authentication Token for Wavefront. Only required if using Direct Ingestion\n  #token = \"DUMMY_TOKEN\"  \n  \n  ## DNS name of the wavefront proxy server. Do not use if url is specified\n  #host = \"wavefront.example.com\"\n\n  ## Port that the Wavefront proxy server listens on. Do not use if url is specified\n  #port = 2878\n\n  ## prefix for metrics keys\n  #prefix = \"my.specific.prefix.\"\n\n  ## whether to use \"value\" for name of simple fields. default is false\n  #simple_fields = false\n\n  ## character to use between metric and field name.  default is . (dot)\n  #metric_separator = \".\"\n\n  ## Convert metric name paths to use metricSeparator character\n  ## When true will convert all _ (underscore) characters in final metric name. default is true\n  #convert_paths = true\n\n  ## Use Strict rules to sanitize metric and tag names from invalid characters\n  ## When enabled forward slash (/) and comma (,) will be accpeted\n  #use_strict = false\n\n  ## Use Regex to sanitize metric and tag names from invalid characters\n  ## Regex is more thorough, but significantly slower. default is false\n  #use_regex = false\n\n  ## point tags to use as the source name for Wavefront (if none found, host will be used)\n  #source_override = [\"hostname\", \"address\", \"agent_host\", \"node_host\"]\n\n  ## whether to convert boolean values to numeric values, with false -\u003e 0.0 and true -\u003e 1.0. default is true\n  #convert_bool = true\n\n  ## Define a mapping, namespaced by metric prefix, from string values to numeric values\n  ##   deprecated in 1.9; use the enum processor plugin\n  #[[outputs.wavefront.string_to_number.elasticsearch]]\n  #  green = 1.0\n  #  yellow = 0.5\n  #  red = 0.0\n\n"
    },
    {
      "type": "output",
      "name": "cloudwatch",
      "description": "Configuration for AWS CloudWatch output.",
      "config": "# Configuration for AWS CloudWatch output.\n[[outputs.cloudwatch]]\n  # alias=\"cloudwatch\"\n  ## Amazon REGION\n  region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Assumed credentials via STS if role_arn is specified\n  ## 2) explicit credentials from 'access_key' and 'secret_key'\n  ## 3) shared profile from 'profile'\n  ## 4) environment variables\n  ## 5) shared credentials file\n  ## 6) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Namespace for the CloudWatch MetricDatums\n  namespace = \"InfluxData/Telegraf\"\n\n  ## If you have a large amount of metrics, you should consider to send statistic \n  ## values instead of raw metrics which could not only improve performance but \n  ## also save AWS API cost. If enable this flag, this plugin would parse the required \n  ## CloudWatch statistic fields (count, min, max, and sum) and send them to CloudWatch. \n  ## You could use basicstats aggregator to calculate those fields. If not all statistic \n  ## fields are available, all fields would still be sent as raw metrics. \n  # write_statistics = false\n\n  ## Enable high resolution metrics of 1 second (if not enabled, standard resolution are of 60 seconds precision)\n  # high_resolution_metrics = false\n\n"
    },
    {
      "type": "output",
      "name": "datadog",
      "description": "Configuration for DataDog API to send metrics to.",
      "config": "# Configuration for DataDog API to send metrics to.\n[[outputs.datadog]]\n  # alias=\"datadog\"\n  ## Datadog API key\n  apikey = \"my-secret-key\" # required.\n\n  # The base endpoint URL can optionally be specified but it defaults to:\n  #url = \"https://app.datadoghq.com/api/v1/series\"\n\n  ## Connection timeout.\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "output",
      "name": "discard",
      "description": "Send metrics to nowhere at all",
      "config": "# Send metrics to nowhere at all\n[[outputs.discard]]\n  # alias=\"discard\"\n"
    },
    {
      "type": "output",
      "name": "health",
      "description": "Configurable HTTP health check resource based on metrics",
      "config": "# Configurable HTTP health check resource based on metrics\n[[outputs.health]]\n  # alias=\"health\"\n  ## Address and port to listen on.\n  ##   ex: service_address = \"http://localhost:8080\"\n  ##       service_address = \"unix:///var/run/telegraf-health.sock\"\n  # service_address = \"http://:8080\"\n\n  ## The maximum duration for reading the entire request.\n  # read_timeout = \"5s\"\n  ## The maximum duration for writing the entire response.\n  # write_timeout = \"5s\"\n\n  ## Username and password to accept for HTTP basic authentication.\n  # basic_username = \"user1\"\n  # basic_password = \"secret\"\n\n  ## Allowed CA certificates for client certificates.\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## TLS server certificate and private key.\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## One or more check sub-tables should be defined, it is also recommended to\n  ## use metric filtering to limit the metrics that flow into this output.\n  ##\n  ## When using the default buffer sizes, this example will fail when the\n  ## metric buffer is half full.\n  ##\n  ## namepass = [\"internal_write\"]\n  ## tagpass = { output = [\"influxdb\"] }\n  ##\n  ## [[outputs.health.compares]]\n  ##   field = \"buffer_size\"\n  ##   lt = 5000.0\n  ##\n  ## [[outputs.health.contains]]\n  ##   field = \"buffer_size\"\n\n"
    },
    {
      "type": "output",
      "name": "kinesis",
      "description": "Configuration for the AWS Kinesis output.",
      "config": "# Configuration for the AWS Kinesis output.\n[[outputs.kinesis]]\n  # alias=\"kinesis\"\n  ## Amazon REGION of kinesis endpoint.\n  region = \"ap-southeast-2\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Assumed credentials via STS if role_arn is specified\n  ## 2) explicit credentials from 'access_key' and 'secret_key'\n  ## 3) shared profile from 'profile'\n  ## 4) environment variables\n  ## 5) shared credentials file\n  ## 6) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Kinesis StreamName must exist prior to starting telegraf.\n  streamname = \"StreamName\"\n  ## DEPRECATED: PartitionKey as used for sharding data.\n  partitionkey = \"PartitionKey\"\n  ## DEPRECATED: If set the paritionKey will be a random UUID on every put.\n  ## This allows for scaling across multiple shards in a stream.\n  ## This will cause issues with ordering.\n  use_random_partitionkey = false\n  ## The partition key can be calculated using one of several methods:\n  ##\n  ## Use a static value for all writes:\n  #  [outputs.kinesis.partition]\n  #    method = \"static\"\n  #    key = \"howdy\"\n  #\n  ## Use a random partition key on each write:\n  #  [outputs.kinesis.partition]\n  #    method = \"random\"\n  #\n  ## Use the measurement name as the partition key:\n  #  [outputs.kinesis.partition]\n  #    method = \"measurement\"\n  #\n  ## Use the value of a tag for all writes, if the tag is not set the empty\n  ## default option will be used. When no default, defaults to \"telegraf\"\n  #  [outputs.kinesis.partition]\n  #    method = \"tag\"\n  #    key = \"host\"\n  #    default = \"mykey\"\n\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n  ## debug will show upstream aws messages.\n  debug = false\n\n"
    },
    {
      "type": "output",
      "name": "riemann_legacy",
      "description": "Configuration for the Riemann server to send metrics to",
      "config": "# Configuration for the Riemann server to send metrics to\n[[outputs.riemann_legacy]]\n  # alias=\"riemann_legacy\"\n  ## URL of server\n  url = \"localhost:5555\"\n  ## transport protocol to use either tcp or udp\n  transport = \"tcp\"\n  ## separator to use between input name and field name in Riemann service name\n  separator = \" \"\n\n"
    },
    {
      "type": "output",
      "name": "stackdriver",
      "description": "Configuration for Google Cloud Stackdriver to send metrics to",
      "config": "# Configuration for Google Cloud Stackdriver to send metrics to\n[[outputs.stackdriver]]\n  # alias=\"stackdriver\"\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## The namespace for the metric descriptor\n  namespace = \"telegraf\"\n\n  ## Custom resource type\n  # resource_type = \"generic_node\"\n\n  ## Additonal resource labels\n  # [outputs.stackdriver.resource_labels]\n  #   node_id = \"$HOSTNAME\"\n  #   namespace = \"myapp\"\n  #   location = \"eu-north0\"\n\n"
    },
    {
      "type": "output",
      "name": "amon",
      "description": "Configuration for Amon Server to send metrics to.",
      "config": "# Configuration for Amon Server to send metrics to.\n[[outputs.amon]]\n  # alias=\"amon\"\n  ## Amon Server Key\n  server_key = \"my-server-key\" # required.\n\n  ## Amon Instance URL\n  amon_instance = \"https://youramoninstance\" # required\n\n  ## Connection timeout.\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "output",
      "name": "application_insights",
      "description": "Send metrics to Azure Application Insights",
      "config": "# Send metrics to Azure Application Insights\n[[outputs.application_insights]]\n  # alias=\"application_insights\"\n  ## Instrumentation key of the Application Insights resource.\n  instrumentation_key = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx\"\n\n  ## Timeout for closing (default: 5s).\n  # timeout = \"5s\"\n\n  ## Enable additional diagnostic logging.\n  # enable_diagnostic_logging = false\n\n  ## Context Tag Sources add Application Insights context tags to a tag value.\n  ##\n  ## For list of allowed context tag keys see:\n  ## https://github.com/Microsoft/ApplicationInsights-Go/blob/master/appinsights/contracts/contexttagkeys.go\n  # [outputs.application_insights.context_tag_sources]\n  #   \"ai.cloud.role\" = \"kubernetes_container_name\"\n  #   \"ai.cloud.roleInstance\" = \"kubernetes_pod_name\"\n\n"
    },
    {
      "type": "output",
      "name": "file",
      "description": "Send telegraf metrics to file(s)",
      "config": "# Send telegraf metrics to file(s)\n[[outputs.file]]\n  # alias=\"file\"\n  ## Files to write to, \"stdout\" is a specially handled file.\n  files = [\"stdout\", \"/tmp/metrics.out\"]\n\n  ## Use batch serialization format instead of line based delimiting.  The\n  ## batch format allows for the production of non line based output formats and\n  ## may more effiently encode metric groups.\n  # use_batch_format = false\n\n  ## The file will be rotated after the time interval specified.  When set\n  ## to 0 no time based rotation is performed.\n  # rotation_interval = \"0d\"\n\n  ## The logfile will be rotated when it becomes larger than the specified\n  ## size.  When set to 0 no size based rotation is performed.\n  # rotation_max_size = \"0MB\"\n\n  ## Maximum number of rotated archives to keep, any older logs are deleted.\n  ## If set to -1, no archives are removed.\n  # rotation_max_archives = 5\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "opentsdb",
      "description": "Configuration for OpenTSDB server to send metrics to",
      "config": "# Configuration for OpenTSDB server to send metrics to\n[[outputs.opentsdb]]\n  # alias=\"opentsdb\"\n  ## prefix for metrics keys\n  prefix = \"my.specific.prefix.\"\n\n  ## DNS name of the OpenTSDB server\n  ## Using \"opentsdb.example.com\" or \"tcp://opentsdb.example.com\" will use the\n  ## telnet API. \"http://opentsdb.example.com\" will use the Http API.\n  host = \"opentsdb.example.com\"\n\n  ## Port of the OpenTSDB server\n  port = 4242\n\n  ## Number of data points to send to OpenTSDB in Http requests.\n  ## Not used with telnet API.\n  http_batch_size = 50\n\n  ## URI Path for Http requests to OpenTSDB.\n  ## Used in cases where OpenTSDB is located behind a reverse proxy.\n  http_path = \"/api/put\"\n\n  ## Debug true - Prints OpenTSDB communication\n  debug = false\n\n  ## Separator separates measurement name from field\n  separator = \"_\"\n\n"
    },
    {
      "type": "output",
      "name": "amqp",
      "description": "Publishes metrics to an AMQP broker",
      "config": "# Publishes metrics to an AMQP broker\n[[outputs.amqp]]\n  # alias=\"amqp\"\n  ## Broker to publish to.\n  ##   deprecated in 1.7; use the brokers option\n  # url = \"amqp://localhost:5672/influxdb\"\n\n  ## Brokers to publish to.  If multiple brokers are specified a random broker\n  ## will be selected anytime a connection is established.  This can be\n  ## helpful for load balancing when not using a dedicated load balancer.\n  brokers = [\"amqp://localhost:5672/influxdb\"]\n\n  ## Maximum messages to send over a connection.  Once this is reached, the\n  ## connection is closed and a new connection is made.  This can be helpful for\n  ## load balancing when not using a dedicated load balancer.\n  # max_messages = 0\n\n  ## Exchange to declare and publish to.\n  exchange = \"telegraf\"\n\n  ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n  # exchange_type = \"topic\"\n\n  ## If true, exchange will be passively declared.\n  # exchange_passive = false\n\n  ## Exchange durability can be either \"transient\" or \"durable\".\n  # exchange_durability = \"durable\"\n\n  ## Additional exchange arguments.\n  # exchange_arguments = { }\n  # exchange_arguments = {\"hash_propery\" = \"timestamp\"}\n\n  ## Authentication credentials for the PLAIN auth_method.\n  # username = \"\"\n  # password = \"\"\n\n  ## Auth method. PLAIN and EXTERNAL are supported\n  ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n  ## described here: https://www.rabbitmq.com/plugins.html\n  # auth_method = \"PLAIN\"\n\n  ## Metric tag to use as a routing key.\n  ##   ie, if this tag exists, its value will be used as the routing key\n  # routing_tag = \"host\"\n\n  ## Static routing key.  Used when no routing_tag is set or as a fallback\n  ## when the tag specified in routing tag is not found.\n  # routing_key = \"\"\n  # routing_key = \"telegraf\"\n\n  ## Delivery Mode controls if a published message is persistent.\n  ##   One of \"transient\" or \"persistent\".\n  # delivery_mode = \"transient\"\n\n  ## InfluxDB database added as a message header.\n  ##   deprecated in 1.7; use the headers option\n  # database = \"telegraf\"\n\n  ## InfluxDB retention policy added as a message header\n  ##   deprecated in 1.7; use the headers option\n  # retention_policy = \"default\"\n\n  ## Static headers added to each published message.\n  # headers = { }\n  # headers = {\"database\" = \"telegraf\", \"retention_policy\" = \"default\"}\n\n  ## Connection timeout.  If not provided, will default to 5s.  0s means no\n  ## timeout (not recommended).\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## If true use batch serialization format instead of line based delimiting.\n  ## Only applies to data formats which are not line based such as JSON.\n  ## Recommended to set to true.\n  # use_batch_format = false\n\n  ## Content encoding for message payloads, can be set to \"gzip\" to or\n  ## \"identity\" to apply no encoding.\n  ##\n  ## Please note that when use_batch_format = false each amqp message contains only\n  ## a single metric, it is recommended to use compression with batch format\n  ## for best results.\n  # content_encoding = \"identity\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "azure_monitor",
      "description": "Send aggregate metrics to Azure Monitor",
      "config": "# Send aggregate metrics to Azure Monitor\n[[outputs.azure_monitor]]\n  # alias=\"azure_monitor\"\n  ## Timeout for HTTP writes.\n  # timeout = \"20s\"\n\n  ## Set the namespace prefix, defaults to \"Telegraf/\u003cinput-name\u003e\".\n  # namespace_prefix = \"Telegraf/\"\n\n  ## Azure Monitor doesn't have a string value type, so convert string\n  ## fields to dimensions (a.k.a. tags) if enabled. Azure Monitor allows\n  ## a maximum of 10 dimensions so Telegraf will only send the first 10\n  ## alphanumeric dimensions.\n  # strings_as_dimensions = false\n\n  ## Both region and resource_id must be set or be available via the\n  ## Instance Metadata service on Azure Virtual Machines.\n  #\n  ## Azure Region to publish metrics against.\n  ##   ex: region = \"southcentralus\"\n  # region = \"\"\n  #\n  ## The Azure Resource ID against which metric will be logged, e.g.\n  ##   ex: resource_id = \"/subscriptions/\u003csubscription_id\u003e/resourceGroups/\u003cresource_group\u003e/providers/Microsoft.Compute/virtualMachines/\u003cvm_name\u003e\"\n  # resource_id = \"\"\n\n  ## Optionally, if in Azure US Government, China or other sovereign\n  ## cloud environment, set appropriate REST endpoint for receiving\n  ## metrics. (Note: region may be unused in this context)\n  # endpoint_url = \"https://monitoring.core.usgovcloudapi.net\"\n\n"
    },
    {
      "type": "output",
      "name": "syslog",
      "description": "Configuration for Syslog server to send metrics to",
      "config": "# Configuration for Syslog server to send metrics to\n[[outputs.syslog]]\n  # alias=\"syslog\"\n  ## URL to connect to\n  ## ex: address = \"tcp://127.0.0.1:8094\"\n  ## ex: address = \"tcp4://127.0.0.1:8094\"\n  ## ex: address = \"tcp6://127.0.0.1:8094\"\n  ## ex: address = \"tcp6://[2001:db8::1]:8094\"\n  ## ex: address = \"udp://127.0.0.1:8094\"\n  ## ex: address = \"udp4://127.0.0.1:8094\"\n  ## ex: address = \"udp6://127.0.0.1:8094\"\n  address = \"tcp://127.0.0.1:8094\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Period between keep alive probes.\n  ## Only applies to TCP sockets.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  # keep_alive_period = \"5m\"\n\n  ## The framing technique with which it is expected that messages are\n  ## transported (default = \"octet-counting\").  Whether the messages come\n  ## using the octect-counting (RFC5425#section-4.3.1, RFC6587#section-3.4.1),\n  ## or the non-transparent framing technique (RFC6587#section-3.4.2).  Must\n  ## be one of \"octet-counting\", \"non-transparent\".\n  # framing = \"octet-counting\"\n\n  ## The trailer to be expected in case of non-trasparent framing (default = \"LF\").\n  ## Must be one of \"LF\", or \"NUL\".\n  # trailer = \"LF\"\n\n  ## SD-PARAMs settings\n  ## Syslog messages can contain key/value pairs within zero or more\n  ## structured data sections.  For each unrecognised metric tag/field a\n  ## SD-PARAMS is created.\n  ##\n  ## Example:\n  ##   [[outputs.syslog]]\n  ##     sdparam_separator = \"_\"\n  ##     default_sdid = \"default@32473\"\n  ##     sdids = [\"foo@123\", \"bar@456\"]\n  ##\n  ##   input =\u003e xyzzy,x=y foo@123_value=42,bar@456_value2=84,something_else=1\n  ##   output (structured data only) =\u003e [foo@123 value=42][bar@456 value2=84][default@32473 something_else=1 x=y]\n\n  ## SD-PARAMs separator between the sdid and tag/field key (default = \"_\")\n  # sdparam_separator = \"_\"\n\n  ## Default sdid used for tags/fields that don't contain a prefix defined in\n  ## the explict sdids setting below If no default is specified, no SD-PARAMs\n  ## will be used for unrecognised field.\n  # default_sdid = \"default@32473\"\n\n  ## List of explicit prefixes to extract from tag/field keys and use as the\n  ## SDID, if they match (see above example for more details):\n  # sdids = [\"foo@123\", \"bar@456\"]\n\n  ## Default severity value. Severity and Facility are used to calculate the\n  ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field\n  ## with key \"severity_code\" is defined.  If unset, 5 (notice) is the default\n  # default_severity_code = 5\n\n  ## Default facility value. Facility and Severity are used to calculate the\n  ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field with\n  ## key \"facility_code\" is defined.  If unset, 1 (user-level) is the default\n  # default_facility_code = 1\n\n  ## Default APP-NAME value (RFC5424#section-6.2.5)\n  ## Used when no metric tag with key \"appname\" is defined.\n  ## If unset, \"Telegraf\" is the default\n  # default_appname = \"Telegraf\"\n\n"
    },
    {
      "type": "output",
      "name": "nsq",
      "description": "Send telegraf measurements to NSQD",
      "config": "# Send telegraf measurements to NSQD\n[[outputs.nsq]]\n  # alias=\"nsq\"\n  ## Location of nsqd instance listening on TCP\n  server = \"localhost:4150\"\n  ## NSQ topic for producer messages\n  topic = \"telegraf\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "socket_writer",
      "description": "Generic socket writer capable of handling multiple socket types.",
      "config": "# Generic socket writer capable of handling multiple socket types.\n[[outputs.socket_writer]]\n  # alias=\"socket_writer\"\n  ## URL to connect to\n  # address = \"tcp://127.0.0.1:8094\"\n  # address = \"tcp://example.com:http\"\n  # address = \"tcp4://127.0.0.1:8094\"\n  # address = \"tcp6://127.0.0.1:8094\"\n  # address = \"tcp6://[2001:db8::1]:8094\"\n  # address = \"udp://127.0.0.1:8094\"\n  # address = \"udp4://127.0.0.1:8094\"\n  # address = \"udp6://127.0.0.1:8094\"\n  # address = \"unix:///tmp/telegraf.sock\"\n  # address = \"unixgram:///tmp/telegraf.sock\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Period between keep alive probes.\n  ## Only applies to TCP sockets.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  # keep_alive_period = \"5m\"\n\n  ## Data format to generate.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "mqtt",
      "description": "Configuration for MQTT server to send metrics to",
      "config": "# Configuration for MQTT server to send metrics to\n[[outputs.mqtt]]\n  # alias=\"mqtt\"\n  servers = [\"localhost:1883\"] # required.\n\n  ## MQTT outputs send metrics to this topic format\n  ##    \"\u003ctopic_prefix\u003e/\u003chostname\u003e/\u003cpluginname\u003e/\"\n  ##   ex: prefix/web01.example.com/mem\n  topic_prefix = \"telegraf\"\n\n  ## QoS policy for messages\n  ##   0 = at most once\n  ##   1 = at least once\n  ##   2 = exactly once\n  # qos = 2\n\n  ## username and password to connect MQTT server.\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## client ID, if not set a random ID is generated\n  # client_id = \"\"\n\n  ## Timeout for write operations. default: 5s\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## When true, metrics will be sent in one MQTT message per flush.  Otherwise,\n  ## metrics are written one metric per MQTT message.\n  # batch = false\n\n  ## When true, metric will have RETAIN flag set, making broker cache entries until someone\n  ## actually reads it\n  # retain = false\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "cloud_pubsub",
      "description": "Publish Telegraf metrics to a Google Cloud PubSub topic",
      "config": "# Publish Telegraf metrics to a Google Cloud PubSub topic\n[[outputs.cloud_pubsub]]\n  # alias=\"cloud_pubsub\"\n  ## Required. Name of Google Cloud Platform (GCP) Project that owns\n  ## the given PubSub topic.\n  project = \"my-project\"\n\n  ## Required. Name of PubSub topic to publish metrics to.\n  topic = \"my-topic\"\n\n  ## Required. Data format to consume.\n  ## Each data format has its own unique set of configuration options.\n  ## Read more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n  ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n  ## Application Default Credentials, which is preferred.\n  # credentials_file = \"path/to/my/creds.json\"\n\n  ## Optional. If true, will send all metrics per write in one PubSub message.\n  # send_batched = true\n\n  ## The following publish_* parameters specifically configures batching\n  ## requests made to the GCP Cloud PubSub API via the PubSub Golang library. Read\n  ## more here: https://godoc.org/cloud.google.com/go/pubsub#PublishSettings\n\n  ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n  ## when it has this many PubSub messages. If send_batched is true,\n  ## this is ignored and treated as if it were 1.\n  # publish_count_threshold = 1000\n\n  ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n  ## when it has this many PubSub messages. If send_batched is true,\n  ## this is ignored and treated as if it were 1\n  # publish_byte_threshold = 1000000\n\n  ## Optional. Specifically configures requests made to the PubSub API.\n  # publish_num_go_routines = 2\n\n  ## Optional. Specifies a timeout for requests to the PubSub API.\n  # publish_timeout = \"30s\"\n\n  ## Optional. If true, published PubSub message data will be base64-encoded.\n  # base64_data = false\n\n  ## Optional. PubSub attributes to add to metrics.\n  # [[inputs.pubsub.attributes]]\n  #   my_attr = \"tag_value\"\n\n"
    },
    {
      "type": "output",
      "name": "influxdb_v2",
      "description": "Configuration for sending metrics to InfluxDB",
      "config": "# Configuration for sending metrics to InfluxDB\n[[outputs.influxdb_v2]]\n  # alias=\"influxdb_v2\"\n  ## The URLs of the InfluxDB cluster nodes.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  ##   ex: urls = [\"https://us-west-2-1.aws.cloud2.influxdata.com\"]\n  urls = [\"http://127.0.0.1:8086\"]\n\n  ## Token for authentication.\n  token = \"\"\n\n  ## Organization is the name of the organization you wish to write to; must exist.\n  organization = \"\"\n\n  ## Destination bucket to write into.\n  bucket = \"\"\n\n  ## The value of this tag will be used to determine the bucket.  If this\n  ## tag is not set the 'bucket' option is used as the default.\n  # bucket_tag = \"\"\n\n  ## If true, the bucket tag will not be added to the metric.\n  # exclude_bucket_tag = false\n\n  ## Timeout for HTTP messages.\n  # timeout = \"5s\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP Proxy override, if unset values the standard proxy environment\n  ## variables are consulted to determine which proxy, if any, should be used.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## HTTP User-Agent\n  # user_agent = \"telegraf\"\n\n  ## Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"gzip\"\n\n  ## Enable or disable uint support for writing uints influxdb 2.0.\n  # influx_uint_support = false\n\n  ## Optional TLS Config for use on HTTP connections.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "output",
      "name": "cratedb",
      "description": "Configuration for CrateDB to send metrics to.",
      "config": "# Configuration for CrateDB to send metrics to.\n[[outputs.cratedb]]\n  # alias=\"cratedb\"\n  # A github.com/jackc/pgx connection string.\n  # See https://godoc.org/github.com/jackc/pgx#ParseDSN\n  url = \"postgres://user:password@localhost/schema?sslmode=disable\"\n  # Timeout for all CrateDB queries.\n  timeout = \"5s\"\n  # Name of the table to store metrics in.\n  table = \"metrics\"\n  # If true, and the metrics table does not exist, create it automatically.\n  table_create = true\n\n"
    },
    {
      "type": "output",
      "name": "kafka",
      "description": "Configuration for the Kafka server to send metrics to",
      "config": "# Configuration for the Kafka server to send metrics to\n[[outputs.kafka]]\n  # alias=\"kafka\"\n  ## URLs of kafka brokers\n  brokers = [\"localhost:9092\"]\n  ## Kafka topic for producer messages\n  topic = \"telegraf\"\n\n  ## Optional Client id\n  # client_id = \"Telegraf\"\n\n  ## Set the minimal supported Kafka version.  Setting this enables the use of new\n  ## Kafka features and APIs.  Of particular interest, lz4 compression\n  ## requires at least version 0.10.0.0.\n  ##   ex: version = \"1.1.0\"\n  # version = \"\"\n\n  ## Optional topic suffix configuration.\n  ## If the section is omitted, no suffix is used.\n  ## Following topic suffix methods are supported:\n  ##   measurement - suffix equals to separator + measurement's name\n  ##   tags        - suffix equals to separator + specified tags' values\n  ##                 interleaved with separator\n\n  ## Suffix equals to \"_\" + measurement name\n  # [outputs.kafka.topic_suffix]\n  #   method = \"measurement\"\n  #   separator = \"_\"\n\n  ## Suffix equals to \"__\" + measurement's \"foo\" tag value.\n  ##   If there's no such a tag, suffix equals to an empty string\n  # [outputs.kafka.topic_suffix]\n  #   method = \"tags\"\n  #   keys = [\"foo\"]\n  #   separator = \"__\"\n\n  ## Suffix equals to \"_\" + measurement's \"foo\" and \"bar\"\n  ##   tag values, separated by \"_\". If there is no such tags,\n  ##   their values treated as empty strings.\n  # [outputs.kafka.topic_suffix]\n  #   method = \"tags\"\n  #   keys = [\"foo\", \"bar\"]\n  #   separator = \"_\"\n\n  ## Telegraf tag to use as a routing key\n  ##  ie, if this tag exists, its value will be used as the routing key\n  routing_tag = \"host\"\n\n  ## Static routing key.  Used when no routing_tag is set or as a fallback\n  ## when the tag specified in routing tag is not found.  If set to \"random\",\n  ## a random value will be generated for each message.\n  ##   ex: routing_key = \"random\"\n  ##       routing_key = \"telegraf\"\n  # routing_key = \"\"\n\n  ## CompressionCodec represents the various compression codecs recognized by\n  ## Kafka in messages.\n  ##  0 : No compression\n  ##  1 : Gzip compression\n  ##  2 : Snappy compression\n  ##  3 : LZ4 compression\n  # compression_codec = 0\n\n  ##  RequiredAcks is used in Produce Requests to tell the broker how many\n  ##  replica acknowledgements it must see before responding\n  ##   0 : the producer never waits for an acknowledgement from the broker.\n  ##       This option provides the lowest latency but the weakest durability\n  ##       guarantees (some data will be lost when a server fails).\n  ##   1 : the producer gets an acknowledgement after the leader replica has\n  ##       received the data. This option provides better durability as the\n  ##       client waits until the server acknowledges the request as successful\n  ##       (only messages that were written to the now-dead leader but not yet\n  ##       replicated will be lost).\n  ##   -1: the producer gets an acknowledgement after all in-sync replicas have\n  ##       received the data. This option provides the best durability, we\n  ##       guarantee that no messages will be lost as long as at least one in\n  ##       sync replica remains.\n  # required_acks = -1\n\n  ## The maximum number of times to retry sending a metric before failing\n  ## until the next flush.\n  # max_retry = 3\n\n  ## The maximum permitted size of a message. Should be set equal to or\n  ## smaller than the broker's 'message.max.bytes'.\n  # max_message_bytes = 1000000\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional SASL Config\n  # sasl_username = \"kafka\"\n  # sasl_password = \"secret\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "librato",
      "description": "Configuration for Librato API to send metrics to.",
      "config": "# Configuration for Librato API to send metrics to.\n[[outputs.librato]]\n  # alias=\"librato\"\n  ## Librator API Docs\n  ## http://dev.librato.com/v1/metrics-authentication\n  ## Librato API user\n  api_user = \"telegraf@influxdb.com\" # required.\n  ## Librato API token\n  api_token = \"my-secret-token\" # required.\n  ## Debug\n  # debug = false\n  ## Connection timeout.\n  # timeout = \"5s\"\n  ## Output source Template (same as graphite buckets)\n  ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n  ## This template is used in librato's source (not metric's name)\n  template = \"host\"\n\n\n"
    },
    {
      "type": "output",
      "name": "elasticsearch",
      "description": "Configuration for Elasticsearch to send metrics to.",
      "config": "# Configuration for Elasticsearch to send metrics to.\n[[outputs.elasticsearch]]\n  # alias=\"elasticsearch\"\n  ## The full HTTP endpoint URL for your Elasticsearch instance\n  ## Multiple urls can be specified as part of the same cluster,\n  ## this means that only ONE of the urls will be written to each interval.\n  urls = [ \"http://node1.es.example.com:9200\" ] # required.\n  ## Elasticsearch client timeout, defaults to \"5s\" if not set.\n  timeout = \"5s\"\n  ## Set to true to ask Elasticsearch a list of all cluster nodes,\n  ## thus it is not necessary to list all nodes in the urls config option.\n  enable_sniffer = false\n  ## Set the interval to check if the Elasticsearch nodes are available\n  ## Setting to \"0s\" will disable the health check (not recommended in production)\n  health_check_interval = \"10s\"\n  ## HTTP basic authentication details\n  # username = \"telegraf\"\n  # password = \"mypassword\"\n\n  ## Index Config\n  ## The target index for metrics (Elasticsearch will create if it not exists).\n  ## You can use the date specifiers below to create indexes per time frame.\n  ## The metric timestamp will be used to decide the destination index name\n  # %Y - year (2016)\n  # %y - last two digits of year (00..99)\n  # %m - month (01..12)\n  # %d - day of month (e.g., 01)\n  # %H - hour (00..23)\n  # %V - week of the year (ISO week) (01..53)\n  ## Additionally, you can specify a tag name using the notation {{tag_name}}\n  ## which will be used as part of the index name. If the tag does not exist,\n  ## the default tag value will be used.\n  # index_name = \"telegraf-{{host}}-%Y.%m.%d\"\n  # default_tag_value = \"none\"\n  index_name = \"telegraf-%Y.%m.%d\" # required.\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Template Config\n  ## Set to true if you want telegraf to manage its index template.\n  ## If enabled it will create a recommended index template for telegraf indexes\n  manage_template = true\n  ## The template name used for telegraf indexes\n  template_name = \"telegraf\"\n  ## Set to true if you want telegraf to overwrite an existing template\n  overwrite_template = false\n\n"
    },
    {
      "type": "output",
      "name": "instrumental",
      "description": "Configuration for sending metrics to an Instrumental project",
      "config": "# Configuration for sending metrics to an Instrumental project\n[[outputs.instrumental]]\n  # alias=\"instrumental\"\n  ## Project API Token (required)\n  api_token = \"API Token\" # required\n  ## Prefix the metrics with a given name\n  prefix = \"\"\n  ## Stats output template (Graphite formatting)\n  ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n  template = \"host.tags.measurement.field\"\n  ## Timeout in seconds to connect\n  timeout = \"2s\"\n  ## Display Communcation to Instrumental\n  debug = false\n\n"
    }
  ]
}
`{
  "version": "1.13.0",
  "os": "linux",
  "plugins": [
    {
      "type": "output",
      "name": "http",
      "description": "A plugin that can transmit metrics over HTTP",
      "config": "# A plugin that can transmit metrics over HTTP\n[[outputs.http]]\n  # alias=\"http\"\n  ## URL is the address to send metrics to\n  url = \"http://127.0.0.1:8080/telegraf\"\n\n  ## Timeout for HTTP message\n  # timeout = \"5s\"\n\n  ## HTTP method, one of: \"POST\" or \"PUT\"\n  # method = \"POST\"\n\n  ## HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## OAuth2 Client Credentials Grant\n  # client_id = \"clientid\"\n  # client_secret = \"secret\"\n  # token_url = \"https://indentityprovider/oauth2/v1/token\"\n  # scopes = [\"urn:opc:idm:__myscopes__\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Data format to output.\n  ## Each data format has it's own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Additional HTTP headers\n  # [outputs.http.headers]\n  #   # Should be set manually to \"application/json\" for json data_format\n  #   Content-Type = \"text/plain; charset=utf-8\"\n\n"
    },
    {
      "type": "output",
      "name": "influxdb",
      "description": "Configuration for sending metrics to InfluxDB",
      "config": "# Configuration for sending metrics to InfluxDB\n[[outputs.influxdb]]\n  # alias=\"influxdb\"\n  ## The full HTTP or UDP URL for your InfluxDB instance.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  # urls = [\"unix:///var/run/influxdb.sock\"]\n  # urls = [\"udp://127.0.0.1:8089\"]\n  # urls = [\"http://127.0.0.1:8086\"]\n\n  ## The target database for metrics; will be created as needed.\n  ## For UDP url endpoint database needs to be configured on server side.\n  # database = \"telegraf\"\n\n  ## The value of this tag will be used to determine the database.  If this\n  ## tag is not set the 'database' option is used as the default.\n  # database_tag = \"\"\n\n  ## If true, the database tag will not be added to the metric.\n  # exclude_database_tag = false\n\n  ## If true, no CREATE DATABASE queries will be sent.  Set to true when using\n  ## Telegraf with a user without permissions to create databases or when the\n  ## database already exists.\n  # skip_database_creation = false\n\n  ## Name of existing retention policy to write to.  Empty string writes to\n  ## the default retention policy.  Only takes effect when using HTTP.\n  # retention_policy = \"\"\n\n  ## Write consistency (clusters only), can be: \"any\", \"one\", \"quorum\", \"all\".\n  ## Only takes effect when using HTTP.\n  # write_consistency = \"any\"\n\n  ## Timeout for HTTP messages.\n  # timeout = \"5s\"\n\n  ## HTTP Basic Auth\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## HTTP User-Agent\n  # user_agent = \"telegraf\"\n\n  ## UDP payload size is the maximum packet size to send.\n  # udp_payload = \"512B\"\n\n  ## Optional TLS Config for use on HTTP connections.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## HTTP Proxy override, if unset values the standard proxy environment\n  ## variables are consulted to determine which proxy, if any, should be used.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## When true, Telegraf will output unsigned integers as unsigned values,\n  ## i.e.: \"42u\".  You will need a version of InfluxDB supporting unsigned\n  ## integer values.  Enabling this option will result in field type errors if\n  ## existing data has been written.\n  # influx_uint_support = false\n\n"
    },
    {
      "type": "output",
      "name": "exec",
      "description": "Send metrics to command as input over stdin",
      "config": "# Send metrics to command as input over stdin\n[[outputs.exec]]\n  # alias=\"exec\"\n  ## Command to injest metrics via stdin.\n  command = [\"tee\", \"-a\", \"/dev/null\"]\n\n  ## Timeout for command to complete.\n  # timeout = \"5s\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "graphite",
      "description": "Configuration for Graphite server to send metrics to",
      "config": "# Configuration for Graphite server to send metrics to\n[[outputs.graphite]]\n  # alias=\"graphite\"\n  ## TCP endpoint for your graphite instance.\n  ## If multiple endpoints are configured, output will be load balanced.\n  ## Only one of the endpoints will be written to with each iteration.\n  servers = [\"localhost:2003\"]\n  ## Prefix metrics name\n  prefix = \"\"\n  ## Graphite output template\n  ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  template = \"host.tags.measurement.field\"\n\n  ## Enable Graphite tags support\n  # graphite_tag_support = false\n\n  ## timeout in seconds for the write connection to graphite\n  timeout = 2\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "output",
      "name": "graylog",
      "description": "Send telegraf metrics to graylog(s)",
      "config": "# Send telegraf metrics to graylog(s)\n[[outputs.graylog]]\n  # alias=\"graylog\"\n  ## UDP endpoint for your graylog instance.\n  servers = [\"127.0.0.1:12201\", \"192.168.1.1:12201\"]\n\n"
    },
    {
      "type": "output",
      "name": "nats",
      "description": "Send telegraf measurements to NATS",
      "config": "# Send telegraf measurements to NATS\n[[outputs.nats]]\n  # alias=\"nats\"\n  ## URLs of NATS servers\n  servers = [\"nats://localhost:4222\"]\n  ## Optional credentials\n  # username = \"\"\n  # password = \"\"\n  ## NATS subject for producer messages\n  subject = \"telegraf\"\n\n  ## Use Transport Layer Security\n  # secure = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "prometheus_client",
      "description": "Configuration for the Prometheus client to spawn",
      "config": "# Configuration for the Prometheus client to spawn\n[[outputs.prometheus_client]]\n  # alias=\"prometheus_client\"\n  ## Address to listen on\n  listen = \":9273\"\n\n  ## Metric version controls the mapping from Telegraf metrics into\n  ## Prometheus format.  When using the prometheus input, use the same value in\n  ## both plugins to ensure metrics are round-tripped without modification.\n  ##\n  ##   example: metric_version = 1; deprecated in 1.13\n  ##            metric_version = 2; recommended version\n  # metric_version = 1\n\n  ## Use HTTP Basic Authentication.\n  # basic_username = \"Foo\"\n  # basic_password = \"Bar\"\n\n  ## If set, the IP Ranges which are allowed to access metrics.\n  ##   ex: ip_range = [\"192.168.0.0/24\", \"192.168.1.0/30\"]\n  # ip_range = []\n\n  ## Path to publish the metrics on.\n  # path = \"/metrics\"\n\n  ## Expiration interval for each metric. 0 == no expiration\n  # expiration_interval = \"60s\"\n\n  ## Collectors to enable, valid entries are \"gocollector\" and \"process\".\n  ## If unset, both are enabled.\n  # collectors_exclude = [\"gocollector\", \"process\"]\n\n  ## Send string metrics as Prometheus labels.\n  ## Unless set to false all string metrics will be sent as labels.\n  # string_as_label = true\n\n  ## If set, enable TLS with the given certificate.\n  # tls_cert = \"/etc/ssl/telegraf.crt\"\n  # tls_key = \"/etc/ssl/telegraf.key\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Export metric collection time.\n  # export_timestamp = false\n\n"
    },
    {
      "type": "output",
      "name": "riemann",
      "description": "Configuration for the Riemann server to send metrics to",
      "config": "# Configuration for the Riemann server to send metrics to\n[[outputs.riemann]]\n  # alias=\"riemann\"\n  ## The full TCP or UDP URL of the Riemann server\n  url = \"tcp://localhost:5555\"\n\n  ## Riemann event TTL, floating-point time in seconds.\n  ## Defines how long that an event is considered valid for in Riemann\n  # ttl = 30.0\n\n  ## Separator to use between measurement and field name in Riemann service name\n  ## This does not have any effect if 'measurement_as_attribute' is set to 'true'\n  separator = \"/\"\n\n  ## Set measurement name as Riemann attribute 'measurement', instead of prepending it to the Riemann service name\n  # measurement_as_attribute = false\n\n  ## Send string metrics as Riemann event states.\n  ## Unless enabled all string metrics will be ignored\n  # string_as_state = false\n\n  ## A list of tag keys whose values get sent as Riemann tags.\n  ## If empty, all Telegraf tag values will be sent as tags\n  # tag_keys = [\"telegraf\",\"custom_tag\"]\n\n  ## Additional Riemann tags to send.\n  # tags = [\"telegraf-output\"]\n\n  ## Description for Riemann event\n  # description_text = \"metrics collected from telegraf\"\n\n  ## Riemann client write timeout, defaults to \"5s\" if not set.\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "output",
      "name": "wavefront",
      "description": "Configuration for Wavefront server to send metrics to",
      "config": "# Configuration for Wavefront server to send metrics to\n[[outputs.wavefront]]\n  # alias=\"wavefront\"\n  ## Url for Wavefront Direct Ingestion or using HTTP with Wavefront Proxy\n  ## If using Wavefront Proxy, also specify port. example: http://proxyserver:2878\n  url = \"https://metrics.wavefront.com\"\n\n  ## Authentication Token for Wavefront. Only required if using Direct Ingestion\n  #token = \"DUMMY_TOKEN\"  \n  \n  ## DNS name of the wavefront proxy server. Do not use if url is specified\n  #host = \"wavefront.example.com\"\n\n  ## Port that the Wavefront proxy server listens on. Do not use if url is specified\n  #port = 2878\n\n  ## prefix for metrics keys\n  #prefix = \"my.specific.prefix.\"\n\n  ## whether to use \"value\" for name of simple fields. default is false\n  #simple_fields = false\n\n  ## character to use between metric and field name.  default is . (dot)\n  #metric_separator = \".\"\n\n  ## Convert metric name paths to use metricSeparator character\n  ## When true will convert all _ (underscore) characters in final metric name. default is true\n  #convert_paths = true\n\n  ## Use Strict rules to sanitize metric and tag names from invalid characters\n  ## When enabled forward slash (/) and comma (,) will be accpeted\n  #use_strict = false\n\n  ## Use Regex to sanitize metric and tag names from invalid characters\n  ## Regex is more thorough, but significantly slower. default is false\n  #use_regex = false\n\n  ## point tags to use as the source name for Wavefront (if none found, host will be used)\n  #source_override = [\"hostname\", \"address\", \"agent_host\", \"node_host\"]\n\n  ## whether to convert boolean values to numeric values, with false -\u003e 0.0 and true -\u003e 1.0. default is true\n  #convert_bool = true\n\n  ## Define a mapping, namespaced by metric prefix, from string values to numeric values\n  ##   deprecated in 1.9; use the enum processor plugin\n  #[[outputs.wavefront.string_to_number.elasticsearch]]\n  #  green = 1.0\n  #  yellow = 0.5\n  #  red = 0.0\n\n"
    },
    {
      "type": "output",
      "name": "cloudwatch",
      "description": "Configuration for AWS CloudWatch output.",
      "config": "# Configuration for AWS CloudWatch output.\n[[outputs.cloudwatch]]\n  # alias=\"cloudwatch\"\n  ## Amazon REGION\n  region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Assumed credentials via STS if role_arn is specified\n  ## 2) explicit credentials from 'access_key' and 'secret_key'\n  ## 3) shared profile from 'profile'\n  ## 4) environment variables\n  ## 5) shared credentials file\n  ## 6) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Namespace for the CloudWatch MetricDatums\n  namespace = \"InfluxData/Telegraf\"\n\n  ## If you have a large amount of metrics, you should consider to send statistic \n  ## values instead of raw metrics which could not only improve performance but \n  ## also save AWS API cost. If enable this flag, this plugin would parse the required \n  ## CloudWatch statistic fields (count, min, max, and sum) and send them to CloudWatch. \n  ## You could use basicstats aggregator to calculate those fields. If not all statistic \n  ## fields are available, all fields would still be sent as raw metrics. \n  # write_statistics = false\n\n  ## Enable high resolution metrics of 1 second (if not enabled, standard resolution are of 60 seconds precision)\n  # high_resolution_metrics = false\n\n"
    },
    {
      "type": "output",
      "name": "datadog",
      "description": "Configuration for DataDog API to send metrics to.",
      "config": "# Configuration for DataDog API to send metrics to.\n[[outputs.datadog]]\n  # alias=\"datadog\"\n  ## Datadog API key\n  apikey = \"my-secret-key\" # required.\n\n  # The base endpoint URL can optionally be specified but it defaults to:\n  #url = \"https://app.datadoghq.com/api/v1/series\"\n\n  ## Connection timeout.\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "output",
      "name": "discard",
      "description": "Send metrics to nowhere at all",
      "config": "# Send metrics to nowhere at all\n[[outputs.discard]]\n  # alias=\"discard\"\n"
    },
    {
      "type": "output",
      "name": "health",
      "description": "Configurable HTTP health check resource based on metrics",
      "config": "# Configurable HTTP health check resource based on metrics\n[[outputs.health]]\n  # alias=\"health\"\n  ## Address and port to listen on.\n  ##   ex: service_address = \"http://localhost:8080\"\n  ##       service_address = \"unix:///var/run/telegraf-health.sock\"\n  # service_address = \"http://:8080\"\n\n  ## The maximum duration for reading the entire request.\n  # read_timeout = \"5s\"\n  ## The maximum duration for writing the entire response.\n  # write_timeout = \"5s\"\n\n  ## Username and password to accept for HTTP basic authentication.\n  # basic_username = \"user1\"\n  # basic_password = \"secret\"\n\n  ## Allowed CA certificates for client certificates.\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## TLS server certificate and private key.\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## One or more check sub-tables should be defined, it is also recommended to\n  ## use metric filtering to limit the metrics that flow into this output.\n  ##\n  ## When using the default buffer sizes, this example will fail when the\n  ## metric buffer is half full.\n  ##\n  ## namepass = [\"internal_write\"]\n  ## tagpass = { output = [\"influxdb\"] }\n  ##\n  ## [[outputs.health.compares]]\n  ##   field = \"buffer_size\"\n  ##   lt = 5000.0\n  ##\n  ## [[outputs.health.contains]]\n  ##   field = \"buffer_size\"\n\n"
    },
    {
      "type": "output",
      "name": "kinesis",
      "description": "Configuration for the AWS Kinesis output.",
      "config": "# Configuration for the AWS Kinesis output.\n[[outputs.kinesis]]\n  # alias=\"kinesis\"\n  ## Amazon REGION of kinesis endpoint.\n  region = \"ap-southeast-2\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Assumed credentials via STS if role_arn is specified\n  ## 2) explicit credentials from 'access_key' and 'secret_key'\n  ## 3) shared profile from 'profile'\n  ## 4) environment variables\n  ## 5) shared credentials file\n  ## 6) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Kinesis StreamName must exist prior to starting telegraf.\n  streamname = \"StreamName\"\n  ## DEPRECATED: PartitionKey as used for sharding data.\n  partitionkey = \"PartitionKey\"\n  ## DEPRECATED: If set the paritionKey will be a random UUID on every put.\n  ## This allows for scaling across multiple shards in a stream.\n  ## This will cause issues with ordering.\n  use_random_partitionkey = false\n  ## The partition key can be calculated using one of several methods:\n  ##\n  ## Use a static value for all writes:\n  #  [outputs.kinesis.partition]\n  #    method = \"static\"\n  #    key = \"howdy\"\n  #\n  ## Use a random partition key on each write:\n  #  [outputs.kinesis.partition]\n  #    method = \"random\"\n  #\n  ## Use the measurement name as the partition key:\n  #  [outputs.kinesis.partition]\n  #    method = \"measurement\"\n  #\n  ## Use the value of a tag for all writes, if the tag is not set the empty\n  ## default option will be used. When no default, defaults to \"telegraf\"\n  #  [outputs.kinesis.partition]\n  #    method = \"tag\"\n  #    key = \"host\"\n  #    default = \"mykey\"\n\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n  ## debug will show upstream aws messages.\n  debug = false\n\n"
    },
    {
      "type": "output",
      "name": "riemann_legacy",
      "description": "Configuration for the Riemann server to send metrics to",
      "config": "# Configuration for the Riemann server to send metrics to\n[[outputs.riemann_legacy]]\n  # alias=\"riemann_legacy\"\n  ## URL of server\n  url = \"localhost:5555\"\n  ## transport protocol to use either tcp or udp\n  transport = \"tcp\"\n  ## separator to use between input name and field name in Riemann service name\n  separator = \" \"\n\n"
    },
    {
      "type": "output",
      "name": "stackdriver",
      "description": "Configuration for Google Cloud Stackdriver to send metrics to",
      "config": "# Configuration for Google Cloud Stackdriver to send metrics to\n[[outputs.stackdriver]]\n  # alias=\"stackdriver\"\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## The namespace for the metric descriptor\n  namespace = \"telegraf\"\n\n  ## Custom resource type\n  # resource_type = \"generic_node\"\n\n  ## Additonal resource labels\n  # [outputs.stackdriver.resource_labels]\n  #   node_id = \"$HOSTNAME\"\n  #   namespace = \"myapp\"\n  #   location = \"eu-north0\"\n\n"
    },
    {
      "type": "output",
      "name": "amon",
      "description": "Configuration for Amon Server to send metrics to.",
      "config": "# Configuration for Amon Server to send metrics to.\n[[outputs.amon]]\n  # alias=\"amon\"\n  ## Amon Server Key\n  server_key = \"my-server-key\" # required.\n\n  ## Amon Instance URL\n  amon_instance = \"https://youramoninstance\" # required\n\n  ## Connection timeout.\n  # timeout = \"5s\"\n\n"
    },
    {
      "type": "output",
      "name": "application_insights",
      "description": "Send metrics to Azure Application Insights",
      "config": "# Send metrics to Azure Application Insights\n[[outputs.application_insights]]\n  # alias=\"application_insights\"\n  ## Instrumentation key of the Application Insights resource.\n  instrumentation_key = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx\"\n\n  ## Timeout for closing (default: 5s).\n  # timeout = \"5s\"\n\n  ## Enable additional diagnostic logging.\n  # enable_diagnostic_logging = false\n\n  ## Context Tag Sources add Application Insights context tags to a tag value.\n  ##\n  ## For list of allowed context tag keys see:\n  ## https://github.com/Microsoft/ApplicationInsights-Go/blob/master/appinsights/contracts/contexttagkeys.go\n  # [outputs.application_insights.context_tag_sources]\n  #   \"ai.cloud.role\" = \"kubernetes_container_name\"\n  #   \"ai.cloud.roleInstance\" = \"kubernetes_pod_name\"\n\n"
    },
    {
      "type": "output",
      "name": "file",
      "description": "Send telegraf metrics to file(s)",
      "config": "# Send telegraf metrics to file(s)\n[[outputs.file]]\n  # alias=\"file\"\n  ## Files to write to, \"stdout\" is a specially handled file.\n  files = [\"stdout\", \"/tmp/metrics.out\"]\n\n  ## Use batch serialization format instead of line based delimiting.  The\n  ## batch format allows for the production of non line based output formats and\n  ## may more effiently encode metric groups.\n  # use_batch_format = false\n\n  ## The file will be rotated after the time interval specified.  When set\n  ## to 0 no time based rotation is performed.\n  # rotation_interval = \"0d\"\n\n  ## The logfile will be rotated when it becomes larger than the specified\n  ## size.  When set to 0 no size based rotation is performed.\n  # rotation_max_size = \"0MB\"\n\n  ## Maximum number of rotated archives to keep, any older logs are deleted.\n  ## If set to -1, no archives are removed.\n  # rotation_max_archives = 5\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "opentsdb",
      "description": "Configuration for OpenTSDB server to send metrics to",
      "config": "# Configuration for OpenTSDB server to send metrics to\n[[outputs.opentsdb]]\n  # alias=\"opentsdb\"\n  ## prefix for metrics keys\n  prefix = \"my.specific.prefix.\"\n\n  ## DNS name of the OpenTSDB server\n  ## Using \"opentsdb.example.com\" or \"tcp://opentsdb.example.com\" will use the\n  ## telnet API. \"http://opentsdb.example.com\" will use the Http API.\n  host = \"opentsdb.example.com\"\n\n  ## Port of the OpenTSDB server\n  port = 4242\n\n  ## Number of data points to send to OpenTSDB in Http requests.\n  ## Not used with telnet API.\n  http_batch_size = 50\n\n  ## URI Path for Http requests to OpenTSDB.\n  ## Used in cases where OpenTSDB is located behind a reverse proxy.\n  http_path = \"/api/put\"\n\n  ## Debug true - Prints OpenTSDB communication\n  debug = false\n\n  ## Separator separates measurement name from field\n  separator = \"_\"\n\n"
    },
    {
      "type": "output",
      "name": "amqp",
      "description": "Publishes metrics to an AMQP broker",
      "config": "# Publishes metrics to an AMQP broker\n[[outputs.amqp]]\n  # alias=\"amqp\"\n  ## Broker to publish to.\n  ##   deprecated in 1.7; use the brokers option\n  # url = \"amqp://localhost:5672/influxdb\"\n\n  ## Brokers to publish to.  If multiple brokers are specified a random broker\n  ## will be selected anytime a connection is established.  This can be\n  ## helpful for load balancing when not using a dedicated load balancer.\n  brokers = [\"amqp://localhost:5672/influxdb\"]\n\n  ## Maximum messages to send over a connection.  Once this is reached, the\n  ## connection is closed and a new connection is made.  This can be helpful for\n  ## load balancing when not using a dedicated load balancer.\n  # max_messages = 0\n\n  ## Exchange to declare and publish to.\n  exchange = \"telegraf\"\n\n  ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n  # exchange_type = \"topic\"\n\n  ## If true, exchange will be passively declared.\n  # exchange_passive = false\n\n  ## Exchange durability can be either \"transient\" or \"durable\".\n  # exchange_durability = \"durable\"\n\n  ## Additional exchange arguments.\n  # exchange_arguments = { }\n  # exchange_arguments = {\"hash_propery\" = \"timestamp\"}\n\n  ## Authentication credentials for the PLAIN auth_method.\n  # username = \"\"\n  # password = \"\"\n\n  ## Auth method. PLAIN and EXTERNAL are supported\n  ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n  ## described here: https://www.rabbitmq.com/plugins.html\n  # auth_method = \"PLAIN\"\n\n  ## Metric tag to use as a routing key.\n  ##   ie, if this tag exists, its value will be used as the routing key\n  # routing_tag = \"host\"\n\n  ## Static routing key.  Used when no routing_tag is set or as a fallback\n  ## when the tag specified in routing tag is not found.\n  # routing_key = \"\"\n  # routing_key = \"telegraf\"\n\n  ## Delivery Mode controls if a published message is persistent.\n  ##   One of \"transient\" or \"persistent\".\n  # delivery_mode = \"transient\"\n\n  ## InfluxDB database added as a message header.\n  ##   deprecated in 1.7; use the headers option\n  # database = \"telegraf\"\n\n  ## InfluxDB retention policy added as a message header\n  ##   deprecated in 1.7; use the headers option\n  # retention_policy = \"default\"\n\n  ## Static headers added to each published message.\n  # headers = { }\n  # headers = {\"database\" = \"telegraf\", \"retention_policy\" = \"default\"}\n\n  ## Connection timeout.  If not provided, will default to 5s.  0s means no\n  ## timeout (not recommended).\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## If true use batch serialization format instead of line based delimiting.\n  ## Only applies to data formats which are not line based such as JSON.\n  ## Recommended to set to true.\n  # use_batch_format = false\n\n  ## Content encoding for message payloads, can be set to \"gzip\" to or\n  ## \"identity\" to apply no encoding.\n  ##\n  ## Please note that when use_batch_format = false each amqp message contains only\n  ## a single metric, it is recommended to use compression with batch format\n  ## for best results.\n  # content_encoding = \"identity\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "azure_monitor",
      "description": "Send aggregate metrics to Azure Monitor",
      "config": "# Send aggregate metrics to Azure Monitor\n[[outputs.azure_monitor]]\n  # alias=\"azure_monitor\"\n  ## Timeout for HTTP writes.\n  # timeout = \"20s\"\n\n  ## Set the namespace prefix, defaults to \"Telegraf/\u003cinput-name\u003e\".\n  # namespace_prefix = \"Telegraf/\"\n\n  ## Azure Monitor doesn't have a string value type, so convert string\n  ## fields to dimensions (a.k.a. tags) if enabled. Azure Monitor allows\n  ## a maximum of 10 dimensions so Telegraf will only send the first 10\n  ## alphanumeric dimensions.\n  # strings_as_dimensions = false\n\n  ## Both region and resource_id must be set or be available via the\n  ## Instance Metadata service on Azure Virtual Machines.\n  #\n  ## Azure Region to publish metrics against.\n  ##   ex: region = \"southcentralus\"\n  # region = \"\"\n  #\n  ## The Azure Resource ID against which metric will be logged, e.g.\n  ##   ex: resource_id = \"/subscriptions/\u003csubscription_id\u003e/resourceGroups/\u003cresource_group\u003e/providers/Microsoft.Compute/virtualMachines/\u003cvm_name\u003e\"\n  # resource_id = \"\"\n\n  ## Optionally, if in Azure US Government, China or other sovereign\n  ## cloud environment, set appropriate REST endpoint for receiving\n  ## metrics. (Note: region may be unused in this context)\n  # endpoint_url = \"https://monitoring.core.usgovcloudapi.net\"\n\n"
    },
    {
      "type": "output",
      "name": "syslog",
      "description": "Configuration for Syslog server to send metrics to",
      "config": "# Configuration for Syslog server to send metrics to\n[[outputs.syslog]]\n  # alias=\"syslog\"\n  ## URL to connect to\n  ## ex: address = \"tcp://127.0.0.1:8094\"\n  ## ex: address = \"tcp4://127.0.0.1:8094\"\n  ## ex: address = \"tcp6://127.0.0.1:8094\"\n  ## ex: address = \"tcp6://[2001:db8::1]:8094\"\n  ## ex: address = \"udp://127.0.0.1:8094\"\n  ## ex: address = \"udp4://127.0.0.1:8094\"\n  ## ex: address = \"udp6://127.0.0.1:8094\"\n  address = \"tcp://127.0.0.1:8094\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Period between keep alive probes.\n  ## Only applies to TCP sockets.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  # keep_alive_period = \"5m\"\n\n  ## The framing technique with which it is expected that messages are\n  ## transported (default = \"octet-counting\").  Whether the messages come\n  ## using the octect-counting (RFC5425#section-4.3.1, RFC6587#section-3.4.1),\n  ## or the non-transparent framing technique (RFC6587#section-3.4.2).  Must\n  ## be one of \"octet-counting\", \"non-transparent\".\n  # framing = \"octet-counting\"\n\n  ## The trailer to be expected in case of non-trasparent framing (default = \"LF\").\n  ## Must be one of \"LF\", or \"NUL\".\n  # trailer = \"LF\"\n\n  ## SD-PARAMs settings\n  ## Syslog messages can contain key/value pairs within zero or more\n  ## structured data sections.  For each unrecognised metric tag/field a\n  ## SD-PARAMS is created.\n  ##\n  ## Example:\n  ##   [[outputs.syslog]]\n  ##     sdparam_separator = \"_\"\n  ##     default_sdid = \"default@32473\"\n  ##     sdids = [\"foo@123\", \"bar@456\"]\n  ##\n  ##   input =\u003e xyzzy,x=y foo@123_value=42,bar@456_value2=84,something_else=1\n  ##   output (structured data only) =\u003e [foo@123 value=42][bar@456 value2=84][default@32473 something_else=1 x=y]\n\n  ## SD-PARAMs separator between the sdid and tag/field key (default = \"_\")\n  # sdparam_separator = \"_\"\n\n  ## Default sdid used for tags/fields that don't contain a prefix defined in\n  ## the explict sdids setting below If no default is specified, no SD-PARAMs\n  ## will be used for unrecognised field.\n  # default_sdid = \"default@32473\"\n\n  ## List of explicit prefixes to extract from tag/field keys and use as the\n  ## SDID, if they match (see above example for more details):\n  # sdids = [\"foo@123\", \"bar@456\"]\n\n  ## Default severity value. Severity and Facility are used to calculate the\n  ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field\n  ## with key \"severity_code\" is defined.  If unset, 5 (notice) is the default\n  # default_severity_code = 5\n\n  ## Default facility value. Facility and Severity are used to calculate the\n  ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field with\n  ## key \"facility_code\" is defined.  If unset, 1 (user-level) is the default\n  # default_facility_code = 1\n\n  ## Default APP-NAME value (RFC5424#section-6.2.5)\n  ## Used when no metric tag with key \"appname\" is defined.\n  ## If unset, \"Telegraf\" is the default\n  # default_appname = \"Telegraf\"\n\n"
    },
    {
      "type": "output",
      "name": "nsq",
      "description": "Send telegraf measurements to NSQD",
      "config": "# Send telegraf measurements to NSQD\n[[outputs.nsq]]\n  # alias=\"nsq\"\n  ## Location of nsqd instance listening on TCP\n  server = \"localhost:4150\"\n  ## NSQ topic for producer messages\n  topic = \"telegraf\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "socket_writer",
      "description": "Generic socket writer capable of handling multiple socket types.",
      "config": "# Generic socket writer capable of handling multiple socket types.\n[[outputs.socket_writer]]\n  # alias=\"socket_writer\"\n  ## URL to connect to\n  # address = \"tcp://127.0.0.1:8094\"\n  # address = \"tcp://example.com:http\"\n  # address = \"tcp4://127.0.0.1:8094\"\n  # address = \"tcp6://127.0.0.1:8094\"\n  # address = \"tcp6://[2001:db8::1]:8094\"\n  # address = \"udp://127.0.0.1:8094\"\n  # address = \"udp4://127.0.0.1:8094\"\n  # address = \"udp6://127.0.0.1:8094\"\n  # address = \"unix:///tmp/telegraf.sock\"\n  # address = \"unixgram:///tmp/telegraf.sock\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Period between keep alive probes.\n  ## Only applies to TCP sockets.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  # keep_alive_period = \"5m\"\n\n  ## Data format to generate.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "mqtt",
      "description": "Configuration for MQTT server to send metrics to",
      "config": "# Configuration for MQTT server to send metrics to\n[[outputs.mqtt]]\n  # alias=\"mqtt\"\n  servers = [\"localhost:1883\"] # required.\n\n  ## MQTT outputs send metrics to this topic format\n  ##    \"\u003ctopic_prefix\u003e/\u003chostname\u003e/\u003cpluginname\u003e/\"\n  ##   ex: prefix/web01.example.com/mem\n  topic_prefix = \"telegraf\"\n\n  ## QoS policy for messages\n  ##   0 = at most once\n  ##   1 = at least once\n  ##   2 = exactly once\n  # qos = 2\n\n  ## username and password to connect MQTT server.\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## client ID, if not set a random ID is generated\n  # client_id = \"\"\n\n  ## Timeout for write operations. default: 5s\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## When true, metrics will be sent in one MQTT message per flush.  Otherwise,\n  ## metrics are written one metric per MQTT message.\n  # batch = false\n\n  ## When true, metric will have RETAIN flag set, making broker cache entries until someone\n  ## actually reads it\n  # retain = false\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "cloud_pubsub",
      "description": "Publish Telegraf metrics to a Google Cloud PubSub topic",
      "config": "# Publish Telegraf metrics to a Google Cloud PubSub topic\n[[outputs.cloud_pubsub]]\n  # alias=\"cloud_pubsub\"\n  ## Required. Name of Google Cloud Platform (GCP) Project that owns\n  ## the given PubSub topic.\n  project = \"my-project\"\n\n  ## Required. Name of PubSub topic to publish metrics to.\n  topic = \"my-topic\"\n\n  ## Required. Data format to consume.\n  ## Each data format has its own unique set of configuration options.\n  ## Read more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n  ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n  ## Application Default Credentials, which is preferred.\n  # credentials_file = \"path/to/my/creds.json\"\n\n  ## Optional. If true, will send all metrics per write in one PubSub message.\n  # send_batched = true\n\n  ## The following publish_* parameters specifically configures batching\n  ## requests made to the GCP Cloud PubSub API via the PubSub Golang library. Read\n  ## more here: https://godoc.org/cloud.google.com/go/pubsub#PublishSettings\n\n  ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n  ## when it has this many PubSub messages. If send_batched is true,\n  ## this is ignored and treated as if it were 1.\n  # publish_count_threshold = 1000\n\n  ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n  ## when it has this many PubSub messages. If send_batched is true,\n  ## this is ignored and treated as if it were 1\n  # publish_byte_threshold = 1000000\n\n  ## Optional. Specifically configures requests made to the PubSub API.\n  # publish_num_go_routines = 2\n\n  ## Optional. Specifies a timeout for requests to the PubSub API.\n  # publish_timeout = \"30s\"\n\n  ## Optional. If true, published PubSub message data will be base64-encoded.\n  # base64_data = false\n\n  ## Optional. PubSub attributes to add to metrics.\n  # [[inputs.pubsub.attributes]]\n  #   my_attr = \"tag_value\"\n\n"
    },
    {
      "type": "output",
      "name": "influxdb_v2",
      "description": "Configuration for sending metrics to InfluxDB",
      "config": "# Configuration for sending metrics to InfluxDB\n[[outputs.influxdb_v2]]\n  # alias=\"influxdb_v2\"\n  ## The URLs of the InfluxDB cluster nodes.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  ##   ex: urls = [\"https://us-west-2-1.aws.cloud2.influxdata.com\"]\n  urls = [\"http://127.0.0.1:8086\"]\n\n  ## Token for authentication.\n  token = \"\"\n\n  ## Organization is the name of the organization you wish to write to; must exist.\n  organization = \"\"\n\n  ## Destination bucket to write into.\n  bucket = \"\"\n\n  ## The value of this tag will be used to determine the bucket.  If this\n  ## tag is not set the 'bucket' option is used as the default.\n  # bucket_tag = \"\"\n\n  ## If true, the bucket tag will not be added to the metric.\n  # exclude_bucket_tag = false\n\n  ## Timeout for HTTP messages.\n  # timeout = \"5s\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP Proxy override, if unset values the standard proxy environment\n  ## variables are consulted to determine which proxy, if any, should be used.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## HTTP User-Agent\n  # user_agent = \"telegraf\"\n\n  ## Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"gzip\"\n\n  ## Enable or disable uint support for writing uints influxdb 2.0.\n  # influx_uint_support = false\n\n  ## Optional TLS Config for use on HTTP connections.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n"
    },
    {
      "type": "output",
      "name": "cratedb",
      "description": "Configuration for CrateDB to send metrics to.",
      "config": "# Configuration for CrateDB to send metrics to.\n[[outputs.cratedb]]\n  # alias=\"cratedb\"\n  # A github.com/jackc/pgx connection string.\n  # See https://godoc.org/github.com/jackc/pgx#ParseDSN\n  url = \"postgres://user:password@localhost/schema?sslmode=disable\"\n  # Timeout for all CrateDB queries.\n  timeout = \"5s\"\n  # Name of the table to store metrics in.\n  table = \"metrics\"\n  # If true, and the metrics table does not exist, create it automatically.\n  table_create = true\n\n"
    },
    {
      "type": "output",
      "name": "kafka",
      "description": "Configuration for the Kafka server to send metrics to",
      "config": "# Configuration for the Kafka server to send metrics to\n[[outputs.kafka]]\n  # alias=\"kafka\"\n  ## URLs of kafka brokers\n  brokers = [\"localhost:9092\"]\n  ## Kafka topic for producer messages\n  topic = \"telegraf\"\n\n  ## Optional Client id\n  # client_id = \"Telegraf\"\n\n  ## Set the minimal supported Kafka version.  Setting this enables the use of new\n  ## Kafka features and APIs.  Of particular interest, lz4 compression\n  ## requires at least version 0.10.0.0.\n  ##   ex: version = \"1.1.0\"\n  # version = \"\"\n\n  ## Optional topic suffix configuration.\n  ## If the section is omitted, no suffix is used.\n  ## Following topic suffix methods are supported:\n  ##   measurement - suffix equals to separator + measurement's name\n  ##   tags        - suffix equals to separator + specified tags' values\n  ##                 interleaved with separator\n\n  ## Suffix equals to \"_\" + measurement name\n  # [outputs.kafka.topic_suffix]\n  #   method = \"measurement\"\n  #   separator = \"_\"\n\n  ## Suffix equals to \"__\" + measurement's \"foo\" tag value.\n  ##   If there's no such a tag, suffix equals to an empty string\n  # [outputs.kafka.topic_suffix]\n  #   method = \"tags\"\n  #   keys = [\"foo\"]\n  #   separator = \"__\"\n\n  ## Suffix equals to \"_\" + measurement's \"foo\" and \"bar\"\n  ##   tag values, separated by \"_\". If there is no such tags,\n  ##   their values treated as empty strings.\n  # [outputs.kafka.topic_suffix]\n  #   method = \"tags\"\n  #   keys = [\"foo\", \"bar\"]\n  #   separator = \"_\"\n\n  ## Telegraf tag to use as a routing key\n  ##  ie, if this tag exists, its value will be used as the routing key\n  routing_tag = \"host\"\n\n  ## Static routing key.  Used when no routing_tag is set or as a fallback\n  ## when the tag specified in routing tag is not found.  If set to \"random\",\n  ## a random value will be generated for each message.\n  ##   ex: routing_key = \"random\"\n  ##       routing_key = \"telegraf\"\n  # routing_key = \"\"\n\n  ## CompressionCodec represents the various compression codecs recognized by\n  ## Kafka in messages.\n  ##  0 : No compression\n  ##  1 : Gzip compression\n  ##  2 : Snappy compression\n  ##  3 : LZ4 compression\n  # compression_codec = 0\n\n  ##  RequiredAcks is used in Produce Requests to tell the broker how many\n  ##  replica acknowledgements it must see before responding\n  ##   0 : the producer never waits for an acknowledgement from the broker.\n  ##       This option provides the lowest latency but the weakest durability\n  ##       guarantees (some data will be lost when a server fails).\n  ##   1 : the producer gets an acknowledgement after the leader replica has\n  ##       received the data. This option provides better durability as the\n  ##       client waits until the server acknowledges the request as successful\n  ##       (only messages that were written to the now-dead leader but not yet\n  ##       replicated will be lost).\n  ##   -1: the producer gets an acknowledgement after all in-sync replicas have\n  ##       received the data. This option provides the best durability, we\n  ##       guarantee that no messages will be lost as long as at least one in\n  ##       sync replica remains.\n  # required_acks = -1\n\n  ## The maximum number of times to retry sending a metric before failing\n  ## until the next flush.\n  # max_retry = 3\n\n  ## The maximum permitted size of a message. Should be set equal to or\n  ## smaller than the broker's 'message.max.bytes'.\n  # max_message_bytes = 1000000\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional SASL Config\n  # sasl_username = \"kafka\"\n  # sasl_password = \"secret\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n\n"
    },
    {
      "type": "output",
      "name": "librato",
      "description": "Configuration for Librato API to send metrics to.",
      "config": "# Configuration for Librato API to send metrics to.\n[[outputs.librato]]\n  # alias=\"librato\"\n  ## Librator API Docs\n  ## http://dev.librato.com/v1/metrics-authentication\n  ## Librato API user\n  api_user = \"telegraf@influxdb.com\" # required.\n  ## Librato API token\n  api_token = \"my-secret-token\" # required.\n  ## Debug\n  # debug = false\n  ## Connection timeout.\n  # timeout = \"5s\"\n  ## Output source Template (same as graphite buckets)\n  ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n  ## This template is used in librato's source (not metric's name)\n  template = \"host\"\n\n\n"
    },
    {
      "type": "output",
      "name": "elasticsearch",
      "description": "Configuration for Elasticsearch to send metrics to.",
      "config": "# Configuration for Elasticsearch to send metrics to.\n[[outputs.elasticsearch]]\n  # alias=\"elasticsearch\"\n  ## The full HTTP endpoint URL for your Elasticsearch instance\n  ## Multiple urls can be specified as part of the same cluster,\n  ## this means that only ONE of the urls will be written to each interval.\n  urls = [ \"http://node1.es.example.com:9200\" ] # required.\n  ## Elasticsearch client timeout, defaults to \"5s\" if not set.\n  timeout = \"5s\"\n  ## Set to true to ask Elasticsearch a list of all cluster nodes,\n  ## thus it is not necessary to list all nodes in the urls config option.\n  enable_sniffer = false\n  ## Set the interval to check if the Elasticsearch nodes are available\n  ## Setting to \"0s\" will disable the health check (not recommended in production)\n  health_check_interval = \"10s\"\n  ## HTTP basic authentication details\n  # username = \"telegraf\"\n  # password = \"mypassword\"\n\n  ## Index Config\n  ## The target index for metrics (Elasticsearch will create if it not exists).\n  ## You can use the date specifiers below to create indexes per time frame.\n  ## The metric timestamp will be used to decide the destination index name\n  # %Y - year (2016)\n  # %y - last two digits of year (00..99)\n  # %m - month (01..12)\n  # %d - day of month (e.g., 01)\n  # %H - hour (00..23)\n  # %V - week of the year (ISO week) (01..53)\n  ## Additionally, you can specify a tag name using the notation {{tag_name}}\n  ## which will be used as part of the index name. If the tag does not exist,\n  ## the default tag value will be used.\n  # index_name = \"telegraf-{{host}}-%Y.%m.%d\"\n  # default_tag_value = \"none\"\n  index_name = \"telegraf-%Y.%m.%d\" # required.\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Template Config\n  ## Set to true if you want telegraf to manage its index template.\n  ## If enabled it will create a recommended index template for telegraf indexes\n  manage_template = true\n  ## The template name used for telegraf indexes\n  template_name = \"telegraf\"\n  ## Set to true if you want telegraf to overwrite an existing template\n  overwrite_template = false\n\n"
    },
    {
      "type": "output",
      "name": "instrumental",
      "description": "Configuration for sending metrics to an Instrumental project",
      "config": "# Configuration for sending metrics to an Instrumental project\n[[outputs.instrumental]]\n  # alias=\"instrumental\"\n  ## Project API Token (required)\n  api_token = \"API Token\" # required\n  ## Prefix the metrics with a given name\n  prefix = \"\"\n  ## Stats output template (Graphite formatting)\n  ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n  template = \"host.tags.measurement.field\"\n  ## Timeout in seconds to connect\n  timeout = \"2s\"\n  ## Display Communcation to Instrumental\n  debug = false\n\n"
    }
  ]
}
`{
  "version": "1.13.0",
  "os": "linux",
  "plugins": [
    {
      "type": "processor",
      "name": "converter",
      "description": "Convert values to another metric value type",
      "config": "# Convert values to another metric value type\n[[processors.converter]]\n  # alias=\"converter\"\n  ## Tags to convert\n  ##\n  ## The table key determines the target type, and the array of key-values\n  ## select the keys to convert.  The array may contain globs.\n  ##   \u003ctarget-type\u003e = [\u003ctag-key\u003e...]\n  [processors.converter.tags]\n    string = []\n    integer = []\n    unsigned = []\n    boolean = []\n    float = []\n\n  ## Fields to convert\n  ##\n  ## The table key determines the target type, and the array of key-values\n  ## select the keys to convert.  The array may contain globs.\n  ##   \u003ctarget-type\u003e = [\u003cfield-key\u003e...]\n  [processors.converter.fields]\n    tag = []\n    string = []\n    integer = []\n    unsigned = []\n    boolean = []\n    float = []\n\n"
    },
    {
      "type": "processor",
      "name": "override",
      "description": "Apply metric modifications using override semantics.",
      "config": "# Apply metric modifications using override semantics.\n[[processors.override]]\n  # alias=\"override\"\n  ## All modifications on inputs and aggregators can be overridden:\n  # name_override = \"new_name\"\n  # name_prefix = \"new_name_prefix\"\n  # name_suffix = \"new_name_suffix\"\n\n  ## Tags to be added (all values must be strings)\n  # [processors.override.tags]\n  #   additional_tag = \"tag_value\"\n\n"
    },
    {
      "type": "processor",
      "name": "strings",
      "description": "Perform string processing on tags, fields, and measurements",
      "config": "# Perform string processing on tags, fields, and measurements\n[[processors.strings]]\n  # alias=\"strings\"\n  ## Convert a tag value to uppercase\n  # [[processors.strings.uppercase]]\n  #   tag = \"method\"\n\n  ## Convert a field value to lowercase and store in a new field\n  # [[processors.strings.lowercase]]\n  #   field = \"uri_stem\"\n  #   dest = \"uri_stem_normalised\"\n\n  ## Trim leading and trailing whitespace using the default cutset\n  # [[processors.strings.trim]]\n  #   field = \"message\"\n\n  ## Trim leading characters in cutset\n  # [[processors.strings.trim_left]]\n  #   field = \"message\"\n  #   cutset = \"\\t\"\n\n  ## Trim trailing characters in cutset\n  # [[processors.strings.trim_right]]\n  #   field = \"message\"\n  #   cutset = \"\\r\\n\"\n\n  ## Trim the given prefix from the field\n  # [[processors.strings.trim_prefix]]\n  #   field = \"my_value\"\n  #   prefix = \"my_\"\n\n  ## Trim the given suffix from the field\n  # [[processors.strings.trim_suffix]]\n  #   field = \"read_count\"\n  #   suffix = \"_count\"\n\n  ## Replace all non-overlapping instances of old with new\n  # [[processors.strings.replace]]\n  #   measurement = \"*\"\n  #   old = \":\"\n  #   new = \"_\"\n\n  ## Trims strings based on width\n  # [[processors.strings.left]]\n  #   field = \"message\"\n  #   width = 10\n\n  ## Decode a base64 encoded utf-8 string\n  # [[processors.strings.base64decode]]\n  #   field = \"message\"\n\n"
    },
    {
      "type": "processor",
      "name": "tag_limit",
      "description": "Restricts the number of tags that can pass through this filter and chooses which tags to preserve when over the limit.",
      "config": "# Restricts the number of tags that can pass through this filter and chooses which tags to preserve when over the limit.\n[[processors.tag_limit]]\n  # alias=\"tag_limit\"\n  ## Maximum number of tags to preserve\n  limit = 10\n\n  ## List of tags to preferentially preserve\n  keep = [\"foo\", \"bar\", \"baz\"]\n\n"
    },
    {
      "type": "processor",
      "name": "date",
      "description": "Dates measurements, tags, and fields that pass through this filter.",
      "config": "# Dates measurements, tags, and fields that pass through this filter.\n[[processors.date]]\n  # alias=\"date\"\n  ## New tag to create\n  tag_key = \"month\"\n\n  ## Date format string, must be a representation of the Go \"reference time\"\n  ## which is \"Mon Jan 2 15:04:05 -0700 MST 2006\".\n  date_format = \"Jan\"\n\n"
    },
    {
      "type": "processor",
      "name": "parser",
      "description": "Parse a value in a specified field/tag(s) and add the result in a new metric",
      "config": "# Parse a value in a specified field/tag(s) and add the result in a new metric\n[[processors.parser]]\n  # alias=\"parser\"\n  ## The name of the fields whose value will be parsed.\n  parse_fields = []\n\n  ## If true, incoming metrics are not emitted.\n  drop_original = false\n\n  ## If set to override, emitted metrics will be merged by overriding the\n  ## original metric using the newly parsed metrics.\n  merge = \"override\"\n\n  ## The dataformat to be read from files\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "processor",
      "name": "pivot",
      "description": "Rotate a single valued metric into a multi field metric",
      "config": "# Rotate a single valued metric into a multi field metric\n[[processors.pivot]]\n  # alias=\"pivot\"\n  ## Tag to use for naming the new field.\n  tag_key = \"name\"\n  ## Field to use as the value of the new field.\n  value_key = \"value\"\n\n"
    },
    {
      "type": "processor",
      "name": "printer",
      "description": "Print all metrics that pass through this filter.",
      "config": "# Print all metrics that pass through this filter.\n[[processors.printer]]\n  # alias=\"printer\"\n\n"
    },
    {
      "type": "processor",
      "name": "clone",
      "description": "Clone metrics and apply modifications.",
      "config": "# Clone metrics and apply modifications.\n[[processors.clone]]\n  # alias=\"clone\"\n  ## All modifications on inputs and aggregators can be overridden:\n  # name_override = \"new_name\"\n  # name_prefix = \"new_name_prefix\"\n  # name_suffix = \"new_name_suffix\"\n\n  ## Tags to be added (all values must be strings)\n  # [processors.clone.tags]\n  #   additional_tag = \"tag_value\"\n\n"
    },
    {
      "type": "processor",
      "name": "enum",
      "description": "Map enum values according to given table.",
      "config": "# Map enum values according to given table.\n[[processors.enum]]\n  # alias=\"enum\"\n  [[processors.enum.mapping]]\n    ## Name of the field to map\n    field = \"status\"\n\n    ## Name of the tag to map\n    # tag = \"status\"\n\n    ## Destination tag or field to be used for the mapped value.  By default the\n    ## source tag or field is used, overwriting the original value.\n    dest = \"status_code\"\n\n    ## Default value to be used for all values not contained in the mapping\n    ## table.  When unset, the unmodified value for the field will be used if no\n    ## match is found.\n    # default = 0\n\n    ## Table of mappings\n    [processors.enum.mapping.value_mappings]\n      green = 1\n      amber = 2\n      red = 3\n\n"
    },
    {
      "type": "processor",
      "name": "rename",
      "description": "Rename measurements, tags, and fields that pass through this filter.",
      "config": "# Rename measurements, tags, and fields that pass through this filter.\n[[processors.rename]]\n  # alias=\"rename\"\n\n"
    },
    {
      "type": "processor",
      "name": "topk",
      "description": "Print all metrics that pass through this filter.",
      "config": "# Print all metrics that pass through this filter.\n[[processors.topk]]\n  # alias=\"topk\"\n  ## How many seconds between aggregations\n  # period = 10\n\n  ## How many top metrics to return\n  # k = 10\n\n  ## Over which tags should the aggregation be done. Globs can be specified, in\n  ## which case any tag matching the glob will aggregated over. If set to an\n  ## empty list is no aggregation over tags is done\n  # group_by = ['*']\n\n  ## Over which fields are the top k are calculated\n  # fields = [\"value\"]\n\n  ## What aggregation to use. Options: sum, mean, min, max\n  # aggregation = \"mean\"\n\n  ## Instead of the top k largest metrics, return the bottom k lowest metrics\n  # bottomk = false\n\n  ## The plugin assigns each metric a GroupBy tag generated from its name and\n  ## tags. If this setting is different than \"\" the plugin will add a\n  ## tag (which name will be the value of this setting) to each metric with\n  ## the value of the calculated GroupBy tag. Useful for debugging\n  # add_groupby_tag = \"\"\n\n  ## These settings provide a way to know the position of each metric in\n  ## the top k. The 'add_rank_field' setting allows to specify for which\n  ## fields the position is required. If the list is non empty, then a field\n  ## will be added to each and every metric for each string present in this\n  ## setting. This field will contain the ranking of the group that\n  ## the metric belonged to when aggregated over that field.\n  ## The name of the field will be set to the name of the aggregation field,\n  ## suffixed with the string '_topk_rank'\n  # add_rank_fields = []\n\n  ## These settings provide a way to know what values the plugin is generating\n  ## when aggregating metrics. The 'add_agregate_field' setting allows to\n  ## specify for which fields the final aggregation value is required. If the\n  ## list is non empty, then a field will be added to each every metric for\n  ## each field present in this setting. This field will contain\n  ## the computed aggregation for the group that the metric belonged to when\n  ## aggregated over that field.\n  ## The name of the field will be set to the name of the aggregation field,\n  ## suffixed with the string '_topk_aggregate'\n  # add_aggregate_fields = []\n\n"
    },
    {
      "type": "processor",
      "name": "regex",
      "description": "Transforms tag and field values with regex pattern",
      "config": "# Transforms tag and field values with regex pattern\n[[processors.regex]]\n  # alias=\"regex\"\n  ## Tag and field conversions defined in a separate sub-tables\n  # [[processors.regex.tags]]\n  #   ## Tag to change\n  #   key = \"resp_code\"\n  #   ## Regular expression to match on a tag value\n  #   pattern = \"^(\\\\d)\\\\d\\\\d$\"\n  #   ## Matches of the pattern will be replaced with this string.  Use ${1}\n  #   ## notation to use the text of the first submatch.\n  #   replacement = \"${1}xx\"\n\n  # [[processors.regex.fields]]\n  #   ## Field to change\n  #   key = \"request\"\n  #   ## All the power of the Go regular expressions available here\n  #   ## For example, named subgroups\n  #   pattern = \"^/api(?P\u003cmethod\u003e/[\\\\w/]+)\\\\S*\"\n  #   replacement = \"${method}\"\n  #   ## If result_key is present, a new field will be created\n  #   ## instead of changing existing field\n  #   result_key = \"method\"\n\n  ## Multiple conversions may be applied for one field sequentially\n  ## Let's extract one more value\n  # [[processors.regex.fields]]\n  #   key = \"request\"\n  #   pattern = \".*category=(\\\\w+).*\"\n  #   replacement = \"${1}\"\n  #   result_key = \"search_category\"\n\n"
    },
    {
      "type": "processor",
      "name": "unpivot",
      "description": "Rotate multi field metric into several single field metrics",
      "config": "# Rotate multi field metric into several single field metrics\n[[processors.unpivot]]\n  # alias=\"unpivot\"\n  ## Tag to use for the name.\n  tag_key = \"name\"\n  ## Field to use for the name of the value.\n  value_key = \"value\"\n\n"
    }
  ]
}
`{
  "version": "1.13.0",
  "os": "linux",
  "plugins": [
    {
      "type": "processor",
      "name": "converter",
      "description": "Convert values to another metric value type",
      "config": "# Convert values to another metric value type\n[[processors.converter]]\n  # alias=\"converter\"\n  ## Tags to convert\n  ##\n  ## The table key determines the target type, and the array of key-values\n  ## select the keys to convert.  The array may contain globs.\n  ##   \u003ctarget-type\u003e = [\u003ctag-key\u003e...]\n  [processors.converter.tags]\n    string = []\n    integer = []\n    unsigned = []\n    boolean = []\n    float = []\n\n  ## Fields to convert\n  ##\n  ## The table key determines the target type, and the array of key-values\n  ## select the keys to convert.  The array may contain globs.\n  ##   \u003ctarget-type\u003e = [\u003cfield-key\u003e...]\n  [processors.converter.fields]\n    tag = []\n    string = []\n    integer = []\n    unsigned = []\n    boolean = []\n    float = []\n\n"
    },
    {
      "type": "processor",
      "name": "override",
      "description": "Apply metric modifications using override semantics.",
      "config": "# Apply metric modifications using override semantics.\n[[processors.override]]\n  # alias=\"override\"\n  ## All modifications on inputs and aggregators can be overridden:\n  # name_override = \"new_name\"\n  # name_prefix = \"new_name_prefix\"\n  # name_suffix = \"new_name_suffix\"\n\n  ## Tags to be added (all values must be strings)\n  # [processors.override.tags]\n  #   additional_tag = \"tag_value\"\n\n"
    },
    {
      "type": "processor",
      "name": "strings",
      "description": "Perform string processing on tags, fields, and measurements",
      "config": "# Perform string processing on tags, fields, and measurements\n[[processors.strings]]\n  # alias=\"strings\"\n  ## Convert a tag value to uppercase\n  # [[processors.strings.uppercase]]\n  #   tag = \"method\"\n\n  ## Convert a field value to lowercase and store in a new field\n  # [[processors.strings.lowercase]]\n  #   field = \"uri_stem\"\n  #   dest = \"uri_stem_normalised\"\n\n  ## Trim leading and trailing whitespace using the default cutset\n  # [[processors.strings.trim]]\n  #   field = \"message\"\n\n  ## Trim leading characters in cutset\n  # [[processors.strings.trim_left]]\n  #   field = \"message\"\n  #   cutset = \"\\t\"\n\n  ## Trim trailing characters in cutset\n  # [[processors.strings.trim_right]]\n  #   field = \"message\"\n  #   cutset = \"\\r\\n\"\n\n  ## Trim the given prefix from the field\n  # [[processors.strings.trim_prefix]]\n  #   field = \"my_value\"\n  #   prefix = \"my_\"\n\n  ## Trim the given suffix from the field\n  # [[processors.strings.trim_suffix]]\n  #   field = \"read_count\"\n  #   suffix = \"_count\"\n\n  ## Replace all non-overlapping instances of old with new\n  # [[processors.strings.replace]]\n  #   measurement = \"*\"\n  #   old = \":\"\n  #   new = \"_\"\n\n  ## Trims strings based on width\n  # [[processors.strings.left]]\n  #   field = \"message\"\n  #   width = 10\n\n  ## Decode a base64 encoded utf-8 string\n  # [[processors.strings.base64decode]]\n  #   field = \"message\"\n\n"
    },
    {
      "type": "processor",
      "name": "tag_limit",
      "description": "Restricts the number of tags that can pass through this filter and chooses which tags to preserve when over the limit.",
      "config": "# Restricts the number of tags that can pass through this filter and chooses which tags to preserve when over the limit.\n[[processors.tag_limit]]\n  # alias=\"tag_limit\"\n  ## Maximum number of tags to preserve\n  limit = 10\n\n  ## List of tags to preferentially preserve\n  keep = [\"foo\", \"bar\", \"baz\"]\n\n"
    },
    {
      "type": "processor",
      "name": "date",
      "description": "Dates measurements, tags, and fields that pass through this filter.",
      "config": "# Dates measurements, tags, and fields that pass through this filter.\n[[processors.date]]\n  # alias=\"date\"\n  ## New tag to create\n  tag_key = \"month\"\n\n  ## Date format string, must be a representation of the Go \"reference time\"\n  ## which is \"Mon Jan 2 15:04:05 -0700 MST 2006\".\n  date_format = \"Jan\"\n\n"
    },
    {
      "type": "processor",
      "name": "parser",
      "description": "Parse a value in a specified field/tag(s) and add the result in a new metric",
      "config": "# Parse a value in a specified field/tag(s) and add the result in a new metric\n[[processors.parser]]\n  # alias=\"parser\"\n  ## The name of the fields whose value will be parsed.\n  parse_fields = []\n\n  ## If true, incoming metrics are not emitted.\n  drop_original = false\n\n  ## If set to override, emitted metrics will be merged by overriding the\n  ## original metric using the newly parsed metrics.\n  merge = \"override\"\n\n  ## The dataformat to be read from files\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n"
    },
    {
      "type": "processor",
      "name": "pivot",
      "description": "Rotate a single valued metric into a multi field metric",
      "config": "# Rotate a single valued metric into a multi field metric\n[[processors.pivot]]\n  # alias=\"pivot\"\n  ## Tag to use for naming the new field.\n  tag_key = \"name\"\n  ## Field to use as the value of the new field.\n  value_key = \"value\"\n\n"
    },
    {
      "type": "processor",
      "name": "printer",
      "description": "Print all metrics that pass through this filter.",
      "config": "# Print all metrics that pass through this filter.\n[[processors.printer]]\n  # alias=\"printer\"\n\n"
    },
    {
      "type": "processor",
      "name": "clone",
      "description": "Clone metrics and apply modifications.",
      "config": "# Clone metrics and apply modifications.\n[[processors.clone]]\n  # alias=\"clone\"\n  ## All modifications on inputs and aggregators can be overridden:\n  # name_override = \"new_name\"\n  # name_prefix = \"new_name_prefix\"\n  # name_suffix = \"new_name_suffix\"\n\n  ## Tags to be added (all values must be strings)\n  # [processors.clone.tags]\n  #   additional_tag = \"tag_value\"\n\n"
    },
    {
      "type": "processor",
      "name": "enum",
      "description": "Map enum values according to given table.",
      "config": "# Map enum values according to given table.\n[[processors.enum]]\n  # alias=\"enum\"\n  [[processors.enum.mapping]]\n    ## Name of the field to map\n    field = \"status\"\n\n    ## Name of the tag to map\n    # tag = \"status\"\n\n    ## Destination tag or field to be used for the mapped value.  By default the\n    ## source tag or field is used, overwriting the original value.\n    dest = \"status_code\"\n\n    ## Default value to be used for all values not contained in the mapping\n    ## table.  When unset, the unmodified value for the field will be used if no\n    ## match is found.\n    # default = 0\n\n    ## Table of mappings\n    [processors.enum.mapping.value_mappings]\n      green = 1\n      amber = 2\n      red = 3\n\n"
    },
    {
      "type": "processor",
      "name": "rename",
      "description": "Rename measurements, tags, and fields that pass through this filter.",
      "config": "# Rename measurements, tags, and fields that pass through this filter.\n[[processors.rename]]\n  # alias=\"rename\"\n\n"
    },
    {
      "type": "processor",
      "name": "topk",
      "description": "Print all metrics that pass through this filter.",
      "config": "# Print all metrics that pass through this filter.\n[[processors.topk]]\n  # alias=\"topk\"\n  ## How many seconds between aggregations\n  # period = 10\n\n  ## How many top metrics to return\n  # k = 10\n\n  ## Over which tags should the aggregation be done. Globs can be specified, in\n  ## which case any tag matching the glob will aggregated over. If set to an\n  ## empty list is no aggregation over tags is done\n  # group_by = ['*']\n\n  ## Over which fields are the top k are calculated\n  # fields = [\"value\"]\n\n  ## What aggregation to use. Options: sum, mean, min, max\n  # aggregation = \"mean\"\n\n  ## Instead of the top k largest metrics, return the bottom k lowest metrics\n  # bottomk = false\n\n  ## The plugin assigns each metric a GroupBy tag generated from its name and\n  ## tags. If this setting is different than \"\" the plugin will add a\n  ## tag (which name will be the value of this setting) to each metric with\n  ## the value of the calculated GroupBy tag. Useful for debugging\n  # add_groupby_tag = \"\"\n\n  ## These settings provide a way to know the position of each metric in\n  ## the top k. The 'add_rank_field' setting allows to specify for which\n  ## fields the position is required. If the list is non empty, then a field\n  ## will be added to each and every metric for each string present in this\n  ## setting. This field will contain the ranking of the group that\n  ## the metric belonged to when aggregated over that field.\n  ## The name of the field will be set to the name of the aggregation field,\n  ## suffixed with the string '_topk_rank'\n  # add_rank_fields = []\n\n  ## These settings provide a way to know what values the plugin is generating\n  ## when aggregating metrics. The 'add_agregate_field' setting allows to\n  ## specify for which fields the final aggregation value is required. If the\n  ## list is non empty, then a field will be added to each every metric for\n  ## each field present in this setting. This field will contain\n  ## the computed aggregation for the group that the metric belonged to when\n  ## aggregated over that field.\n  ## The name of the field will be set to the name of the aggregation field,\n  ## suffixed with the string '_topk_aggregate'\n  # add_aggregate_fields = []\n\n"
    },
    {
      "type": "processor",
      "name": "regex",
      "description": "Transforms tag and field values with regex pattern",
      "config": "# Transforms tag and field values with regex pattern\n[[processors.regex]]\n  # alias=\"regex\"\n  ## Tag and field conversions defined in a separate sub-tables\n  # [[processors.regex.tags]]\n  #   ## Tag to change\n  #   key = \"resp_code\"\n  #   ## Regular expression to match on a tag value\n  #   pattern = \"^(\\\\d)\\\\d\\\\d$\"\n  #   ## Matches of the pattern will be replaced with this string.  Use ${1}\n  #   ## notation to use the text of the first submatch.\n  #   replacement = \"${1}xx\"\n\n  # [[processors.regex.fields]]\n  #   ## Field to change\n  #   key = \"request\"\n  #   ## All the power of the Go regular expressions available here\n  #   ## For example, named subgroups\n  #   pattern = \"^/api(?P\u003cmethod\u003e/[\\\\w/]+)\\\\S*\"\n  #   replacement = \"${method}\"\n  #   ## If result_key is present, a new field will be created\n  #   ## instead of changing existing field\n  #   result_key = \"method\"\n\n  ## Multiple conversions may be applied for one field sequentially\n  ## Let's extract one more value\n  # [[processors.regex.fields]]\n  #   key = \"request\"\n  #   pattern = \".*category=(\\\\w+).*\"\n  #   replacement = \"${1}\"\n  #   result_key = \"search_category\"\n\n"
    },
    {
      "type": "processor",
      "name": "unpivot",
      "description": "Rotate multi field metric into several single field metrics",
      "config": "# Rotate multi field metric into several single field metrics\n[[processors.unpivot]]\n  # alias=\"unpivot\"\n  ## Tag to use for the name.\n  tag_key = \"name\"\n  ## Field to use for the name of the value.\n  value_key = \"value\"\n\n"
    }
  ]
}
`{
  "version": "1.13.0",
  "os": "linux",
  "plugins": [
    {
      "type": "aggregator",
      "name": "merge",
      "description": "Merge metrics into multifield metrics by series key",
      "config": "# Merge metrics into multifield metrics by series key\n[[aggregators.merge]]\n  # alias=\"merge\"\n"
    },
    {
      "type": "aggregator",
      "name": "minmax",
      "description": "Keep the aggregate min/max of each metric passing through.",
      "config": "# Keep the aggregate min/max of each metric passing through.\n[[aggregators.minmax]]\n  # alias=\"minmax\"\n  ## General Aggregator Arguments:\n  ## The period on which to flush \u0026 clear the aggregator.\n  period = \"30s\"\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  drop_original = false\n\n"
    },
    {
      "type": "aggregator",
      "name": "valuecounter",
      "description": "Count the occurrence of values in fields.",
      "config": "# Count the occurrence of values in fields.\n[[aggregators.valuecounter]]\n  # alias=\"valuecounter\"\n  ## General Aggregator Arguments:\n  ## The period on which to flush \u0026 clear the aggregator.\n  period = \"30s\"\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  drop_original = false\n  ## The fields for which the values will be counted\n  fields = []\n\n"
    },
    {
      "type": "aggregator",
      "name": "basicstats",
      "description": "Keep the aggregate basicstats of each metric passing through.",
      "config": "# Keep the aggregate basicstats of each metric passing through.\n[[aggregators.basicstats]]\n  # alias=\"basicstats\"\n  ## The period on which to flush \u0026 clear the aggregator.\n  period = \"30s\"\n\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  drop_original = false\n\n  ## Configures which basic stats to push as fields\n  # stats = [\"count\", \"min\", \"max\", \"mean\", \"stdev\", \"s2\", \"sum\"]\n\n"
    },
    {
      "type": "aggregator",
      "name": "final",
      "description": "Report the final metric of a series",
      "config": "# Report the final metric of a series\n[[aggregators.final]]\n  # alias=\"final\"\n  ## The period on which to flush \u0026 clear the aggregator.\n  period = \"30s\"\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  drop_original = false\n\n  ## The time that a series is not updated until considering it final.\n  series_timeout = \"5m\"\n\n"
    },
    {
      "type": "aggregator",
      "name": "histogram",
      "description": "Create aggregate histograms.",
      "config": "# Create aggregate histograms.\n[[aggregators.histogram]]\n  # alias=\"histogram\"\n  ## The period in which to flush the aggregator.\n  period = \"30s\"\n\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  drop_original = false\n\n  ## If true, the histogram will be reset on flush instead\n  ## of accumulating the results.\n  reset = false\n\n  ## Example config that aggregates all fields of the metric.\n  # [[aggregators.histogram.config]]\n  #   ## The set of buckets.\n  #   buckets = [0.0, 15.6, 34.5, 49.1, 71.5, 80.5, 94.5, 100.0]\n  #   ## The name of metric.\n  #   measurement_name = \"cpu\"\n\n  ## Example config that aggregates only specific fields of the metric.\n  # [[aggregators.histogram.config]]\n  #   ## The set of buckets.\n  #   buckets = [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\n  #   ## The name of metric.\n  #   measurement_name = \"diskio\"\n  #   ## The concrete fields of metric\n  #   fields = [\"io_time\", \"read_time\", \"write_time\"]\n\n"
    }
  ]
}
`{
  "version": "1.13.0",
  "os": "linux",
  "plugins": [
    {
      "type": "aggregator",
      "name": "merge",
      "description": "Merge metrics into multifield metrics by series key",
      "config": "# Merge metrics into multifield metrics by series key\n[[aggregators.merge]]\n  # alias=\"merge\"\n"
    },
    {
      "type": "aggregator",
      "name": "minmax",
      "description": "Keep the aggregate min/max of each metric passing through.",
      "config": "# Keep the aggregate min/max of each metric passing through.\n[[aggregators.minmax]]\n  # alias=\"minmax\"\n  ## General Aggregator Arguments:\n  ## The period on which to flush \u0026 clear the aggregator.\n  period = \"30s\"\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  drop_original = false\n\n"
    },
    {
      "type": "aggregator",
      "name": "valuecounter",
      "description": "Count the occurrence of values in fields.",
      "config": "# Count the occurrence of values in fields.\n[[aggregators.valuecounter]]\n  # alias=\"valuecounter\"\n  ## General Aggregator Arguments:\n  ## The period on which to flush \u0026 clear the aggregator.\n  period = \"30s\"\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  drop_original = false\n  ## The fields for which the values will be counted\n  fields = []\n\n"
    },
    {
      "type": "aggregator",
      "name": "basicstats",
      "description": "Keep the aggregate basicstats of each metric passing through.",
      "config": "# Keep the aggregate basicstats of each metric passing through.\n[[aggregators.basicstats]]\n  # alias=\"basicstats\"\n  ## The period on which to flush \u0026 clear the aggregator.\n  period = \"30s\"\n\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  drop_original = false\n\n  ## Configures which basic stats to push as fields\n  # stats = [\"count\", \"min\", \"max\", \"mean\", \"stdev\", \"s2\", \"sum\"]\n\n"
    },
    {
      "type": "aggregator",
      "name": "final",
      "description": "Report the final metric of a series",
      "config": "# Report the final metric of a series\n[[aggregators.final]]\n  # alias=\"final\"\n  ## The period on which to flush \u0026 clear the aggregator.\n  period = \"30s\"\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  drop_original = false\n\n  ## The time that a series is not updated until considering it final.\n  series_timeout = \"5m\"\n\n"
    },
    {
      "type": "aggregator",
      "name": "histogram",
      "description": "Create aggregate histograms.",
      "config": "# Create aggregate histograms.\n[[aggregators.histogram]]\n  # alias=\"histogram\"\n  ## The period in which to flush the aggregator.\n  period = \"30s\"\n\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  drop_original = false\n\n  ## If true, the histogram will be reset on flush instead\n  ## of accumulating the results.\n  reset = false\n\n  ## Example config that aggregates all fields of the metric.\n  # [[aggregators.histogram.config]]\n  #   ## The set of buckets.\n  #   buckets = [0.0, 15.6, 34.5, 49.1, 71.5, 80.5, 94.5, 100.0]\n  #   ## The name of metric.\n  #   measurement_name = \"cpu\"\n\n  ## Example config that aggregates only specific fields of the metric.\n  # [[aggregators.histogram.config]]\n  #   ## The set of buckets.\n  #   buckets = [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\n  #   ## The name of metric.\n  #   measurement_name = \"diskio\"\n  #   ## The concrete fields of metric\n  #   fields = [\"io_time\", \"read_time\", \"write_time\"]\n\n"
    }
  ]
}
` Plugin defines a Telegraf plugin. Type of the plugin. Name of the plugin. Description of the plugin. Config contains the toml config of the plugin. TelegrafPlugins defines a Telegraf version's collection of plugins. Version of telegraf plugins are for. OS the plugins apply to. Plugins this version of telegraf supports. ListAvailablePlugins lists available plugins based on type. GetPlugin returns the plugin's sample config, if available. findPluginByName returns a plugin named "name". This should only be run on TelegrafPlugins containing the same type of plugin. AvailablePlugins returns the base list of available plugins. AvailableInputs returns the base list of available input plugins. AvailableOutputs returns the base list of available output plugins. AvailableProcessors returns the base list of available processor plugins. AvailableAggregators returns the base list of available aggregator plugins. AvailableBundles returns the base list of available bundled plugins. AgentConfig contains the default agent config./Users/austinjaybecker/projects/abeck-go-testing/telegraf/plugins/type.go Type is a telegraf plugin type. available types. Input is an input plugin. Output is an output plugin. Processor is a processor plugin. Aggregator is an aggregator plugin. Config interface for all plugins. Type is the plugin type PluginName is the string value of telegraf plugin package name./Users/austinjaybecker/projects/abeck-go-testing/telegraf/service/Users/austinjaybecker/projects/abeck-go-testing/telegraf/service/telegraf.goCorruptTelegrafErrorErrInvalidTelegrafIDErrInvalidTelegrafOrgIDErrTelegrafNotFoundErrUnprocessableTelegrafInternalTelegrafServiceErrorUnavailableTelegrafServiceErrormarshalTelegrafmarshalTelegrafPluginsunmarshalTelegraftelegraf configuration not found"telegraf configuration not found"provided telegraf configuration ID has invalid format"provided telegraf configuration ID has invalid format"provided telegraf configuration organization ID is missing or invalid"provided telegraf configuration organization ID is missing or invalid"Unable to connect to telegraf service. Please try again; Err: %v"Unable to connect to telegraf service. Please try again; Err: %v"kv/telegraf"kv/telegraf"Unknown internal telegraf data error; Err: %v"Unknown internal telegraf data error; Err: %v"unable to convert telegraf configuration into JSON; Err %v"unable to convert telegraf configuration into JSON; Err %v"Unable to connect to telegraf config stats service. Please try again; Err: %v"Unable to connect to telegraf config stats service. Please try again; Err: %v" ErrTelegrafNotFound is used when the telegraf configuration is not found. ErrInvalidTelegrafID is used when the service was provided ErrInvalidTelegrafOrgID is the error message for a missing or invalid organization ID. UnavailableTelegrafServiceError is used if we aren't able to interact with the InternalTelegrafServiceError is used when the error comes from an CorruptTelegrafError is used when the config cannot be unmarshalled from the ErrUnprocessableTelegraf is used when a telegraf is not able to be converted to JSON. Service is a telegraf config service. New constructs and configures a new telegraf config service. skip until offset reached stop cursing when limit is reached forward cursor entire bucket cursors do not support numeric offset but we can at least constrain the response size by the offset + limit since we are not doing any other filtering REMOVE this cursor option if you do any other filtering PutTelegrafConfig put a telegraf config to storage. insert index entry for orgID -> id removing index entry for orgID -> id unmarshalTelegraf turns the stored byte slice in the kv into a *influxdb.TelegrafConfig./Users/austinjaybecker/projects/abeck-go-testing/telegraf/service/testing/Users/austinjaybecker/projects/abeck-go-testing/telegraf/service/testing/testing.goTelegrafConfigFieldstelegrafCmpOptionstelegrafTestFactoryFuncgithub.com/influxdata/influxdb/v2/telegraf/plugins/inputs"github.com/influxdata/influxdb/v2/telegraf/plugins/inputs"github.com/influxdata/influxdb/v2/telegraf/plugins/outputs"github.com/influxdata/influxdb/v2/telegraf/plugins/outputs"IgnoreUnexported"CreateTelegrafConfig""FindTelegrafConfigByID""FindTelegrafConfigs""UpdateTelegrafConfig""DeleteTelegrafConfig"telegrafConfigcreate telegraf config without organization ID should error"create telegraf config without organization ID should error"invalid org IDcreate telegraf config with empty set"create telegraf config with empty set"[[inputs.cpu]]
[[outputs.influxdb_v2]]
"[[inputs.cpu]]\n[[outputs.influxdb_v2]]\n"basic create telegraf config"basic create telegraf config"tc1"tc1"[[inputs.mem_stats]]
"[[inputs.mem_stats]]\n"expected error '%v' got '%v'"expected error '%v' got '%v'"telegraf config ID not set from CreateTelegrafConfig"telegraf config ID not set from CreateTelegrafConfig"expected error messages to match '%v' got '%v'"expected error messages to match '%v' got '%v'"failed to retrieve telegraf configs: %v"failed to retrieve telegraf configs: %v"telegraf configs are different -got/+want
diff %s"telegraf configs are different -got/+want\ndiff %s"[[inputs.cpu]]
"[[inputs.cpu]]\n""tc2"[[inputs.file]]
[[inputs.mem]]
"[[inputs.file]]\n[[inputs.mem]]\n"expected errors to be equal '%v' got '%v'"expected errors to be equal '%v' got '%v'"expected error '%s' got '%s'"expected error '%s' got '%s'"find all telegraf configs (across orgs)"find all telegraf configs (across orgs)"filter by organization only"filter by organization only"tc3"tc3"tc4"tc4"empty for provided org"empty for provided org"find with limit and offset"find with limit and offset"telegraf config with ID %v not found"telegraf config with ID %v not found"config update"config update"[[inputs.file]]
[[inputs.kubernetes]]
[[inputs.kubernetes]]
"[[inputs.file]]\n[[inputs.kubernetes]]\n[[inputs.kubernetes]]\n"none existing config"none existing config" TelegrafConfigFields includes prepopulated data for mapping tests. TelegrafConfigStore tests all the service functions. CreateTelegrafConfig testing. for inmem test as it doesn't unmarshal.. FindTelegrafConfigByID testing. FindTelegrafConfigs testing UpdateTelegrafConfig testing. notice this get ignored - ie., resulting TelegrafConfig will have OrgID equal to fourID DeleteTelegrafConfig testing./Users/austinjaybecker/projects/abeck-go-testing/telegraf.gofoundstcdconfigDataArraydataOktpFn"invalid org ID"the telegraf plugin is name %s doesn't match the config %s"the telegraf plugin is name %s doesn't match the config %s"there is no telegraf plugin in the config"there is no telegraf plugin in the config"unsupported telegraf plugin type %s"unsupported telegraf plugin type %s"unsupported telegraf plugin %s, type %s"unsupported telegraf plugin %s, type %s"MustCompilePOSIX\[\[(inputs\..*|outputs\..*|aggregators\..*|processors\..*)\]\]`\[\[(inputs\..*|outputs\..*|aggregators\..*|processors\..*)\]\]`no config to get buckets"no config to get buckets""outputs"no plugins in config to get buckets"no plugins in config to get buckets"influxdb_v2 output has no config"influxdb_v2 output has no config"unmarshal telegraf config raw plugin"unmarshal telegraf config raw plugin" ErrTelegrafConfigInvalidOrgID is the error message for a missing or invalid organization ID. ErrTelegrafConfigNotFound is the error message for a missing telegraf config. TelegrafConfigFilter represents a set of filter that restrict the returned telegraf configs. TelegrafConfig stores telegraf config for one telegraf instance. ID of this config object. OrgID is the id of the owning organization. Name of this config object. Decription of this config object. ConfigTOML contains the raw toml config. Metadata for the config. CountPlugins returns a map of the number of times each plugin is used. UnmarshalJSON implement the json.Unmarshaler interface. Gets called when reading from the kv db. mostly legacy so loading old/stored configs still work. May not remove for a while. Primarily will get hit when user views/downloads config. Prefer new structure; use full toml config. legacy, remove after some moons. or a migration. Handles legacy adding of default plugins (agent and output). Get buckets from the config. return bucket, config, error if pr.Config if empty, make it a blank obj, so it will still go to the unmarshalling process to validate. telegrafConfigDecode is the helper struct for json decoding. legacy. telegrafPluginDecode is the helper struct for json decoding. legacy. Alias of the plugin. Config is the currently stored plugin configuration./Users/austinjaybecker/projects/abeck-go-testing/telemetry/Users/austinjaybecker/projects/abeck-go-testing/telemetry/handler.goAddTimestampsDefaultMaxBytesDefaultTimeoutErrMetricsTimestampPresentNewPusherdecodePostMetricsRequestmetricsFormatnsPerMillisecondtelemetryMatcherxformsmaxBytes1024000pushed metrics must not have timestamp"pushed metrics must not have timestamp"MethodHeadhttps://www.influxdata.com/telemetry"https://www.influxdata.com/telemetry"StatusSeeOtherGET, HEAD, PUT, POST"GET, HEAD, PUT, POST"FmtTexttext/plain; version=0.0.4; charset=utf-8Metrics format not support"Metrics format not support"Unable to decode metrics"Unable to decode metrics"Invalid metrics"Invalid metrics"Unable to encode metric families"Unable to encode metric families"Unable to write to store"Unable to write to store"StatusAcceptedFmtUnknown<unknown>unknown format metrics format"unknown format metrics format"LimitReader DefaultTimeout is the length of time servicing the metrics before canceling. DefaultMaxBytes is the largest request body read. ErrMetricsTimestampPresent is returned when the prometheus metrics has timestamps set. Not sure why, but, pushgateway does not allow timestamps. PushGateway handles receiving prometheus push metrics and forwards them to the Store. If Format is not set, the format of the inbound metrics are used. handler returns after this duration with an error; defaults to 5 seconds maximum number of bytes to read from the body; defaults to 1024000 NewPushGateway constructs the PushGateway. Handler accepts prometheus metrics send via the Push client and sends those metrics into the store. redirect to agreement to give our users information about this collected data. protect against reading too many bytes prom's pushgateway does not allow timestamps for some reason. Checks if any timestamps have been specified./Users/austinjaybecker/projects/abeck-go-testing/telemetry/metrics.gotask_scheduler_claims_active"task_scheduler_claims_active"http_api_requests_total"http_api_requests_total"storage_wal_writes_total"storage_wal_writes_total"query_control_requests_total"query_control_requests_total"query_control_functions_total"query_control_functions_total"query_control_all_duration_seconds"query_control_all_duration_seconds"http_api_request_duration_seconds_bucket"http_api_request_duration_seconds_bucket"storage_tsi_index_series_total"storage_tsi_index_series_total"storage_series_file_disk_bytes"storage_series_file_disk_bytes"storage_wal_current_segment_bytes"storage_wal_current_segment_bytes"storage_tsm_files_disk_bytes"storage_tsm_files_disk_bytes"
	 *   Runtime stats
	  includes version, os, etc.
	 * Resource Counts
	  Count of currently active tasks
	 * Count of API requests including success and failure
	 
	 * Count of writes and queries
	 
	 * Query analysis
	  Count of functions in queries (e.g. mean, median) Total query duration per org.
	 * Write analysis
	  Count only the durations of the /write endpoint.
	 * Storage cardinality
	 
	 * Storage disk usage
	  All families need to be aggregated to get a true idea of disk usage./Users/austinjaybecker/projects/abeck-go-testing/telemetry/push.gohttps://telemetry.influxdata.com/metrics/job/influxdb"https://telemetry.influxdata.com/metrics/job/influxdb"unable to POST metrics; received status %s: %s"unable to POST metrics; received status %s: %s" Pusher pushes metrics to a prometheus push gateway. NewPusher sends usage metrics to a prometheus push gateway. Push POSTs prometheus metrics in protobuf delimited format to a push gateway. when there are no metrics to send, then, no need to POST. FIXME: consider why we're checking for cancellation here./Users/austinjaybecker/projects/abeck-go-testing/telemetry/reporter.go"telemetry""interval"Starting"Starting"Failure reporting telemetry metrics"Failure reporting telemetry metrics"Reporting"Reporting" Reporter reports telemetry metrics to a prometheus push gateway every interval. NewReporter reports telemetry every 24 hours. Report starts periodic telemetry reporting each interval./Users/austinjaybecker/projects/abeck-go-testing/telemetry/store.go"Write" Store records usage data. WriteMessage stores data into the store. LogStore logs data written to the store. WriteMessage logs data at Info level./Users/austinjaybecker/projects/abeck-go-testing/telemetry/timestamps.gonowMilliseconds AddTimestamps enriches prometheus metrics by adding timestamps. Transform adds now as a timestamp to all metrics./Users/austinjaybecker/projects/abeck-go-testing/tenant/Users/austinjaybecker/projects/abeck-go-testing/tenant/doc.goAggregateErrorAuthedBucketServiceAuthedOrgServiceAuthedUserServiceBucketAlreadyExistsErrorBucketLoggerBucketMetricsBucketSvcCorruptURMErrorEIncorrectPasswordEIncorrectUserEShortPasswordErrBucketNameNotUniqueErrBucketNotFoundByNameErrCorruptBucketErrCorruptOrgErrCorruptUserErrIDNotUniqueErrInvalidURMIDErrNameisEmptyErrNotFoundErrOnboardInvalidErrOnboardingNotAllowedErrURMNotFoundErrUnprocessableBucketErrUnprocessableMappingErrUnprocessableOrgErrUnprocessableUserInvalidOrgIDErrorInvalidUserIDErrorNewAggregateErrorNewAuthedBucketServiceNewAuthedOrgServiceNewAuthedUserServiceNewBucketLoggerNewBucketMetricsNewBucketSvcNewHTTPBucketHandlerNewHTTPOrgHandlerNewHTTPUserHandlerNewOnboardingResponseNewOrgLoggerNewOrgMetricsNewOrganizationSvcNewPasswordLoggerNewPasswordMetricsNewURMLoggerNewUrmMetricsNewUserLoggerNewUserMetricsNewUserResourceMappingSvcNewUserSvcNonUniqueMappingErrorOrgAlreadyExistsErrorOrgLoggerOrgMetricsOrgNotFoundByNameOrgSvcPasswordLoggerPasswordMetricsURMLoggerURMSvcUnavailablePasswordServiceErrorUnavailableURMServiceErrorUnexpectedUserBucketErrorUnexpectedUserIndexErrorUrmMetricsUserAlreadyExistsErrorUserLoggerUserMetricsUserSvcbucketIndexbucketIndexKeybucketOperationLogKeyPrefixbucketUpdatectxInternaldecodeOnboardRequestencodeBucketOperationLogKeyencodeOrganizationOperationLogKeyencodeUserOperationLogKeyencryptPassworderrDeleteSystemBucketerrRenameSystemBucketerrSlicegetRequestinternalCtxinvalidBucketListRequestisInternalisOnboardingResponsemarshalBucketmarshalOrgmarshalUsernewBucketUpdatenewOrgResponsenewOrgsResponseonboardingResponseorgOperationLogKeyPrefixorgResponseorganizationIndexorganizationIndexKeyorgsResponsepostBucketRequestpostRequestprefixOnboardunmarshalBucketunmarshalOrgunmarshalUseruserIndexuserOperationLogKeyPrefixuserResourceKeyuserResourcePrefixKeyuserTypeFromPathuserpasswordBucketvalidBucketName
The tenant domain encapsulates all the storage critical metadata services:
User
Organization
Bucket
URM's

These services are the cornerstone of all other metadata services. The intent is to have
a single location for all tenant related code. THis should facilitate faster bug resolution and
allow us to make changes to this service without effecting any dependant services.

When a new request for the tenant service comes in it should follow this pattern:
1 http_server_resource - this is where the request is parsed and rejected if the client didn't send
	the right information
2 middleware_resource_auth - We now confirm the user that generated the request has sufficient permission
	to accomplish this task, in some cases we adjust the request if the user is without the correct permissions
3 middleware_resource_metrics - Track RED metrics for this request
4 middleware_resource_logging - add logging around request duration and status.
5 service_resource - When a request reaches the service we verify the content for compatibility with the existing dataset,
	for instance if a resource has a "orgID" we will ensure the organization exists
6 storage_resource - Basic CRUD actions for the system.

This pattern of api -> middleware -> service -> basic crud helps us to break down the responsibilities into digestible
chunks and allows us to swap in or out any pieces we need depending on the situation. Currently the storage layer is using
a kv store but by breaking the crud actions into its own independent set of concerns we allow ourselves to move away from kv
if the need arises without having to be concerned about messing up some other pieces of logic.
getURMsByTypedecodeGetRequestpostURMByTypedecodePostRequestdeleteURMtoInfluxDBremoveResourceRelationspwdService/Users/austinjaybecker/projects/abeck-go-testing/tenant/error.goname is empty"name is empty"onboarding has already been completed"onboarding has already been completed"onboard failed, missing value"onboard failed, missing value"error %d/%d: %s"error %d/%d: %s" ErrNameisEmpty is when a name is empty ErrIDNotUnique is used when attempting to create an org or bucket that already ErrOnboardingNotAllowed occurs when request to onboard comes in and we are not allowing this request AggregateError enables composing multiple errors. This is ideal in the case that you are applying functions with side effects to a slice of elements. E.g., deleting/updating a slice of resources. NewAggregateError returns a new AggregateError. Add adds an error to the aggregate. Err returns a proper error from this aggregate error./Users/austinjaybecker/projects/abeck-go-testing/tenant/error_bucket.goinvalid bucket list action, call should be GetBucketByName"invalid bucket list action, call should be GetBucketByName"kv/listBucket"kv/listBucket"system buckets cannot be renamed"system buckets cannot be renamed"system buckets cannot be deleted"system buckets cannot be deleted"bucket name is not unique"bucket name is not unique"bucket %q not found"bucket %q not found"user could not be unmarshalled"user could not be unmarshalled"kv/UnmarshalBucket"kv/UnmarshalBucket"bucket with name %s already exists"bucket with name %s already exists"user could not be marshalled"user could not be marshalled"kv/MarshalBucket"kv/MarshalBucket" ErrBucketNotFoundByName is used when the user is not found. ErrCorruptBucket is used when the user cannot be unmarshalled from the bytes stored in the kv. BucketAlreadyExistsError is used when attempting to create a user with a name that already exists. ErrUnprocessableBucket is used when a org is not able to be processed./Users/austinjaybecker/projects/abeck-go-testing/tenant/error_org.goorganization with name %s already exists"organization with name %s already exists"organization name "%s" not found"organization name \"%s\" not found"kv/UnmarshalOrg"kv/UnmarshalOrg"kv/MarshalOrg"kv/MarshalOrg"org id provided is invalid"org id provided is invalid" ErrOrgNotFound is used when the user is not found. OrgAlreadyExistsError is used when creating a new organization with a name that has already been used. Organization names must be unique. ErrCorruptOrg is used when the user cannot be unmarshalled from the bytes ErrUnprocessableOrg is used when a org is not able to be processed. InvalidOrgIDError is used when a service was provided an invalid ID. This is some sort of internal server error./Users/austinjaybecker/projects/abeck-go-testing/tenant/error_urm.goprovided user resource mapping ID has invalid format"provided user resource mapping ID has invalid format"user to resource mapping not found"user to resource mapping not found"Unable to connect to resource mapping service. Please try again; Err: %v"Unable to connect to resource mapping service. Please try again; Err: %v"kv/userResourceMapping"kv/userResourceMapping"Unknown internal user resource mapping data error; Err: %v"Unknown internal user resource mapping data error; Err: %v"unable to convert mapping of user to resource into JSON; Err %v"unable to convert mapping of user to resource into JSON; Err %v"Unexpected error when assigning user to a resource: mapping for user %s already exists"Unexpected error when assigning user to a resource: mapping for user %s already exists" ErrInvalidURMID is used when the service was provided ErrURMNotFound is used when the user resource mapping is not found. UnavailableURMServiceError is used if we aren't able to interact with the CorruptURMError is used when the config cannot be unmarshalled from the ErrUnprocessableMapping is used when a user resource mapping  is not able to be converted to JSON. NonUniqueMappingError is an internal error when a user already has been mapped to a resource/Users/austinjaybecker/projects/abeck-go-testing/tenant/error_user.goyour username or password is incorrect"your username or password is incorrect"your userID is incorrect"your userID is incorrect"passwords must be at least 8 characters long"passwords must be at least 8 characters long"user with name %s already exists"user with name %s already exists"unexpected error retrieving user bucket; Err: %v"unexpected error retrieving user bucket; Err: %v"kv/userBucket"kv/userBucket"unexpected error retrieving user index; Err: %v"unexpected error retrieving user index; Err: %v"kv/userIndex"kv/userIndex"user id provided is invalid"user id provided is invalid"kv/UnmarshalUser"kv/UnmarshalUser"kv/MarshalUser"kv/MarshalUser"Unable to connect to password service. Please try again; Err: %v"Unable to connect to password service. Please try again; Err: %v"kv/setPassword"kv/setPassword" ErrUserNotFound is used when the user is not found. EIncorrectPassword is returned when any password operation fails in which we do not want to leak information. EIncorrectUser is returned when any user is failed to be found which indicates the userID provided is for a user that does not exist. EShortPassword is used when a password is less than the minimum acceptable password length. UserAlreadyExistsError is used when attempting to create a user with a name UnexpectedUserBucketError is used when the error comes from an internal system. UnexpectedUserIndexError is used when the error comes from an internal system. InvalidUserIDError is used when a service was provided an invalid ID. ErrCorruptUser is used when the user cannot be unmarshalled from the bytes ErrUnprocessableUser is used when a user is not able to be processed. UnavailablePasswordServiceError is used if we aren't able to add the password to the store, it means the store is not available at the moment (e.g. network)./Users/austinjaybecker/projects/abeck-go-testing/tenant/http_client_bucket.gobucket name is required"bucket name is required" BucketClientService connects to Influx via HTTP using tokens to manage buckets OpPrefix is an additional property for error find bucket service, when finds nothing. FindBucketByName returns a single bucket by name TODO(@jsteenb2): are tracing/Users/austinjaybecker/projects/abeck-go-testing/tenant/http_client_onboarding.go OnboardClientService connects to Influx via HTTP to perform onboarding operations/Users/austinjaybecker/projects/abeck-go-testing/tenant/http_client_org.go OrgClientService connects to Influx via HTTP using tokens to manage organizations OpPrefix is for not found errors. FindOrganizationByID gets a single organization with a given id using HTTP. FindOrganization gets a single organization matching the filter using HTTP. FindOrganizations returns all organizations that match the filter via HTTP. CreateOrganization creates an organization. UpdateOrganization updates the organization over HTTP. DeleteOrganization removes organization id over HTTP./Users/austinjaybecker/projects/abeck-go-testing/tenant/http_client_urm.go/Users/austinjaybecker/projects/abeck-go-testing/tenant/http_client_user.go"permissions" PasswordClientService is an http client to speak to the password service./Users/austinjaybecker/projects/abeck-go-testing/tenant/http_handler_urm.gouSvc/{userID}"/{userID}" NewURMHandler generates a mountable handler for URMs. It needs to know how it will be looking up your resource id this system assumes you are using chi syntax for query string params `/orgs/{id}/` so it can use chi.URLParam(). determine the type of request from the path./Users/austinjaybecker/projects/abeck-go-testing/tenant/http_server_bucket.gobucketsRequestjson:"retentionRules,omitempty"`json:"retentionRules,omitempty"`Bucket created"Bucket created"organization id must be provided"organization id must be provided"Bucket retrieved"Bucket retrieved"Bucket deleted"Bucket deleted"Buckets retrieved"Buckets retrieved"Bucket updated"Bucket updated" BucketHandler represents an HTTP API handler for users. we may need this for now but we dont want it permanently NewHTTPBucketHandler constructs a new http server. RESTy routes for "articles" resource bucket is used for serialization/deserialization with duration string syntax. zero value implies infinite retention policy Only support a single retention period for the moment bucketUpdate is used for serialization/deserialization with retention rules. For now, only use a single retention rule. allow for no label svc handlePostBucket is the HTTP handler for the POST /api/v2/buckets route. handleGetBucket is the HTTP handler for the GET /api/v2/buckets/:id route. handleDeleteBucket is the HTTP handler for the DELETE /api/v2/buckets/:id route. handleGetBuckets is the HTTP handler for the GET /api/v2/buckets route. handlePatchBucket is the HTTP handler for the PATCH /api/v2/buckets route./Users/austinjaybecker/projects/abeck-go-testing/tenant/http_server_onboarding.goOnboarding eligibility check finished"Onboarding eligibility check finished"Onboarding setup completed"Onboarding setup completed" OnboardHandler represents an HTTP API handler for users. NewHTTPOnboardHandler constructs a new http server. isOnboarding is the HTTP handler for the POST /api/v2/setup route. handleInitialOnboardRequest is the HTTP handler for the GET /api/v2/setup route./Users/austinjaybecker/projects/abeck-go-testing/tenant/http_server_org.gosecretHandler/secrets"/secrets"/api/v2/orgs/%s/logs"/api/v2/orgs/%s/logs"/api/v2/orgs/%s/members"/api/v2/orgs/%s/members"/api/v2/orgs/%s/owners"/api/v2/orgs/%s/owners"/api/v2/orgs/%s/labels"/api/v2/orgs/%s/labels"/api/v2/buckets?org=%s"/api/v2/buckets?org=%s"/api/v2/tasks?org=%s"/api/v2/tasks?org=%s"/api/v2/dashboards?org=%s"/api/v2/dashboards?org=%s"json:"orgs"`json:"orgs"`Org created"Org created"Org retrieved"Org retrieved"Orgs retrieved"Orgs retrieved"Org updated"Org updated"Org deleted"Org deleted" OrgHandler represents an HTTP API handler for organizations. NewHTTPOrgHandler constructs a new http server. handlePostOrg is the HTTP handler for the POST /api/v2/orgs route. handleGetOrg is the HTTP handler for the GET /api/v2/orgs/:id route. handleGetOrgs is the HTTP handler for the GET /api/v2/orgs route. handlePatchOrg is the HTTP handler for the PATH /api/v2/orgs route. handleDeleteOrganization is the HTTP handler for the DELETE /api/v2/orgs/:id route./Users/austinjaybecker/projects/abeck-go-testing/tenant/http_server_user.gopasswordService/password"/password"/permissions"/permissions" NewHTTPUserHandler constructs a new http server. because this is a mounted path in both the /users and the /me route we can get a me request through this handler/Users/austinjaybecker/projects/abeck-go-testing/tenant/index/Users/austinjaybecker/projects/abeck-go-testing/tenant/index/index.gouserresourcemappingsbyuserindexv1"userresourcemappingsbyuserindexv1" URMByUserIndeMappingx is the mapping description of an index between a user and a URM/Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_bucket_auth.go TODO (al): remove authorizer/bucket when the bucket service moves to tenant AuthedBucketService wraps a influxdb.BucketService and authorizes actions NewAuthedBucketService constructs an instance of an authorizing bucket service./Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_bucket_logging.gofailed to create bucket"failed to create bucket"bucket create"bucket create"failed to find bucket with ID %v"failed to find bucket with ID %v"bucket find by ID"bucket find by ID"failed to find bucket with name %v in org %v"failed to find bucket with name %v in org %v"bucket find by name"bucket find by name"failed to find bucket matching the given filter"failed to find bucket matching the given filter"bucket find"bucket find"buckets find"buckets find"failed to update bucket"failed to update bucket"bucket update"bucket update"failed to delete bucket with ID %v"failed to delete bucket with ID %v"bucket delete"bucket delete" NewBucketLogger returns a logging service middleware for the Bucket Service./Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_bucket_metrics.goupdatedBucketfind_bucket_by_id"find_bucket_by_id"find_bucket"find_bucket"find_buckets"find_buckets"create_bucket"create_bucket"update_bucket"update_bucket"delete_bucket"delete_bucket"find_bucket_by_name"find_bucket_by_name" NewBucketMetrics returns a metrics service middleware for the Bucket Service. Returns a single bucket by ID. Returns the first bucket that matches filter. Creates a new bucket and sets b.ID with the new identifier. Updates a single bucket with changeset and returns the new bucket state after update. Removes a bucket by ID. FindBucketByName finds a Bucket given its name and Organization ID/Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_onboarding_auth.go TODO (al): remove authorizer/org when the org service moves to tenant AuthedOnboardSvc wraps a influxdb.OnboardingService and authorizes actions NewAuthedOnboardSvc constructs an instance of an authorizing org service. IsOnboarding pass through. this is handled by the underlying service layer OnboardInitialUser pass through. this is handled by the underlying service layer OnboardUser needs to confirm this user has access to do global create for multiple resources/Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_onboarding_logging.goavailablefailed to check onboarding"failed to check onboarding"is onboarding"is onboarding"failed to onboard user %s"failed to onboard user %s"onboard initial user"onboard initial user"onboard user"onboard user" NewOnboardingLogger returns a logging service middleware for the Bucket Service./Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_onboarding_metrics.gois_onboarding"is_onboarding"onboard_initial_user"onboard_initial_user"onboard_user"onboard_user" NewOnboardingMetrics returns a metrics service middleware for the User Service./Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_org_auth.go AuthedOrgService wraps a influxdb.OrganizationService and authorizes actions NewAuthedOrgService constructs an instance of an authorizing org service./Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_org_logging.gofailed to create org"failed to create org"org create"org create"failed to find org with ID %v"failed to find org with ID %v"org find by ID"org find by ID"failed to find org matching the given filter"failed to find org matching the given filter"org find"org find"orgs find"orgs find"failed to update org"failed to update org"org update"org update"failed to delete org with ID %v"failed to delete org with ID %v"org delete"org delete" NewOrgLogger returns a logging service middleware for the Organization Service./Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_org_metrics.goupdatedOrgfind_org_by_id"find_org_by_id"find_org"find_org"find_orgs"find_orgs"create_org"create_org"update_org"update_org"delete_org"delete_org" NewOrgMetrics returns a metrics service middleware for the Organization Service./Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_urm_auth.goauthedUrms resource's orgID Check if user making request has read access to organization prior to listing URMs. There should only be one because resourceID and userID are used to create the primary key for urms/Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_urm_logging.gofailed to create urm"failed to create urm"urm create"urm create"failed to find urms matching the given filter"failed to find urms matching the given filter"urm find"urm find"failed to delete urm for resource %v and user %v"failed to delete urm for resource %v and user %v"urm delete"urm delete" NewUrmLogger returns a logging service middleware for the User Resource Mapping Service./Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_urm_metrics.gofind_urms"find_urms"create_urm"create_urm"delete_urm"delete_urm" NewUrmMetrics returns a metrics service middleware for the User Resource Mapping Service./Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_user_auth.go TODO (al): remove authorizer/user when the user service moves to tenant AuthedUserService wraps a influxdb.UserService and authorizes actions NewAuthedUserService constructs an instance of an authorizing user service. TODO (desa): we'll likely want to push this operation into the database eventually since fetching the whole list of data AuthedPasswordService is a new authorization middleware for a password service. NewAuthedPasswordService wraps an existing password service with auth middleware./Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_user_logging.gofailed to create user"failed to create user"user create"user create"failed to find user with ID %v"failed to find user with ID %v"user find by ID"user find by ID"failed to find user matching the given filter"failed to find user matching the given filter"user find"user find"failed to find users matching the given filter"failed to find users matching the given filter"users find"users find"user update"user update"failed to delete user with ID %v"failed to delete user with ID %v"find permission for user"find permission for user"failed to set password for user with ID %v"failed to set password for user with ID %v"set password"set password"failed to compare password for user with ID %v"failed to compare password for user with ID %v"compare password"compare password"failed to compare and set password for user with ID %v"failed to compare and set password for user with ID %v"compare and set password"compare and set password" NewUserLogger returns a logging service middleware for the User Service. NewPasswordLogger returns a logging service middleware for the Password Service./Users/austinjaybecker/projects/abeck-go-testing/tenant/middleware_user_metrics.goupdatedUserfind_user_by_id"find_user_by_id"find_user"find_user"find_users"find_users"create_user"create_user"update_user"update_user"delete_user"delete_user"find_permission_for_user"find_permission_for_user"set_password"set_password"compare_password"compare_password"compare_and_set_password"compare_and_set_password" NewUserMetrics returns a metrics service middleware for the User Service. NewPasswordMetrics returns a metrics service middleware for the Password Service./Users/austinjaybecker/projects/abeck-go-testing/tenant/service.goinflux/tenant/internal"influx/tenant/internal" NewService creates a new base tenant service. creates a new Service with logging and metrics middleware wrappers./Users/austinjaybecker/projects/abeck-go-testing/tenant/service_bucket.gobucket name %s is invalid. Buckets may not start with underscore"bucket name %s is invalid. Buckets may not start with underscore"bucket name %s is invalid. Bucket names may not include quotation marks"bucket name %s is invalid. Bucket names may not include quotation marks" we need a valid org id make sure the org exists TODO: I think we should allow bucket deletes but maybe im wrong. removeResourceRelations allows us to clean up any resource relationship that would have normally been left over after a delete action of a resource. validBucketName reports any errors with bucket names names starting with an underscore are reserved for system buckets quotation marks will cause queries to fail/Users/austinjaybecker/projects/abeck-go-testing/tenant/service_onboarding.gopermFnub%s's Token"%s's Token" WithAlwaysAllowInitialUser configures the OnboardService to always return true for IsOnboarding to allow multiple initial onboard requests. we are allowed to onboard a user if we have no users or orgs OnboardInitialUser allows us to onboard a new user if is onboarding is allowed OnboardUser allows us to onboard a new user if is onboarding is allowed onboardUser allows us to onboard new users. create a user create users password set the new user in the context create users org create orgs buckets bolt doesn't lock per collection or record so we have to close our transaction before we can reach out to the auth service./Users/austinjaybecker/projects/abeck-go-testing/tenant/service_op_log.go OpLogService is a type which stores operation logs for buckets, users and orgs. NewOpLogService constructs and configures a new op log service. GetOrganizationOperationLog retrieves a organization operation log. GetBucketOperationLog retrieves a buckets operation log. GetUserOperationLog retrieves a user operation log./Users/austinjaybecker/projects/abeck-go-testing/tenant/service_org.gombSystem bucket for monitoring logs"System bucket for monitoring logs" if im given a id or a name I know I can only return 1 find urms for orgs with this user find orgs by the urm's resource ids. if there is an error then this is a crufty urm and we should just move on create associated URM if I am given a userid i can associate the user as the org owner DeleteOrganization removes a organization by ID and its dependent resources. clean up the buckets for this organization/Users/austinjaybecker/projects/abeck-go-testing/tenant/service_urm.go FindUserResourceMappings returns a list of UserResourceMappings that match filter and the total count of matching mappings. CreateUserResourceMapping creates a user resource mapping. DeleteUserResourceMapping deletes a user resource mapping./Users/austinjaybecker/projects/abeck-go-testing/tenant/service_user.gopassHashbcryptgolang.org/x/crypto/bcrypt"golang.org/x/crypto/bcrypt"CompareHashAndPasswordGenerateFromPassword Returns a single user by ID. Returns the first user that matches filter. if im given no filters its not a valid find user request. (leaving it unchecked seems dangerous) Returns a list of users that match filter and the total count of matching users. Additional options provide pagination & sorting. { if a id is provided we will reroute to findUserByID if a name is provided we will reroute to findUser with a name filter Creates a new user and sets u.ID with the new identifier. Updates a single user with changeset. Returns the new user state after update. { Removes a user by ID. FindPermissionForUser gets the full set of permission for a specified user id set password get password compare password/Users/austinjaybecker/projects/abeck-go-testing/tenant/storage.go/Users/austinjaybecker/projects/abeck-go-testing/tenant/storage_bucket.gounameikeynewIkeyoldIkey uniqueBucketName ensures this bucket is unique for this organization allow for hard coded bucket names that dont exist in the system this isn't a list action its a `GetBucketByName` if we dont have any options it would be irresponsible to just give back all orgs in the system if an organization is passed we need to use the index check to see if it matches the filter get the prefix key (org id with an empty name) generate new bucket ID validation/Users/austinjaybecker/projects/abeck-go-testing/tenant/storage_org.go if ID is provided then ensure it is unique/Users/austinjaybecker/projects/abeck-go-testing/tenant/storage_urm.goreachedOffsetcursorOptionsencodedResourceIDencodedUserID NOTE(affo): On URM creation, we check that the user exists. We do not check that the resource it is pointing to exists. This decision takes into account that different resources could not be in the same store. To perform that kind of check, we must rely on the service layer. However, we do not want having the storage layer depend on the service layer above. insert urm into by user index urm by user index lookup respect offset parameter for now the best we can do is use the resourceID if we have that as a forward cursor option remove user resource mapping from by user index/Users/austinjaybecker/projects/abeck-go-testing/tenant/storage_user.goaggErr if we dont have any options it would be irresponsible to just give back all users in the system Clean up users password. Clean up user URMs. Do not fail fast on error. Try to avoid as much as possible the effects of partial deletion./Users/austinjaybecker/projects/abeck-go-testing/tenant.go TenantService is a service that exposes the functionality of the embedded services./Users/austinjaybecker/projects/abeck-go-testing/testing/Users/austinjaybecker/projects/abeck-go-testing/testing/auth.goAuthTestOptsAuthorizationFieldsBenchmarkIndexWalkBucketFieldsCleanupDBRPMappingsCleanupDBRPMappingsV2CreateDBRPMappingCreateDBRPMappingV2DBRPMappingCmpOptionsV2DBRPMappingFieldsDBRPMappingFieldsV2DeleteDBRPMappingDeleteDBRPMappingV2DeleteSecretsFindDBRPMappingFindDBRPMappingByIDV2FindDBRPMappingByKeyFindDBRPMappingsFindManyDBRPMappingsV2IDPtrKVConcurrentUpdateKVCursorKVCursorWithHintsKVDeleteKVForwardCursorKVGetKVGetBatchKVPutKVStoreFieldsKVUpdateKVViewKeyValueLogFieldsLabelFieldsMiscDBRPMappingV2NowFuncOnboardingFieldsOrganizationFieldsPasswordFieldsSecretServiceFieldsSessionFieldsTargetFieldsTenantFieldsTestIndexTrimWhitespaceUpdateDBRPMappingV2UpdateUser_IndexHygieneUserFieldsUserResourceFieldsVariableFieldsVariableSvcOptsWithHTTPValidationWithoutFindByTokenallKVsallUsersPermissionauthOneIDauthThreeIDauthTwoIDauthZeroIDauthorizationCmpOptionsbaseUserResourceFieldsboolPtrbucketCmpOptionsbucketServiceFbucketsByNamecreateUsersPermissiondbrpBucket1IDdbrpBucket2IDdbrpBucketAIDdbrpBucketBIDdbrpMappingCmpOptionsdbrpOrg1IDdbrpOrg2IDdbrpOrg3IDdefaultSourceIDdefaultSourceOrganizationIDdeleteUsersPermissionidAidBidCidDidFiveidFouridOneidThreeidTwokeyValueLogCmpOptionslabelCmpOptionslabelOneIDlabelServiceFlabelThreeIDlabelTwoIDloopIDGeneratormappingCmpOptionsnewMigrationnewNResourcesnewNResourcesWithUserCountnewResourcenewSomeResourceStoreoldFakeDateonboardCmpOptionsoneTokenorgBucketsIDGeneratororganizationCmpOptionssecretCmpOptionssessionCmpOptionssessionCompareOptionssessionOneIDsessionServiceFuncsessionTwoIDsomeResourcesomeResourceBucketsomeResourceStoresourceCmpOptionssourceOneIDsourceOrgOneIDsourceTwoIDspyMigrationSpecstringPtrtarget1target2target3targetCmpOptionstargetOneIDtargetThreeIDtargetTwoIDtestPopulateAndVerifytestWalktesterurmByResourceIDurmByUserIDuserCmpOptionsuserOneIDuserResourceMappingServiceFuserThreeIDuserTwoIDvariableCmpOptionsupdatedAuth020f755c3c081000"020f755c3c081000"IgnoreFieldsOrgIDGeneratorbasic create authorization"basic create authorization""rand"cooluser"cooluser""o1"supersecret"supersecret"already existing auth"already existing auth"new auth"new auth"providing a non existing user is invalid"providing a non existing user is invalid"auth with non-existent user"auth with non-existent user"providing a non existing org is invalid"providing a non existing org is invalid"auth with non-existent org"auth with non-existent org"failed to retrieve authorizations: %v"failed to retrieve authorizations: %v"authorizations are different -got/+want
diff %s"authorizations are different -got/+want\ndiff %s"basic find authorization by id"basic find authorization by id"regularuser"regularuser"rand1"rand1"rand2"rand2"authorization is different -got/+want
diff %s"authorization is different -got/+want\ndiff %s""o2"rand0"rand0"rand3"rand3"desc1"desc1"update with id not found"update with id not found"update with unknown status"update with unknown status"%s failed, got error %s"%s failed, got error %s"basic find authorization by token"basic find authorization by token"find authorization by token"find authorization by token"rand4"rand4"find all authorizations"find all authorizations"find authorization by user id"find authorization by user id"find authorization by org id"find authorization by org id"find authorization by org id and user id"find authorization by org id and user id"delete authorizations using exist id"delete authorizations using exist id"delete authorizations using id that does not exist"delete authorizations using id that does not exist" Copy input to avoid mutating it WithoutFindByToken allows the Find By Token test case to be skipped when we are testing the http server, since finding by token is not supported by the HTTP API AuthorizationFields will include the IDGenerator, and authorizations AuthorizationService tests all the service functions. CreateAuthorization testing FindAuthorizationByID testing ID(1) UpdateAuthorization testing FindAuthorizationByToken testing FindAuthorizations testing DeleteAuthorization testingLogEntriesSessionsUserResourceMappingsPairsDBRPMappingsV2ownerIDIndexFindByOwnerOrgBucketIDsBucketIDGeneratorPasswordsBucketIDsDBRPMappingsupCalleddownCalledassertUpCalledassertDownCalledHTTPValidationbenchContextmaxLenextLenprocessBenchdurationOrCountFlagallowZeroBenchmarkResultMemAllocsMemBytesNsPerOpmbPerSecAllocsPerOpAllocedBytesPerOpMemStringimportPathpreviousNpreviousDurationbenchFuncbenchTimemissingBytestimerOnshowAllocResultstartAllocsstartBytesnetAllocsnetBytesStartTimerStopTimerResetTimerReportAllocsrunNrun1doBenchlaunchReportMetrictrimOutputRunParallelSetParallelismPBglobalNgrainbN/Users/austinjaybecker/projects/abeck-go-testing/testing/bucket_service.gofilteredBucketscreate buckets with empty set"create buckets with empty set"theorg"theorg"basic create bucket"basic create bucket"otherorg"otherorg"bucket1"bucket1"bucket2"bucket2"names should be unique within an organization"names should be unique within an organization"bucket with name bucket1 already exists"bucket with name bucket1 already exists"names should not be unique across organizations"names should not be unique across organizations"create bucket with orgID not exist"create bucket with orgID not exist"create bucket with illegal quotation mark"create bucket with illegal quotation mark"namewith"quote"namewith\"quote"bucket name namewith"quote is invalid. Bucket names may not include quotation marks"bucket name namewith\"quote is invalid. Bucket names may not include quotation marks"failed to retrieve buckets: %v"failed to retrieve buckets: %v"buckets are different -got/+want
diff %s"buckets are different -got/+want\ndiff %s"basic find bucket by id"basic find bucket by id"find bucket by id not exist"find bucket by id not exist"bucket is different -got/+want
diff %s"bucket is different -got/+want\ndiff %s"find all buckets"find all buckets"find all buckets by offset and limit"find all buckets by offset and limit"find all buckets by after and limit"find all buckets by after and limit"find all buckets by descending"find all buckets by descending"find buckets by organization name"find buckets by organization name""123"find buckets by organization id"find buckets by organization id"find bucket by name"find bucket by name"missing bucket returns no buckets"missing bucket returns no buckets"delete buckets using exist id"delete buckets using exist id"delete buckets using id that does not exist"delete buckets using id that does not exist"1234567890654321"1234567890654321"delete system buckets"delete system buckets"find bucket by id"find bucket by id"missing bucket returns error"missing bucket returns error"bucket "xyz" not found"bucket \"xyz\" not found"update name unique"update name unique"update system bucket name"update system bucket name"update retention"update retention"6000000000000update retention and name"update retention and name"6060000000000update retention and same name"update retention and same name"update bucket with illegal quotation mark"update bucket with illegal quotation mark"valid name"valid name" BucketFields will include the IDGenerator, and buckets BucketService tests all the service functions. CreateBucket testing ID(2) CRUDLog is missing because seed data is created through storage layer and not service layer (where CRUDLog is populated) Delete only newly created buckets - ie., with a not nil ID if tt.args.bucket.ID.Valid() { remove system buckets FindBucketByID testing FindBuckets testing ID(3) DeleteBucket testing remove built in system buckets FindBucket testing UpdateBucket testing/Users/austinjaybecker/projects/abeck-go-testing/testing/dbrp_mapping.godbrpMappingsdbrpMappingba55ba55ba55ba55"ba55ba55ba55ba55"beadbeadbeadbead"beadbeadbeadbead"1005e1eaf1005e1e"1005e1eaf1005e1e"cab00d1ecab00d1e"cab00d1ecab00d1e"ca1fca1fca1fca1f"ca1fca1fca1fca1f"a55e55eda55e55ed"a55e55eda55e55ed"b1077edb1077eded"b1077edb1077eded"failed to populate dbrp mappings"failed to populate dbrp mappings"failed to retrieve all dbrp mappings"failed to retrieve all dbrp mappings"Wrapffailed to remove dbrp mapping %s/%s/%s"failed to remove dbrp mapping %s/%s/%s"create dbrpMappings with empty set"create dbrpMappings with empty set"cluster1"cluster1"database1"database1"retention_policy1"retention_policy1"basic create dbrpMapping"basic create dbrpMapping"cluster2"cluster2"database2"database2"retention_policy2"retention_policy2"idempotent create dbrpMapping"idempotent create dbrpMapping"error on create existing dbrpMapping"error on create existing dbrpMapping"failed to retrieve dbrpMappings: %v"failed to retrieve dbrpMappings: %v"dbrpMappings are different -got/+want
diff %s"dbrpMappings are different -got/+want\ndiff %s"find all dbrpMappings"find all dbrpMappings"find dbrpMappings by cluster"find dbrpMappings by cluster"find default rp from dbrpMappings"find default rp from dbrpMappings"retention_policyA"retention_policyA"retention_policyB"retention_policyB"find dbrpMappings by cluster db and rp"find dbrpMappings by cluster db and rp"find non existing dbrpMapping"find non existing dbrpMapping"clusterX"clusterX"find dbrpMapping with invalid filter"find dbrpMapping with invalid filter"delete existing dbrpMapping"delete existing dbrpMapping"delete dbrpMappings using key that does not exist"delete dbrpMappings using key that does not exist"cluster3"cluster3" Copy input slice to avoid mutating it DBRPMappingFields will include the dbrpMappings Populate creates all entities in DBRPMappingFields CleanupDBRPMappings finds and removes all dbrp mappings CreateDBRPMapping testing FindDBRPMappings testing FindDBRPMappingByKey testing FindDBRPMapping testing DeleteDBRPMapping testing/Users/austinjaybecker/projects/abeck-go-testing/testing/dbrp_mapping_v2.gofind by ID"find by ID"miscellaneous"miscellaneous"failed to remove dbrp mapping %v"failed to remove dbrp mapping %v"basic create dbrp"basic create dbrp"create mapping for same db does not change default"create mapping for same db does not change default"create mapping for same db changes default"create mapping for same db changes default"error on create existing dbrp with same ID"error on create existing dbrp with same ID"error on create dbrp with same orgID, db and rp"error on create dbrp with same orgID, db and rp"error bucket does not exist"error bucket does not exist"failed to retrieve dbrps: %v"failed to retrieve dbrps: %v"want dbrpMappings count of %d, got %d"want dbrpMappings count of %d, got %d"dbrpMappings are different -want/+got
diff %s"dbrpMappings are different -want/+got\ndiff %s"find all dbrps"find all dbrps"1111111111111111"1111111111111111"2222222222222222"2222222222222222"find by bucket ID"find by bucket ID"find by orgID"find by orgID"find by db"find by db"find by rp"find by rp"find by default"find by default"find default"find default"mixed"mixed"retention_policyC"retention_policyC"find existing dbrp"find existing dbrp"find non existing dbrp"find non existing dbrp"find existing dbrp but wrong orgID"find existing dbrp but wrong orgID"basic update"basic update"update invalid dbrp"update invalid dbrp"./"./"error dbrp not found"error dbrp not found"update unchangeable fields"update unchangeable fields"wont_change"wont_change"update to same orgID, db, and rp"update to same orgID, db, and rp"update default when only one dbrp is present"update default when only one dbrp is present"set default when more dbrps are present"set default when more dbrps are present"retention_policy3"retention_policy3"unset default when more dbrps are present"unset default when more dbrps are present"delete existing dbrp"delete existing dbrp"delete default dbrp"delete default dbrp"delete non-existing dbrp"delete non-existing dbrp"defaults are ok"defaults are ok"should be default"should be default"what is inited is present"what is inited is present"delete works"delete works"failed to delete: %v"failed to delete: %v"nothing left"nothing left"new one is still ok"new one is still ok"failed to create: %v"failed to create: %v"failed to retrieve dbrp: %v"failed to retrieve dbrp: %v" Populate creates all entities in DBRPMappingFieldsV2. DBRPMappingServiceV2 tests all the service functions. CleanupDBRPMappingsV2 finds and removes all dbrp mappings. If there is only one mapping for a database, that is the default one. NOTE(affo): in the "same ID" concept, orgID must match too! This one will substitute 200 as default for "database2". invalid db name. The first one becomes the default one./Users/austinjaybecker/projects/abeck-go-testing/testing/id.go IDPtr returns a pointer to an influxdb.ID./Users/austinjaybecker/projects/abeck-go-testing/testing/index.gouserCountallKvsresourceStoretestCasecasesfetchCountresourceCountaresource"aresource"aresourcebyowneridv1"aresourcebyowneridv1"create the aresource bucket"create the aresource bucket"resource %d"resource %d"owner %d"owner %d"Test_PopulateAndVerify"Test_PopulateAndVerify"Test_Walk"Test_Walk"expected index to be empty, found %d items"expected index to be empty, found %d items"owner 0"owner 0"resource 0"resource 0"resource 5"resource 5"owner 1"owner 1"resource 1"resource 1"resource 6"resource 6"owner 2"owner 2"resource 2"resource 2"resource 7"resource 7"owner 3"owner 3"resource 3"resource 3"resource 8"resource 8"owner 4"owner 4"resource 4"resource 4"resource 9"resource 9"resource 10"resource 10"resource 15"resource 15"resource 11"resource 11"resource 16"resource 16"resource 12"resource 12"resource 17"resource 17"resource 13"resource 13"resource 18"resource 18"resource 14"resource 14"resource 19"resource 19"expected %#v, found %#v
"expected %#v, found %#v\n"unexpected err %v"unexpected err %v"owner 0/resource 0"owner 0/resource 0"owner 0/resource 10"owner 0/resource 10"owner 0/resource 15"owner 0/resource 15"owner 0/resource 5"owner 0/resource 5"owner 1/resource 1"owner 1/resource 1"owner 1/resource 11"owner 1/resource 11"owner 1/resource 16"owner 1/resource 16"owner 1/resource 6"owner 1/resource 6"owner 2/resource 12"owner 2/resource 12"owner 2/resource 17"owner 2/resource 17"owner 2/resource 2"owner 2/resource 2"owner 2/resource 7"owner 2/resource 7"owner 3/resource 13"owner 3/resource 13"owner 3/resource 18"owner 3/resource 18"owner 3/resource 3"owner 3/resource 3"owner 3/resource 8"owner 3/resource 8"owner 4/resource 14"owner 4/resource 14"owner 4/resource 19"owner 4/resource 19"owner 4/resource 4"owner 4/resource 4"owner 4/resource 9"owner 4/resource 9"expected %#v to be empty"expected %#v to be empty"entries must not be nil"entries must not be nil" insert 20 resources, but only index the first half check that the index is populated with only 10 items ensure verify identifies the 10 missing items from the index populate the missing indexes check the contents of the index remove the last 10 items from the source, but leave them in the index ensure verify identifies the last 10 items as missing from the source configure resource store with read disabled insert all 20 resources with indexing enabled expect resources to be empty while read path disabled disabled configure index read path enabled/Users/austinjaybecker/projects/abeck-go-testing/testing/keyvalue_log.gologEntrieslogEntry"AddLogEntry""ForEachLogEntry""FirstLogEntry""LastLogEntry"Add entry to empty log"Add entry to empty log"Add entry to non-empty log"Add entry to non-empty log"hat"hat"failed to retrieve log entries: %v"failed to retrieve log entries: %v"logEntries are different -got/+want
diff %s"logEntries are different -got/+want\ndiff %s"all log entries"all log entries""3""4""5"all log entries descending order"all log entries descending order"all log entries with offset"all log entries with offset"for each log entry with limit"for each log entry with limit"log entries with offset and limit"log entries with offset and limit"descending log entries with offset and limit"descending log entries with offset and limit"offset exceeds log range"offset exceeds log range"offset exceeds log range descending"offset exceeds log range descending"get first log entry"get first log entry"get last log entry"get last log entry" A log entry is a comparable data structure that is used for testing KeyValueLogFields will include the IDGenerator, and keyValueLogs KeyValueLog tests all the service functions. AddLogEntry tests the AddLogEntry for the KeyValueLog contract ForEachLogEntry tests the AddLogEntry for the KeyValueLog contract FirstLogEntry tests the FirstLogEntry method for the KeyValueLog contract. LastLogEntry tests the LastLogEntry method for the KeyValueLog contract./Users/austinjaybecker/projects/abeck-go-testing/testing/kv.gofinreturnErrcloseFn"Get""GetBatch""Put""Cursor"CursorWithHints"CursorWithHints""ForwardCursor""View"ConcurrentUpdate"ConcurrentUpdate"get key"get key"get missing key"get missing key"unexpected error retrieving bucket: %v"unexpected error retrieving bucket: %v"exptected to get value %s got %s"exptected to get value %s got %s"error during view transaction: %v"error during view transaction: %v"get keys"get keys"orange"orange"get keys with missing"get keys with missing"exptected to get value %q got %q"exptected to get value %q got %q"put pair"put pair"unexpected error retrieving value: %v"unexpected error retrieving value: %v"delete key"delete key"expected key not found error got %v"expected key not found error got %v"basic cursor"basic cursor""ab"abcd"abcd"abcde"abcde"bcd"bcd""6"cd"cd""7"exptected to get key %s got %s"exptected to get key %s got %s"val:"val:"no hints"no hints"aa/00"aa/00"aa/01"aa/01"aaa/00"aaa/00"aaa/01"aaa/01"aaa/02"aaa/02"aaa/03"aaa/03"bbb/00"bbb/00"bbb/01"bbb/01"bbb/02"bbb/02"aaa"aaa"prefix hint"prefix hint"aaa/"aaa/"start hint"start hint"predicate for key"predicate for key"predicate for value"predicate for value"val:aa/"val:aa/"unexpected error: %v"unexpected error: %v"unexpected cursor values: -got/+exp
%v"unexpected cursor values: -got/+exp\n%v"expErrprefix - no hints"prefix - no hints"prefix with limit"prefix with limit"prefix - skip first"prefix - skip first"prefix - skip first with limit"prefix - skip first with limit"prefix - skip first (one item)"prefix - skip first (one item)"prefix - does not prefix seek"prefix - does not prefix seek"aab"aab"no hints - descending"no hints - descending"no hints - descending - with limit"no hints - descending - with limit"prefixed - no hints - descending"prefixed - no hints - descending"aa/"aa/"prefixed - no hints - descending - with limit"prefixed - no hints - descending - with limit"start hint - descending"start hint - descending"predicate for key - descending"predicate for key - descending"predicate for value - descending"predicate for value - descending"expected error to be %v, got %v"expected error to be %v, got %v"expected cursor to close with nil error, found %v"expected cursor to close with nil error, found %v"basic view"basic view"cruel world"cruel world"basic view with delete"basic view with delete"basic view with put"basic view with put"expected transaction to fail"expected transaction to fail"basic update with delete"basic update with delete"expected key not found"expected key not found"error during update transaction: %v"error during update transaction: %v"valueAvalueBbasic concurrent update"basic concurrent update"darkness my new friend"darkness my new friend"https://github.com/influxdata/platform/issues/2371"https://github.com/influxdata/platform/issues/2371" KVStoreFields are background data that has to be set before the test runs. KVStore tests the key value store contract KVGet tests the get method contract for the key value store. KVGetBatch tests the get batch method contract for the key value store. KVPut tests the get method contract for the key value store. KVDelete tests the delete method contract for the key value store. KVCursor tests the cursor contract for the key value store. KVForwardCursor tests the forward cursor contract for the key value store. successfully returned expected error KVView tests the view method contract for the key value store. If len(value) == 0 the test will not attempt a put If true, the test will attempt to delete the provided key KVUpdate tests the update method contract for the key value store. TODO: add case with failed update transaction that doesn't apply all of the changes. KVConcurrentUpdate tests concurrent calls to update. To ensure that a is scheduled before b/Users/austinjaybecker/projects/abeck-go-testing/testing/label_service.go41a9f7288d4e2d64"41a9f7288d4e2d64"b7c5355e1134b11c"b7c5355e1134b11c"c8d6466f2245c22d"c8d6466f2245c22d"names should be unique"names should be unique"label_1"label_1"fff000"fff000"label with name label_1 already exists"label with name label_1 already exists"names should be trimmed of spacing"names should be trimmed of spacing"tag_1"tag_1"     tag_1     "     tag_1     "label with name tag_1 already exists"label with name tag_1 already exists"labels should be unique and case-agnostic"labels should be unique and case-agnostic"TAG_1"TAG_1"label with name TAG_1 already exists"label with name TAG_1 already exists"basic create label"basic create label"Tag2"Tag2"failed to retrieve labels: %v"failed to retrieve labels: %v"labels are different -got/+want
diff %s"labels are different -got/+want\ndiff %s"basic find labels"basic find labels"Tag1"Tag1"find labels filtering"find labels filtering"find a label by name is case-agnostic"find a label by name is case-agnostic"tag1"tag1"TAG1"TAG1"find label by ID"find label by ID"label does not exist"label does not exist"update label name"update label name"NotTag1"NotTag1"cant update a label with a name that already exists"cant update a label with a name that already exists"tag_2"tag_2"should trim space but fails to update existing label"should trim space but fails to update existing label" tag_1 " tag_1 "update label properties"update label properties"replacing a label property"replacing a label property"abc123"abc123"deleting a label property"deleting a label property"updating a non-existent label"updating a non-existent label"basic delete label"basic delete label"deleting a non-existent label"deleting a non-existent label"create label mapping"create label mapping"mapping to a nonexistent label"mapping to a nonexistent label"delete label mapping"delete label mapping" LabelFields include the IDGenerator, labels and their mappings LabelService tests all the service functions./Users/austinjaybecker/projects/abeck-go-testing/testing/migration.gomigrationFourmigrationOnemigrationThreemigrationTwonewMigratortimesmigration one"migration one"migration two"migration two"migration three"migration three"migration four"migration four"List() shows all migrations in down state"List() shows all migrations in down state"Up() runs each migration in turn"Up() runs each migration in turn"List() after adding new migration it reports as expected"List() after adding new migration it reports as expected"Up() only applies the single down migration"Up() only applies the single down migration"Down() calls down for each migration"Down() calls down for each migration"Up() re-applies all migrations"Up() re-applies all migrations"List() missing migration spec errors as expected"List() missing migration spec errors as expected"expected migration spec error, found %v"expected migration spec error, found %v"expected Up() to be called %d times, instead found %d times"expected Up() to be called %d times, instead found %d times"expected Down() to be called %d times, instead found %d times"expected Down() to be called %d times, instead found %d times" NowFunc is a function which returns a time Migrator tests a migrator against a provided store. The migrator is constructed via a provided constructor function which takes a logger and a now function used to derive time. mocking now time ts returns a point to a time at N unix seconds. all migrations excluding number four (for now) apply all migrations list migration again assert each migration was called assert each migration was called only once assert each migration up was called for a second time remove last specification from migration list/Users/austinjaybecker/projects/abeck-go-testing/testing/onboarding.goerrCodedenied"denied"020f755c3c082008missing username"missing username"missing org"missing org"missing bucket"missing bucket"missing password should fail"missing password should fail"valid onboarding json should create a user, org, bucket, and authorization"valid onboarding json should create a user, org, bucket, and authorization"password1"password1"admin's Token"admin's Token"Error: %v"Error: %v"expected error code '%s' got '%v'"expected error code '%s' got '%v'"expected error code to match '%s' got '%v'"expected error code to match '%s' got '%v'"onboarding results are different -got/+want
diff %s"onboarding results are different -got/+want\ndiff %s"020f755c3c082004"020f755c3c082004"020f755c3c082005"020f755c3c082005""020f755c3c082008" its possible auth wont exist on the basic service level OnboardingFields will include the IDGenerator, TokenGenerator and IsOnboarding OnboardInitialUser testing 1 week/Users/austinjaybecker/projects/abeck-go-testing/testing/organization_service.gocreate organizations with empty set"create organizations with empty set"basic create organization"basic create organization"organization1"organization1"organization2"organization2"empty name"empty name"name only have spaces"name only have spaces"organization with name organization1 already exists"organization with name organization1 already exists"create organization with no id"create organization with no id"organizations are different -got/+want
diff %s"organizations are different -got/+want\ndiff %s"basic find organization by id"basic find organization by id"didn't find organization by id"didn't find organization by id"organization is different -got/+want
diff %s"organization is different -got/+want\ndiff %s"find all organizations"find all organizations"desc xyz"desc xyz"find all organizations by offset and limit"find all organizations by offset and limit"ijk"ijk"find organization by id"find organization by id"find organization by name"find organization by name"find organization by id not exists"find organization by id not exists"find organization by name not exists"find organization by name not exists"na"na"organization name "na" not found"organization name \"na\" not found"delete organizations using exist id"delete organizations using exist id"orgA"orgA"orgB"orgB"delete organizations using id that does not exist"delete organizations using id that does not exist"find organization in which no name filter matches should return no org"find organization in which no name filter matches should return no org"organization name "unknown" not found"organization name \"unknown\" not found"find organization in which no id filter matches should return no org"find organization in which no id filter matches should return no org"find organization no filter is set returns an error about filters not provided"find organization no filter is set returns an error about filters not provided"missing organization returns error"missing organization returns error"organization name "abc" not found"organization name \"abc\" not found"update id not exists"update id not exists"update name to same name"update name to same name"update name not unique"update name not unique"organization with name organization2 already exists"organization with name organization2 already exists"update name is empty"update name is empty"update name only has space"update name only has space"            "            "organization1 description"organization1 description"organization2 description"organization2 description" OrganizationFields will include the IDGenerator, and organizations OrganizationService tests all the service functions. CreateOrganization testing Delete only newly created organizations if tt.args.organization.ID != nil { FindOrganizationByID testing FindOrganizations testing DeleteOrganization testing FindOrganization testing UpdateOrganization testing/Users/austinjaybecker/projects/abeck-go-testing/testing/passwords.gosetting password longer than 8 characters works"setting password longer than 8 characters works"user1"user1"howdydoody"howdydoody"passwords that are too short have errors"passwords that are too short have errors"short"short"setting a password for a non-existent user is a generic-like error"setting a password for a non-existent user is a generic-like error"expected SetPassword error %v got %v"expected SetPassword error %v got %v"comparing same password is not an error"comparing same password is not an error"comparing different password is an error"comparing different password is an error"wrongpassword"wrongpassword"comparing a password to a non-existent user is a generic-like error"comparing a password to a non-existent user is a generic-like error"user exists but no password has been set"user exists but no password has been set"expected ComparePassword error %v got %v"expected ComparePassword error %v got %v"setting a password to the existing password is valid"setting a password to the existing password is valid"providing an incorrect old password is an error"providing an incorrect old password is an error"not used"not used"<invalid> a new password that is less than 8 characters is an error"<invalid> a new password that is less than 8 characters is an error"expected CompareAndSetPassword error %v got %v"expected CompareAndSetPassword error %v got %v" PasswordFields will include the IDGenerator, and users and their passwords. passwords are indexed against the Users field PasswordsService tests all the service functions. SetPassword tests overriding the password of a known user ComparePassword tests setting and comparing passwords. CompareAndSetPassword tests implementations of PasswordsService./Users/austinjaybecker/projects/abeck-go-testing/testing/scraper_target.gourl1"url1"url2"url2"url3"url3"org%d"org%d"create targets with empty set"create targets with empty set"create target with invalid org id"create target with invalid org id"create target with invalid bucket id"create target with invalid bucket id"basic create target"basic create target"failed to retrieve scraper targets: %v"failed to retrieve scraper targets: %v"scraper targets are different -got/+want
diff %s"scraper targets are different -got/+want\ndiff %s"get all targets"get all targets"filter by name"filter by name"filter by id"filter by id"filter targets by orgID"filter targets by orgID"filter targets by org name"filter targets by org name"filter targets by org name not exist"filter targets by org name not exist"org2"org2"organization name "org2" not found`organization name "org2" not found`targets are different -got/+want
diff %s"targets are different -got/+want\ndiff %s"basic find target by id"basic find target by id""target1""target2"find target by id not find"find target by id not find"target is different -got/+want
diff %s"target is different -got/+want\ndiff %s"delete targets using exist id"delete targets using exist id"delete targets using id that does not exist"delete targets using id that does not exist"failed to retrieve targets: %v"failed to retrieve targets: %v"update url with blank id"update url with blank id"update url with non exist id"update url with non exist id"update url"update url"scraper target is different -got/+want
diff %s"scraper target is different -got/+want\ndiff %s" TargetFields will include the IDGenerator, and targets ScraperService tests all the service functions. AddTarget testing. ListTargets testing GetTargetByID testing RemoveTarget testing UpdateTarget testing/Users/austinjaybecker/projects/abeck-go-testing/testing/secret.go"LoadSecret""PutSecret""PutSecrets""PatchSecrets""GetSecretKeys""DeleteSecrets"load secret field"load secret field"api_key"api_key"abc123xyz"abc123xyz"expected value to be %s, got %s"expected value to be %s, got %s"put secret"put secret"unexpected error %v"unexpected error %v"put secrets"put secrets"api_key2"api_key2"batman"batman"potato"potato"keys are different -got/+want
diff %s"keys are different -got/+want\ndiff %s"patch secrets"patch secrets"get secret keys for one org"get secret keys for one org"zyx321cba"zyx321cba"delete secret keys"delete secret keys"foo"foo" A secret is a comparable data structure that is used for testing SecretServiceFields contain the SecretService will test all methods for the secrets service. LoadSecret tests the LoadSecret method for the SecretService interface. PutSecret tests the PutSecret method for the SecretService interface. PutSecrets tests the PutSecrets method for the SecretService interface. PatchSecrets tests the PatchSecrets method for the SecretService interface. skip value checking for http service testing GetSecretKeys tests the GetSecretKeys method for the SecretService interface. DeleteSecrets tests the DeleteSecrets method for the SecretService interface./Users/austinjaybecker/projects/abeck-go-testing/testing/session.gocmpOptions"ExpiresAt"create sessions with empty set"create sessions with empty set"sessions are different -got/+want
diff %s"sessions are different -got/+want\ndiff %s"basic find session"basic find session"2030look for not existing session"look for not existing session"session is different -got/+want
diff %s"session is different -got/+want\ndiff %s"expected session to be expired got %v"expected session to be expired got %v"expected a nil session but got: %v"expected a nil session but got: %v"basic renew session"basic renew session"2031renew session with an earlier time than existing expiration"renew session with an earlier time than existing expiration"renew nil session"renew nil session"err in find session %v"err in find session %v" SessionFields will include the IDGenerator, TokenGenerator, Sessions, and Users SessionService tests all the service functions. CreateSession testing FindSession testing ExpireSession testing RenewSession testing/Users/austinjaybecker/projects/abeck-go-testing/testing/source.go61726920617a696f"61726920617a696f"create sources with empty set"create sources with empty set"failed to retrieve sources: %v"failed to retrieve sources: %v"sources are different -got/+want
diff %s"sources are different -got/+want\ndiff %s"find default source by ID"find default source by ID"find source by ID"find source by ID"find all sources"find all sources"delete source by ID"delete source by ID"delete default source by ID"delete default source by ID" SourceFields will include the IDGenerator, and sources CreateSource testing FindSourceByID testing FindSources testing DeleteSource testing/Users/austinjaybecker/projects/abeck-go-testing/testing/tenant.gonbsnorgsnurmsnusrsnnbsnnurmscheckInvariancepreDeletionBucketscreating an org creates system buckets"creating an org creates system buckets"expected 1 org, got: %v"expected 1 org, got: %v"expected no user, got: %v"expected no user, got: %v"expected no urm, got: %+v"expected no urm, got: %+v"expected 2 buckets, got: %v"expected 2 buckets, got: %v"unexpected nam for bucket: %s"unexpected nam for bucket: %s"creating user creates only the user"creating user creates only the user"expected no org, got: %v"expected no org, got: %v"expected 1 user, got: %v"expected 1 user, got: %v"expected no urm, got: %v"expected no urm, got: %v"expected no bucket created, got: %+v"expected no bucket created, got: %+v"creating urm pointing to non existing user fails"creating urm pointing to non existing user fails"expected %d urms got %d: %+v"expected %d urms got %d: %+v"expected error got none"expected error got none"should not be possible to create bucket without org"should not be possible to create bucket without org"expected bucket created, got: %+v"expected bucket created, got: %+v"making user part of org creates mapping to org only"making user part of org creates mapping to org only"unexpected name for bucket: %s"unexpected name for bucket: %s"unexpected name for bucket: %v"unexpected name for bucket: %v"unexpected urms -want/+got:
	%s"unexpected urms -want/+got:\n\t%s"user2"user2"password2"password2"deleting bucket deletes urm"deleting bucket deletes urm"expected 1 urm, got: %v"expected 1 urm, got: %v"deleting bucket urm does create dangling bucket"deleting bucket urm does create dangling bucket"expected 1 buckets, got: %v"expected 1 buckets, got: %v"expected bucket2, to be dangling, got: %+v"expected bucket2, to be dangling, got: %+v"expected bucket1, to be dangling, got: %+v"expected bucket1, to be dangling, got: %+v"deleting a user deletes every related urm and nothing else"deleting a user deletes every related urm and nothing else"expected that user deletion would remove dangling urms, got: %+v"expected that user deletion would remove dangling urms, got: %+v"deleting a bucket deletes every related urm"deleting a bucket deletes every related urm"expected that bucket deletion would remove dangling urms, got: %+v"expected that bucket deletion would remove dangling urms, got: %+v"deleting an organization should delete everything that depends on it"deleting an organization should delete everything that depends on it"expected org buckets to be deleted, got: %+v"expected org buckets to be deleted, got: %+v"expected this urm to be deleted, got %+v instead"expected this urm to be deleted, got %+v instead"expected 2 users, got: %v"expected 2 users, got: %v"expected buckets to be deleted, got: %+v"expected buckets to be deleted, got: %+v" TenantService tests the tenant service functions. These tests stress the relation between the services embedded by the TenantService. The individual functionality of services is tested elsewhere. Create tests various cases of creation for the services in the TenantService. For example, when you create a user, do you create system buckets? How are URMs organized? Blank fields, we are testing creation. NOTE(affo)(*kv.Service): tests that contain s.CreateOrganization() generate error in logs:   Failed to make user owner of organization: {"error": "could not find authorizer on context when adding user to resource type orgs"}. This happens because kv requires an authorization to be in context. This is a bad dependency pattern (store -> auth) and should not be there. Anyways this does not prevent the org to be created. If you add the urm manually you'll obtain the same result. NOTE(affo)(*kv.Service): it also creates urms for the non existing user found in context. Check existence NOTE(affo)(*kv.Service): nope, it does create system buckets with invalid OrgIDs. Number of buckets prior to user creation. This is because, for now, system buckets always get returned for compatibility with the old system. Compare new number of buckets with the one prior to user creation. NOTE(affo)(*kv.Service): nope, it does create a useless URM, no existence check.  Apparently, system buckets are created too :thinking. First create an org and a user. Wrong userID. Wrong orgID. The URM gets created successfully. NOTE(affo)(*kv.Service): errors on bucket creation.  But, apparently, system buckets are created too :thinking. Number of buckets prior to bucket creation. Compare new number of buckets with the one prior to bucket creation. Now add a new bucket and check the URMs. Delete tests various cases of deletion for the services in the TenantService. An example: if you delete a bucket the corresponding user resource mapping is not present. URM are userID + resourceID (they do not include resource type) so same IDs across different resources leads to collisions therefore, we need to start bucket IDs at higher offset for test. 2 organizations create 2 system buckets each so start at 14 ID(14) ID(15) NOTE(affo): bucket URMs should not be here, create them only for deletion purposes. user 1 owns org1 (and so bucket1) user 1 is member of org2 (and so bucket2) user 2 owns org2 (and so bucket2) NOTE(affo): those resources could not be dangling (URM could be inferred from an user being in the owner org). We do not want to automatically propagate this kind of delete because an resource will always have an owner org. Pre-check the current situation. bucket1 is owned by user1. Check it. bucket2 is owned by user2. bucket2 is readable by user2. Now delete user2 -> bucket2. Still expect bucket2 to exist (user1 still points to it). Now delete user1 -> bucket2. Still expect bucket2 to exist (nobody points to it). Now delete user1 -> bucket1. Still expect bucket1 to exist (nobody points to it). Delete user1. We expect his urms deleted but not bucket1. Delete bucket2. We expect its urms deleted. NOTE(affo)(*kv.Service): buckets, users, and urms survive. Delete org1. We expect its buckets to be deleted. We expect urms to those buckets to be deleted too. No user should be deleted. Delete org2. Everything should disappear./Users/austinjaybecker/projects/abeck-go-testing/testing/user_resource_mapping_service.go"CreateUserResourceMapping""FindUserResourceMappings""DeleteUserResourceMapping"basic create user resource mapping"basic create user resource mapping"duplicate mappings are not allowed"duplicate mappings are not allowed"failed to retrieve mappings: %v"failed to retrieve mappings: %v"mappings are different -got/+want
diff %s"mappings are different -got/+want\ndiff %s"basic delete user resource mapping"basic delete user resource mapping"deleting a non-existent user"deleting a non-existent user"delete user resource mapping for org"delete user resource mapping for org"basic find mappings"basic find mappings"find mappings filtered by user"find mappings filtered by user"find mappings filtered by resource"find mappings filtered by resource"find mappings filtered by user type"find mappings filtered by user type"find mappings filtered by resource type"find mappings filtered by resource type" UserResourceFields includes prepopulated data for mapping tests UserResourceMappingService tests all the service functions. baseUserResourceFields creates base fields to create URMs. Users for URMs must exist in order not to fail on creation.lint:ignore ST1005 Error is capitalized in the tested code./Users/austinjaybecker/projects/abeck-go-testing/testing/user_service.gonerrnewUserNameoerroldUserName"CreateUser""FindUserByID""FindUsers""DeleteUser""FindUser""UpdateUser""UpdateUser_IndexHygiene"create users with empty set"create users with empty set"basic create user"basic create user"user with name user1 already exists"user with name user1 already exists"failed to retrieve users: %v"failed to retrieve users: %v"users are different -got/+want
diff %s"users are different -got/+want\ndiff %s"basic find user by id"basic find user by id"find user by id not exists"find user by id not exists"user is different -got/+want
diff %s"user is different -got/+want\ndiff %s"find all users"find all users"find user by id"find user by id"find user by name"find user by name"find user by name not exists"find user by name not exists"no_exist"no_exist"delete users using exist id"delete users using exist id"delete users using id that does not exist"delete users using id that does not exist"find existing user by its id"find existing user by its id"user with name does not exist"user with name does not exist"user with id does not exist"user with id does not exist"filter with both name and ID prefers ID"filter with both name and ID prefers ID"filter with no name nor id returns error"filter with no name nor id returns error"filter both name and non-existent id returns no user"filter both name and non-existent id returns no user"update status"update status"update name with id not exists"update name with id not exists"user1Updated"user1Updated"unexpected error when finding user by name"unexpected error when finding user by name" UserFields will include the IDGenerator, and users UserService tests all the service functions. CreateUser testing Delete only created users - ie., having a not nil ID FindUserByID testing FindUsers testing DeleteUser testing FindUser testing UpdateUser testing Ensure we can find the user with the new name. Ensure we cannot find a user with the old name. The index used when searching by name should have been cleared out by the UpdateUser operation./Users/austinjaybecker/projects/abeck-go-testing/testing/util.go/Users/austinjaybecker/projects/abeck-go-testing/testing/variable.gogithub.com/influxdata/influxdb/v2/pkg/testing/assert"github.com/influxdata/influxdb/v2/pkg/testing/assert"2002"CreateVariable""TrimWhitespace""FindVariableByID""FindVariables""UpdateVariable""ReplaceVariable""DeleteVariable"trimwhitespace"trimwhitespace"existing-variable"existing-variable"   existing-variable   "   existing-variable   "variable is not unique"variable is not unique"failed to retrieve variables: %v"failed to retrieve variables: %v"found unexpected variables -got/+want
diff %s"found unexpected variables -got/+want\ndiff %s"basic create with missing id"basic create with missing id"already there"already there"basic variable"basic variable"creating a variable assigns the variable an id and adds it to the store"creating a variable assigns the variable an id and adds it to the store"MY-variable"MY-variable"cant create a new variable with a name that exists"cant create a new variable with a name that exists"variable names should be unique and case-insensitive"variable names should be unique and case-insensitive"EXISTING-variable"EXISTING-variable"variable is not unique for key "variable is not unique for key "cant create a new variable when variable name exists with a different type"cant create a new variable when variable name exists with a different type"finding a variable that exists by id"finding a variable that exists by id"existing-variable-a"existing-variable-a"existing-variable-b"existing-variable-b"finding a non-existent variable"finding a non-existent variable"variable not foundfound unexpected variable -got/+want
diff %s"found unexpected variable -got/+want\ndiff %s"find all variables"find all variables"find variables by wrong org id"find variables by wrong org id"find all variables by org 22"find all variables by org 22""c"variables are different -got/+want
diff %s"variables are different -got/+want\ndiff %s"updating a variable's name"updating a variable's name"new-variable-b-name"new-variable-b-name"updating a non-existent variable fails"updating a non-existent variable fails"howdy"howdy"updating fails when variable name already exists"updating fails when variable name already exists"variable-a"variable-a"variable-b"variable-b"variable entity update conflicts with an existing entity"variable entity update conflicts with an existing entity"trims the variable name but updating fails when variable name already exists"trims the variable name but updating fails when variable name already exists"    variable-a    "    variable-a    "variable name not updated"variable name not updated"renamed-variable"renamed-variable"deleting a variable"deleting a variable"deleting a variable that doesn't exist"deleting a variable that doesn't exist" VariableFields defines fields for a variable test VariableService tests all the service functions. trims white space, but fails when variable name already exists CreateVariable tests influxdb.VariableService CreateVariable interface method FindVariableByID tests influxdb.VariableService FindVariableByID interface method FindVariables tests influxdb.variableService FindVariables interface method todo(leodido) todo(leodido) > use VariableFilter as arg UpdateVariable tests influxdb.VariableService UpdateVariable interface method ReplaceVariable tests influxdb.VariableService ReplaceVariable interface method DeleteVariable tests influxdb.VariableService DeleteVariable interface method/Users/austinjaybecker/projects/abeck-go-testing/tests/Users/austinjaybecker/projects/abeck-go-testing/tests/auth_helpers.goClientConfigDefaultBucketNameDefaultDocumentsNamespaceDefaultOrgNameDefaultPasswordDefaultPipelineDefaultUsernameMakeAuthorizationMakeBucketPermMakeBucketRWPermMockCheckNewDefaultPipelineNewPipelineOperTokenPipelinePipelineOptionQueryRequestBodyValidCustomNotificationEndpointValidNotificationEndpointValidNotificationRuleVeryVerbosefluxPathmergePermspipelineConfigpipelineOptionfoo user auth"foo user auth"makeLauncherOptionDocumentsNamespaceMustWriteBatchWriteBatchMustCreateAuthMustCreateBucketMustCreateOrgMustCreateLabelMustCreateCheckMustCreateTelegrafMustCreateUserMustCreateVariableMustCreateNotificationEndpointMustCreateNotificationRuleMustCreateDBRPMappingMustCreateResourceDeleteResourceMustDeleteResourceMustFindAllAddURMAddOwnerMustAddOwnerAddMemberMustAddMemberRemoveURMRemoveSpecificURMMustRemoveURMMustCreateLabelMappingFindLabelMappingsMustFindLabelMappingsMustDeleteLabelMappingDefaultOrgIDDefaultBucketIDDefaultUserIDMustOpenMustCloseMustNewAdminClientMustNewClientNewBrowserClientBrowserFormakeLauncherOptionFn/Users/austinjaybecker/projects/abeck-go-testing/tests/client.gocsvRespaccessinfluxhttpunable to create auth: %v"unable to create auth: %v""n1"unable to create bucket: %v"unable to create bucket: %v"unable to create org: %v"unable to create org: %v"unable to create label: %v"unable to create label: %v"unable to create check: %v"unable to create check: %v""d1"[[howdy]]"[[howdy]]"unable to create telegraf config: %v"unable to create telegraf config: %v"unable to create user: %v"unable to create user: %v"unable to create variable: %v"unable to create variable: %v"unable to create notification endpoint: %v"unable to create notification endpoint: %v"unable to create notification rule: %v"unable to create notification rule: %v"unable to delete notification endpoint: %v"unable to delete notification endpoint: %v"unable to create DBRP mapping: %v"unable to create DBRP mapping: %v"I think sources are going to be removed right?"I think sources are going to be removed right?"Task go client is not yet created"Task go client is not yet created"Scraper go client is not yet created"Scraper go client is not yet created"Secrets go client is not yet created"Secrets go client is not yet created"Are views still a thing?"Are views still a thing?"unable to delete resource %v %v: %v"unable to delete resource %v %v: %v"unexpected error finding resources %v: %v"unexpected error finding resources %v: %v"unexpected error adding owner %v to %v: %v"unexpected error adding owner %v to %v: %v"unexpected error adding member %v to %v"unexpected error adding member %v to %v"unexpected error removing org/resource mapping: %v"unexpected error removing org/resource mapping: %v"unexpected error attaching label %v to %v: %v"unexpected error attaching label %v to %v: %v"unexpected error finding label mappings: %v"unexpected error finding label mappings: %v"unexpected error deleting label %v from %v"unexpected error deleting label %v from %v" If Session is provided, Token is ignored. Client provides an API for writing, querying, and interacting with resources like authorizations, buckets, and organizations. NewClient initialises a new Client which is ready to write points to the HTTP write endpoint. Open opens the client Close closes the client MustWriteBatch calls WriteBatch, panicking if an error is encountered. WriteBatch writes the current batch of points to the HTTP endpoint. Query returns the CSV response from a flux query to the HTTP API. This also remove all the \r to make it easier to write tests. remove the \r to simplify testing against a body of CSV. This is the only namespace for documents present after init. QueryRequestBody creates a body for a flux query using common CSV output params. Headers are included, but, annotations are not. MustCreateAuth creates an auth  or is a fatal error. Used in tests where the content of the bucket does not matter. This authorization token is an operator token for the default organization for the default user. MustCreateBucket creates a bucket or is a fatal error. MustCreateOrg creates an org or is a fatal error. Used in tests where the content of the org does not matter. MustCreateLabel creates a label or is a fatal error. Used in tests where the content of the label does not matter. MustCreateCheck creates a check or is a fatal error. Used in tests where the content of the check does not matter. MustCreateTelegraf creates a telegraf config or is a fatal error. Used in tests where the content of the telegraf config does not matter. this id is not used in the API  MustCreateUser creates a user or is a fatal error. Used in tests where the content of the user does not matter. MustCreateVariable creates a variable or is a fatal error. Used in tests where the content of the variable does not matter. MustCreateNotificationEndpoint creates a notification endpoint or is a fatal error. Used in tests where the content of the notification endpoint does not matter. MustCreateNotificationRule creates a Notification Rule or is a fatal error Used in tests where the content of the notification rule does not matter we don't need this endpoint, so delete it to be compatible with other tests MustCreateDBRPMapping creates a DBRP Mapping or is a fatal error. Used in tests where the content of the mapping does not matter. The created mapping points to the user's default bucket. MustCreateResource will create a generic resource via the API. Used in tests where the content of the resource does not matter.  // Create one of each org resource  for _, r := range influxdb.OrgResourceTypes {      client.MustCreateResource(t, r)  // Create a variable:  id := client.MustCreateResource(t, influxdb.VariablesResourceType)  defer client.MustDeleteResource(t, influxdb.VariablesResourceType, id) DeleteResource will remove a resource using the API. Ignore the other results as suggested by goDoc. MustDeleteResource requires no error when deleting a resource. FindAll returns all the IDs of a specific resource type. MustFindAll returns all the IDs of a specific resource type; any error is fatal. AddOwner associates the user as owner of the resource. MustAddOwner requires that the user is associated with the resource or the test will be stopped fatally. AddMember associates the user as member of the resource. MustAddMember requires that the user is associated with the resource RemoveURM removes association of the user to the resource. Interestingly the URM service does not make difference on the user type. I.e. removing an URM from a user to a resource, will delete every URM of every type from that user to that resource. Or, put in another way, there can only be one resource mapping from a user to a resource at a time: either you are a member, or an owner (in that case you are a member too). RemoveSpecificURM gets around a client issue where deletes doesn't have enough context to remove a urm from a specific resource type MustRemoveURM requires that the user is removed as owner/member from the resource. CreateLabelMapping creates a label mapping for label `l` to the resource with `id`. MustCreateLabelMapping requires that the label is associated with the resource FindLabelMappings finds the labels for the specified resource. MustFindLabelMappings makes the test fail if an error is found. DeleteLabelMapping deletes the label for the specified resource. MustDeleteLabelMapping makes the test fail if an error is found./Users/austinjaybecker/projects/abeck-go-testing/tests/defaults.gomyorg"myorg"db/rp"db/rp"opertoken"opertoken" Default values created when calling NewPipeline. Since we can only write data via 1.x path we need to have a 1.x bucket name OperToken has permissions to do anything. VeryVerbose when set to true, will enable very verbose logging of services./Users/austinjaybecker/projects/abeck-go-testing/tests/doc.go
	Package tests contains a set of integration tests, which run in-memory versions
	of various 2.0 services. They're not intended to be full end-to-end tests,
	but are a suitable place to write tests that need to flex the logic of
	multiple 2.0 components.
/Users/austinjaybecker/projects/abeck-go-testing/tests/mock.gohttps://howdy.com"https://howdy.com"little rule"little rule"pipeline test check"pipeline test check"1m"1m"0m"0m"Check: ${ r._check_name } is: ${ r._level }"Check: ${ r._check_name } is: ${ r._level }"from(bucket: "db/rp") |> range(start: v.timeRangeStart, stop: v.timeRangeStop) |> filter(fn: (r) => r._measurement == "my_measurement") |> filter(fn: (r) => r._field == "my_field") |> count() |> yield(name: "count")`from(bucket: "db/rp") |> range(start: v.timeRangeStart, stop: v.timeRangeStop) |> filter(fn: (r) => r._measurement == "my_measurement") |> filter(fn: (r) => r._field == "my_field") |> count() |> yield(name: "count")`"builder"my_measurement"my_measurement"my_field"my_field"9999 ValidCustomNotificationEndpoint creates a NotificationEndpoint with a custom name ValidNotificationEndpoint returns a valid notification endpoint. This is the easiest way of "mocking" a influxdb.NotificationEndpoint. ValidNotificationRule returns a valid Notification Rule of type HTTP for testing MockCheck returns a valid check to be used in tests./Users/austinjaybecker/projects/abeck-go-testing/tests/pipeline/Users/austinjaybecker/projects/abeck-go-testing/tests/pipeline/fixture.goAdminTagAllClientTagsBaseFixtureClientTagMemberTagNewBaseFixtureNoAccessTagOwnerTagfxNoAccessGetClientgithub.com/influxdata/influxdb/v2/tests"github.com/influxdata/influxdb/v2/tests"no_access"no_access"error while creating browser client: %v"error while creating browser client: %v"unknown tag %s"unknown tag %s" BaseFixture is a Fixture with multiple users in the system. NewBaseFixture creates a BaseFixture with and admin, an org owner, a member, and an outsider for the given orgID and bucketID. GetClient returns the client associated with the given tag./Users/austinjaybecker/projects/abeck-go-testing/tests/pipeline_helpers.golauncherOptionszaptestgo.uber.org/zap/zaptest"go.uber.org/zap/zaptest"LoggerOptionloggerOptionszapOptionsapplyLoggerOptiontest_name"test_name" A Pipeline is responsible for configuring launcher.TestLauncher with default values so it may be used for end-to-end integration tests. pipelineConfig tracks the pre-configuration for a pipeline. NewDefaultPipeline creates a Pipeline with default It is retained for compatibility with cloud tests. NewPipeline returns a pipeline with the given options applied to the configuration as appropriate. A single user, org, bucket and token are created. This is left here mainly for retro compatibility setup default operator infinite retention period Open opens all the components of the pipeline. MustOpen opens the pipeline, panicking if any error is encountered. Close closes all the components of the pipeline. MustClose closes the pipeline, panicking if any error is encountered. MustNewAdminClient returns a default client that will direct requests to Launcher. The operator token is authorized to do anything in the system. MustNewClient returns a client that will direct requests to Launcher. NewBrowserClient returns a client with a cookie session that will direct requests to Launcher. BrowserFor will create a user, session, and browser client. The generated browser points to the given org and bucket. The user and session are inserted directly into the backing store. Flush is a no-op and retained for compatibility with tests from cloud. DefaultPipeline is a wrapper for Pipeline and is retained for compatibility with cloud tests./Users/austinjaybecker/projects/abeck-go-testing/tests/pipeline_option.go PipelineOption configures a pipeline. WithDefaults returns a slice of options for a default pipeline. WithReplicas sets the number of replicas in the pipeline./Users/austinjaybecker/projects/abeck-go-testing/token.go TokenGenerator represents a generator for API tokens. Token generates a new API token./Users/austinjaybecker/projects/abeck-go-testing/toml/Users/austinjaybecker/projects/abeck-go-testing/toml/toml.goApplyEnvOverridesapplyEnvOverridesgrpNamegetenvintValueboolValuefloatValueenvKeystructFieldtypeOfSpecstructKeyIBytes"Size"size was empty"size was empty"ParseBytesfile mode cannot be zero"file mode cannot be zero"%04o"%04o"LookupGroupgroup must be a name (string) or id (int)"group must be a name (string) or id (int)"failed to apply %v to %v using type %v and value '%v': %s"failed to apply %v to %v using type %v and value '%v': %s"%s_%d"%s_%d""toml" Package toml adds support to marshal and unmarshal types not in the official TOML spec. Duration is a TOML wrapper type for time.Duration. String returns the string representation of the duration. UnmarshalText parses a TOML value into a duration value. Ignore if there is no value set. Otherwise parse as a duration formatted string. Set duration and return. MarshalText converts a duration to a string for decoding toml Size represents a TOML parsable file size. Users can specify size using "k" or "K" for kibibytes, "m" or "M" for mebibytes, and "g" or "G" for gibibytes. If a size suffix isn't specified then bytes are assumed. UnmarshalText parses a byte size from text. If spec is a named type and is addressable, check the address to see if it implements encoding.TextUnmarshaler. If we have a pointer, dereference it If the type is s slice, apply to each using the index as a suffix, e.g. GRAPHITE_0, GRAPHITE_0_TEMPLATES_0 or GRAPHITE_0_TEMPLATES="item1,item2" If the type is s slice but have value not parsed as slice e.g. GRAPHITE_0_TEMPLATES="item1,item2" Skip any fields that we cannot set Skip fields with tag `toml:"-"`. Embedded field without a toml tag. Don't modify prefix. Replace hyphens with underscores to avoid issues with shells If it's a sub-config, recursively apply Skip any fields we don't have a value to set/Users/austinjaybecker/projects/abeck-go-testing/tools/Users/austinjaybecker/projects/abeck-go-testing/tools/tmpl/Users/austinjaybecker/projects/abeck-go-testing/tools/tmpl/main.goStripCommentserrExitfileModeformatSourcelistValuemustReadAllparsePathpathSpecreadDataIsGoFilenvdataArggeneratedos/exec"os/exec"text/template"text/template".tmpl".tmpl" â " â ".go".go"template file '%s' must have .tmpl extension"template file '%s' must have .tmpl extension"expected NAME=VALUE, got %s"expected NAME=VALUE, got %s"input JSON data"input JSON data"run goimports"run goimports"-d NAME=VALUE"-d NAME=VALUE"data option is required"data option is required"LookPathgoimports"goimports"failed to find goimports: %s"failed to find goimports: %s"no tmpl files specified"no tmpl files specified"invalid JSON data: %s"invalid JSON data: %s"lower"lower"upper"upper""gen"error processing template '%s': %s"error processing template '%s': %s"// Code generated by %s. DO NOT EDIT.
"// Code generated by %s. DO NOT EDIT.\n"error executing template '%s': %s"error executing template '%s': %s"error formatting '%s': %s"error formatting '%s': %s"CmdSysProcAttrNoSetGroupsChrootPtraceSetsidSetpgidSetcttyNocttyCttyForegroundPgidPidisdonesigMusetDoneKillkillblockUntilWaitableProcessStateWaitStatusExitedExitStatusSignaledCoreDumpStoppedContinuedStopSignalTrapCauseRusageTimevalUsecUtimeStimeMaxrssIxrssIdrssIsrssMinfltMajfltNswapInblockOublockMsgsndMsgrcvNsignalsNvcswNivcswpidrusageUserTimeSystemTimeSysUsageexitedsysUsageExitCodeuserTimesystemTimectxResultExtraFilesWaitDelaychildIOFilesparentIOPipesgoroutinegoroutineErrcreatedByStacklookPathErrargvchildStdinchildStdoutchildStderrwriterDescriptorwatchCtxawaitGoroutinesCombinedOutputStdinPipeStdoutPipeStderrPipeExitErrorerror running goimports: %s"error running goimports: %s"'*'*/"*/" preamble keep new line unexpected state, so return raw bytes/Users/austinjaybecker/projects/abeck-go-testing/tsdb/Users/austinjaybecker/projects/abeck-go-testing/tsdb/config.goAppendSeriesEntryAppendSeriesKeyCloneSeriesSegmentsCompareSeriesKeysCreateCursorIteratorsCreateSeriesSegmentDefaultCacheSnapshotMemorySizeDefaultCacheSnapshotWriteColdDurationDefaultCompactFullWriteColdDurationDefaultCompactThroughputDefaultCompactThroughputBurstDefaultEngineDefaultIndexDefaultMaxConcurrentCompactionsDefaultMaxSeriesPerDatabaseDefaultMaxValuesPerTagDefaultSeriesFileMaxConcurrentSnapshotCompactionsDefaultSeriesIDSetCacheSizeDefaultSeriesPartitionCompactThresholdDifferenceSeriesIDIteratorsEngineFormatErrFieldNotFoundErrFieldOverflowErrFieldTypeConflictErrFieldUnmappedIDErrFormatNotFoundErrIndexClosingErrInvalidSeriesIndexErrInvalidSeriesPartitionIDErrInvalidSeriesSegmentErrInvalidSeriesSegmentVersionErrMultipleIndexTypesErrSeriesFileClosedErrSeriesPartitionClosedErrSeriesPartitionCompactionCancelledErrSeriesSegmentNotWritableErrShardDeletionErrShardDisabledErrShardNotFoundErrShardNotIdleErrStoreClosedErrUnknownEngineFormatErrUnknownFieldTypeErrUnknownFieldsFormatFilterUndeletedSeriesIDIteratorFindSegmentGenerateSeriesKeysInMemFormatIndexFormatIndexSetInmemIndexNameIntersectSeriesIDIteratorsIsValidSeriesEntryFlagIsValidSeriesSegmentFilenameLimitErrorMakeTagsKeyMarshalTagsMeasurementIteratorsMergeMeasurementIteratorsMergeSeriesIDIteratorsMergeTagKeyIteratorsMergeTagValueIteratorsMustOpenIndexNewEngineFuncNewEngineOptionsNewFieldKeysIteratorNewIndexFuncNewInmemIndexNewMeasurementFieldSetNewMeasurementFieldsNewMeasurementSliceIteratorNewSeriesIDSetNewSeriesIDSetIteratorNewSeriesIDSetIteratorsNewSeriesIDSliceIteratorNewSeriesIndexHeaderNewSeriesIteratorAdapterNewSeriesPointIteratorNewSeriesQueryAdapterIteratorNewSeriesSegmentHeaderNewShardNewShardErrorNewTagKeySliceIteratorNewTagKeysIteratorNewTagValueSliceIteratorParseSeriesKeyIntoPartialWriteErrorReadAllSeriesIDIteratorReadSeriesIndexHeaderReadSeriesKeyReadSeriesKeyFromSegmentsReadSeriesKeyLenReadSeriesKeyMeasurementReadSeriesKeyTagReadSeriesKeyTagNReadSeriesSegmentHeaderRegisterEngineRegisterIndexRegisteredEnginesRegisteredIndexesSeriesEntryFlagSizeSeriesEntryHeaderSizeSeriesFilePartitionNSeriesIDElemsSeriesIDIteratorsSeriesIDSetIteratorSeriesIDSizeSeriesIDSliceIteratorSeriesIndexElemSizeSeriesIndexHeaderSeriesIndexHeaderSizeSeriesIndexLoadFactorSeriesIndexMagicSeriesIndexVersionSeriesKeySizeSeriesKeysSizeSeriesSegmentHeaderSeriesSegmentMagicSeriesSegmentSizeSeriesSegmentVersionShardErrorSplitSeriesOffsetTSI1FormatTSI1IndexNameTSM1FormatTagKeyIteratorsTagKeysSliceTagValueIteratorsTagValuesSliceUnionSeriesIDIteratorsbyDatabasebyTagKeydataTypeFromModelsFieldTypedecodeStorePathdefaultFieldValidatorfieldKeysIteratorfieldsIndexMagicNumberfilterUndeletedSeriesIDIteratormeasurementKeyFuncmeasurementKeysIteratormeasurementMergeIteratormeasurementSliceIteratormergeTagValuesnewBinaryExprGuardnewEngineFuncsnewEpochTrackernewExprGuardnewGuardnewIndexFuncsnewMeasurementKeysIteratornewSeriesCursornewSeriesIDExprIteratornopparseSeriesKeyseriesElemAdapterseriesIDDifferenceIteratorseriesIDExprIteratorseriesIDIntersectIteratorseriesIDMergeIteratorseriesIDSetIteratorseriesIDUnionIteratorseriesIteratorAdapterseriesKeysseriesPointIteratorseriesQueryAdapterIteratorseriesSegmentFilenameRegexshardSetstatDatabaseMeasurementsstatDatabaseSeriesstatDiskBytesstatFieldsCreatestatSeriesCreatestatWriteBytesstatWritePointsDroppedstatWritePointsErrstatWritePointsOKstatWriteReqstatWriteReqErrstatWriteReqOKtagKeyMergeIteratortagKeySliceIteratortagValueMergeIteratortagValueSliceIteratortagValuesSlicetimeBytesdiagnosticsgithub.com/influxdata/influxdb/v2/v1/monitor/diagnostics"github.com/influxdata/influxdb/v2/v1/monitor/diagnostics""tsm1"2560026214400144000000000004915250331648100000toml:"-"`toml:"-"`toml:"index-version"`toml:"index-version"`toml:"wal-fsync-delay"`toml:"wal-fsync-delay"`toml:"validate-keys"`toml:"validate-keys"`toml:"query-log-enabled"`toml:"query-log-enabled"`toml:"cache-max-memory-size"`toml:"cache-max-memory-size"`toml:"cache-snapshot-memory-size"`toml:"cache-snapshot-memory-size"`toml:"cache-snapshot-write-cold-duration"`toml:"cache-snapshot-write-cold-duration"`toml:"compact-full-write-cold-duration"`toml:"compact-full-write-cold-duration"`toml:"compact-throughput"`toml:"compact-throughput"`toml:"compact-throughput-burst"`toml:"compact-throughput-burst"`toml:"max-series-per-database"`toml:"max-series-per-database"`toml:"max-values-per-tag"`toml:"max-values-per-tag"`toml:"max-concurrent-compactions"`toml:"max-concurrent-compactions"`toml:"max-index-log-file-size"`toml:"max-index-log-file-size"`toml:"series-id-set-cache-size"`toml:"series-id-set-cache-size"`toml:"series-file-max-concurrent-snapshot-compactions"`toml:"series-file-max-concurrent-snapshot-compactions"`toml:"trace-logging-enabled"`toml:"trace-logging-enabled"`toml:"tsm-use-madv-willneed"`toml:"tsm-use-madv-willneed"`Data.Dir must be specified"Data.Dir must be specified"Data.WALDir must be specified"Data.WALDir must be specified"max-concurrent-compactions must be non-negative"max-concurrent-compactions must be non-negative"series-id-set-cache-size must be non-negative"series-id-set-cache-size must be non-negative"series-file-max-concurrent-compactions must be non-negative"series-file-max-concurrent-compactions must be non-negative"unrecognized engine %s"unrecognized engine %s"unrecognized index %s"unrecognized index %s"RowFromMapwal-dir"wal-dir"wal-fsync-delay"wal-fsync-delay"cache-max-memory-size"cache-max-memory-size"cache-snapshot-memory-size"cache-snapshot-memory-size"cache-snapshot-write-cold-duration"cache-snapshot-write-cold-duration"compact-full-write-cold-duration"compact-full-write-cold-duration"max-series-per-database"max-series-per-database"max-values-per-tag"max-values-per-tag"max-concurrent-compactions"max-concurrent-compactions"max-index-log-file-size"max-index-log-file-size"series-id-set-cache-size"series-id-set-cache-size"series-file-max-concurrent-compactions"series-file-max-concurrent-compactions" DefaultEngine is the default engine for new shards DefaultIndex is the default index for new shards tsdb/engine/wal configuration options Default settings for TSM DefaultCacheMaxMemorySize is the maximum size a shard's cache can reach before it starts rejecting writes. 1GB DefaultCacheSnapshotMemorySize is the size at which the engine will snapshot the cache and write it to a TSM file, freeing up memory 25MB DefaultCacheSnapshotWriteColdDuration is the length of time at which the engine will snapshot the cache and write it to a new TSM file if the shard hasn't received writes or deletes DefaultCompactFullWriteColdDuration is the duration at which the engine will compact all TSM files in a shard if it hasn't received a write or delete DefaultCompactThroughput is the rate limit in bytes per second that we will allow TSM compactions to write to disk. Not that short bursts are allowed to happen at a possibly larger value, set by DefaultCompactThroughputBurst. A value of 0 here will disable compaction rate limiting DefaultCompactThroughputBurst is the rate limit in bytes per second that we will allow TSM compactions to write to disk. If this is not set, the burst value will be set to equal the normal throughput DefaultMaxPointsPerBlock is the maximum number of points in an encoded block in a TSM file DefaultMaxSeriesPerDatabase is the maximum number of series a node can hold per database. This limit only applies to the "inmem" index. DefaultMaxValuesPerTag is the maximum number of values a tag can have within a measurement. DefaultMaxConcurrentCompactions is the maximum number of concurrent full and level compactions that can run at one time.  A value of 0 results in 50% of runtime.GOMAXPROCS(0) used at runtime. DefaultMaxIndexLogFileSize is the default threshold, in bytes, when an index write-ahead log file will compact into an index file. 1MB DefaultSeriesIDSetCacheSize is the default number of series ID sets to cache in the TSI index. DefaultSeriesFileMaxConcurrentSnapshotCompactions is the maximum number of concurrent series partition snapshot compactions that can run at one time. A value of 0 results in runtime.GOMAXPROCS(0). Config holds the configuration for the tsbd package. General WAL configuration options WALFsyncDelay is the amount of time that a write will wait before fsyncing.  A duration greater than 0 can be used to batch up multiple fsync calls.  This is useful for slower disks or when WAL write contention is seen.  A value of 0 fsyncs every write to the WAL. Enables unicode validation on series keys on write. Query logging Compaction options for tsm1 (descriptions above with defaults) Limits MaxSeriesPerDatabase is the maximum number of series a node can hold per database. When this limit is exceeded, writes return a 'max series per database exceeded' error. A value of 0 disables the limit. This limit only applies when using the "inmem" index. MaxValuesPerTag is the maximum number of tag values a single tag key can have within a measurement.  When the limit is exceeded, writes return an error. A value of 0 disables the limit. MaxConcurrentCompactions is the maximum number of concurrent level and full compactions that can be running at one time across all shards.  Compactions scheduled to run when the limit is reached are blocked until a running compaction completes.  Snapshot compactions are not affected by this limit.  A value of 0 limits compactions to runtime.GOMAXPROCS(0). MaxIndexLogFileSize is the threshold, in bytes, when an index write-ahead log file will compact into an index file. Lower sizes will cause log files to be compacted more quickly and result in lower heap usage at the expense of write throughput. Higher sizes will be compacted less frequently, store more series in-memory, and provide higher write throughput. SeriesIDSetCacheSize is the number items that can be cached within the TSI index. TSI caching can help with query performance when the same tag key/value predicates are commonly used on queries. Setting series-id-set-cache-size to 0 disables the cache. SeriesFileMaxConcurrentSnapshotCompactions is the maximum number of concurrent snapshot compactions that can be running at one time across all series partitions in a database. Snapshots scheduled to run when the limit is reached are blocked until a running snapshot completes.  Only snapshot compactions are affected by this limit. A value of 0 limits snapshot compactions to the lesser of 8 (series file partition quantity) and runtime.GOMAXPROCS(0). TSMWillNeed controls whether we hint to the kernel that we intend to page in mmap'd sections of TSM files. This setting defaults to off, as it has been found to be problematic in some cases. It may help users who have slow disks. NewConfig returns the default configuration for tsdb. Validate validates the configuration hold by c. Diagnostics returns a diagnostics representation of a subset of the Config.fieldSetsHasInmemIndexDedupeInmemIndexesMeasurementNamesByExprmeasurementNamesByExprmeasurementNamesByNameFiltermeasurementNamesByPredicatemeasurementNamesByTagFiltermeasurementNamesByTagPredicatemeasurementAuthorizedSeriesmeasurementHasTagValuemeasurementHasEmptyTagValuemeasurementHasTagValueRegexhasTagKeymeasurementIteratortagKeyIteratortagValueIteratorTagKeyHasAuthorizedSeriesmeasurementSeriesIDIteratortagKeySeriesIDIteratortagValueSeriesIDIteratorMeasurementSeriesByExprIteratormeasurementSeriesByExprIteratorMeasurementSeriesKeysByExprseriesByExprIteratorseriesByBinaryExprIteratorseriesByBinaryExprStringIteratorseriesByBinaryExprRegexIteratorseriesByBinaryExprVarRefIteratorMatchTagValueSeriesIDIteratormatchTagValueSeriesIDIteratormatchTagValueEqualEmptySeriesIDIteratormatchTagValueEqualNotEmptySeriesIDIteratormatchTagValueNotEqualEmptySeriesIDIteratormatchTagValueNotEqualNotEmptySeriesIDIteratorTagValuesByKeyAndExprtagValuesByKeyAndExprTagSetsindexSetmitrreadSeriesKeysMaxOffsetKeyIDMapIDOffsetMapofsDroppedKeysFieldKeysByPredicatecreateSeriesIterator/Users/austinjaybecker/projects/abeck-go-testing/tsdb/cursor.go EOF represents a "not found" key returned by a Cursor. possible errors are ErrEngineClosed or ErrShardDisabled, so we can safely skip those shards/Users/austinjaybecker/projects/abeck-go-testing/tsdb/cursors/Users/austinjaybecker/projects/abeck-go-testing/tsdb/cursors/arrayvalues.gen.goEmptyMeasurementFieldsIteratorEmptyStringIteratorFieldTypeToDataTypeMeasurementFieldMeasurementFieldSliceMeasurementFieldsIteratorMeasurementFieldsIteratorFlatMapMeasurementFieldsSliceIteratorModelsFieldTypeToFieldTypeNewMeasurementFieldsSliceIteratorNewMeasurementFieldsSliceIteratorWithStatsNewStringSliceIteratorNewStringSliceIteratorWithStatsNewTimestampArrayLenStringIteratorToSliceStringSliceIteratorTimestampArrayUndefinedfieldTypeToDataTypeMappingmeasurementFieldsIteratormodelsFieldTypeToFieldTypeMappingstringIteratorminValrmaxrmin Source: arrayvalues.gen.go.tmpl search performs a binary search for UnixNano() v in a and returns the position, i, where v would be inserted. An additional check of a.Timestamps[i] == v is necessary to determine if the value v exists. Define: f(x) â a.Timestamps[x] < v Define: f(-1) == true, f(n) == false Invariant: f(lo-1) == true, f(hi) == false preserves f(lo-1) == true preserves f(hi) == false lo == hi FindRange returns the positions where min and max would be inserted into the array. If a[0].UnixNano() > max or a[len-1].UnixNano() < min then FindRange returns (-1, -1) indicating the array is outside the [min, max]. The values must be deduplicated and sorted before calling FindRange or the results are undefined. Exclude removes the subset of values in [min, max]. The values must be deduplicated and sorted before calling Exclude or the results are undefined. a.Timestamps[rmin] â¥ min a.Timestamps[rmax] â¥ max Include returns the subset values between min and max inclusive. The values must be deduplicated and sorted before calling Include or the results are undefined. Merge overlays b to top of a.  If two values conflict with the same timestamp, b is used.  Both a and b must be sorted in ascending order. Normally, both a and b should not contain duplicates.  Due to a bug in older versions, it's possible stored blocks might contain duplicate values.  Remove them if they exists before merging. a = a.Deduplicate() b = b.Deduplicate() Exclude removes the subset of timestamps in [min, max]. The timestamps must Contains returns true if values exist between min and max inclusive. The values must be sorted before calling Contains or the results are undefined.IsLowertoSliceUniqueByKey/Users/austinjaybecker/projects/abeck-go-testing/tsdb/cursors/arrayvalues.go size of timestamps + values/Users/austinjaybecker/projects/abeck-go-testing/tsdb/cursors/cursor.go Stats returns the aggregate stats of all cursor iterators. CursorStats represents stats collected by a cursor. number of values scanned number of uncompressed bytes scanned Add adds other to s and updates s./Users/austinjaybecker/projects/abeck-go-testing/tsdb/cursors/fieldtype_string.goFloatIntegerUnsignedStringBooleanUndefined"FloatIntegerUnsignedStringBooleanUndefined" Code generated by "stringer -type FieldType"; DO NOT EDIT./Users/austinjaybecker/projects/abeck-go-testing/tsdb/cursors/gen.gogo:generate env GO111MODULE=on go run github.com/benbjohnson/tmpl -data=@arrayvalues.gen.go.tmpldata arrayvalues.gen.go.tmplgo:generate stringer -type FieldType/Users/austinjaybecker/projects/abeck-go-testing/tsdb/cursors/mock/Users/austinjaybecker/projects/abeck-go-testing/tsdb/cursors/mock/cursor_iterator.goMockCursorIteratorMockCursorIteratorMockRecorderMockIntegerArrayCursorMockIntegerArrayCursorMockRecorderNewMockCursorIteratorNewMockIntegerArrayCursor"Next""Stats" Source: github.com/influxdata/influxdb/v2/tsdb/cursors (interfaces: CursorIterator) MockCursorIterator is a mock of CursorIterator interface MockCursorIteratorMockRecorder is the mock recorder for MockCursorIterator NewMockCursorIterator creates a new mock instance Next mocks base method Next indicates an expected call of Next Stats mocks base method Stats indicates an expected call of Stats/Users/austinjaybecker/projects/abeck-go-testing/tsdb/cursors/mock/integer_array_cursor.go"Err" Source: github.com/influxdata/influxdb/v2/tsdb/cursors (interfaces: IntegerArrayCursor) MockIntegerArrayCursor is a mock of IntegerArrayCursor interface MockIntegerArrayCursorMockRecorder is the mock recorder for MockIntegerArrayCursor NewMockIntegerArrayCursor creates a new mock instance Err mocks base method Err indicates an expected call of Err/Users/austinjaybecker/projects/abeck-go-testing/tsdb/cursors/schema.goiijj FieldType represents the primitive field data types available in tsm. means the data type is a float means the data type is an integer means the data type is an unsigned integer means the data type is a string of text means the data type is a boolean means the data type in unknown or undefined FieldTypeToDataType returns the equivalent influxql DataType for the field type ft. If ft is an invalid FieldType, the results are undefined. IsLower returns true if the other FieldType has greater precedence than the current value. Undefined has the lowest precedence. ModelsFieldTypeToFieldType returns the equivalent FieldType for ft. Key is the name of the field Type is field type Timestamp refers to the maximum timestamp observed for the given field MeasurementFieldSlice implements sort.Interface and sorts the slice from lowest to highest precedence. Use sort.Reverse to sort from highest to lowest. UniqueByKey performs an in-place update of m, removing duplicate elements by Key, keeping the first occurrence of each. If the slice is not sorted, the behavior of UniqueByKey is undefined. optimization: skip copy if j == i Next advances the iterator to the next value. It returns false when there are no more values. Value returns the current value. EmptyMeasurementFieldsIterator is an implementation of MeasurementFieldsIterator that returns no values. MeasurementFieldsIteratorFlatMap reads the remainder of i, flattening the results to a single slice./Users/austinjaybecker/projects/abeck-go-testing/tsdb/cursors/string.go StringIterator describes the behavior for enumerating a sequence of string values. Next advances the StringIterator to the next value. It returns false EmptyStringIterator is an implementation of StringIterator that returns StringIteratorToSlice reads the remainder of i into a slice and returns the result./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/engine.go Package engine can be imported to initialize and register all available TSDB engines. Alternatively, you can import any individual subpackage underneath engine. import "github.com/influxdata/influxdb/v2/tsdb/engine" Initialize and register tsm1 engine/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/array_cursor.gen.goBadTSMFileExtensionBatchDeletersBitReaderBlockBooleanBlockCountBlockFloat64BlockIntegerBlockStringBlockTypeBlockTypeToInfluxQLDataTypeBlockUnsignedBooleanArrayDecodeAllBooleanArrayEncodeAllBooleanDecoderBooleanEncoderCompactionGroupCompactionPlannerCompactionTempExtensionCompactorCountTimestampsDecodeBlockDecodeBooleanArrayBlockDecodeBooleanBlockDecodeFloatArrayBlockDecodeFloatBlockDecodeIntegerArrayBlockDecodeIntegerBlockDecodeStringArrayBlockDecodeStringBlockDecodeUnsignedArrayBlockDecodeUnsignedBlockDefaultFormatFileNameDefaultParseFileNameDefaultPlannerDefaultSegmentSizeDeleteRangeWALEntryDeleteRangeWALEntryTypeDeleteWALEntryDeleteWALEntryTypeDigestFilenameDigestFreshDigestManifestDigestManifestEntriesDigestManifestEntryDigestOptionsDigestReaderDigestTimeRangeDigestTimeSpanDigestWithOptionsDigestWriterEmptyValueEngineStatisticsErrCacheMemorySizeLimitExceededErrDigestAlreadyWrittenErrDigestManifestAlreadyReadErrFileInUseErrMaxBlocksExceededErrMaxKeyLengthExceededErrNoDigestManifestErrNoValuesErrSnapshotInProgressErrStringArrayEncodeTooLargeErrTSMClosedErrWALClosedErrWALCorruptFileStoreFileStoreStatisticsFloatArrayDecodeAllFloatArrayEncodeAllFloatDecoderFloatEncoderFormatFileNameFuncIndexWriterIntegerArrayDecodeAllIntegerArrayEncodeAllIntegerDecoderIntegerEncoderKeyCursorKeyIteratorMagicNumberMetricsGroupFromContextNewBitReaderNewBooleanArrayFromValuesNewBooleanEncoderNewBooleanValueNewCacheKeyIteratorNewCompactorNewContextWithMetricsGroupNewDefaultPlannerNewDigestManifestNewDigestManifestEntryNewDigestReaderNewDigestWriterNewDiskIndexWriterNewFileStoreNewFloatArrayFromValuesNewFloatEncoderNewFloatValueNewIndexWriterNewIndirectIndexNewIntegerArrayFromValuesNewIntegerEncoderNewIntegerValueNewStringArrayFromValuesNewStringEncoderNewStringValueNewTSMBatchKeyIteratorNewTSMKeyIteratorNewTSMWriterNewTSMWriterWithDiskBufferNewTimeEncoderNewUnsignedArrayFromValuesNewUnsignedValueNewValueNewWALNewWALSegmentReaderNewWALSegmentWriterParseFileNameFuncSeriesFieldKeySeriesFieldKeyBytesStringArrayDecodeAllStringArrayEncodeAllStringDecoderStringEncoderTSMErrorsTSMFileTSMWriterTimeArrayDecodeAllTimeArrayEncodeAllTimeDecoderTmpTSMFileExtensionUnmarshalPredicateUnsignedArrayDecodeAllUnsignedArrayEncodeAllWALWALEntryWALFilePrefixWALSegmentReaderWALSegmentWriterWALStatisticsWalEntryTypeWithMadviseWillNeedWriteWALEntryWriteWALEntryTypeZigZagDecodeZigZagEncodearrayCursorIteratorascLocationsbatchDeletebitMaskblockToFieldTypeblocksbooleanArrayAscendingCursorbooleanArrayDescendingCursorbooleanAscendingCursorbooleanBlocksDecodedCounterbooleanBlocksSizeCounterbooleanCompressedBitPackedbooleanCursorbooleanDecoderPoolbooleanDescendingCursorbooleanEncoderPoolbooleanEntryTypebooleanFinalizerIteratorbooleanInstrumentedIteratorbooleanIteratorbufCursorbufPoolbuildPredicateNodebytesPoolcacheBlockcacheKeyIteratorcompactionStrategycursorAtcursorsAtdefaultWaitingWALWritesdefaultWeightsdeleteFlushThresholddescLocationsdirectIndexemptyBytesemptyStoreencodeBooleanBlockencodeBooleanBlockUsingencodeBooleanValuesBlockencodeFloatBlockencodeFloatBlockUsingencodeFloatValuesBlockencodeIntegerBlockencodeIntegerBlockUsingencodeIntegerValuesBlockencodeStringBlockencodeStringBlockUsingencodeStringValuesBlockencodeUnsignedBlockencodeUnsignedBlockUsingencodeUnsignedValuesBlockencodedBlockHeaderSizeerrBlockReaderrCompactionAbortederrCompactionInProgresserrCompactionsDisablederrIncompatibleVersionerrMaxFileExceedederrSnapshotsDisablederrStringBatchDecodeInvalidStringLengtherrStringBatchDecodeLengthOverflowerrStringBatchDecodeShortBufferexcludeTombstonesBooleanArrayexcludeTombstonesBooleanValuesexcludeTombstonesFloatArrayexcludeTombstonesFloatValuesexcludeTombstonesIntegerArrayexcludeTombstonesIntegerValuesexcludeTombstonesStringArrayexcludeTombstonesStringValuesexcludeTombstonesUnsignedArrayexcludeTombstonesUnsignedValuesfileStorefloat64EntryTypefloatArrayAscendingCursorfloatArrayDescendingCursorfloatAscendingCursorfloatBlocksDecodedCounterfloatBlocksSizeCounterfloatCastIntegerCursorfloatCastUnsignedCursorfloatCompressedGorillafloatCursorfloatDecoderPoolfloatDescendingCursorfloatEncoderPoolfloatFinalizerIteratorfloatInstrumentedIteratorfloatIteratorfsyncEverygetBooleanEncodergetBufgetFloatEncodergetIntegerEncodergetStringEncodergetTimeEncodergetUnsignedEncoderidFromFileNameindexCountSizeindexEntriesindexEntrySizeindexTagSetsindexTypeSizeintCompressedRLEintCompressedSimpleintUncompressedintegerArrayAscendingCursorintegerArrayDescendingCursorintegerAscendingCursorintegerBatchDecodeAllInvalidintegerBatchDecodeAllRLEintegerBatchDecodeAllSimpleintegerBatchDecodeAllUncompressedintegerBatchDecoderFuncintegerBlocksDecodedCounterintegerBlocksSizeCounterintegerCastFloatCursorintegerCastUnsignedCursorintegerCursorintegerDecoderPoolintegerDescendingCursorintegerEncoderPoolintegerEntryTypeintegerFinalizerIteratorintegerInstrumentedIteratorintegerIteratorkeyFieldSeparatorkeyFieldSeparatorByteskeyIteratorkeyIteratorslatenciesliteralValueCursormadvisemadviseDontNeedmatchTagValuesmaxIndexEntriesmaxKeyLengthmaxTSMFileSizemergeKeyIteratormmapAccessornewBooleanArrayAscendingCursornewBooleanArrayDescendingCursornewBooleanAscendingCursornewBooleanCursornewBooleanDescendingCursornewBooleanFinalizerIteratornewBooleanInstrumentedIteratornewBooleanIteratornewBufCursornewEntryValuesnewFinalizerIteratornewFloatArrayAscendingCursornewFloatArrayDescendingCursornewFloatAscendingCursornewFloatCursornewFloatDescendingCursornewFloatFinalizerIteratornewFloatInstrumentedIteratornewFloatIteratornewInstrumentedIteratornewIntegerArrayAscendingCursornewIntegerArrayDescendingCursornewIntegerAscendingCursornewIntegerCursornewIntegerDescendingCursornewIntegerFinalizerIteratornewIntegerInstrumentedIteratornewIntegerIteratornewKeyCursornewKeyIteratornewLimitIteratornewMergeFinalizerIteratornewMergeKeyIteratornewPredicateCachenewPredicateStatenewSchedulernewStringArrayAscendingCursornewStringArrayDescendingCursornewStringAscendingCursornewStringCursornewStringDescendingCursornewStringFinalizerIteratornewStringInstrumentedIteratornewStringIteratornewTsmGenerationnewUnsignedArrayAscendingCursornewUnsignedArrayDescendingCursornewUnsignedAscendingCursornewUnsignedCursornewUnsignedDescendingCursornewUnsignedFinalizerIteratornewUnsignedInstrumentedIteratornewUnsignedIteratornewringnilBooleanLiteralValueCursornilCursornilFloatLiteralValueCursornilIntegerLiteralValueCursornilOffsetnilStringLiteralValueCursornilUnsignedLiteralValueCursornoFileStoreObservernumberOfAuxCursorsCounternumberOfCondCursorsCounternumberOfRefCursorsCounterpackBlockplanningTimerpredicateCachepredicateEvalpredicateMatcherpredicateNodepredicateNodeAndpredicateNodeComparisonpredicateNodeOrpredicatePopTagpredicatePopTagEscapepredicateResponsepredicateResponse_falsepredicateResponse_needMorepredicateResponse_truepredicateStatepredicateVersionZeropurgerputBooleanEncoderputBufputFloatEncoderputIntegerEncoderputStringEncoderputTimeEncoderputUnsignedEncoderreadEntriesreadKeyreinterpretInt64ToUint64SlicereinterpretUint64ToInt64SliceringShardssegmentFileNamesstatCacheAgeMsstatCacheCompactionDurationstatCacheCompactionErrorstatCacheCompactionsstatCacheCompactionsActivestatCacheDiskBytesstatCacheMemoryBytesstatCacheWriteDroppedstatCacheWriteErrstatCacheWriteOKstatCachedBytesstatFileStoreBytesstatFileStoreCountstatSnapshotsstatTSMFullCompactionDurationstatTSMFullCompactionErrorstatTSMFullCompactionQueuestatTSMFullCompactionsstatTSMFullCompactionsActivestatTSMLevel1CompactionDurationstatTSMLevel1CompactionErrorstatTSMLevel1CompactionQueuestatTSMLevel1CompactionsstatTSMLevel1CompactionsActivestatTSMLevel2CompactionDurationstatTSMLevel2CompactionErrorstatTSMLevel2CompactionQueuestatTSMLevel2CompactionsstatTSMLevel2CompactionsActivestatTSMLevel3CompactionDurationstatTSMLevel3CompactionErrorstatTSMLevel3CompactionQueuestatTSMLevel3CompactionsstatTSMLevel3CompactionsActivestatTSMOptimizeCompactionDurationstatTSMOptimizeCompactionErrorstatTSMOptimizeCompactionQueuestatTSMOptimizeCompactionsstatTSMOptimizeCompactionsActivestatWALCompactionTimeMsstatWALCurrentBytesstatWALOldBytesstatWriteErrstatWriteOkstatsBufferCopyIntervalNstringArrayAscendingCursorstringArrayDescendingCursorstringAscendingCursorstringBlocksDecodedCounterstringBlocksSizeCounterstringCompressedSnappystringCursorstringDecoderPoolstringDescendingCursorstringEncoderPoolstringEntryTypestringFinalizerIteratorstringInstrumentedIteratorstringSliceCursorsyncertimeBatchDecodeAllInvalidtimeBatchDecodeAllRLEtimeBatchDecodeAllSimpletimeBatchDecodeAllUncompressedtimeBatchDecoderFunctimeCompressedPackedSimpletimeCompressedRLEtimeDecoderPooltimeEncoderPooltimeUncompressedtsmBatchKeyIteratortsmGenerationtsmGenerationstsmGrouptsmKeyIteratortsmReaderstsmWriterunpackBlockunsignedArrayAscendingCursorunsignedArrayDescendingCursorunsignedAscendingCursorunsignedBlocksDecodedCounterunsignedBlocksSizeCounterunsignedCastFloatCursorunsignedCastIntegerCursorunsignedCursorunsignedDescendingCursorunsignedEntryTypeunsignedFinalizerIteratorunsignedInstrumentedIteratorunsignedIteratoruvnanv2headerv3headerv4headervalueTypeBooleanvalueTypeFloat64valueTypeIntegervalueTypeStringvalueTypeUndefinedvalueTypeUnsignedvarRefSliceContainsvarRefSliceRemoveverifyVersionwalEncodeBufSizewalkPredicateNodesreadMinreadMaxmarkReadseeksReadFloatBlockReadIntegerBlockReadUnsignedBlockReadStringBlockReadBooleanBlockseekAscendingseekDescendingnextAscendingnextDescendingReadFloatArrayBlockReadIntegerArrayBlockReadUnsignedArrayBlockReadStringArrayBlockReadBooleanArrayBlockkeyCursornextTSMcacheValuestsmKeyCursorckeytkeycvalstvals Array Cursors close closes the cursor and any dependent cursors. Next returns the next key/value for the cursor. optimization: all points can be served from TSM data because we need the entire block and the block completely fits within the buffer. copy as much as we can TSM was exhausted cache was exhaustedBitWriterResumeWriteBitWriteBitsleadingtrailingpeekCachepeekTSMnextUnsignednextCacheParseFileNameparseFileNamehasTombstonescompactFullWriteColdDurationlastPlanChecklastFindGenerationslastGenerationsforceFullfilesInUseSetFileStoreFullyCompactedForceFullPlanLevelPlanOptimizefindGenerationsacquireMarshalSizenextStringnextFloatlocscloseGCCanReadBitFastReadBitFastReadBitReadBitsaccessCountfreeCountmmapWillNeedincAccessCacheCompactionsCacheCompactionsActiveCacheCompactionErrorsCacheCompactionDurationTSMCompactionsTSMCompactionsActiveTSMCompactionErrorsTSMCompactionDurationTSMCompactionsQueueTSMOptimizeCompactionsTSMOptimizeCompactionsActiveTSMOptimizeCompactionErrorsTSMOptimizeCompactionDurationTSMOptimizeCompactionsQueueTSMFullCompactionsTSMFullCompactionsActiveTSMFullCompactionErrorsTSMFullCompactionDurationTSMFullCompactionsQueueNextGenerationformatFileNamesnapshotsEnabledlastSnapshotDurationsnapshotLatenciessnapshotsInterruptcompactionsInterruptWithFormatFileNameFuncWithParseFileNameFuncDisableSnapshotsEnableSnapshotsWriteSnapshotCompactFullCompactFastremoveTmpFileswriteNewFilesEstimatedIndexSizeoverlapsTimeRangepartiallyReadtsmFilescurrentTsmiteratorsmergedFloatValuesmergedIntegerValuesmergedUnsignedValuesmergedBooleanValuesmergedStringValuesmergeFloatcombineFloatchunkFloatmergeIntegercombineIntegerchunkIntegermergeUnsignedcombineUnsignedchunkUnsignedmergeStringcombineStringchunkStringmergeBooleancombineBooleanchunkBooleanAppendErrorhasMergedValueshandleEncodeErrorhandleDecodeErrornextBooleannextIntegerreadFullmanifestReadReadManifestReadTimeSpancurscondsstatsLockstatsBufcopyStatsOldBytesCurrentBytessyncCountsyncWaiterscurrentSegmentIDcurrentSegmentWritersyncDelaytraceLoggertraceLoggingSegmentSizeenableTraceLoggingscheduleSyncClosedSegmentswriteToLogrollSegmentCloseSegmentnewSegmentFileFileCountpurgelastFileStatscurrentGenerationtsmMMAPWillNeedopenLimitercurrentTempDirIDCurrentGenerationWalkKeysReplaceWithCallbackqueuesweightssetDepthlevelWorkerssnapDonesnapWGCompactionPlanCacheFlushMemorySizeThresholdCacheFlushWriteColdDurationenableCompactionsOnOpenseriesIDSetsseriesTypeMapmuDigestbuildFloatCursorbuildIntegerCursorbuildUnsignedCursorbuildStringCursorbuildBooleanCursorenableLevelCompactionsdisableLevelCompactionsenableSnapshotCompactionsdisableSnapshotCompactionstimeStampFilterTarFilefilterFileToBackupoverlayreadFileFromBackupaddToIndexFromKeydeleteSeriesRangecleanupMeasurementwriteSnapshotAndCommitcompactCacheShouldCompactCachecompactHiPriorityLevelcompactLoPriorityLevelcompactFulllevelCompactionStrategyfullCompactionStrategyreloadCachecleanupTempTSMFilescreateCallIteratorcreateVarRefIteratorcreateTagSetIteratorscreateTagSetGroupIteratorscreateVarRefSeriesIteratorseriesCostfilledkeysHintgetPartitionencodePackedencodeRawencodeRLEWriteBlockWriteIndexrleencodeUncompressedibufobufwroteStreamHeadermanifestWrittenWriteManifestWriteTimeSpanReadSeekerlastSyncCachedkeyCountcomprightRegleftLiteralrightLiteralleftIndexrightIndexdurationStatactiveStatsuccessStaterrorStatcompactGrouprleDeltadecodePackeddecodeRLEdecodeRawbuildFloatArrayCursorbuildIntegerArrayCursorbuildUnsignedArrayCursorbuildStringArrayCursorbuildBooleanArrayCursorseriesFieldKeyBytesrleFirstdecodeUncompressedCRCRanges/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/array_cursor_iterator.gen.go Source: array_cursor_iterator.gen.go.tmpl buildFloatArrayCursor creates an array cursor for a float field. buildIntegerArrayCursor creates an array cursor for a integer field. buildUnsignedArrayCursor creates an array cursor for a unsigned field. buildStringArrayCursor creates an array cursor for a string field. buildBooleanArrayCursor creates an array cursor for a boolean field./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/array_cursor_iterator.gogithub.com/influxdata/influxdb/v2/pkg/metrics"github.com/influxdata/influxdb/v2/pkg/metrics" Look up fields for measurement. Find individual field. field doesn't exist for this measurement Return appropriate cursor based on type./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/array_encoding.goblockTypeinvalid block type: exp %d, got %d"invalid block type: exp %d, got %d" DecodeBooleanArrayBlock decodes the boolean block from the byte slice and writes the values to a. DecodeFloatArrayBlock decodes the float block from the byte slice DecodeIntegerArrayBlock decodes the integer block from the byte slice DecodeUnsignedArrayBlock decodes the unsigned integer block from the byte slice DecodeStringArrayBlock decodes the string block from the byte slice/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/batch_boolean.goPutUvarintUvarintBooleanBatchDecoder: invalid count"BooleanBatchDecoder: invalid count" BooleanArrayEncodeAll encodes src into b, returning b and any error encountered. The returned slice may be of a different length and capacity to b. Header + Num bools + bool data. Store the encoding type in the 4 high bits of the first byte Current bit in current byte. Encode the number of booleans written. Set current bit on current byte. Clear current bit on current byte. Add an extra byte to capture overflowing bits. First byte stores the encoding type, only have 1 bit-packet format currently ignore for now. Shouldn't happen - TSM file was truncated/corrupted/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/batch_float.gosigbitsvDeltaprevLeadingprevTrailingbits01leadingNleadingTrailingBitCountlmBitsmBitssBitsbrCachedValbrValidBitsmeaningfulNtrailingNunsupported value: NaN"unsupported value: NaN"TrailingZeros640x1F9221120237041090561RotateLeft64ERRORREAD0READ120470x7ff FloatArrayEncodeAll encodes src into b, returning b and any error encountered. Currently only the float compression scheme used in Facebook's Gorilla is supported, so this method implements a batch oriented version of that. Enough room for the header and one value. Write sentinel value to terminate batch. Number of bits written. Write first value. Encode remaining values. Encode sentinel value to terminate batch Write a zero bit. Nothing else to do. First the current bit of the current byte is set to indicate we're writing a delta value to the stream. Keep growing b until we can fit all bits in. n&7 - current bit in current byte. n>>3 - the current byte. Sets the current bit of the current byte. Write the delta to b. Determine the leading and trailing zeros. Clamp number of leading zeros to avoid overflow when encoding At least 2 further bits will be required. Write a zero bit. Write the l least significant bits of vDelta to b, most significant bit first. Full value to write. l least significant bits of v. In this case the current byte is not full. Move 8 MSB to 8 LSB Move written bits out of the way. TODO(edd): Optimise this. It's unlikely we actually have 8 bytes to write. Set a single bit to indicate a value will follow. Set current bit on current byte Write 5 bits of leading. Enough room to write the 5 bits in the current byte? 5 LSB of leading. Move 5 MSB to 8 LSB 5 bits fit into current byte. In this case there are fewer than 5 bits available in current byte. First step is to fill current byte Some of mask will get lost. Second step is to write the lost part of mask into the next byte. Move written bits in previous byte out of way. Recompute current bit. Note that if leading == trailing == 0, then sigbits == 64.  But that value doesn't actually fit into the 6 bits we have. Luckily, we never need to encode 0 significant bits, since that would put us in the other case (vdelta == 0).  So instead we write out a 0 and adjust it back to 64 on unpacking. Move 6 LSB of sigbits to MSB Move 6 MSB to 8 LSB The 6 bits fit into the current byte. In this case there are fewer than 6 bits available in current byte. First step is to fill the current byte. Write to the current bit. Write l remaining bits into current byte. Remove bits written in previous byte out of way. Write final value. Move l LSB into MSB Shift remaining bits and write out in one go. Remove bits written in previous byte. TODO(edd): Optimise this. bitMask contains a lookup table where the index is the number of bits and the value is a mask. The table is always read by ANDing the index with 0x3f, such that if the index is 64, position 0 will be read, which is a 0xffffffffffffffff, thus returning all bits. 00 = 0xffffffffffffffff 01 = 0x0000000000000001 02 = 0x0000000000000003 03 = 0x0000000000000007 ... 62 = 0x3fffffffffffffff 63 = 0x7fffffffffffffff current value trailing zero count meaningful bit count first byte is the compression type; always Gorilla special case: there were no values to decode convert the []float64 to []uint64 to avoid calling math.Float64Frombits, which results in unnecessary moves between Xn registers before moving the value into the float64 slice. This change increased performance from 320 MB/s to 340 MB/s on an Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz The bit reader code uses brCachedVal to store up to the next 8 bytes of MSB data read from b. brValidBits stores the number of remaining unread bits starting from the MSB. Before N bits are read from brCachedVal, they are left-rotated N bits, such that they end up in the left-most position. Using bits.RotateLeft64 results in a single instruction on many CPU architectures. This approach permits simple tests, such as for the two control bits:    brCachedVal&1 > 0 The alternative was to leave brCachedValue alone and perform shifts and masks to read specific bits. The original approach looked like the following:    brCachedVal&(1<<(brValidBits&0x3f)) > 0 a buffer of up to the next 8 bytes read from b in MSB order the number of unread bits remaining in brCachedVal Refill brCachedVal, reading up to 8 bytes from b fast path reads 8 bytes directly The expected exit condition is for a uvnan to be decoded. Any other error (EOF) indicates a truncated stream. brValidBits > 0 is impossible to predict, so we place the most likely case inside the if and immediately jump, keeping the instruction pipeline consistently full. This is a similar approach to using the GCC __builtin_expect intrinsic, which modifies the order of branches such that the likely case follows the conditional jump. Written as if brValidBits == 0 and placing the Refill brCachedVal code inside reduces benchmarks from 318 MB/s to 260 MB/s on an Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz read control bit 0 read control bit 1 read 5 bits for leading zero count and 6 bits for the meaningful data count leading + meaningful data counts decode 5 bits leading + 6 bits meaningful for a total of 11 bits 5 bits leading 6 bits meaningful meaningfulN == 0 is a special case, such that all bits are meaningful significant bits IsNaN, eof/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/batch_integer.godeltassrcintgithub.com/influxdata/influxdb/v2/pkg/encoding/simple8b"github.com/influxdata/influxdb/v2/pkg/encoding/simple8b"IntegerArrayDecodeAll: expected multiple of 8 bytes"IntegerArrayDecodeAll: expected multiple of 8 bytes"IntegerArrayDecodeAll: not enough data to decode packed value"IntegerArrayDecodeAll: not enough data to decode packed value"IntegerArrayDecodeAll: unexpected number of values decoded; got=%d, exp=%d"IntegerArrayDecodeAll: unexpected number of values decoded; got=%d, exp=%d"IntegerArrayDecodeAll: not enough data to decode RLE starting value"IntegerArrayDecodeAll: not enough data to decode RLE starting value"IntegerArrayDecodeAll: invalid RLE delta value"IntegerArrayDecodeAll: invalid RLE delta value"IntegerArrayDecodeAll: invalid RLE repeat value"IntegerArrayDecodeAll: invalid RLE repeat value"unknown encoding %v"unknown encoding %v" IntegerArrayEncodeAll encodes src into b, returning b and any error encountered. IntegerArrayEncodeAll implements batch oriented versions of the three integer encoding types we support: uncompressed, simple8b and RLE. Important: IntegerArrayEncodeAll modifies the contents of src by using it as scratch space for delta encoded values. It is NOT SAFE to use src after passing it into IntegerArrayEncodeAll. To prevent an allocation of the entire block we're encoding reuse the src slice to store the encoded deltas. Large varints can take up to 10 bytes.  We're storing 3 + 1 type byte. 4 high bits used for the encoding type The first value The first delta The number of times the delta is repeated There is an encoded value that's too big to simple8b encode. Encode uncompressed. 4 high bits of first byte store the encoding type for the block Encode with simple8b - fist value is written unencoded using 8 bytes. Write the first value since it's not part of the encoded values Write the encoded values UnsignedArrayEncodeAll encodes src into b, returning b and any error encountered. UnsignedArrayEncodeAll implements batch oriented versions of the three integer integerBatchDecodeAllInvalid first value decode compressed values calculate prefix sum Next 8 bytes is the starting value Next 1-10 bytes is the delta value Last 1-10 bytes is how many times the value repeats/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/batch_string.gomlecompressedSzdtasrcSzsrcSz64totSzsnappy"github.com/golang/snappy"StringArrayDecodeAll: invalid encoded string length"StringArrayDecodeAll: invalid encoded string length"StringArrayDecodeAll: length overflow"StringArrayDecodeAll: length overflow"StringArrayDecodeAll: short buffer"StringArrayDecodeAll: short buffer"StringArrayEncodeAll: source length too large"StringArrayEncodeAll: source length too large"MaxVarintLen32MaxUint32MaxEncodedLenfailed to decode string block: %v"failed to decode string block: %v" ErrStringArrayEncodeTooLarge reports that the encoded length of a slice of strings is too large. StringArrayEncodeAll encodes src into b, returning b and any error encountered. Currently only the string compression scheme used snappy. strings shouldn't be longer than 64kb 32-bit systems determine the maximum possible length needed for the buffer, which includes the compressed size header  Shortcut to snappy encoding nothing. write the data to be compressed *after* the space needed for snappy compression. The compressed data is at the start of the allocated buffer, ensuring the entire capacity is returned and available for subsequent use. First byte stores the encoding type, only have snappy format currently so ignore for now. it is important that to note that `snappy.Decode` always returns a newly allocated slice as the final strings reference this slice directly. The length of this string plus the length of the variable byte encoded length NOTE: this optimization is critical for performance and to reduce allocations. This is just as "safe" as string.Builder, which returns a string mapped to the original byte slice force a resize/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/batch_timestamp.godgap1e121000000000000TimeArrayDecodeAll: expected multiple of 8 bytes"TimeArrayDecodeAll: expected multiple of 8 bytes"TimeArrayDecodeAll: not enough data to decode packed timestamps"TimeArrayDecodeAll: not enough data to decode packed timestamps"Pow100xFTimeArrayDecodeAll: unexpected number of values decoded; got=%d, exp=%d"TimeArrayDecodeAll: unexpected number of values decoded; got=%d, exp=%d"TimeArrayDecodeAll: not enough data to decode RLE starting value"TimeArrayDecodeAll: not enough data to decode RLE starting value"TimeArrayDecodeAll: invalid run length in decodeRLE"TimeArrayDecodeAll: invalid run length in decodeRLE"TimeDecoder: invalid repeat value in decodeRLE"TimeDecoder: invalid repeat value in decodeRLE" TimeArrayEncodeAll encodes src into b, returning b and any error encountered. TimeArrayEncodeAll implements batch oriented versions of the three integer Timestamp values to be encoded should be sorted before encoding.  When encoded, the values are first delta-encoded.  The first value is the starting timestamp, subsequent values are the difference from the prior value. Important: TimeArrayEncodeAll modifies the contents of src by using it as passing it into TimeArrayEncodeAll. Deltas are the same - encode with RLE The first delta, checking the divisor given all deltas are the same, we can do a single check for the divisor 4 low bits are the log10 divisor We can't compress this time-range, the deltas exceed 1 << 60 find divisor only if we're compressing with simple8b If our value is divisible by 10, break.  Otherwise, try the next smallest divisor. Only apply the divisor if it's greater than 1 since division is expensive. timeBatchDecodeAllInvalid multiplier Compute the prefix sum and scale the deltas back up Lower 4 bits hold the 10 based exponent so we can scale the values back up Next 8 bytes is the starting timestamp Next 1-10 bytes is our (scaled down by factor of 10) run length delta Scale the delta back up/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/bit_reader.gobufNbyteN BitReader reads bits from an io.Reader. bit buffer available bits NewBitReader returns a new instance of BitReader that reads from data. Reset sets the underlying reader on b and reinitializes. CanReadBitFast returns true if calling ReadBitFast() is allowed. Fast bit reads are allowed when at least 2 values are in the buffer. This is because it is not required to refilled the buffer and the caller can inline the calls. ReadBitFast is an optimized bit read. IMPORTANT: Only allowed if CanReadFastBit() is true! ReadBit returns the next bit from the underlying data. ReadBits reads nbits from the underlying data into a uint64. nbits must be from 1 to 64, inclusive. Return EOF if there is no more data. Return bits from buffer if less than available bits. Return all bits, if requested. Otherwise mask returned bits. Otherwise read all available bits in current buffer. Read new buffer. Append new buffer to previous buffer and shift to remove unnecessary bits. Remove used bits from new buffer. Determine number of bytes to read to fill buffer. Limit to the length of our data. Optimized 8-byte read. Otherwise append bytes to buffer. Move data forward./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/bool.goBooleanDecoder: invalid count"BooleanDecoder: invalid count" boolean encoding uses 1 bit per value.  Each compressed byte slice contains a 1 byte header indicating the compression type, followed by a variable byte encoded length indicating how many booleans are packed in the slice.  The remaining bytes contains 1 byte for every 8 boolean values encoded. Note: an uncompressed boolean format is not yet implemented. booleanCompressedBitPacked is a bit packed format using 1 bit per boolean BooleanEncoder encodes a series of booleans to an in-memory buffer. The encoded bytes The current byte being encoded The number of bools packed into b The total number of bools written NewBooleanEncoder returns a new instance of BooleanEncoder. Reset sets the encoder to its initial state. Write encodes b to the underlying buffer. If we have filled the current byte, flush it Use 1 bit for each boolean value, shift the current byte by 1 and set the least significant bit accordingly Increment the current boolean count Increment the total boolean count Pad remaining byte w/ 0s If we have bits set, append them to the byte slice Flush is no-op Bytes returns a new byte slice containing the encoded booleans from previous calls to Write. Ensure the current byte is flushed Encode the number of booleans written Append the packed booleans BooleanDecoder decodes a series of booleans from an in-memory buffer. SetBytes initializes the decoder with a new set of bytes to read from. This must be called before calling any other methods. Next returns whether there are any bits remaining in the decoder. It returns false if there was an error decoding. The error is available on the Error method. Read returns the next bit from the decoder. Index into the byte slice integer division by 8 Bit position The mask to select the bit The packed byte Returns true if the bit is set Error returns the error encountered during decoding, if one occurred./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/cache.goaddedSizesnapStorecachesstorerssnapshotEntriesorigSizeageStatsnapshot in progress"snapshot in progress"cache-max-memory-size exceeded: (%d/%d)"cache-max-memory-size exceeded: (%d/%d)"memBytes"memBytes"diskBytes"diskBytes"snapshotCount"snapshotCount"cacheAgeMs"cacheAgeMs"cachedBytes"cachedBytes""WALCompactionTimeMs"writeOk"writeOk"writeErr"writeErr"writeDropped"writeDropped"tsm1_cache"tsm1_cache"CompareAndSwapUint32StoreUint64Reading file"Reading file"File corrupt"File corrupt""pos"cacheloader"cacheloader" ringShards specifies the number of partitions that the hash ring used to store the entry mappings contains. It must be a power of 2. From empirical testing, a value above the number of cores on the machine does not provide any additional benefit. For now we'll set it to the number of cores on the largest box we could imagine running influx. ErrSnapshotInProgress is returned if a snapshot is attempted while one is already running. ErrCacheMemorySizeLimitExceeded returns an error indicating an operation could not be completed due to exceeding the cache-max-memory-size setting. entry is a set of values and some metadata. All stored values. The type of values stored. Read only so doesn't need to be protected by mu. newEntryValues returns a new instance of entry with the given values.  If the values are not valid, an error is returned. No values, don't check types and ordering Make sure all the values are the same type Set the type of values stored. add adds the given values to the entry. Are any of the new values the wrong type? entry currently has no values, so add the new ones and we're done. Append the new values to the existing ones... deduplicate sorts and orders the entry's values. If values are already deduped and sorted, the function does no work and simply returns. count returns the number of values in this entry. filter removes all values with timestamps between min and max inclusive. size returns the size of this entry in bytes. InfluxQLType returns for the entry the data type of its values. Statistics gathered by the Cache. levels - point in time measures level: Size of in-memory cache in bytes level: Size of on-disk snapshots in bytes level: Number of active snapshots. level: Number of milliseconds since cache was last snapshoted at sample time counters - accumulative measures counter: Total number of bytes written into snapshots. counter: Total number of milliseconds spent compacting snapshots storer is the interface that descibes a cache's store. Get an entry by its key. Write an entry to the store. Add a new entry to the store. Remove an entry from the store. Return an optionally sorted slice of entry keys. Apply f to all entries in the store in parallel. Apply f to all entries in serial. Reset the store to an initial unused state. Split splits the store into n stores Count returns the number of keys in the store Cache maintains an in-memory store of Values for a set of keys. Due to a bug in atomic  size needs to be the first word in the struct, as that's the only place where you're guaranteed to be 64-bit aligned on a 32 bit system. See: https://golang.org/pkg/sync/atomic/#pkg-note-BUG snapshots are the cache objects that are currently being written to tsm files they're kept in memory while flushing so they can be queried along with the cache. they are read only and should never be modified This number is the number of pending or failed WriteSnaphot attempts since the last successful one. A one time synchronization used to initial the cache with a store.  Since the store can allocate a a large amount memory across shards, we lazily create it. NewCache returns an instance of a cache which will use a maximum of maxSize bytes of memory. Only used for engine caches, never for snapshots. CacheStatistics hold statistics related to the cache. Statistics returns statistics for periodic monitoring. init initializes the cache and allocates the underlying store.  Once initialized, the store re-used until Freed. Free releases the underlying store and memory held by the Cache. Write writes the set of values for the key to the cache. This function is goroutine-safe. It returns an error if the cache will exceed its max size by adding the new values. Enough room in the cache? Update the cache size and the memory size stat. WriteMulti writes the map of keys and associated values to the cache. This function is goroutine-safe. It returns an error if the cache will exceeded its max size by adding the new values.  The write attempts to write as many values as possible.  If one key fails, the others can still succeed and an error will be returned. maxSize is safe for reading without a lock. We'll optimistially set size here, and then decrement it for write errors. The write failed, hold onto the error and adjust the size delta. Some points in the batch were dropped.  An error is returned so error stat is incremented as well. Update the memory size stat Snapshot takes a snapshot of the current cache, adds it to the slice of caches that are being flushed, and resets the current cache with new values. increment the number of times we tried to do this If no snapshot exists, create a new one, otherwise update the existing snapshot Did a prior snapshot exist that failed?  If so, return the existing snapshot to retry. Save the size of the snapshot on the snapshot cache Save the size of the snapshot on the live cache Reset the cache's store. increment the number of bytes added to the snapshot Deduplicate sorts the snapshot before returning it. The compactor and any queries coming in while it writes will need the values sorted. Apply a function that simply calls deduplicate on each entry in the ring. apply cannot return an error in this invocation. ClearSnapshot removes the snapshot cache from the list of flushing caches and adjusts the size. reset the snapshot store outside of the write lock decrement the number of bytes in cache Reset the snapshot to a fresh Cache. Size returns the number of point-calcuated bytes the cache currently uses. increaseSize increases size by delta. decreaseSize decreases size by delta. Per sync/atomic docs, bit-flip delta minus one to perform subtraction within AddUint64. MaxSize returns the maximum number of bytes the cache may consume. Keys returns a sorted slice of all keys under management by the cache. Type returns the series type for a key. Values returns a copy of all values, deduped and sorted, for the given key. No values in hot cache or snapshots. Build the sequence of entries that will be returned, in the correct order. Calculate the required size of the destination buffer. guarantee we are deduplicated Any entries? If not, return. Create the buffer, and copy all hot values and snapshots. Individual entries are sorted at this point, so now the code has to check if the resultant buffer will be sorted from start to finish. Delete removes all values for the given keys from the cache. DeleteRange removes the values for all keys containing points with timestamps between between min and max from the cache. TODO(edd): Lock usage could possibly be optimised if necessary. Make sure key exist in the cache, skip if it does not SetMaxSize updates the memory limit of the cache. values returns the values for the key. It assumes the data is already sorted. It doesn't lock the cache but it does read-lock the entry if there is one for the key. values should only be used in compact.go in the CacheKeyIterator. ApplyEntryFn applies the function f to each entry in the Cache. ApplyEntryFn calls f on each entry in turn, within the same goroutine. It is safe for use by multiple goroutines. CacheLoader processes a set of WAL segment files, and loads a cache with the data contained within those files.  Processing of the supplied files take place in the order they exist in the files slice. NewCacheLoader returns a new instance of a CacheLoader. Load returns a cache loaded with the data contained within the segment files. If, during reading of a segment file, corruption is encountered, that segment file is truncated up to and including the last valid byte, and processing continues with the next segment file. Log some information about the segments. Nothing to read, skip it WithLogger sets the logger on the CacheLoader. UpdateAge updates the age statistic based on the current time. UpdateCompactTime updates WAL compaction time statistic based on d. updateCachedBytes increases the cachedBytes counter by b. updateMemSize updates the memSize level by b. updateSnapshots updates the snapshotsCount and the diskSize levels. Update disk stats/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/compact.gen.godedupcb Source: compact.gen.go.tmpl merge combines the next set of blocks into merged blocks. No blocks left, or pending merged values, we're done If we have more than one block or any partially tombstoned blocks, we many need to dedup Quickly scan each block to see if any overlap with the prior block, if they overlap then we need to dedup as there may be duplicate points now combine returns a new set of blocks using the current blocks in the buffers.  If dedup is true, all the blocks will be decoded, dedup and sorted in in order.  If dedup is false, only blocks that are smaller than the chunk size will be decoded and combined. Adjust the min time to the start of any overlapping blocks. We have some overlapping blocks so decode all, append in order and then dedup Remove values we already read Filter out only the values for overlapping block Record that we read a subset of the block Apply each tombstone to the block Since we combined multiple blocks, we could have more values than we should put into a single block.  We need to chunk them up into groups and re-encode them. skip this block if it's values were already read If this block is already full, just add it as is accumulate all errors to tsmKeyIterator.err If we only have 1 blocks left, just append it as is and avoid decoding/recoding The remaining blocks can be combined and we know that they do not overlap and so we can just append each, sort and re-encode. Re-encode the remaining values into the last block Invariant: v.MaxTime() == k.blocks[i].maxTime if this block is already full, just add it as is if we only have 1 blocks left, just append it as is and avoid decoding/recoding TODO(edd): pool this buffer/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/compact.golastWritewriteColdDurationparseFileNameFuncgenscGroupcGroupscurrentGengenerationslevelGroupsminGenerationsgenCountendIndexskipGroupcompactablegenTimelastGenorderedGenerationsskipInUsetsmStatsformatFileNameFunccardintCresCsplitsmaxGenerationmaxSequencetrsgenerationinProgressmaxBlocksmaxFileSizelimitWritersyncingWriterblockKeyminTypeckibencfenciencsenctencuenclevchunksgithub.com/influxdata/influxdb/v2/pkg/limiter"github.com/influxdata/influxdb/v2/pkg/limiter"2048"tmp""tsm"max file exceeded"max file exceeded"snapshots disabled"snapshots disabled"compactions disabled"compactions disabled"compaction in progress: %s"compaction in progress: %s"compaction in progress"compaction in progress"compaction aborted: %s"compaction aborted: %s"compaction aborted"compaction aborted"block read error on %s: %s"block read error on %s: %s"block read error on %s"block read error on %s"3e63000000150000000002e6bad plan: %s"bad plan: %s"error removing temp compaction file: %v"error removing temp compaction file: %v"O_EXCL256267108864invalid index entry for block. min=%d, max=%d"invalid index entry for block. min=%d, max=%d"RETRYunknown block type: %v"unknown block type: %v"encode error: unable to compress block type %s for key '%s': %v"encode error: unable to compress block type %s for key '%s': %v"decode error: unable to decompress block type %s for key '%s': %v"decode error: unable to decompress block type %s for key '%s': %v" Compactions are the process of creating read-optimized TSM files. The files are created by converting write-optimized WAL entries to read-optimized TSM format.  They can also be created from existing TSM files when there are tombstone records that need to be removed, points that were overwritten by later writes and need to updated, or multiple smaller TSM files need to be merged to reduce file counts and improve compression ratios. The compaction process is stream-oriented using multiple readers and iterators.  The resulting stream is written sorted and chunked to allow for one-pass writing of a new TSM file. 2GB CompactionTempExtension is the extension used for temporary files created during compaction. TSMFileExtension is the extension used for TSM files. Error returns the string representation of the error, to satisfy the error interface. CompactionGroup represents a list of files eligible to be compacted together. CompactionPlanner determines what TSM files and WAL segments to include in a given compaction run. ForceFull causes the planner to return a full compaction plan the next time Plan() is called if there are files that could be compacted. DefaultPlanner implements CompactionPlanner using a strategy to roll up multiple generations of TSM files into larger files in stages.  It attempts to minimize the number of TSM files on disk while rolling up a bounder number of files. compactFullWriteColdDuration specifies the length of time after which if no writes have been committed to the WAL, the engine will do a full compaction of the TSM files in this shard. This duration should always be greater than the CacheFlushWriteColdDuration lastPlanCheck is the last time Plan was called lastFindGenerations is the last time findGenerations was run lastGenerations is the last set of generations found by findGenerations forceFull causes the next full plan requests to plan any files that may need to be compacted.  Normally, these files are skipped and scheduled infrequently as the plans are more expensive to run. filesInUse is the set of files that have been returned as part of a plan and might be being compacted.  Two plans should not return the same file at any given time. tsmGeneration represents the TSM files within a generation. 000001-01.tsm, 000001-02.tsm would be in the same generation 000001 each with different sequence numbers. size returns the total size of the files in the generation. compactionLevel returns the level of the files in this generation. Level 0 is always created from the result of a cache compaction.  It generates 1 file with a sequence num of 1.  Level 2 is generated by compacting multiple level 1 files.  Level 3 is generate by compacting multiple level 2 files.  Level 4 is for anything else. count returns the number of files in the generation. hasTombstones returns true if there are keys removed for any of the files. FullyCompacted returns true if the shard is fully compacted. ForceFull causes the planner to return a full compaction plan the next time a plan is requested.  When ForceFull is called, level and optimize plans will not return plans until a full plan is requested and released. PlanLevel returns a set of TSM files to rewrite for a specific level. If a full plan has been requested, don't plan any levels which will prevent the full plan from acquiring them. Determine the generations from all files on disk.  We need to treat a generation conceptually as a single file even though it may be split across several files in sequence. If there is only one generation and no tombstones, then there's nothing to do. Group each generation by level such that two adjacent generations in the same level become part of the same group. See if this generation is orphan'd which would prevent it from being further compacted until a final full compaction runs. Remove any groups in the wrong level PlanOptimize returns all TSM files if they are in different generations in order to optimize the index across TSM files.  Each returned compaction group can be compacted concurrently. Skip the file if it's over the max size and contains a full block and it does not have any tombstones Only optimize level 4 files since using lower-levels will collide with the level planners Skip the group if it's not worthwhile to optimize it Plan returns a set of TSM files to rewrite for level 4 or higher.  The planning returns multiple groups if possible to allow compactions to run concurrently. first check if we should be doing a full compaction because nothing has been written in a long time Reset the full schedule if we planned because of it. We need to look at the level of the next file because it may need to be combined with this generation but won't get picked up on it's own if this generation is skipped.  This allows the most recently created files to get picked up by the full compaction planner and avoids having a few less optimally compressed files. Make sure we have more than 1 file and more than 1 generation don't plan if nothing has changed in the filestore If there is only one generation, return early to avoid re-compacting the same file over and over again. Need to find the ending point for level 4 files.  They will be the oldest files. We scan each generation in descending break once we see a file less than 4. As compactions run, the oldest files get bigger.  We don't want to re-compact them during this planning if they are maxed out so skip over any we see. Skip the file if it's over the max size and contains a full block or the generation is split over multiple files.  In the latter case, that would mean the data in the file spilled over the 2GB limit. This is an edge case that can happen after multiple compactions run.  The files at the beginning can become larger faster than ones after them.  We want to skip those really big ones and just compact the smaller ones until they are closer in size. step is how may files to compact in a group.  We want to clamp it at 4 but also stil return groups smaller than 4. slice off the generations that we'll examine Loop through the generations in groups of size step and see if we can compact all (or some of them as group) Skip compacting this group if there happens to be any lower level files in the middle.  These will get picked up by the level compactors. Skip the file if it's over the max size and it contains a full block With the groups, we need to evaluate whether the group as a whole can be compacted if we don't have enough generations to compact, skip it All the files to be compacted must be compacted in order.  We need to convert each group to the actual set of files in that group to be compacted. findGenerations groups all the TSM files by generation based on their filename, then returns the generations in descending order (newest first). If skipInUse is true, tsm files that are part of an existing compaction plan are not returned. Skip any files that are assigned to a current compaction plan See if the new files are already in use Mark all the new files in use Release removes the files reference in each compaction group allowing new plans to be able to use them. Compactor merges multiple TSM files into new files or writes a Cache into 1 or more TSM files. RateLimit is the limit for disk writes for all concurrent compactions. lastSnapshotDuration is the amount of time the last snapshot took to complete. The channel to signal that any in progress snapshots should be aborted. The channel to signal that any in progress level compactions should be aborted. NewCompactor returns a new instance of Compactor. Open initializes the Compactor. Close disables the Compactor. DisableSnapshots disables the compactor from performing snapshots. EnableSnapshots allows the compactor to perform snapshots. DisableSnapshots disables the compactor from performing compactions. EnableCompactions allows the compactor to perform compactions. WriteSnapshot writes a Cache snapshot to one or more new TSM files. Enable throttling if we have lower cardinality or snapshots are going fast. Write snapshost concurrently if cardinality is relatively high. Special case very high cardinality, use max concurrency and don't throttle writes. See if we were disabled while writing a snapshot compact writes multiple smaller TSM files into 1 or more larger files. The new compacted files need to added to the max generation in the set.  We need to find that max generation as well as the max sequence number to ensure we write to the next unique location. For each TSM file, create a TSM reader This would be a bug if this occurred as tsmFiles passed in should only be assigned to one compaction at any one time.  A nil tr would mean the file doesn't exist. inform that we're done with this reader when this method returns. CompactFull writes multiple smaller TSM files into 1 or more larger files. CompactFast writes multiple smaller TSM files into 1 or more larger files. removeTmpFiles is responsible for cleaning up a compaction that was started, but then abandoned before the temporary files were dealt with. writeNewFiles writes from the iterator into new TSM files, rotating to a new file once it has reached the max TSM file size. These are the new TSM files written New TSM files are written to a temp file and renamed when fully completed. Write as much as possible to this file We've hit the max file limit and there is more to write.  Create a new file and continue. If the file only contained tombstoned entries, then it would be a 0 length file that we can drop. Don't clean up the file as another compaction is using it.  This should not happen as the planner keeps track of which files are assigned to compaction plans now. Remove any tmp files we already completed We hit an error and didn't finish the compaction.  Remove the temp file and abort. syncingWriter ensures that whatever we wrap the above file descriptor in it will always be able to be synced by the tsm writer, since it does type assertions to attempt to sync. Create the write for the new TSM file. Use a disk based TSM buffer if it looks like we might create a big index in memory. Check for errors where we should not remove the file Each call to read returns the next sorted key (or the prior one if there are more values to write).  The size of values will be less than or equal to our chunk size (1000) Write the key and value If we have a max file size configured and we're over it, close out the file and return the error. Were there any errors encountered during iteration? We're all done.  Close out the file. KeyIterator allows iteration over set of keys and values in sorted order. Next returns true if there are any values remaining in the iterator. Read returns the key, time range, and raw data for the next block, or any error that occurred. Close closes the iterator. Err returns any errors encountered during iteration. EstimatedIndexSize returns the estimated size of the index that would be required to store all the series and entries in the KeyIterator. tsmKeyIterator implements the KeyIterator for set of TSMReaders.  Iteration produces keys in sorted order and the values between the keys sorted and deduped.  If any of the readers have associated tombstone entries, they are returned as part of iteration. readers is the set of readers it produce a sorted key run with values is the temporary buffers for each key that is returned by a reader pos is the current key position within the corresponding readers slice.  A value of pos[0] = 1, means the reader[0] is currently at key 1 in its ordered index. TSMError wraps any error we received while iterating values. indicates whether the iterator should choose a faster merging strategy over a more optimally compressed one.  If fast is true, multiple blocks will just be added as is and not combined.  In some cases, a slower path will need to be utilized even when fast is true to prevent overlapping blocks of time for the same key. If false, the blocks will be decoded and duplicated (if needed) and then chunked into the maximally sized blocks. size is the maximum number of values to encode in a single block key is the current key lowest key across all readers that has not be fully exhausted of values. mergeValues are decoded blocks that have been combined merged are encoded blocks that have been combined or used as is without decode readMin, readMax are the timestamps range of values have been read and encoded from this block. If readMin and readMax are still the initial values, nothing has been read. NewTSMKeyIterator returns a new TSM key iterator from readers. size indicates the maximum number of values to encode in a single block. Any merged blocks pending? Any merged values pending? If we still have blocks from the last read, merge them Read the next block from each TSM iterator This block may have ranges of time removed from it that would reduce the block min and max time. Each reader could have a different key that it's currently at, need to find the next smallest one to keep the sort ordering. block could be nil if the iterator has been exhausted for that file Now we need to find all blocks that match the min key so we can combine and dedupe the blocks if necessary After merging all the values for this key, we might not have any.  (e.g. they were all deleted through many tombstones).  In this case, move on to the next key instead of ending iteration. See if compactions were disabled while we were running. Error returns any errors encountered during iteration. tsmBatchKeyIterator implements the KeyIterator for set of TSMReaders.  Iteration produces errs is any error we received while iterating values. tsmFiles are the string names of the files for use in tracking errors, ordered the same as iterators and buf currentTsm is the current TSM file being iterated over NewTSMBatchKeyIterator returns a new TSM key iterator from readers. NewCacheKeyIterator returns a new KeyIterator from a Cache. Divide the keyset across each CPU Run one goroutine per CPU and encode a section of the key space concurrently Notify this key is fully encoded See if snapshot compactions were disabled while we were running./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/digest.gocntcrckstrdwtsmnamedigestPathmfestshardLastModdigest.tsd"digest.tsd"Can't open digest file: %s"Can't open digest file: %s"Can't stat digest file: %s"Can't stat digest file: %s"Shard modified: shard_time=%v, digest_time=%v"Shard modified: shard_time=%v, digest_time=%v"Can't read digest: err=%s"Can't read digest: err=%s"Can't read manifest: err=%s"Can't read manifest: err=%s"Digest belongs to another shard. Manually copied?: manifest_dir=%s, shard_dir=%s"Digest belongs to another shard. Manually copied?: manifest_dir=%s, shard_dir=%s"Number of tsm files differ: engine=%d, manifest=%d"Number of tsm files differ: engine=%d, manifest=%d"Names don't match: manifest_entry=%d, engine_name=%s, manifest_name=%s"Names don't match: manifest_entry=%d, engine_name=%s, manifest_name=%s"Can't stat tsm file: manifest_entry=%d, path=%s"Can't stat tsm file: manifest_entry=%d, path=%s"TSM file size changed: manifest_entry=%d, path=%s, tsm=%d, manifest=%d"TSM file size changed: manifest_entry=%d, path=%s, tsm=%d, manifest=%d"TSM file modified: manifest_entry=%d, path=%s, tsm_time=%v, digest_time=%v"TSM file modified: manifest_entry=%d, path=%s, tsm_time=%v, digest_time=%v"json:"dir"`json:"dir"`json:"entries"`json:"entries"`json:"filename"`json:"filename"` DigestWithOptions writes a digest of dir to w using options to filter by time and key range. Write the manifest. Write the digest data. Filter blocks that are outside the time filter.  If they overlap, we still include them. Digest writes a digest of dir to w of a full shard dir. DigestFresh returns true if digest cached in dir is still fresh and returns false if it is stale. If the digest is stale, a string description of the reason is also returned. files is a list of filenames the caller expects the digest to contain, usually from the engine's FileStore. Open the digest file. Get digest file info. See if shard was modified after digest was generated. Read the manifest from the digest file. Make sure the digest file belongs to this shard. See if the number of tsm files matches what's listed in the manifest. See if all the tsm files match the manifest. Check filename. Get tsm file info. See if tsm file size has changed. See if tsm file was modified after the digest was created. This should be covered by the engine mod time check above but we'll check each file to be sure. It's better to regenerate the digest than use a stale one. Digest is fresh. DigestManifest contains a list of tsm files used to generate a digest and information about those files which can be used to verify the associated digest file is still valid. Dir is the directory path this manifest describes. Entries is a list of files used to generate a digest. NewDigestManifest creates a digest manifest for a shard directory and list of tsm files from that directory. Filename is the name of one .tsm file used in digest generation. Size is the size, in bytes, of the .tsm file. NewDigestManifestEntry creates a digest manifest entry initialized with a tsm filename and its size. DigestManifestEntries is a list of entries in a manifest file, ordered by tsm filename./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/digest_reader.godigest manifest already read"digest manifest already read"read %d bytes, expected %d, data %v"read %d bytes, expected %d, data %v" ErrDigestManifestAlreadyRead is returned if the client attempts to read a manifest from a digest more than once on the same reader. Read manifest length./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/digest_writer.gono digest manifest"no digest manifest"digest manifest already written"digest manifest already written"NewBufferedWriter ErrNoDigestManifest is returned if an attempt is made to write other parts of a digest before writing the manifest. ErrDigestAlreadyWritten is returned if the client attempts to write more than one manifest. DigestWriter allows for writing a digest of a shard.  A digest is a condensed representation of the contents of a shard.  It can be scoped to one or more series keys, ranges of times or sets of files. Write length of manifest. Write manifest./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/encoding.gen.goavneedSorttsencvencnot ordered: %d %d >= %d"not ordered: %d %d >= %d" Source: encoding.gen.go.tmpllint:file-ignore U1000 generated code Values represents a slice of  values. Deduplicate returns a new slice with any values that have the same timestamp removed. The Value that appears last in the slice is the one that is kept.  The returned Values are sorted if necessary. See if we're already sorted and deduped Exclude returns the subset of values not in [min, max].  The values must a[rmin].UnixNano() â¥ min a[rmax].UnixNano() â¥ max An additional check of a[i].UnixNano() == v is necessary Define: f(x) â a[x].UnixNano() < v be deduplicated and sorted before calling Exclude or the results Sort methods FloatValues represents a slice of Float values. TODO(edd): These need to be pooled. Prepend the first timestamp of the block in the first 8 bytes and the block in the next byte, followed by the block Encoded timestamp values Encoded values IntegerValues represents a slice of Integer values. UnsignedValues represents a slice of Unsigned values. StringValues represents a slice of String values. BooleanValues represents a slice of Boolean values./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/encoding.gotdecvdectsIdxtsLengithub.com/influxdata/influxdb/v2/pkg/pool"github.com/influxdata/influxdb/v2/pkg/pool"unable to encode block type"unable to encode block type"unsupported value type %T"unsupported value type %T"no values to infer type"no values to infer type"unknown block type: %d"unknown block type: %d"count of short block: got %v, exp %v"count of short block: got %v, exp %v"BlockCount: error unpacking block: %v"BlockCount: error unpacking block: %v"decode of short block: got %v, exp %v"decode of short block: got %v, exp %v"error decoding block type: %v"error decoding block type: %v"%v %v"%v %v"MaxVarintLen64unpackBlock: unable to read timestamp block length"unpackBlock: unable to read timestamp block length"unpackBlock: not enough data for timestamp"unpackBlock: not enough data for timestamp" BlockFloat64 designates a block encodes float64 values. BlockInteger designates a block encodes int64 values. BlockBoolean designates a block encodes boolean values. BlockString designates a block encodes string values. BlockUnsigned designates a block encodes uint64 values. encodedBlockHeaderSize is the size of the header for an encoded block.  There is one byte encoding the type of the block. Prime the pools with one encoder/decoder for each available CPU. Check one out to force the allocation now and hold onto it Add them all back encoder pools decoder pools Value represents a TSM-encoded value. UnixNano returns the timestamp of the value in nanoseconds since unix epoch. Value returns the underlying value. Size returns the number of bytes necessary to represent the value and its timestamp. String returns the string representation of the value and its timestamp. internalOnly is unexported to ensure implementations of Value can only originate in this package. NewValue returns a new Value with the underlying type dependent on value. NewIntegerValue returns a new integer value. NewUnsignedValue returns a new unsigned integer value. NewFloatValue returns a new float value. NewBooleanValue returns a new boolean value. NewStringValue returns a new string value. EmptyValue is used when there is no appropriate other value. UnixNano returns tsdb.EOF. Value returns nil. Size returns 0. String returns the empty string. Encode converts the values to a byte slice.  If there are no values, this function panics. InfluxQLType returns the influxql.DataType the values map to. BlockType returns the type of value encoded in a block or an error if the block type is unknown. BlockCount returns the number of timestamps encoded in block. first byte is the block type DecodeBlock takes a byte slice and decodes it into values of the appropriate type based on the block. FloatValue represents a float64 value. UnixNano returns the timestamp of the value. Value returns the underlying float64 value. A float block is encoded using different compression strategies for timestamps and values. Encode values using Gorilla float compression Encode timestamps using an adaptive encoder that uses delta-encoding, frame-or-reference and run length encoding. Encoded float values DecodeFloatBlock decodes the float block from the byte slice and appends the float values to a. Block type is the next block, make sure we actually have a float block Setup our timestamp and value decoders Decode both a timestamp and value Did timestamp decoding have an error? Did float decoding have an error? BooleanValue represents a boolean value. Value returns the underlying boolean value. A boolean block is encoded using different compression strategies Encode timestamps using an adaptive encoder DecodeBooleanBlock decodes the boolean block from the byte slice and appends the boolean values to a. Did boolean decoding have an error? IntegerValue represents an int64 value. Value returns the underlying int64 value. Encoded int64 values Prepend the first timestamp of the block in the first 8 bytes DecodeIntegerBlock decodes the integer block from the byte slice and appends the integer values to a. The first 8 bytes is the minimum timestamp of the block Did int64 decoding have an error? UnsignedValue represents an int64 value. DecodeUnsignedBlock decodes the unsigned integer block from the byte slice and appends the unsigned integer values to a. StringValue represents a string value. Value returns the underlying string value. Encoded string values DecodeStringBlock decodes the string block from the byte slice and appends the string values to a. Did string decoding have an error? We encode the length of the timestamp block using a variable byte encoding. This allows small byte slices to take up 1 byte while larger ones use 2 or more. block is <len timestamp bytes>, <ts bytes>, <value bytes> We don't encode the value length because we know it's the rest of the block after the timestamp block. Unpack the timestamp block length Unpack the timestamp bytes Unpack the value bytes ZigZagEncode converts a int64 to a uint64 by zig zagging negative and positive values across even and odd numbers.  Eg. [0,-1,1,-2] becomes [0, 1, 2, 3]. ZigZagDecode converts a previously zigzag encoded uint64 back to a int64./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/engine.gen.go Source: engine.gen.go.tmpl buildFloatCursor creates a cursor for a float field. buildIntegerCursor creates a cursor for a integer field. buildUnsignedCursor creates a cursor for a unsigned field. buildStringCursor creates a cursor for a string field. buildBooleanCursor creates a cursor for a boolean field./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/engine.gobackoffrefOpttagsSliceshouldDeletedisableOnceflushBatchnewMaxnewMinfreshlogEndtsmfileswalDiskSizeBytescacheEmptyrunningCompactionstagSetsfsTimefieldTypesbaseLenseriesErrplannerwaitCheunstuntmpFinewFilesasNewoverlapstsmMaxtsmMincacheSeriesKeyhasCacheValuesfielsetChangedcacheKeysdeleteKeyshasDeletedoverlapsTimeRangeMinMaxoverlapsTimeRangeMinMaxLockencodedNameabortErrclosedFilesquitrunnablelevel1Groupslevel2Groupslevel3Groupslevel4GroupsoptimizeallfilesitrNconditionFieldsauxCountercondCountercondNamescurCounteritrOpttfsintargithub.com/influxdata/influxdb/v2/pkg/bytesutil"github.com/influxdata/influxdb/v2/pkg/bytesutil"github.com/influxdata/influxdb/v2/pkg/radix"github.com/influxdata/influxdb/v2/pkg/radix"github.com/influxdata/influxdb/v2/pkg/tar"github.com/influxdata/influxdb/v2/pkg/tar"github.com/influxdata/influxdb/v2/pkg/tracing"github.com/influxdata/influxdb/v2/pkg/tracing"github.com/influxdata/influxdb/v2/tsdb/index"github.com/influxdata/influxdb/v2/tsdb/index"#!~#cursors_ref"cursors_ref"cursors_aux"cursors_aux"cursors_cond"cursors_cond"planning_time"planning_time""#!~#"5120052428800cacheCompactions"cacheCompactions"cacheCompactionsActive"cacheCompactionsActive"cacheCompactionErr"cacheCompactionErr"cacheCompactionDuration"cacheCompactionDuration"tsmLevel1Compactions"tsmLevel1Compactions"tsmLevel1CompactionsActive"tsmLevel1CompactionsActive"tsmLevel1CompactionErr"tsmLevel1CompactionErr"tsmLevel1CompactionDuration"tsmLevel1CompactionDuration"tsmLevel1CompactionQueue"tsmLevel1CompactionQueue"tsmLevel2Compactions"tsmLevel2Compactions"tsmLevel2CompactionsActive"tsmLevel2CompactionsActive"tsmLevel2CompactionErr"tsmLevel2CompactionErr"tsmLevel2CompactionDuration"tsmLevel2CompactionDuration"tsmLevel2CompactionQueue"tsmLevel2CompactionQueue"tsmLevel3Compactions"tsmLevel3Compactions"tsmLevel3CompactionsActive"tsmLevel3CompactionsActive"tsmLevel3CompactionErr"tsmLevel3CompactionErr"tsmLevel3CompactionDuration"tsmLevel3CompactionDuration"tsmLevel3CompactionQueue"tsmLevel3CompactionQueue"tsmOptimizeCompactions"tsmOptimizeCompactions"tsmOptimizeCompactionsActive"tsmOptimizeCompactionsActive"tsmOptimizeCompactionErr"tsmOptimizeCompactionErr"tsmOptimizeCompactionDuration"tsmOptimizeCompactionDuration"tsmOptimizeCompactionQueue"tsmOptimizeCompactionQueue"tsmFullCompactions"tsmFullCompactions"tsmFullCompactionsActive"tsmFullCompactionsActive"tsmFullCompactionErr"tsmFullCompactionErr"tsmFullCompactionDuration"tsmFullCompactionDuration"tsmFullCompactionQueue"tsmFullCompactionQueue"INFLUXDB_SERIES_TYPE_CHECK_ENABLED"INFLUXDB_SERIES_TYPE_CHECK_ENABLED"Engine digest"Engine digest"tsm1_digest"tsm1_digest"Starting digest"Starting digest"tsm1_path"tsm1_path"Digest aborted, couldn't stat digest file"Digest aborted, couldn't stat digest file"Digest is fresh"Digest is fresh"Digest stale"Digest stale""reason"Digest aborted, problem creating shard directory path"Digest aborted, problem creating shard directory path"Digest aborted, problem creating tmp digest"Digest aborted, problem creating tmp digest"Digest aborted, problem writing tmp digest"Digest aborted, problem writing tmp digest"Digest aborted, problem renaming tmp digest"Digest aborted, problem renaming tmp digest"Digest aborted, opening new digest"Digest aborted, opening new digest"Digest aborted, can't stat new digest"Digest aborted, can't stat new digest"Digest written"Digest written"tsm1_digest_path"tsm1_digest_path"tsm1_engine"tsm1_engine"fields.idx"fields.idx"error opening fields.idx: %v.  Rebuilding."error opening fields.idx: %v.  Rebuilding."IndexNameError getting the data type of values for key"Error getting the data type of values for key"Meta data index for shard loaded"Meta data index for shard loaded"Snapshotter busy: Backup proceeding without snapshot contents."Snapshotter busy: Backup proceeding without snapshot contents."".tsm".%s".%s"unknown field type for %s: %s"unknown field type for %s: %s"fields not supported in WHERE clause during deletion"fields not supported in WHERE clause during deletion"ShardIndextagKeyValuetagKeyValueEntryRejectsetIDsInsertSeriesIDByteRangeAllNameBytesfieldNamesseriesByIDseriesByTagKeyValuesortedSeriesIDsAuthorizedSeriesByIDSeriesByIDMapSeriesByIDSliceAppendSeriesKeysByIDSeriesKeysByIDHasTagKeyValueCardinalityBytesAddSeriesSeriesIDsByTagKeySeriesIDsByTagValueIDsForExpridsForExprWalkWhereForSeriesIdsSeriesIDsAllOrByExprTagKeysByExprSeriesByTagKeyValueseriesSketchseriesTSSketchmeasurementsSketchmeasurementsTSSketchrebuildQueueMeasurementsByNameCreateMeasurementIndexIfNotExistsTagsForSeriesmeasurementNamesByTagFiltersdropMeasurementMeasurementSeriesKeysByExprIteratorassignExistingSeriesmeasurements still exist"measurements still exist"Cache snapshot"Cache snapshot"tsm1_cache_snapshot"tsm1_cache_snapshot"Snapshot for path written"Snapshot for path written"Snapshot for path deduplicated"Snapshot for path deduplicated"Error writing snapshot from compactor"Error writing snapshot from compactor"Error adding new TSM files from snapshot. Removing temp files."Error adding new TSM files from snapshot. Removing temp files."Unable to remove file"Unable to remove file"Error removing closed WAL segments"Error removing closed WAL segments"Compacting cache"Compacting cache"Error writing snapshot"Error writing snapshot"TSM compaction"TSM compaction"tsm1_compact_group"tsm1_compact_group"Beginning compaction"Beginning compaction"tsm1_files_n"tsm1_files_n"Compacting file"Compacting file"tsm1_index"tsm1_index"tsm1_file"tsm1_file"Aborted compaction"Aborted compaction"Error compacting TSM files"Error compacting TSM files"Renaming a corrupt TSM file due to compaction error"Renaming a corrupt TSM file due to compaction error"Error removing bad TSM file"Error removing bad TSM file"Error renaming corrupt TSM file"Error renaming corrupt TSM file"Error replacing new TSM files"Error replacing new TSM files"Compacted file"Compacted file"Finished compacting files"Finished compacting files"tsm1_level"tsm1_level"tsm1_strategy"tsm1_strategy"tsm1_optimize"tsm1_optimize"Reloaded WAL cache"Reloaded WAL cache"error removing tmp snapshot directory %q: %s"error removing tmp snapshot directory %q: %s"*.%s"*.%s"error getting compaction temp files: %s"error getting compaction temp files: %s"error removing temp compaction files: %v"error removing temp compaction files: %v""cond"create_iterator"create_iterator"max-select-series limit exceeded: (%d/%d)"max-select-series limit exceeded: (%d/%d)"AnyField Package tsm1 provides a TSDB in the Time Structured Merge tree format. import "github.com/influxdata/influxdb/v2/tsdb/engine/tsm1"go:generate -command tmpl go run github.com/benbjohnson/tmplgo:generate tmpl -data=@iterator.gen.go.tmpldata iterator.gen.go.tmpl engine.gen.go.tmpl array_cursor.gen.go.tmpl array_cursor_iterator.gen.go.tmpl The file store generate uses a custom modified tmpl to support adding templated data from the command line. This can probably be worked into the upstream tmpl but isn't at the moment.go:generate go run ../../../tools/tmpl -i -data=file_store.gen.go.tmpldata file_store.gen.go.tmpl=file_store.gen.gogo:generate go run ../../../tools/tmpl -i -d isArray=y -data=file_store.gen.go.tmpldata file_store.gen.go.tmpl=file_store_array.gen.gogo:generate tmpl -data=@encoding.gen.go.tmpldata encoding.gen.go.tmplgo:generate tmpl -data=@compact.gen.go.tmpldata compact.gen.go.tmplgo:generate tmpl -data=@reader.gen.go.tmpldata reader.gen.go.tmpl Ensure Engine implements the interface. Static objects to prevent small allocs. NewContextWithMetricsGroup creates a new context with a tsm1 metrics.Group for tracking various metrics when accessing TSM data. MetricsGroupFromContext returns the tsm1 metrics.Group associated with the context or nil if no group has been assigned. keyFieldSeparator separates the series key from the field name in the composite key that identifies a specific field in series deleteFlushThreshold is the size in bytes of a batch of series keys to delete. Statistics gathered by the engine. Engine represents a storage engine with compressed blocks. The following group of fields is used to track the state of level compactions within the Engine. The WaitGroup is used to monitor the compaction goroutines, the 'done' channel is used to signal those goroutines to shutdown. Every request to disable level compactions will call 'Wait' on 'wg', with the first goroutine to arrive (levelWorkers == 0 while holding the lock) will close the done channel and re-assign 'nil' to the variable. Re-enabling will decrease 'levelWorkers', and when it decreases to zero, level compactions will be started back up again. waitgroup for active level compaction goroutines channel to signal level compactions to stop Number of "workers" that expect compactions to be in a disabled state channel to signal snapshot compactions to stop waitgroup for running snapshot compactions Logger to be used for important messages Logger to be used when trace-logging is on. CacheFlushMemorySizeThreshold specifies the minimum size threshold for the cache when the engine should write a snapshot to a TSM file CacheFlushWriteColdDuration specifies the length of time after which if no writes have been committed to the WAL, the engine will write a snapshot of the cache to a TSM file WALEnabled determines whether writes to the WAL are enabled.  If this is false, writes will only exist in the cache and can be lost if a snapshot has not occurred. Invoked when creating a backup file "as new". Controls whether to enabled compactions when the engine is open Limiter for concurrent compactions. provides access to the total set of series IDs seriesTypeMap maps a series key to field type muDigest ensures only one goroutine can generate a digest at a time. NewEngine returns a new instance of Engine. Feature flag to enable per-series type checking, by default this is off and e.seriesTypeMap will be nil. Digest returns a reader for the shard's digest. Get a list of tsm file paths from the FileStore. See if there's a fresh digest cached on disk. Return the cached digest. Either no digest existed or the existing one was stale so generate a new digest. Make sure the directory exists, in case it was deleted for some reason. Create a tmp file to write the digest to. Write the new digest to the tmp file. Rename the temporary digest file to the actual digest file. Create and return a reader for the new digest file. SetEnabled sets whether the engine is enabled. SetCompactionsEnabled enables compactions on the engine.  When disabled all running compactions are aborted and new compactions stop running. enableLevelCompactions will request that level compactions start back up again 'wait' signifies that a corresponding call to disableLevelCompactions(true) was made at some point, and the associated task that required disabled compactions is now complete If we don't need to wait, see if we're already enabled still waiting on more workers or already enabled last one to enable, start things back up disableLevelCompactions will stop level compactions before returning. If 'wait' is set to true, then a corresponding call to enableLevelCompactions(true) will be required before level compactions will start back up again. Hold onto the current done channel so we can wait on it if necessary It's possible we have closed the done channel and released the lock and another goroutine has attempted to disable compactions.  We're current in the process of disabling them so check for this and wait until the original completes. Prevent new compactions from starting Stop all background compaction goroutines Signal that all goroutines have exited. Compaction were already disabled. We were not the first caller to disable compactions and they were in the process of being disabled.  Wait for them to complete before returning. Check if already enabled under read lock Check again under write lock We may be in the process of stopping snapshots.  See if the channel was closed. first one here, disable and wait for completion Wait for the snapshot goroutine to exit. Signal that the goroutines are exit and everything is stopped by setting snapDone to nil. If the cache is empty, free up its resources as well. ScheduleFullCompaction will force the engine to fully compact all data stored. This will cancel and running compactions and snapshot any data in the cache to TSM files.  This is an expensive operation. Snapshot any data in the cache Cancel running compactions Ensure compactions are restarted Force the planner to only create a full plan. Path returns the path the engine was opened with. MeasurementFieldSet returns the measurement field set. MeasurementFields returns the measurement fields for a measurement. SeriesN returns the unique number of series in the index. MeasurementsSketches returns sketches that describe the cardinality of the measurements in this shard and measurements that were in this shard, but have been tombstoned. SeriesSketches returns sketches that describe the cardinality of the series in this shard and series that were in this shard, but have LastModified returns the time when this shard was last modified. EngineStatistics maintains statistics for the engine. Counter of cache compactions that have ever run. Gauge of cache compactions currently running. Counter of cache compactions that have failed due to error. Counter of number of wall nanoseconds spent in cache compactions. Counter of TSM compactions (by level) that have ever run. Gauge of TSM compactions (by level) currently running. Counter of TSM compcations (by level) that have failed due to error. Counter of number of wall nanoseconds spent in TSM compactions (by level). Gauge of TSM compactions queues (by level). Counter of optimize compactions that have ever run. Gauge of optimize compactions currently running. Counter of optimize compactions that have failed due to error. Counter of number of wall nanoseconds spent in optimize compactions. Gauge of optimize compactions queue. Counter of full compactions that have ever run. Gauge of full compactions currently running. Counter of full compactions that have failed due to error. Counter of number of wall nanoseconds spent in full compactions. Gauge of full compactions queue. DiskSize returns the total size in bytes of all TSM and WAL segments on disk. Open opens and initializes the engine. TODO(edd): plumb context Close closes the engine. Subsequent calls to Close are a nop. Lock now and close everything else down. Ensures that the channel will not be closed again. WithLogger sets the logger for the engine. LoadMetadataIndex loads the shard metadata into memory. Note, it not safe to call LoadMetadataIndex concurrently. LoadMetadataIndex should only be called when initialising a new Engine. Save reference to index for iterator creation. If we have the cached fields index on disk and we're using TSI, we can skip scanning all the TSM files. Send batch of keys to the index. Reset buffers. Add remaining partial batch from FileStore. load metadata from the Cache Save the field set index so we don't have to rebuild it next time IsIdle returns true if the cache is empty, there are no running compactions and the shard is fully compacted. Free releases any resources held by the engine to free up memory or CPU. Backup writes a tar archive of any TSM files modified since the passed in time to the passed in writer. The basePath will be prepended to the names of the files in the archive. It will force a snapshot of the WAL first then perform the backup with a read lock against the file store. This means that new TSM files will not be able to be created in this shard while the backup is running. For shards that are still acively getting writes, this could cause the WAL to backup, increasing memory usage and evenutally rejecting writes. Remove the temporary snapshot dir Grab the tombstone file if one exists. We overlap time ranges, we need to filter the file overlap to the right overlap to the left TSM file has a range LARGER than the boundary above is the only case where we need to keep the reader open. the TSM file is 100% inside the range, so we can just write it without scanning each block implicit else: here we iterate over the blocks and only keep the ones we really want. not concerned with typ or checksum since we are just blindly writing back, with no decoding make sure the whole file is out to disk Restore reads a tar archive generated by Backup(). Only files that match basePath will be copied into the directory. This obtains a write lock so no operations can be performed while restoring. Import reads a tar archive generated by Backup() and adds each file matching basePath as a new TSM file.  This obtains a write lock so no operations can be performed while Importing. If the import is successful, a full compaction is scheduled. overlay reads a tar archive generated by Backup() and adds each file from the archive matching basePath to the shard. If asNew is true, each file will be installed as a new TSM file even if an existing file with the same name in the backup exists. Copy files from archive while under lock to prevent reopening. The filestore will only handle tsm files. Other file types will be ignored. Load any new series keys to the index If asNew is true, the files created from readFileFromBackup will be new ones having a temp extension. This isn't a .tsm file. Merge and dedup all the series keys across each reader to reduce lock contention on the index. Add remaining partial batch. readFileFromBackup copies the next file from the archive into the shard. The file is skipped if it does not have a matching shardRelativePath prefix. addToIndexFromKey will pull the measurement names, series keys, and field names from composite keys, and add them to the database index and measurement fields. Replace tsm key format with index key format. Build in-memory index, if necessary. WritePoints writes metadata and point data into the engine. It returns an error if new points are added to an existing key. Skip fields name "time", they are illegal Fast-path check to see if the field for the series already exists. Field type is unknown, we can try to add it. Existing type is different from what was passed in, we need to drop this write and refresh the series type map. Doesn't exist, so try to insert We didn't insert and the type that exists isn't what we tried to insert, so we have a conflict and must drop this field/series. The series already exists, but with a different type.  This is also a type conflict and we need to drop this field/series. first try to write to the cache DeleteSeriesRange removes the values between min and max (inclusive) from all series DeleteSeriesRangeWithPredicate removes the values between min and max (inclusive) from all series for which predicate() returns true. If predicate() is nil, then all values in range are removed. Ensure that the index does not compact away the measurement or series we're going to delete before we're done with them. Indicator that the min/max time for the current batch has changed and we need to flush the current batch before appending to it. These are reversed from min/max to ensure they are different the first time through. There is no predicate, so setup newMin/newMax to delete the full time range. See if the series should be deleted and if so, what range of time. If the min/max happens to change for the batch, we need to flush the current batch and start a new one. Disable and abort running compactions so that tombstones added existing tsm files don't get removed.  This would cause deleted measurements/series to re-appear once the compaction completed.  We only disable the level compactions so that snapshotting does not stop while writing out tombstones.  If it is stopped, and writing tombstones takes a long time, writes can get rejected due to the cache filling up. Delete all matching batch. Use the new min/max time for the next iteration deleteSeriesRange removes the values between min and max (inclusive) from all series.  This does not update the index or disable compactions.  This should mainly be called by DeleteSeriesRange and not directly. Min and max time in the engine are slightly different from the query language values. Ensure keys are sorted since lower layers require them to be. Run the delete on each TSM file in parallel See if this TSM file contains the keys and time range Delete each key we find in the file.  We seek to the min key and walk from there. find the keys in the cache and remove them ApplySerialEntryFn cannot return an error in this invocation. Cache does not walk keys in sorted order, so search the sorted series we need to delete to see if any of the cache keys match. k is the measurement + tags + sep + field Sort the series keys because ApplyEntryFn iterates over the keys randomly. delete from the WAL The series are deleted on disk, but the index may still say they exist. Depending on the the min,max time passed in, the series may or not actually exists now.  To reconcile the index, we walk the series keys that still exists on disk and cross out any keys that match the passed in series.  Any series left in the slice at the end do not exist and can be deleted from the index. Note: this is inherently racy if writes are occurring to the same measurement/series are being removed.  A write could occur and exist in the cache at this point, but we would delete it from the index. Apply runs this func concurrently.  The seriesKeys slice is mutated concurrently by different goroutines setting positions to nil. Start from the min deleted key that exists in this file. Skip over any deleted keys that are less than our tsm key We've found a matching key, cross it out so we do not remove it from the index. The seriesKeys slice is mutated if they are still found in the cache. Already crossed out Have we deleted all values for the series? If so, we need to remove the series from the index. For use when accessing series file. This key was wiped because it shouldn't be removed from index. See if this series was found in the cache earlier If there are multiple fields, they will have the same prefix.  If any field has values, then we can't delete it from the index. Remove the series from the local index. Add the id to the set of delete ids. Remove any series IDs for our set that still exist in other shards. We cannot remove these from the series file yet. Remove the remaining ids from the series file as they no longer exist in any shard. In the case of the inmem index the series can be removed across the global index (all shards). A sentinel error message to cause DeleteWithLock to not delete the measurement Under write lock, delete the measurement if we no longer have any data stored for the measurement.  If data exists, we can't delete the field set yet as there were writes to the measurement while we are deleting it. First scan the cache to see if any series exists for this measurement. Check the filestore. Something else failed, return it DeleteMeasurement deletes a measurement and all related series. Attempt to find the series keys. ForEachMeasurementName iterates over each measurement name in the engine. WriteTo is not implemented. WriteSnapshot will snapshot the cache and write a new TSM file with its contents, releasing the snapshot when done. Lock and grab the cache snapshot along with all the closed WAL filenames associated with the snapshot The snapshotted cache may have duplicate points and unsorted data.  We need to deduplicate it before writing the snapshot.  This can be very expensive so it's done while we are not holding the engine write lock. CreateSnapshot will create a temp directory that holds temporary hardlinks to the underylyng shard files. Generate a snapshot of the index. writeSnapshotAndCommit will write the passed cache to a new TSM file and remove the closed WAL segments. write the new snapshot files update the file store with these new files Remove the new snapshot files. We will try again. clear the snapshot from the in-memory cache, then the old WAL files compactCache continually checks if the WAL cache should be written to disk. ShouldCompactCache returns true if the Cache is over its flush threshold or if the passed in lastWriteTime is older than the write cold threshold. Find our compaction plans If no full compactions are need, see if an optimize is needed Update the level plan queue stats Set the queue depths on the scheduler Find the next compaction that can run and try to kick it off Release all the plans we didn't start. compactHiPriorityLevel kicks off compactions using the high priority policy. It returns true if the compaction was started Try hi priority limiter, otherwise steal a little from the low priority if we can. Release the files in the compaction plan Return the unused plans compactLoPriorityLevel kicks off compactions using the lo priority policy. It returns the plans that were not able to be started Try the lo priority limiter, otherwise steal a little from the high priority if we can. compactFull kicks off full and optimize compactions using the lo priority policy. It returns the plans that were not able to be started. compactionStrategy holds the details of what to do in a compaction. Apply concurrently compacts all the groups in a compaction strategy. compactGroup executes the compaction strategy against a single CompactionGroup. We hit a bad TSM file - rename so the next compaction can proceed. levelCompactionStrategy returns a compactionStrategy for the given level. It returns nil if there are no TSM files to compact. fullCompactionStrategy returns a compactionStrategy for higher level generations of TSM files. reloadCache reads the WAL segment files and loads them into the cache. Disable the max size during loading cleanup removes all temp files and dirs that exist on disk.  This is should only be run at startup to avoid removing tmp files that are still in use. Check to see if there are any `.tmp` directories that were left over from failed shard snapshots KeyCursor returns a KeyCursor for the given key starting at time t. CreateIterator returns an iterator for the measurement based on opt. Determine tagsets for this measurement based on dimensions and filters. Reverse the tag sets if we are ordering by descending. Calculate tag sets and apply SLIMIT/SOFFSET. Abort if the query was killed Wrap each series in a call iterator. createVarRefIterator creates an iterator for a variable reference. If we have a LIMIT or OFFSET and the grouping of the outer query is different than the current grouping, we need to perform the limit on each of the individual series keys instead to improve performance. Apply a limit on the merged iterator. When the final dimensions and the current grouping are the same, we will only produce one series so we can use the faster limit iterator. When the dimensions are different than the current grouping, we need to account for the possibility there will be multiple series. The limit iterator in the influxql package handles that scenario. createTagSetIterators creates a set of iterators for a tagset. Set parallelism by number of logical cpus. Create series key groupings w/ return error. Group series keys. Read series groups in parallel. Determine total number of iterators so we can allocate only once. Combine all iterators together and check for errors. If an error occurred, make sure we close all created iterators. createTagSetGroupIterators creates a set of iterators for a subset of a tagset's series. Retrieve non-time fields from this series filter and filter out tags. Enforce series limit at creation time. createVarRefSeriesIterator creates an iterator for a variable reference for a series. Create options specific for this series. Build main cursor. If the field doesn't exist then don't build an iterator. Build auxiliary cursors. Tag values should be returned if the field doesn't exist. Create cursor from field if a tag wasn't requested. If a field was requested, use a nil cursor of the requested type. If field doesn't exist, use the tag value. However, if the tag value is blank then return a null. Remove _tagKey condition field. We can't seach on it because we can't join it to _tagValue based on time. Remove _tagKey conditional references from iterator. Build conditional field cursors. If a conditional field doesn't exist then ignore the series. Limit tags to only the dimensions selected. If it's only auxiliary fields then it doesn't matter what type of iterator we use. Remove name if requested. buildCursor creates an untyped cursor for a field. Check if this is a system field cursor. Check for system field for field keys. Check if we need to perform a cast. Performing a cast in the engine (if it is possible) is much more efficient than an automatic cast. Populate map with tag values. Match against each specific tag. IteratorCost produces the cost of an iterator. Determine if this measurement exists. If it does not, then no shards are accessed to begin with. Determine all of the tag sets for this query. Attempt to retrieve the ref from the main expression (if it exists). Count the number of series concatenated from the tag set. Retrieve the cost for the main expression (if it exists). Retrieve the cost for every auxiliary field since these are also iterators that we may have to look through. We may want to separate these though as we are unlikely to incur anywhere close to the full costs of the auxiliary iterators because many of the selected values are usually skipped. Retrieve the expression names in the condition (if there is a condition). We will also create cursors for these too. Type returns FieldType for a series.  If the series does not exist, ErrUnknownFieldType is returned. Retrieve the range of values within the cache. SeriesFieldKey combine a series key and field name for a unique string to be hashed to a numeric ID. SeriesAndFieldFromCompositeKey returns the series key and the field key extracted from the composite key. No field???stringSetintersectFilterExprsDeleteBoolLiteralTruesTagFilter/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/engine_cursor.go/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/file_store.gen.gomaxTminT Code generated by file_store.gen.go.tmpl. DO NOT EDIT. ReadFloatBlock reads the next block as a set of float values. No matching blocks to decode First block is the oldest block containing the points we're searching for. Remove any tombstones If there are no values in this first block (all tombstoned or previously read) and we have more potential blocks too search.  Try again. Only one block with this key and time range so return it Use the current block time range as our overlapping window Blocks are ordered by generation, we may have values in the past in later blocks, if so, expand the window to include the min time range to ensure values are returned in ascending order Find first block that overlaps our window Shrink our window so it's the intersection of the first overlapping block and the first block.  We do this to minimize the region that overlaps and needs to be merged. Search the remaining blocks that overlap our window and append their values so we can merge them. Skip this block if it doesn't contain points we looking for or they have already been read Remove any tombstoned values Only use values in the overlapping window Merge the remaining values with the existing expand the window to include the max time range to ensure values are returned in descending If the block we decoded should have all of it's values included, mark it as read so we don't use it again. ReadIntegerBlock reads the next block as a set of integer values. ReadUnsignedBlock reads the next block as a set of unsigned values. ReadStringBlock reads the next block as a set of string values. ReadBooleanBlock reads the next block as a set of boolean values./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/file_store.gouniqueKeysapplyErrbatchesdfreaderCtmpfilesoldFilesupdatedFnoldNameinusetotalSizetsmTmpExtietsmf"bad"numFiles"numFiles"float_blocks_decoded"float_blocks_decoded"float_blocks_size_bytes"float_blocks_size_bytes"integer_blocks_decoded"integer_blocks_decoded"integer_blocks_size_bytes"integer_blocks_size_bytes"unsigned_blocks_decoded"unsigned_blocks_decoded"unsigned_blocks_size_bytes"unsigned_blocks_size_bytes"string_blocks_decoded"string_blocks_decoded"string_blocks_size_bytes"string_blocks_size_bytes"boolean_blocks_decoded"boolean_blocks_decoded"boolean_blocks_size_bytes"boolean_blocks_size_bytes""filestore"tsm1_filestore"tsm1_filestore"unknown type for %v"unknown type for %v"cannot open FileStore without an OpenLimiter (is EngineOptions.OpenLimiter set?)"cannot open FileStore without an OpenLimiter (is EngineOptions.OpenLimiter set?)"*."*."*.tsmerror opening file %s: %v"error opening file %s: %v"Opened file"Opened file"Cannot read corrupt tsm file, renaming"Cannot read corrupt tsm file, renaming"Cannot rename corrupt tsm file"Cannot rename corrupt tsm file"cannot rename corrupt file %s: %v"cannot rename corrupt file %s: %v"cannot read corrupt file %s: %v"cannot read corrupt file %s: %v"ENTRIESCreating snapshot"Creating snapshot"%d.%s"%d.%s"error creating tsm hard link: %q"error creating tsm hard link: %q"error creating tombstone hard link: %q"error creating tombstone hard link: %q"%09d-%09d"%09d-%09d"file %s is named incorrectly"file %s is named incorrectly"Purge: close file"Purge: close file"Purge: remove file"Purge: remove file" The extension used to describe temporary snapshot files. The extension used to describe corrupt snapshot files. TSMFile represents an on-disk TSM file. Path returns the underlying file path for the TSMFile.  If the file has not be written or loaded from disk, the zero value is returned. Read returns all the values in the block where time t resides. ReadAt returns all the values in the block identified by entry. Entries returns the index entries for all blocks for the given key. Returns true if the TSMFile may contain a value with the specified key and time. Contains returns true if the file contains any values for the given key. OverlapsTimeRange returns true if the time range of the file intersect min and max. OverlapsKeyRange returns true if the key range of the file intersects min and max. TimeRange returns the min and max time across all keys in the file. TombstoneRange returns ranges of time that are deleted for the given key. KeyRange returns the min and max keys in the file. KeyCount returns the number of distinct keys in the file. Seek returns the position in the index with the key <= key. KeyAt returns the key located at index position idx. Type returns the block type of the values stored for the key.  Returns one of BlockFloat64, BlockInt64, BlockBoolean, BlockString.  If key does not exist, an error is returned. BatchDelete return a BatchDeleter that allows for multiple deletes in batches and group commit or rollback. Delete removes the keys from the set of keys available in this file. DeleteRange removes the values for keys between timestamps min and max. HasTombstones returns true if file contains values that have been deleted. TombstoneFiles returns the tombstone filestats if there are any tombstones written for this file. Close closes the underlying file resources. Size returns the size of the file on disk in bytes. Rename renames the existing TSM file to a new name and replaces the mmap backing slice using the new file name.  Index and Reader state are not re-initialized. Remove deletes the file from the filesystem. InUse returns true if the file is currently in use by queries. Ref records that this file is actively in use. Unref records that this file is no longer in use. Stats returns summary information about the TSM file. BlockIterator returns an iterator pointing to the first block in the file and allows sequential iteration to each and every block. Free releases any resources held by the FileStore to free up system resources. Statistics gathered by the FileStore. FileStore is an abstraction around multiple TSM files. Most recently known file stats. If nil then stats will need to be recalculated If true then the kernel will be advised MMAP_WILLNEED for TSM files. limit the number of concurrent opening TSM files. FileStat holds information about a TSM file on disk. OverlapsKeyRange returns true if the min and max keys of the file overlap the arguments min and max. ContainsKey returns true if the min and max keys of the file overlap the arguments min and max. NewFileStore returns a new instance of FileStore based on the given directory. WithObserver sets the observer for the file store. enableTraceLogging must be called before the FileStore is opened. WithLogger sets the logger on the file store. FileStoreStatistics keeps statistics about the file store. Count returns the number of TSM files currently loaded. Files returns the slice of TSM files currently loaded. This is only used for tests, and the files aren't guaranteed to stay valid in the presence of compactions. Free releases any resources held by the FileStore.  The resources will be re-acquired if necessary if they are needed after freeing them. CurrentGeneration returns the current generation of the TSM files. NextGeneration increments the max file ID and returns the new value. WalkKeys calls fn for every key in every TSM file known to the FileStore.  If the key exists in multiple files, it will be invoked for each file. Ensure files are not unmapped while we're iterating over them. Keys returns all keys and types for all files in the file store. Type returns the type of values store at the block for key. Limit apply fn to number of cores DeleteRange removes the values for keys between timestamps min and max.  This should only be used with smaller batches of series keys. Rollback the deletes Open loads all the TSM files in the configured directory. Not loading files from disk so nothing to do find the current max ID for temp directories ascertain the current temp directory number by examining the existing directories and choosing the one with the higest basename when converted to an integer. i must be a valid integer and greater than f.currentTempDirID at this point struct to hold the result of opening each reader in a goroutine Keep track of the latest ID Ensure a limited number of TSM files are loaded at once. Systems which have very large datasets (1TB+) can have thousands of TSM files which can cause extremely long load times. If we are unable to read a TSM file then log the error, rename the file, and continue loading the shard without it. Accumulate file store size stats Re-initialize the lastModified time for the file store Close closes the file store. Make the object appear closed to other method calls. Let other methods access this closed object while we do the actual closing. Read returns the slice of values for the given key and the given timestamp, if any file matches those constraints. Can this file possibly contain this key and timestamp? May have the key and time we are looking for so try to find Reader returns a TSMReader for path if one is currently managed by the FileStore. Otherwise it returns nil. If it returns a file, you must call Unref on it when you are done, and never use it after that. KeyCursor returns a KeyCursor for key and t across the files in the FileStore. Stats returns the stats of the underlying files, preferring the cached version if it is still valid. The file stats cache is invalid due to changes to files. Need to recalculate. If lastFileStats's capacity is far away from the number of entries we need to add, then we'll reallocate. ReplaceWithCallback replaces oldFiles with newFiles and calls updatedFn with the files to be added the FileStore. Replace replaces oldFiles with newFiles. Rename all the new files to make them live on restart This isn't a .tsm or .tsm.tmp file. give the observer a chance to process the file first. The new TSM files have a tmp extension.  First rename them. Any error after this point should result in the file being bein named back to the original name. The caller then has the opportunity to remove it. Keep track of the new mod time Copy the current set of active files while we rename and load the new files.  We copy the pointers here to minimize the time that locks are held as well as to ensure that the replacement is atomic.Â© We need to prune our set of active files now If queries are running against this file, then we need to move it out of the way and let them complete.  We'll then delete the original file to avoid blocking callers upstream.  If the process crashes, the temp file is cleaned up at startup automatically. In order to ensure that there are no races with this (file held externally calls Ref after we check InUse), we need to maintain the invariant that every handle to a file is handed out in use (Ref'd), and handlers only ever relinquish the file once (call Unref exactly once, and never use it again). InUse is only valid during a write lock, since we allow calls to Ref and Unref under the read lock and no lock at all respectively. Copy all the tombstones related to this TSM file Rename the TSM file used by this reader Remove the old file and tombstones.  We can't use the normal TSMReader.Remove() because it now refers to our temp file which we can't remove. Tell the purger about our in-use files we need to remove If times didn't change (which can happen since file mod times are second level), then add a ns to the time to ensure that lastModified changes since files on disk actually did change Recalculate the disk size stat LastModified returns the last time the file store was updated with new TSM files or a delete. BlockCount returns number of values stored in the block at location idx in the file at path.  If path does not match any file in the store, 0 is returned.  If idx is out of range for the number of blocks in the file, 0 is returned. on Error, BlockCount(block) returns 0 for cnt We need to determine the possible files that may be accessed by this query given Skip any blocks only contain values that are tombstoned. locations returns the files and index blocks for a key and time.  ascending indicates whether the key will be scan in ascending time order or descenging time order. This function assumes the read-lock has been taken. If we ascending and the max time of the file is before where we want to start skip it. If we are descending and the min time of the file is after where we want to start, then skip it. This file could potential contain points we are looking for so find the blocks for the given key. If we ascending and the max time of a block is before where we are looking, skip it since the data is out of our range If we descending and the min time of a block is after where we are looking, skip For an ascending cursor, mark everything before the seek time as read so we can filter it out at query time For an ascending cursort, mark everything after the seek time as read Otherwise, add this file and block location CreateSnapshot creates hardlinks for all tsm and tombstone files in the path provided. create a copy of the files slice and ensure they aren't closed out from under us, nor the slice mutated. increment and keep track of the current temp dir for when we drop the lock. this ensures we are the only writer to the directory. create the tmp directory and add the hard links. there is no longer any shared mutable state. FormatFileNameFunc is executed when generating a new TSM filename. Source filenames are provided via src. DefaultFormatFileName is the default implementation to format TSM filenames. ParseFileNameFunc is executed when parsing a TSM filename into generation & sequence. DefaultParseFileName is used to parse the filenames of TSM files. KeyCursor allows iteration through keys in a set of files within a FileStore. seeks is all the file locations that we need to return during iteration. current is the set of blocks possibly containing the next set of points. Normally this is just one entry, but there may be multiple if points have been overwritten. pos is the index within seeks.  Based on ascending, it will increment or decrement through the size of seeks slice. newKeyCursor returns a new instance of KeyCursor. Determine the distinct set of TSM files in use and mark then as in-use Close removes all references on the cursor. Remove all of our in-use references since we're done seek positions the cursor at the given time. Record the position of the first block matching our seek time Next moves the cursor to the next position. Data should be read by the ReadBlock functions. Do we still have unread values in the current block Append the first matching block If we have ovelapping blocks, append all their values so we can dedup/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/file_store_array.gen.go ReadFloatArrayBlock reads the next block as a set of float values. ReadIntegerArrayBlock reads the next block as a set of integer values. ReadUnsignedArrayBlock reads the next block as a set of unsigned values. ReadStringArrayBlock reads the next block as a set of string values. ReadBooleanArrayBlock reads the next block as a set of boolean values./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/file_store_key_iterator.gomerging current key index key count remove iterator from heap same as previous key, keep iterating/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/file_store_observer.go/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/float.gombitsvbitsbitstream"github.com/dgryski/go-bitstream"0x7FF80000000000010x0One
This code is originally from: https://github.com/dgryski/go-tsz and has been modified to remove
the timestamp compression functionality.

It implements the float compression as presented in: http://www.vldb.org/pvldb/vol8/p1816-teller.pdf.
This implementation uses a sentinel value of NaN which means that float64 NaN cannot be stored using
this version.
 Note: an uncompressed format is not yet implemented. floatCompressedGorilla is a compressed format using the gorilla paper encoding uvnan is the constant returned from math.NaN(). FloatEncoder encodes multiple float64s into a byte slice. NewFloatEncoder returns a new FloatEncoder. Reset sets the encoder back to its initial state. Bytes returns a copy of the underlying byte buffer used in the encoder. Flush indicates there are no more values to encode. write an end-of-stream record Write encodes v to the underlying buffer. Only allow NaN as a sentinel value first point TODO(dgryski): check if it's 'cheaper' to reset the leading/trailing bits instead FloatDecoder decodes a byte slice into multiple float64 values. SetBytes initializes the decoder with b. Must call before calling Next(). first byte is the compression type. we currently just have gorilla compression. Reset all fields. Next returns true if there are remaining values to read. mark as finished if there were no values. IsNaN read compressed value it.val = it.val reuse leading/trailing zero bits it.leading, it.trailing = it.leading, it.trailing 0 significant bits here means we overflowed and we actually need 64; see comment in encoder Values returns the current float64 value. Error returns the current decoding error./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/int.gogithub.com/jwilder/encoding/simple8b"github.com/jwilder/encoding/simple8b"IntegerDecoder: not enough data to decode RLE starting value"IntegerDecoder: not enough data to decode RLE starting value"IntegerDecoder: invalid RLE delta value"IntegerDecoder: invalid RLE delta value"IntegerDecoder: invalid RLE repeat value"IntegerDecoder: invalid RLE repeat value"IntegerDecoder: not enough data to decode packed value"IntegerDecoder: not enough data to decode packed value"failed to decode value %v: %v"failed to decode value %v: %v"IntegerDecoder: not enough data to decode uncompressed value"IntegerDecoder: not enough data to decode uncompressed value" Integer encoding uses two different strategies depending on the range of values in the uncompressed data.  Encoded values are first encoding used zig zag encoding. This interleaves positive and negative integers across a range of positive integers. For example, [-2,-1,0,1] becomes [3,1,0,2]. See https://developers.google.com/protocol-buffers/docs/encoding?hl=en#signed-integers for more information. If all the zig zag encoded values are less than 1 << 60 - 1, they are compressed using simple8b encoding.  If any value is larger than 1 << 60 - 1, the values are stored uncompressed. Each encoded byte slice contains a 1 byte header followed by multiple 8 byte packed integers or 8 byte uncompressed integers.  The 4 high bits of the first byte indicate the encoding type for the remaining bytes. There are currently two encoding types that can be used with room for 16 total.  These additional encoding slots are reserved for future use.  One improvement to be made is to use a patched encoding such as PFOR if only a small number of values exceed the max compressed value range.  This should improve compression ratios with very large integers near the ends of the int64 range. intUncompressed is an uncompressed format using 8 bytes per point intCompressedSimple is a bit-packed format using simple8b encoding intCompressedRLE is a run-length encoding format IntegerEncoder encodes int64s into byte slices. NewIntegerEncoder returns a new integer encoder with an initial buffer of values sized at sz. Write encodes v to the underlying buffers. Delta-encode each value as it's written.  This happens before ZigZagEncoding because the deltas could be negative. Bytes returns a copy of the underlying buffer. Only run-length encode if it could reduce storage size. Value is too large to encode using packed format Encode all but the first value.  Fist value is written unencoded using 8 bytes. IntegerDecoder decodes a byte slice into int64s. 240 is the maximum number of values that can be encoded into a single uint64 using simple8b The first value for a run-length encoded byte slice The delta value for a run-length encoded byte slice SetBytes sets the underlying byte slice of the decoder. Next returns true if there are any values remaining to be decoded. Error returns the last error encountered by the decoder. Read returns the next value from the decoder. v is the delta encoded value, we need to add the prior value to get the original Store the first value and delta value so we do not need to allocate a large values slice.  We can compute the value at position d.i on demand. We've process all the bytes The first value is always unencoded Should never happen, only error that could be returned is if the the value to be decoded was not actually encoded by simple8b encoder./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/iterator.gen.gocvaluetvalueFloatIterator finalized by GC"FloatIterator finalized by GC"unexpected metrics"unexpected metrics"IntegerIterator finalized by GC"IntegerIterator finalized by GC"UnsignedIterator finalized by GC"UnsignedIterator finalized by GC"StringIterator finalized by GC"StringIterator finalized by GC"BooleanIterator finalized by GC"BooleanIterator finalized by GC" cursorAt provides a bufferred cursor interface. This required for literal value cursors which don't have a time value. bufCursor implements a bufferred cursor. newBufCursor returns a bufferred wrapper for cur. next returns the buffer, if filled. Otherwise returns the next key/value from the cursor. unread pushes k and v onto the buffer. peek reads next next key/value without removing them from the cursor. nextAt returns the next value where key is equal to seek. Skips over any keys that are less than seek. If the key doesn't exist then a nil value is returned instead. Return "nil" value for type. statsBufferCopyIntervalN is the number of points that are read before copying the stats buffer to the iterator's stats field. This is used to amortize the cost of using a mutex when updating stats. map used for condition evaluation reusable buffer Read from the main cursor if we have one. Otherwise find lowest aux timestamp. Exit if we have no more points or we are outside our time range. Read from each auxiliary cursor. Read from condition field cursors. Evaluate condition, if one exists. Retry if it fails. Track points returned. Copy buffer to stats periodically. copyStats copies from the itr stats buffer to the stats under lock. Stats returns stats on the points processed. floatLimitIterator Check if we are beyond the limit. Read the next point. Offsets are handled by a higher level iterator so return all points. floatCursor represents an object for iterating over a single float field. peekCache returns the current time/value from the cache. peekTSM returns the current time/value from tsm. next returns the next key/value for the cursor. nextFloat returns the next key/value for the cursor. No more data in cache or in TSM files. Both cache and tsm files have the same key, cache takes precedence. Buffered cache key precedes that in TSM file. Buffered TSM key precedes that in cache. nextCache returns the next value from the cache. nextTSM returns the next value from the TSM files. integerLimitIterator integerCursor represents an object for iterating over a single integer field. nextInteger returns the next key/value for the cursor. unsignedLimitIterator unsignedCursor represents an object for iterating over a single unsigned field. nextUnsigned returns the next key/value for the cursor. stringLimitIterator stringCursor represents an object for iterating over a single string field. nextString returns the next key/value for the cursor. booleanLimitIterator booleanCursor represents an object for iterating over a single boolean field. nextBoolean returns the next key/value for the cursor./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/iterator.gounsupported finalizer iterator type: %T"unsupported finalizer iterator type: %T"unsupported instrumented iterator type: %T"unsupported instrumented iterator type: %T" literalValueCursor represents a cursor that always returns a single value. It doesn't not have a time value so it can only be used with nextAt(). preallocate and cast to cursorAt to avoid allocations stringSliceCursor is a cursor that outputs a slice of string values. newMergeFinalizerIterator creates a new Merge iterator from the inputs. If the call to Merge succeeds, the resulting Iterator will be wrapped in a finalizer iterator. If Merge returns an error, the inputs will be closed. newFinalizerIterator creates a new iterator that installs a runtime finalizer to ensure close is eventually called if the iterator is garbage collected. This additional guard attempts to protect against clients of CreateIterator not correctly closing them and leaking cursors./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/mmap_unix.goadvicePROT_WRITEMAP_ANONMAP_PRIVATE4098MADV_WILLNEEDMADV_DONTNEEDMadvise +build !windows,!plan9 anonymous mapping madviseWillNeed gives the kernel the mmap madvise value MADV_WILLNEED, hinting that we plan on using the provided buffer in the near future. From: github.com/boltdb/bolt/bolt_unix.go/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/pools.go getBuf returns a buffer with length size from the buffer pool. putBuf returns a buffer to the pool./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/predicate.gopopTagunescapedTagunescapedValueunknown tag byte: %x"unknown tag byte: %x"invalid number of children for logical expression: %v"invalid number of children for logical expression: %v"invalid tag ref in comparison: %v"invalid tag ref in comparison: %v"invalid left literal in comparison: %v"invalid left literal in comparison: %v"invalid left node in comparison: %v"invalid left node in comparison: %v"invalid right literal in comparison: %v"invalid right literal in comparison: %v"invalid right node in comparison: %v"invalid right node in comparison: %v"invalid comparison involving regex: %v"invalid comparison involving regex: %v"invalid comparison not against regex: %v"invalid comparison not against regex: %v"unknown logical type: %v"unknown logical type: %v"unsupported predicate type: %v"unsupported predicate type: %v" Enumeration of all predicate versions we support unmarshalling. UnmarshalPredicate takes stored predicate bytes from a Marshal call and returns a Predicate. Design Predicates lazily evaluate with memoization so that we can walk a series key by the tags without parsing them into a structure and allocating. Each node in a predicate tree keeps a cache if it has enough information to have a definite value. The predicate state keeps track of all of the tag key/value pairs passed to it, and has a reset function to start over for a new series key. For example, imagine a query like	("tag1" == "val1" AND "tag2" == "val2") OR "tag3" == "val3" The state would have tag values set on it like	state.Set("tag1", "val1")        => NeedMore	state.Set("tag2", "not-val2")    => NeedMore	state.Set("tag3", "val3")        => True where after the first Set, the AND and OR clauses are both NeedMore, after the second Set, the AND clause is False and the OR clause is NeedMore, and after the last Set, the AND clause is still False, and the OR clause is True. Fast resetting is achieved by having each cache maintain a pointer to the state and both having a generation number. When the state resets, it bumps the generation number, and when the value is set in the cache, it is set with the current generation of the state. When querying the cache, it checks if the generation still matches. Protobuf Implementation NewProtobufPredicate returns a Predicate that matches based on the comparison structure described by the incoming protobuf. Walk the predicate to collect the tag refs Only add to the matcher locations the first time we encounter the tag key reference. This prevents problems with redundant predicates like:   foo = a AND foo = b   foo = c AND foo = d Construct the shared state and root predicate node. predicateMatcher implements Predicate for a protobuf. Clone returns a deep copy of p's state and root node. It is not safe to modify p.pred on the returned clone. Matches checks if the key matches the predicate by feeding individual tags into the state and returning as soon as the root node has a definite answer. Extract the series from the composite key Determine which popping algorithm to use. If there are no escape characters we can use the quicker method that only works in that case. Feed tag pairs into the state and update until we have a definite response. If it always needed more then it didn't match. For example, consider if the predicate matches `tag1=val1` but tag1 is not present in the key. Marshal returns a buffer representing the protobuf predicate. Prefix it with the version byte so that we can change in the future if necessary walkPredicateNodes recursively calls the function for each node. buildPredicateNode takes a protobuf node and converts it into a predicateNode. It is strict in what it accepts. Fill in the left side of the comparison Tag refs look up the location of the tag in the state Left literals are only allowed to be strings Fill in the right side of the comparison Right literals are allowed to be regexes as well as strings Ensure that a regex is set on the right if and only if the comparison is a regex Predicate Responses Predicate State predicateState keeps track of tag key=>value mappings with cheap methods to reset to a blank state. newPredicateState creates a predicateState given a map of keys to indexes into an an array. so that caches start out unfilled since they start at 0 Clone returns a deep copy of p. Reset clears any set values for the state. Set sets the key to be the value and returns true if the key is part of the considered set of keys. Predicate Cache predicateCache interacts with the predicateState to keep determined responses memoized until the state has been Reset to avoid recomputing nodes. newPredicateCache constructs a predicateCache for the provided state. Cached returns the cached response and a boolean indicating if it is valid. Store sets the cache to the provided response until the state is Reset. Predicate Nodes predicateNode is the interface that any parts of a predicate tree implement. Update informs the node that the state has been updated and asks it to return a response. Clone returns a deep copy of the node. predicateNodeAnd combines two predicate nodes with an And. Update checks if both of the left and right nodes are true. If either is false then the node is definitely false. Otherwise, it needs more information. predicateNodeOr combines two predicate nodes with an Or. Update checks if either the left and right nodes are true. If both nodes are false, then the node is definitely false. Otherwise, it needs more information. predicateNodeComparison compares values of tags. Update checks if both sides of the comparison are determined, and if so, evaluates the comparison to a determined truth value. predicateEval is a helper to do the appropriate comparison depending on which comparison enumeration value was passed. Popping Tags The models package has some of this logic as well, but doesn't export ways to get at individual tags one at a time. In the common, no escape characters case, popping the first tag off of a series key takes around ~10ns. predicatePopTag pops a tag=value pair from the front of series, returning the remainder in rest. it assumes there are no escaped characters in the series. find the first ',' find the first '=' predicatePopTagEscape pops a tag=value pair from the front of series, returning the remainder in rest. it assumes there are possibly/likely escaped characters in the series. find the first unescaped ',' this is the last tag pair make index relative to full series slice the comma is escaped find the first unescaped '=' there is no tag value the equals is escaped sad time: it's possible this tag/value has escaped characters, so we have to find an unescape them. since the byte slice may refer to read-only memory, we can't do this in place, so we make copies./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/reader.gen.go Source: reader.gen.go.tmpl ReadFloatBlockAt returns the float values corresponding to the given index entry. ReadFloatArrayBlockAt fills vals with the float values corresponding to the given index entry. ReadIntegerBlockAt returns the integer values corresponding to the given index entry. ReadIntegerArrayBlockAt fills vals with the integer values corresponding to the given index entry. ReadUnsignedBlockAt returns the unsigned values corresponding to the given index entry. ReadUnsignedArrayBlockAt fills vals with the unsigned values corresponding to the given index entry. ReadStringBlockAt returns the string values corresponding to the given index entry. ReadStringArrayBlockAt fills vals with the string values corresponding to the given index entry. ReadBooleanBlockAt returns the boolean values corresponding to the given index entry. ReadBooleanArrayBlockAt fills vals with the boolean values corresponding to the given index entry. blockAccessor abstracts a method of accessing blocks from a TSM file./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/reader.gowillNeeddErrprevTsmaxTsminTsnewTsfullKeysfirstOfsiMaxlastOfsindexOfsPosindexStarttempfile still in use"file still in use"delete during iteration"delete during iteration"init: read tombstones: %v"init: read tombstones: %v"error reading entries: %v"error reading entries: %v"SliceIsSortedkey does not exist: %s"key does not exist: %s"indirectIndex: not enough data for key length value"indirectIndex: not enough data for key length value"indirectIndex: not enough data for index entries count"indirectIndex: not enough data for index entries count"indirectIndex: not enough data for min time"indirectIndex: not enough data for min time"indirectIndex: not enough data for max time"indirectIndex: not enough data for max time"windows"windows"mmapAccessor: byte slice too small for indirectIndex"mmapAccessor: byte slice too small for indirectIndex"mmapAccessor: invalid indexStart"mmapAccessor: invalid indexStart"readEntries: data too short for headers"readEntries: data too short for headers"readEntries: unmarshal error: %v"readEntries: unmarshal error: %v" ErrFileInUse is returned when attempting to remove or close a TSM file that is still being used. nilOffset is the value written to the offsets to indicate that position is deleted.  The value is the max uint32 which is an invalid position.  We don't use 0 as 0 is actually a valid position. TSMReader is a reader for a TSM file. refs is the count of active references to this reader. Hint to the kernel with MADV_WILLNEED. accessor provides access and decoding of blocks for the reader. index is the index of all blocks. tombstoner ensures tombstoned keys are not available by the index. size is the size of the file on disk. lastModified is the last time this file was modified on disk deleteMu limits concurrent deletes TSMIndex represent the index section of a TSM file.  The index records all blocks, their locations, sizes, min and max times. Delete removes the given keys from the index. DeleteRange removes the given keys with data between minTime and maxTime from the index. ContainsKey returns true if the given key may exist in the index.  This func is faster than Contains but, may return false positives. Contains return true if the given key exists in the index. ContainsValue returns true if key and time might exist in this file.  This function could return true even though the actual point does not exists.  For example, the key may exist in this file, but not have a point exactly at time t. Entries returns all index entries for a key. ReadEntries reads the index entries for key into entries. Entry returns the index entry for the specified key and timestamp.  If no entry matches the key and timestamp, nil is returned. Key returns the key in the index at the given position, using entries to avoid allocations. KeyAt returns the key in the index at the given position. KeyCount returns the count of unique keys in the index. Seek returns the position in the index where key <= value in the index. Size returns the size of the current index in bytes. BlockFloat64, BlockInt64, BlockBool, BlockString.  If key does not exist, UnmarshalBinary populates an index from an encoded byte slice representation of an index. Close closes the index and releases any resources. BlockIterator allows iterating over each block in a TSM file in order.  It provides raw access to the block bytes without decoding them. i is the current key index n is the total number of keys PeekNext returns the next key to be iterated or an empty string. Next returns true if there are more blocks to iterate through. If there were deletes on the TSMReader, then our index is now off and we can't proceed.  What we just read may not actually the next block. Read reads information about the next block to be iterated. Err returns any errors encounter during iteration. WithMadviseWillNeed is an option for specifying whether to provide a MADV_WILL need hint to the kernel. NewTSMReader returns a new TSMReader from the given file. WithObserver sets the observer for the TSM reader. Copy the tombstone key and re-use the buffers to avoid allocations Path returns the path of the file the TSMReader was initialized with. Key returns the key and the underlying entry at the numeric index. KeyAt returns the key and key type at position idx in the index. ReadAt returns the values corresponding to the given index entry. Read returns the values corresponding to the block at the given key and timestamp. ReadAll returns all values for a key in all blocks. Type returns the type of values stored at the given key. Close closes the TSMReader. Ref records a usage of this TSMReader.  If there are active references when the reader is closed or removed, the reader will remain open until there are no more references. Unref removes a usage record of this TSMReader.  If the Reader was closed by another goroutine while there were active references, the file will be closed and remove InUse returns whether the TSMReader currently has any active references. Remove removes any underlying files stored on disk for this reader. Rename renames the underlying file to the new path. Contains returns whether the given key is present in the index. ContainsValue returns true if key and time might exists in this file.  This function could return true even though the actual point does not exist.  For example, the key may DeleteRange removes the given points for keys between minTime and maxTime.   The series keys passed in must be sorted. Delete deletes blocks indicated by keys. OverlapsKeyRange returns true if the key range of the file intersect min and max. KeyRange returns the min and max key across all keys in the file. KeyCount returns the count of unique keys in the TSMReader. Entries returns all index entries for key. IndexSize returns the size of the index in bytes. Size returns the size of the underlying file in bytes. LastModified returns the last time the underlying file was modified. HasTombstones return true if there are any tombstone entries recorded. TombstoneFiles returns any tombstone files associated with this TSM file. Stats returns the FileStat for the TSMReader's underlying file. BlockIterator returns a BlockIterator for the underlying TSM file. If the keys can't exist in this TSM file, skip it. If the timerange can't exist in this TSM file, skip it. BatchDelete returns a BatchDeleter.  Only a single goroutine may run a BatchDelete at a time. Callers must either Commit or Rollback the operation. indirectIndex is a TSMIndex that uses a raw byte slice representation of an index.  This implementation can be used for indexes that may be MMAPed into memory. indirectIndex works a follows.  Assuming we have an index structure in memory as the diagram below: ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ â                               Index                                â âââ¬âââââââââââââââââââââââ¬âââ¬ââââââââââââââââââââââââ¬ââââ¬âââââââââââââ â0â                      â62â                       â145â âââ´ââââââââ¬ââââââââââ¬âââââ¼âââ´âââââââ¬ââââââââââ¬âââââââ¼ââââ´ââââââ¬âââââââ âKey 1 Lenâ   Key   â... âKey 2 Lenâ  Key 2  â ...  â  Key 3  â ...  â â 2 bytes â N bytes â    â 2 bytes â N bytes â      â 2 bytes â      â âââââââââââ´ââââââââââ´âââââ´ââââââââââ´ââââââââââ´âââââââ´ââââââââââ´âââââââ We would build an `offsets` slices where each element pointers to the byte location for the first key in the index slice. â                              Offsets                               â ââââââ¬âââââ¬âââââ¬ââââââââââââââââââââââââââââââââââââââââââââââââââââââ â 0  â 62 â145 â ââââââ´âââââ´âââââ Using this offset slice we can find `Key 2` by doing a binary search over the offsets slice.  Instead of comparing the value in the offsets (e.g. `62`), we use that as an index into the underlying index to retrieve the key at position `62` and perform our comparisons with that. When we have identified the correct position in the index for a given key, we could perform another binary search or a linear scan.  This should be fast as well since each index entry is 28 bytes and all contiguous in memory.  The current implementation uses a linear scan since the number of block entries is expected to be < 100 per key. b is the underlying index byte slice.  This could be a copy on the heap or an MMAP slice reference offsets contains the positions in b for each key.  It points to the 2 byte length of minKey, maxKey are the minium and maximum (lexicographically sorted) contained in the file minTime, maxTime are the minimum and maximum times contained in the file across all series. tombstones contains only the tombstoned keys with subset of time values deleted.  An entry would exist here if a subset of the points for a key were deleted and the file had not be re-compacted to remove the points on disk. TimeRange holds a min and max timestamp. NewIndirectIndex returns a new indirect index. searchOffset searches the offsets slice for key and returns the position in offsets where key would exist. We use a binary search across our indirect offsets (pointers to all the keys in the index slice). i is the position in offsets we are at so get offset it points to It's pointing to the start of the key which is a 2 byte length See if it matches See if we might have found the right index The key is not in the index.  i is the index where it would be inserted so return a value outside our offset range. search returns the byte position of key in the index.  If key is not in the index, len(index) is returned. TODO(sgc): this should be inlined to `indirectIndex` as it is only used here The search may have returned an i == 0 which could indicated that the value searched should be inserted at position 0.  Make sure the key in the index matches the search value. ContainsKey returns true of key may exist in this index. Read and return all the entries ReadEntries returns all index entries for a key. The key is not in the index.  i is the index where it would be inserted. matches the key an timestamp, nil is returned. Key returns the key in the index at the given position. Both keys and offsets are sorted.  Walk both in order and skip any keys that exist in both. No keys, nothing to do If we're deleting the max time range, just use tombstoning to remove the key from the offsets slice Is the range passed in outside of the time range for the file? Skip any keys that don't exist.  These are less than the current key. No more keys to delete, we're done. If the current key is greater than the index one, continue to the next index key. If multiple tombstones are saved for the same key Is the time range passed outside of the time range we've have stored for this key? Does the range passed in cover every value for the key? Append the new tombonstes to the existing ones Sort the updated tombstones if necessary We need to see if all the tombstones end up deleting the entire series.  This could happen if their is one tombstore with min,max time spanning all the block time ranges or from multiple smaller tombstones the delete segments.  To detect this cases, we use a window starting at the first tombstone and grow it be each tombstone that is immediately adjacent to the current window or if it overlaps. If there are any gaps, we abort. Make sure all the tombstone line up for a continuous range.  We don't want to have two small deletes on each edges end up causing us to remove the full key. If we have a fully deleted series, delete it all of it. Delete all the keys that fully deleted in bulk ContainsValue returns true if key and time might exist in this file. Type returns the block type of the values stored for the key. KeyRange returns the min and max keys in the index. TimeRange returns the min and max time across all keys in the index. MarshalBinary returns a byte slice encoded version of the index. Keep a reference to the actual index bytesvar minKey, maxKey []byte To create our "indirect" index, we need to find the location of all the keys in the raw byte slice.  The keys are listed once each (in sorted order).  Following each key is a time ordered list of index entry blocks for that key.  The loop below basically skips across the slice keeping track of the counter when we are at a key field. Skip to the start of the values key length value (2) + type (1) + length of key count of index entries Find the min time for the block Find the max time for the block Windows doesn't use the anonymous map for the offsets index mmapAccess is mmap based block accessor.  It access blocks through an MMAP file interface. Counter incremented everytime the mmapAccessor is accessed Counter to determine whether the accessor can free its resources If true then mmap advise value MADV_WILLNEED will be provided the kernel for b. Hint to the kernel that we will be reading the file.  It would be better to hint that we will be reading the index section, but that's not been implemented as yet. Allow resources to be freed immediately if requested Already freed everything. Were there accesses after the last time we tried to free? If so, don't free anything and record the access count that we see now for the next check. Reset both counters to zero to indicate that we have freed everything.TODO: Validate checksum return the bytes after the 4 byte checksum readAll returns all values for a key in all blocks. Should we skip this block because it contains points that have been deleted The +4 is the 4 byte checksum length Filter out any values that were deleted 2 byte size of key N byte key 1 byte block type 2 byte count of index entries/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/ring.gonewStoreinvalid number of paritions: %d"invalid number of paritions: %d" partitions is the number of partitions we used in the ring's continuum. It basically defines the maximum number of partitions you can have in the ring. If a smaller number of partitions are chosen when creating a ring, then they're evenly spread across this many partitions in the ring. ring is a structure that maps series keys to entries. ring is implemented as a crude hash ring, in so much that you can have variable numbers of members in the ring, and the appropriate member for a given series key can always consistently be found. Unlike a true hash ring though, this ring is not resizeableâthere must be at most 256 members in the ring, and the number of members must always be a power of 2. ring works as follows: Each member of the ring contains a single store, which contains a map of series keys to entries. A ring always has 256 partitions, and a member takes up one or more of these partitions (depending on how many members are specified to be in the ring) To determine the partition that a series key should be added to, the series key is hashed and the first 8 bits are used as an index to the ring. Number of keys within the ring. This is used to provide a hint for allocating the return values in keys(). It will not be perfectly accurate since it doesn't consider adding duplicate keys, or trying to remove non- existent keys. The unique set of partitions in the ring. len(partitions) <= len(continuum) newring returns a new ring initialised with n partitions. n must always be a power of 2, and for performance reasons should be larger than the number of cores on the host. The supported set of values for n is:     {1, 2, 4, 8, 16, 32, 64, 128, 256}. maximum number of partitions. The trick here is to map N partitions to all points on the continuum, such that the first eight bits of a given hash will map directly to one of the N partitions. reset resets the ring so it can be reused. Before removing references to entries within each partition it gathers sizing information to provide hints when reallocating entries in partition maps. reset is not safe for use by multiple goroutines. getPartition retrieves the hash ring partition associated with the provided entry returns the entry for the given key. entry is safe for use by multiple goroutines. write writes values to the entry in the ring's partition associated with key. If no entry exists for the key then one will be created. write is safe for use by multiple goroutines. add adds an entry to the ring. remove deletes the entry for the given key. remove is safe for use by multiple goroutines. keys returns all the keys from all partitions in the hash ring. The returned keys will be in order if sorted is true. apply applies the provided function to every entry in the ring under a read lock using a separate goroutine for each partition. The provided function will be called with each key and the corresponding entry. The first error encountered will be returned, if any. apply is safe for use by multiple goroutines. Collect results. applySerial is similar to apply, but invokes f on each partition in the same goroutine. apply is safe for use by multiple goroutines. partition provides safe access to a map of series keys to entries. entry returns the partition's entry for the provided key. It's safe for use by multiple goroutines. write writes the values to the entry in the partition, creating the entry if it does not exist. Hot path. Check again. Create a new entry using a preallocated size if we have a hint available. add adds a new entry for key to the partition. remove deletes the entry associated with the provided key. keys returns an unsorted slice of the keys in the partition. reset resets the partition by reinitialising the store. reset returns hints about sizes that the entries within the store could be reallocated with./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/scheduler.golevel1Runninglevel2Runninglevel3Runninglevel4RunningloLimitweighthiLimit0.20.20000000000000001113602879701896397/18014398509481984 queues is the depth of work pending for each compaction level/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/string.goStringDecoder: invalid encoded string length"StringDecoder: invalid encoded string length"StringDecoder: length overflow"StringDecoder: length overflow"StringDecoder: not enough data to represent encoded string"StringDecoder: not enough data to represent encoded string" String encoding uses snappy compression to compress each string.  Each string is appended to byte slice prefixed with a variable byte length followed by the string bytes.  The bytes are compressed using snappy compressor and a 1 byte header is used to indicate the type of encoding. stringCompressedSnappy is a compressed encoding using Snappy compression StringEncoder encodes multiple strings into a byte slice. NewStringEncoder returns a new StringEncoder with an initial buffer ready to hold sz bytes. Write encodes s to the underlying buffer. Append the length of the string using variable byte encoding Append the string bytes Compress the currently appended bytes using snappy and prefix with a 1 byte header for future extension StringDecoder decodes a byte slice into strings. SetBytes initializes the decoder with bytes to read from. This must be called before calling any other method. Read the length of the string/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/timestamp.godtsunknown encoding: %v"unknown encoding: %v"TimeDecoder: not enough data to decode packed timestamps"TimeDecoder: not enough data to decode packed timestamps"TimeDecoder: not enough data for initial RLE timestamp"TimeDecoder: not enough data for initial RLE timestamp"TimeDecoder: invalid run length in decodeRLE"TimeDecoder: invalid run length in decodeRLE" Timestamp encoding is adaptive and based on structure of the timestamps that are encoded.  It uses a combination of delta encoding, scaling and compression using simple8b, run length encoding as well as falling back to no compression if needed. Timestamp values to be encoded should be sorted before encoding.  When encoded, the values are first delta-encoded.  The first value is the starting timestamp, subsequent values are the difference from the prior value. Timestamp resolution can also be in the nanosecond.  Many timestamps are monotonically increasing and fall on even boundaries of time such as every 10s.  When the timestamps have this structure, they are scaled by the largest common divisor that is also a factor of 10.  This has the effect of converting very large integer deltas into very small one that can be reversed by multiplying them by the scaling factor. Using these adjusted values, if all the deltas are the same, the time range is stored using run length encoding.  If run length encoding is not possible and all values are less than 1 << 60 - 1 (~36.5 yrs in nanosecond resolution), then the timestamps are encoded using simple8b encoding.  If any value exceeds the maximum values, the deltas are stored uncompressed using 8b each. Each compressed byte slice has a 1 byte header indicating the compression type.  The 4 high bits indicate the encoding type.  The 4 low bits are used by the encoding type. For run-length encoding, the 4 low bits store the log10 of the scaling factor.  The next 8 bytes are the starting timestamp, next 1-10 bytes is the delta value using variable-length encoding, finally the next 1-10 bytes is the count of values. For simple8b encoding, the 4 low bits store the log10 of the scaling factor.  The next 8 bytes is the first delta value stored uncompressed, the remaining bytes are 64bit words containing compressed delta For uncompressed encoding, the delta values are stored using 8 bytes each. timeUncompressed is a an uncompressed format using 8 bytes per timestamp timeCompressedPackedSimple is a bit-packed format using simple8b encoding timeCompressedRLE is a run-length encoding format TimeEncoder encodes time.Time to byte slices. NewTimeEncoder returns a TimeEncoder with an initial buffer ready to hold sz bytes. Write adds a timestamp to the compressed stream. Compute the deltas in place to avoid allocating another slice Starting values for a max and divisor Indicates whether the the deltas can be run-length encoded Iterate in reverse so we can apply deltas in place First differential encode the values We also need to keep track of the max value and largest common divisor Skip the first value || see if prev = curr.  The deltas can be RLE if the are all equal. Bytes returns the encoded bytes of all written times. Maximum and largest common divisor.  rle is true if dts (the delta timestamps), are all the same. The deltas are all the same, so we can run-length encode them The compressed deltas The first delta value Large varints can take up to 10 bytes, we're encoding 3 + 1 byte type The first timestamp TimeDecoder decodes a byte slice into timestamps. Init initializes the decoder with bytes to read from. Encoding type is stored in the 4 high bits of the first byte Next returns true if there are any timestamps remaining to be decoded. Read returns the next timestamp from the decoder. Next 1-10 bytes is our (scaled down by factor of 10) run length values Scale the value back up Uncompressed timestamps are just 8 bytes each First 9 bytes are the starting timestamp and scaling factor, skip over them +1 is for the first uncompressed timestamp, starting timestamp in b[1:9]/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/tombstone.goremoveTmptmpFilenamemaxBufminBuf"tombstone"0x150253790x150353800x1504incompatible v4 version"incompatible v4 version"SeekStartNewWriterSizeNewReaderSizeSeekCurrent Tombstoner records tombstones when entries are deleted. Path is the location of the file to record tombstone. This should be the full path to a TSM file. cache of the stats for this tombstone indicates that the stats may be out of sync with what is on disk and they should be refreshed. Tombstones that have been written but not flushed to disk yet. These are references used for pending writes that have not been committed.  If these are nil, then no pending writes are in progress. Optional observer for when tombstone files are written. NewTombstoner constructs a Tombstoner for the given path. FilterFn can be nil. Tombstone represents an individual deletion. Key is the tombstoned series key. Min and Max are the min and max unix nanosecond time ranges of Key that are deleted.  If the full range is deleted, both values are -1. WithObserver sets a FileStoreObserver for when the tombstone file is written. Add adds the all keys, across all timestamps, to the tombstone. AddRange adds all keys to the tombstone specifying only the data between min and max to be removed. If this TSMFile has not been written (mainly in tests), don't write a tombstone because the keys will not be written when it's actually saved. Reset our temp references and clean up. Delete removes all the tombstone files from disk. TombstoneFiles returns any tombstone files associated with Tombstoner's TSM file. The file doesn't exist so record that we tried to load it so we don't continue to keep trying.  This is the common case. Walk calls fn for every Tombstone under the Tombstoner. Might be a zero length file which should not exist, but an old bug allowed them to occur.  Treat it as an empty v1 tombstone file so we don't abort loading the TSM file. Copy the existing v4 file if it exists There is an existing tombstone on disk and it's not a v3.  Just rewrite it as a v3 version again. Seek back to the beginning we copy the header Copy the while file Write the header only if the file is new Write the tombstones No pending writes fsync the file to flush the write readTombstoneV1 reads the first version of tombstone files that were not capable of storing a min and max time for a key.  This is used for backwards compatibility with versions prior to 0.13.  This format is a simple newline separated text file. readTombstoneV2 reads the second version of tombstone files that are capable of storing keys and the range of time for the key that points were deleted. This format is binary. Skip header, already checked earlier readTombstoneV3 reads the third version of tombstone files that are capable format is a binary and compressed with gzip. Copy the key since b is re-used readTombstoneV4 reads the fourth version of tombstone files that are capable of storing multiple v3 files appended together. Save the position of tombstone file so we don't re-apply the same set again if there are more deletes. Filename is 0000001.tsm1 Strip off the tsm1 Append the "tombstone" suffix to create a 0000001.tombstone file/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/wal.golastSegmentsegtotalOldDiskSizetimerChrwtimerChcurrentFileencBufsegIDsyncErrencLencurTypeunnvalsentryTypedecBufdecLennReadOK10240104857604194304WAL closed"WAL closed"corrupted WAL entry"corrupted WAL entry"8388608oldSegmentsDiskBytes"oldSegmentsDiskBytes"currentSegmentDiskBytes"currentSegmentDiskBytes"tsm1_wal"tsm1_wal"tsm1 WAL starting"tsm1 WAL starting"segment_size"segment_size"tsm1 WAL writing"tsm1 WAL writing"SeekEndRemoving WAL file"Removing WAL file"error rolling WAL segment: %v"error rolling WAL segment: %v"error writing WAL entry: %v"error writing WAL entry: %v"error syncing wal"error syncing wal"error opening new segment file for wal (2): %v"error opening new segment file for wal (2): %v"error opening new segment file for wal (1): %v"error opening new segment file for wal (1): %v"Closing WAL file"Closing WAL file"%s*.%s"%s*.%s"%s%05d.%s"%s%05d.%s"incorrect value found in %T slice: %T"incorrect value found in %T slice: %T"unsupported value found in %T slice: %T"unsupported value found in %T slice: %T"unsupported value type: %#v"unsupported value type: %#v"unknown wal entry type: %v"unknown wal entry type: %v"file %s has wrong name format to have an id"file %s has wrong name format to have an id" DefaultSegmentSize of 10MB is the size at which segment files will be rolled over. WALFileExtension is the file extension we expect for wal segments. WALFilePrefix is the prefix on all wal segment files. walEncodeBufSize is the size of the wal entry encoding buffer WalEntryType is a byte written to a wal segment file that indicates what the following compressed block contains. WriteWALEntryType indicates a write entry. DeleteWALEntryType indicates a delete entry. DeleteRangeWALEntryType indicates a delete range entry. ErrWALClosed is returned when attempting to write to a closed WAL file. ErrWALCorrupt is returned when reading a corrupt WAL entry. bytePool is a shared bytes pool buffer re-cycle []byte slices to reduce allocations. Statistics gathered by the WAL. WAL represents the write-ahead log used for writing TSM files. goroutines waiting for the next fsync write variables cache and flush variables syncDelay sets the duration to wait before fsyncing writes.  A value of 0 (default) will cause every write to be fsync'd.  This must be set before the WAL is opened if a non-default value is required. WALOutput is the writer used by the logger. SegmentSize is the file size at which a segment file will be rotated statistics for the WAL NewWAL initializes a new WAL at the given directory. these options should be overridden by any options in the config enableTraceLogging must be called before the WAL is opened. WithLogger sets the WAL's logger. WALStatistics maintains statistics about the WAL. Path returns the directory the log was initialized with. Open opens and initializes the Log. Open can recover from previous unclosed shutdowns. Set the correct size on the segment writer scheduleSync will schedule an fsync to the current wal segment and notify any waiting gorutines.  If an fsync is already scheduled, subsequent calls will not schedule a new fsync and will be handle by the existing scheduled fsync. If we're not the first to sync, then another goroutine is fsyncing the wal for us. Fsync the wal and notify all pending waiters time.NewTicker requires a > 0 delay, since 0 indicates no delay, use a closed channel which will always be ready to read from. Create a RW chan and close it Convert it to a read-only sync fsyncs the current wal segments and notifies any waiters.  Callers must ensure a write lock on the WAL is obtained before calling sync. WriteMulti writes the given values to the WAL. It returns the WAL segment ID to which the points were written. If an error is returned the segment ID should be ignored. ClosedSegments returns a slice of the names of the closed segment files. Skip the current path Remove deletes the given segment file paths from disk and cleans up any associated objects. Refresh the on-disk size stats LastWriteTime is the last time anything was written to the WAL. limit how many concurrent encodings can be in flight.  Since we can only write one at a time to disk, a slow disk can cause the allocations below to increase quickly.  If we're backed up, wait until others have completed. Make sure the log has not been closed roll the segment file if needed write and sync Update stats for current segment size schedule an fsync and wait for it to complete rollSegment checks if the current segment is due to roll over to a new segment; and if so, opens a new segment file for future writes. A drop database or RP call could trigger this error if writes were in-flight when the drop statement executes. CloseSegment closes the current segment if it is non-empty and opens a new one. Delete deletes the given keys, returning the segment ID for the operation. DeleteRange deletes the given keys within the given time range, returning the segment ID for the operation. Close will finish any flush that is currently in progress and close file handles. Close, but don't set to nil so future goroutines can still be signaled segmentFileNames will return all files that are WAL segment files in sorted order by ascending ID. newSegmentFile will close the current segment file and open a new one, updating bookkeeping info on the log. Reset the current segment size stat WALEntry is record stored in each WAL segment.  Each entry has a type and an opaque, type dependent byte slice data attribute. WriteWALEntry represents a write of points. Type (1), Key Length (2), and Count (4) for each key determine required length timestamps (8) Encode converts the WriteWALEntry into a byte stream using dst if it is large enough.  If dst is too small, the slice will be grown to fit the encoded entry. The entries values are encode as follows: For each key and slice of values, first a 1 byte type for the []Values slice is written.  Following the type, the length and key bytes are written. Following the key, a 4 byte count followed by each value as a 8 byte time and N byte value.  The value is dependent on the type being encoded.  float64, int64, use 8 bytes, boolean uses 1 byte, and string is similar to the key encoding, except that string values have a 4-byte length, and keys only use 2 bytes. This structure is then repeated for each key an value slices. â                           WriteWALEntry                            â ââââââââ¬ââââââââââ¬âââââââââ¬ââââââââ¬ââââââââââ¬ââââââââââ¬ââââ¬âââââââ¬ââââ¤ â Type â Key Len â   Key  â Count â  Time   â  Value  â...â Type â...â â1 byteâ 2 bytes â N bytesâ4 bytesâ 8 bytes â N bytes â   â1 byteâ   â ââââââââ´ââââââââââ´âââââââââ´ââââââââ´ââââââââââ´ââââââââââ´ââââ´âââââââ´ââââ allocate or re-slice to correct size Finally, encode the entry MarshalBinary returns a binary representation of the entry in a new byte slice. Temp buffer to write marshaled points into UnmarshalBinary deserializes the byte slice into w. Type returns WriteWALEntryType. DeleteWALEntry represents the deletion of multiple series. b originates from a pool. Copy what needs to be retained. newlines Encode converts the DeleteWALEntry into a byte slice, appending to dst. We return n-1 to strip off the last newline so that unmarshalling the value does not produce an empty string Type returns DeleteWALEntryType. DeleteRangeWALEntry represents the deletion of multiple series. Encode converts the DeleteRangeWALEntry into a byte slice, appending to b. Type returns DeleteRangeWALEntryType. WALSegmentWriter writes WAL segments. NewWALSegmentWriter returns a new WALSegmentWriter writing to w. Write writes entryType and the buffer containing compressed entry data. Sync flushes the file systems in-memory copy of recently written data to disk, if w is writing to an os.File. WALSegmentReader reads WAL segments. NewWALSegmentReader returns a new WALSegmentReader reading from r. Next indicates if there is a value to read. read the type and the length of the entry We return true here because we want the client code to call read which will return the this error to be handled. read the compressed block and decompress it and marshal it and send it to the cache Read and decode of this entry was successful. Read returns the next entry in the reader. Count returns the total number of bytes read successfully from the segment, as of the last call to Read(). The segment is guaranteed to be valid up to and including this number of bytes. Error returns the last error encountered by the reader. Close closes the underlying io.Reader. idFromFileName parses the segment file ID from its name./Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine/tsm1/writer.goewnwn64fwindexPosnameCloser3828016170x16D116D1no values written"no values written"tsm file closed"tsm file closed"max key length exceeded"max key length exceeded"max blocks exceeded"max blocks exceeded"unmarshalBinary: short buf: %v < %v"unmarshalBinary: short buf: %v < %v"min=%s max=%s ofs=%d siz=%d"min=%s max=%s ofs=%d siz=%d"keys must be added in sorted order: %s < %s"keys must be added in sorted order: %s < %s"ErrShortWritekey '%s' exceeds max index entries: %d > %d"key '%s' exceeds max index entries: %d > %d"write: writer key length error: %v"write: writer key length error: %v"write: writer key error: %v"write: writer key error: %v"write: writer block type and count error: %v"write: writer block type and count error: %v"write: writer entries error: %v"write: writer entries error: %v".tsm.tmp".tsm.tmp".idx.tmp".idx.tmp"init: failed to seek: %v"init: failed to seek: %v"init: error reading magic number of file: %v"init: error reading magic number of file: %v"can only read from tsm file"can only read from tsm file"init: error reading version: %v"init: error reading version: %v"init: file is version %b. expected %b"init: file is version %b. expected %b"
A TSM file is composed for four sections: header, blocks, index and the footer.

ââââââââââ¬âââââââââââââââââââââââââââââââââââââ¬ââââââââââââââ¬âââââââââââââââ
â Header â               Blocks               â    Index    â    Footer    â
â5 bytes â              N bytes               â   N bytes   â   4 bytes    â
ââââââââââ´âââââââââââââââââââââââââââââââââââââ´ââââââââââââââ´âââââââââââââââ

Header is composed of a magic number to identify the file type and a version
number.

âââââââââââââââââââââ
â      Header       â
âââââââââââ¬ââââââââââ¤
â  Magic  â Version â
â 4 bytes â 1 byte  â
âââââââââââ´ââââââââââ

Blocks are sequences of pairs of CRC32 and data.  The block data is opaque to the
file.  The CRC32 is used for block level error detection.  The length of the blocks
is stored in the index.

âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
â                          Blocks                           â
âââââââââââââââââââââ¬ââââââââââââââââââââ¬ââââââââââââââââââââ¤
â      Block 1      â      Block 2      â      Block N      â
âââââââââââ¬ââââââââââ¼ââââââââââ¬ââââââââââ¼ââââââââââ¬ââââââââââ¤
â  CRC    â  Data   â  CRC    â  Data   â  CRC    â  Data   â
â 4 bytes â N bytes â 4 bytes â N bytes â 4 bytes â N bytes â
âââââââââââ´ââââââââââ´ââââââââââ´ââââââââââ´ââââââââââ´ââââââââââ

Following the blocks is the index for the blocks in the file.  The index is
composed of a sequence of index entries ordered lexicographically by key and
then by time.  Each index entry starts with a key length and key followed by a
count of the number of blocks in the file.  Each block entry is composed of
the min and max time for the block, the offset into the file where the block
is located and the the size of the block.

The index structure can provide efficient access to all blocks as well as the
ability to determine the cost associated with accessing a given key.  Given a key
and timestamp, we can determine whether a file contains the block for that
timestamp as well as where that block resides and how much data to read to
retrieve the block.  If we know we need to read all or multiple blocks in a
file, we can use the size to determine how much to read in a given IO.

ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
â                                   Index                                    â
âââââââââââ¬ââââââââââ¬âââââââ¬ââââââââ¬ââââââââââ¬ââââââââââ¬âââââââââ¬âââââââââ¬ââââ¤
â Key Len â   Key   â Type â Count âMin Time âMax Time â Offset â  Size  â...â
â 2 bytes â N bytes â1 byteâ2 bytesâ 8 bytes â 8 bytes â8 bytes â4 bytes â   â
âââââââââââ´ââââââââââ´âââââââ´ââââââââ´ââââââââââ´ââââââââââ´âââââââââ´âââââââââ´ââââ

The last section is the footer that stores the offset of the start of the index.

âââââââââââ
â Footer  â
âââââââââââ¤
âIndex Ofsâ
â 8 bytes â
âââââââââââ
 MagicNumber is written as the first 4 bytes of a data file to identify the file as a tsm1 formatted file Version indicates the version of the TSM file format. Size in bytes of an index entry Size in bytes used to store the count of index entries for a key Size in bytes used to store the type of block encoded Max number of blocks for a given key that can exist in a single file max length of a key in an index entry (measurement + tags) The threshold amount data written before we periodically fsync a TSM file.  This helps avoid long pauses due to very large fsyncs at the end of writing a TSM file.ErrNoValues is returned when TSMWriter.WriteIndex is called and there are no values to write. ErrTSMClosed is returned when performing an operation against a closed TSM file. ErrMaxKeyLengthExceeded is returned when attempting to write a key that is too long. ErrMaxBlocksExceeded is returned when attempting to write a block past the allowed number. TSMWriter writes TSM formatted key and values. Write writes a new block for key containing and values.  Writes append blocks in the order that the Write function is called.  The caller is responsible for ensuring keys and blocks are sorted appropriately. Values are encoded as a full block.  The caller is responsible for ensuring a fixed number of values are encoded in each block as well as ensuring the Values are sorted. The first and last timestamp values are used as the minimum and maximum values for the index entry. WriteBlock writes a new block for key containing the bytes in block.  WriteBlock appends blocks in the order that the WriteBlock function is called.  The caller is responsible for ensuring keys and blocks are sorted appropriately, and that the block and index information is correct for the block.  The minTime and maxTime timestamp values are used as the minimum and maximum values for the index entry. WriteIndex finishes the TSM write streams and writes the index. Flushes flushes all pending changes to the underlying file resources. Close closes any underlying file resources. Size returns the current size in bytes of the file. IndexWriter writes a TSMIndex. Add records a new block entry for a key in the index. Size returns the size of a the current index in bytes. WriteTo writes the index contents to a writer. IndexEntry is the index information for a given block in a TSM file. The min and max time of all points stored in the block. The absolute position in the file where this block is located. The size in bytes of the block in the file. UnmarshalBinary decodes an IndexEntry from a byte slice. AppendTo writes a binary-encoded version of IndexEntry to b, allocating and returning a new slice, if necessary. Contains returns true if this IndexEntry may contain values for the given time. The min and max times are inclusive. OverlapsTimeRange returns true if the given time ranges are completely within the entry's time bounds. String returns a string representation of the entry. NewIndexWriter returns a new IndexWriter. directIndex is a simple in-memory index implementation for a TSM file.  The full index must fit in memory. The bytes written count of when we last fsync'd Is this the first block being added? size of the key stored in the index size of the count of entries stored in the index size of the encoded index entry See if were still adding to the same series key. The last block is still this key We have a new key that is greater than the last one so we need to add a new index block section. Keys can't be added out of order. copyBuffer is the actual implementation of Copy and CopyBuffer. if buf is nil, one is allocated.  This is copied from the Go stdlib in order to remove the fast path WriteTo calls which circumvent any IO throttling as well as to add periodic fsyncs to avoid long stalls. For each key, individual entries are sorted by time Append the key length and key Append the block type and count Append each index entry for all blocks for this key If this is a disk based index and we've written more than the fsync threshold, fsync the data to avoid long pauses later on. Flush anything remaining in the index Remove removes the index from any temporary storage Close the file handle to prevent leaking.  We ignore the error because we just want to cleanup and remove the file. tsmWriter writes keys and values in the TSM format NewTSMWriter returns a new TSMWriter writing to w. NewTSMWriterWithDiskBuffer returns a new TSMWriter writing to w and will use a disk based buffer for the TSM index if possible. Make sure is a File so we can write the temp index alongside it. w is not a file, just use an inmem index Write writes a new block containing key and values. Nothing to write Write header only after we have some data to write. Record this block in index Increment file position pointer WriteBlock writes block for the given key and time range to the TSM file.  If the write exceeds max entries for a given key, ErrMaxBlocksExceeded is returned.  This indicates that the index is now full for this key and no future writes to this key will succeed. Increment file position pointer (checksum + block len) fsync the file periodically to avoid long pauses with very big files. WriteIndex writes the index section of the file.  If there are no index entries to write, this returns ErrNoValues. Set the destination file on the index so we can periodically fsync while writing the index. Write the index Write the index index position sync is a minimal interface to make sure we can sync the wrapped value. we use a minimal interface to be as robust as possible for syncing these files. Remove removes any temporary storage used by the writer. nameCloser is the most permissive interface we can close the wrapped value with. verifyVersion verifies that the reader's bytes are a TSM byte stream of the correct version (1)/Users/austinjaybecker/projects/abeck-go-testing/tsdb/engine.goformat not found"format not found"unknown engine format"unknown engine format"engine already registered: "engine already registered: "invalid engine format: %q"invalid engine format: %q" ErrFormatNotFound is returned when no format can be determined from a path. ErrUnknownEngineFormat is returned when the engine format is unknown. ErrUnknownEngineFormat is currently returned if a format other than tsm1 is encountered. Engine represents a swappable storage engine for the shard. Statistics will return statistics relevant to this engine. SeriesIDSets provides access to the total set of series IDs EngineFormat represents the format for an engine. TSM1Format is the format used by the tsm1 engine. NewEngineFunc creates a new engine. newEngineFuncs is a lookup of engine constructors by name. RegisterEngine registers a storage engine initializer by name. RegisteredEngines returns the slice of currently registered engines. NewEngine returns an instance of an engine based on its format. If the path does not exist then the DefaultFormat is used. Create a new engine If it's a dir then it's a tsm1 engine Lookup engine by format. EngineOptions represents the options used to initialize the engine. shared in-memory index Limits the concurrent number of TSM files that can be loaded at once. CompactionDisabled specifies shards should not schedule compactions. This option is intended for offline tooling. DatabaseFilter is a predicate controlling which databases may be opened. If no function is set, all databases will be opened. RetentionPolicyFilter is a predicate controlling which combination of database and retention policy may be opened. nil will allow all combinations to pass. ShardFilter is a predicate controlling which combination of database, retention policy and shard group may be opened. NewEngineOptions constructs an EngineOptions object with safe default values. This should only be used in tests; production environments should read from a config file. NewInmemIndex returns a new "inmem" index type. FileStoreObserver is passed notifications before the file store adds or deletes files. In this way, it can be sure to observe every file that is added or removed even in the presence of process death. FileFinishing is called before a file is renamed to it's final name. FileUnlinking is called before a file is unlinked./Users/austinjaybecker/projects/abeck-go-testing/tsdb/epoch_tracker.goguardsdgenNewCond TODO(jeff): using a mutex is easiest, but there may be a way to do this with atomics only, and in a way such that writes are minimally blocked. epochTracker keeps track of epochs for write and delete operations allowing a delete to block until all previous writes have completed. current epoch largest delete possible pending writes pending deletes waiting on writes newEpochTracker constructs an epochTracker. epochDeleteState keeps track of the state for a pending delete. done signals that an earlier write has finished. Wait blocks until all earlier writes have finished. next bumps the epoch and returns it. StartWrite should be called before a write is going to start, and after it has checked for guards. EndWrite should be called when the write ends for any reason. TODO(jeff): at the cost of making waitDelete more complicated, we can keep a sorted slice which would allow this to exit early rather than go over the whole map. epochWaiter is a type that can be waited on for prior writes to finish. Wait blocks until all writes prior to the creation of the waiter finish. Done marks the delete as completed, removing its guard. WaitDelete should be called after any delete guards have been installed. The returned epochWaiter will not be affected by any future writes. record our pending delete/Users/austinjaybecker/projects/abeck-go-testing/tsdb/field_validator.go%s: input field "%s" on measurement "%s" is type %s, already exists as type %s"%s: input field \"%s\" on measurement \"%s\" is type %s, already exists as type %s" FieldValidator should return a PartialWriteError if the point should not be written. defaultFieldValidator ensures that points do not use different types for fields that already exist. Validate will return a PartialWriteError if the point has inconsistent fields. Skip fields name "time", they are illegal. If the fields is not present, there cannot be a conflict. If the types are not the same, there is a conflict. dataTypeFromModelsFieldType returns the influxql.DataType that corresponds to the passed in field type. If there is no good match, it returns Unknown./Users/austinjaybecker/projects/abeck-go-testing/tsdb/guard.go guard lets one match a set of points and block until they are done. newGuard constructs a guard that will match any points in the given min and max time range, with the given set of measurement names, or the given expression. The expression is optional. Matches returns true if any of the points match the guard. Wait blocks until the guard has been marked Done. Done signals to anyone waiting on the guard that they can proceed. exprGuard is a union of influxql.Expr based guards. a nil exprGuard matches everything, while the zero value matches nothing. empty returns true if the exprGuard is empty, meaning that it matches no points. newExprGuard scrutinizes the expression and returns an efficient guard. matches everything matches nothing reduce short circuit if we couldn't analyze, match everything newBinaryExprGuard scrutinizes the binary expression and returns an efficient guard. if it's a nested binary expression, always match. ensure one of the expressions is a VarRef, and make that the key. check the key for situations we know we can't filter. scrutinize the value to return an efficient guard. any other operator isn't valid. conservatively match everything. There's a tradeoff between being precise and being fast. For example, if the delete includes a very expensive regex, we don't want to run that against every incoming point. The decision here is to match any point that has a possibly expensive match if there is any overlap on the tags. In other words, expensive matches get transformed into trivially matching everything. We could do a better job here by encoding the two names and checking the points against them, but I'm not quite sure how to do that. Be conservative and match any points that contain either the key or value. since every point has a measurement, always match if either are on the measurement. any other value type matches everything matches checks if the exprGuard matches the point./Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/index.go import "github.com/influxdata/influxdb/v2/tsdb/index"/Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/inmem/Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/inmem/inmem.goNewShardIndexerrMaxSeriesPerDatabaseExceededevictSeriesIDsintersectSeriesFiltersnewEvictSeriesIDsnewMeasurementnewSeriesnewStringSetnewTagKeyValuenewTagKeyValueEntryseriesIDseriesIDIteratorunionSeriesFiltersskeyhasNewSeriesignoreLimitsnewSeriesNseriesListsIDsauthorizedkeyIdxskeysSortedtagValstagMatchvalEqualshardSeriesIDsmaxValuesPerTagdroppedKeysnextKeysgithub.com/influxdata/influxdb/v2/pkg/estimator/hll"github.com/influxdata/influxdb/v2/pkg/estimator/hll"left side of '%s' must be a tag key"left side of '%s' must be a tag key"IsRegexOpright side of '%s' must be a regular expression"right side of '%s' must be a regular expression"right side of '%s' must be a tag value string"right side of '%s' must be a tag value string"invalid tag comparison operator"invalid tag comparison operator"max-values-per-tag limit exceeded (%d/%d): measurement=%q tag=%q value=%q"max-values-per-tag limit exceeded (%d/%d): measurement=%q tag=%q value=%q"max-series-per-database limit exceeded: (%d)"max-series-per-database limit exceeded: (%d)"
Package inmem implements a shared, in-memory index for each database.

The in-memory index is the original index implementation and provides fast
access to index data. However, it also forces high memory usage for large
datasets and can cause OOM errors.

Index is the shared index structure that provides most of the functionality.
However, ShardIndex is a light per-shard wrapper that adapts this original
shared index format to the new per-shard format.
 IndexName is the name of this index. Index is the in memory index of a collection of measurements, time series, and their tags.Â Exported functions are goroutine safe while un-exported functions assume the caller will use the appropriate locks. In-memory metadata index, built on load and updated when new series come in measurement name to object and index map series key to the Series object Mutex to control rebuilds of the index NewIndex returns a new initialized Index. Bytes estimates the memory footprint of this Index, in bytes. mu RWMutex is 24 bytes Do not count SeriesFile because it belongs to the code that constructed this Index. rebuildQueue Mutex is 8 bytes Database returns the name of the database the index was initialized with. Series returns a series by key. SeriesSketches returns the sketches for the series. Measurement returns the measurement object from the index by the name MeasurementExists returns true if the measurement exists. MeasurementsSketches returns the sketches for the measurements. MeasurementsByName returns a list of measurements. MeasurementIterator returns an iterator over all measurements in the index. MeasurementIterator does not support authorization. CreateSeriesListIfNotExists adds the series for the given measurement to the index and sets its ID or returns the existing series object Verify that the series will not exceed limit. If there is a series for this ID, it's already been added. This series might need to be added to the local bitset, if the series was created on another shard. get or create the measurement index Check for the series again under a write lock Note, keys may contain duplicates (e.g., because of points for the same series in the same batch). If the duplicate series are new, the index must be rechecked on each iteration. set the in memory ID for query processing on this shard The series key and tags are clone to prevent a memory leak Add the series to the series sketch. This series needs to be added to the bitset tracking undeleted series IDs. CreateMeasurementIndexIfNotExists creates or retrieves an in memory index object for the measurement See if the measurement exists using a read-lock Doesn't exist, so lock the index to create it Make sure it was created in between the time we released our read-lock and acquire the write lock Add the measurement to the measurements sketch. HasTagKey returns true if tag key exists. HasTagValue returns true if tag value exists. TagValueN returns the cardinality of a tag value. MeasurementTagKeysByExpr returns an ordered set of tag keys filtered by an expression. TagKeyHasAuthorizedSeries determines if there exists an authorized series for the provided measurement name and tag key. TODO(edd): This looks like it's inefficient. Since a series can have multiple tag key/value pairs on it, it's possible that the same unauthorised series will be checked multiple times. It would be more efficient if it were possible to get the set of unique series IDs for a given measurement name and tag key. This tag key/value combination doesn't have any authorised series, so keep checking other tag values. MeasurementTagKeyValuesByExpr returns a set of tag values filtered by an expression. See tsm1.Engine.MeasurementTagKeyValuesByExpr for a fuller description of this method. If we haven't been provided sorted keys, then we need to sort them. This is the case where we have filtered series by some WHERE condition. We only care about the tag values for the keys given the filtered set of series ids. Iterate all series to collect tag values. Iterate the tag keys we're interested in and collect values from this series, if they exist. The tag key is > the largest key we're interested in. ForEachMeasurementTagKey iterates over all tag keys for a measurement. Ensure we do not hold a lock on the index while fn executes in case fn tries to acquire a lock on the index again.  If another goroutine has Lock, this will deadlock. TagKeyCardinality returns the number of values for a measurement/tag key. TagsForSeries returns the tag map for the passed in series MeasurementNamesByExpr takes an expression containing only tags and returns a list of matching measurement names. TODO(edd): Remove authorisation from these methods. There shouldn't need to be any auth passed down into the index. Return all measurement names if no expression is provided. Match on name, if specified. measurementNamesByNameFilter returns the sorted measurements matching a name. measurementNamesByTagFilters returns the sorted measurements matching the filters on tag values. Build a list of measurements matching the filters. Iterate through all measurements in the database. Authorization must be explicitly granted when an authorizer is present. Check the tag values belonging to the tag key for equivalence to the tag value being filtered on. No match. Keep checking. No need to continue checking series, there is a match. Is there a series with this matching tag value that is authorized to be read? If the series is deleted then it can't be used to authorise against. The Range call can return early as a matching tag value with an authorized series has been found. The matching tag value doesn't have any authorized series. Check for other matching tag values if this is a regex check. For negation operators, to determine if the measurement is authorized, an authorized series belonging to the measurement must be located. Then, the measurement can be added iff !tagMatch && authorized. tags match | operation is EQ | measurement matches --------------------------------------------------     True   |       True      |      True     True   |       False     |      False     False  |       True      |      False     False  |       False     |      True MeasurementNamesByRegex returns the measurements that match the regex. DropMeasurement removes the measurement and all of its underlying series from the database index Update the tombstone sketch. DropMeasurementIfSeriesNotExist drops a measurement only if there are no more series for the measurement. DropSeriesGlobal removes the series key and its tags from the index. Remove from the index. Remove the measurement's reference. Mark the series as deleted. If the measurement no longer has any series, remove it as well. TagSets returns a list of tag sets. SetFieldSet sets a shared field set from the engine. FieldSet returns the assigned fieldset. SetFieldName adds a field name to a measurement. ForEachMeasurementName iterates over each measurement name. TagValueIterator provides an iterator over all the tag values belonging to series with the provided measurement name and tag key. TagValueIterator does not currently support authorization. Return all series if no condition specified. Get series IDs that match the WHERE clause. Delete boolean literal true filter expressions. These are returned for `WHERE tagKey = 'tagVal'` type expressions and are okay. Check for unsupported field filters. Any remaining filters means there were fields (e.g., `WHERE value = 1.2`). SeriesIDIterator returns an influxql iterator over matching series ids. Read and sort all measurements. DiskSizeBytes always returns zero bytes, since this is an in-memory index. Rebuild recreates the measurement indexes to allow deleted series to be removed and garbage collected. Only allow one rebuild at a time.  This will cause all subsequent rebuilds to queue.  The measurement rebuild is idempotent and will not be rebuilt if it does not need to be. Measurement never returns an error assignExistingSeries assigns the existing series to shardID and returns the series, names and tags that do not exists yet. Add the existing series to this shard's bitset, since this may be the first time the series is added to this shard. Ensure index implements interface. ShardIndex represents a shim between the TSDB index interface and the shared in-memory index. This is required because per-shard in-memory indexes will grow the heap size too large. shard id Shared reference to global database-wide index. Bitset storing all undeleted series IDs associated with this shard. mapping of measurements to the count of series ids in the set. protected by the seriesIDSet lock. DropSeries removes the provided series id from the local bitset that tracks series in this shard only. Remove from shard-local bitset if it exists. we always report the measurement was dropped if it does not exist in our measurements mapping. CreateSeriesListIfNotExists creates a list of series if they doesn't exist in bulk. Ensure that no tags go over the maximum cardinality. Skip if the tag value already exists. Read cardinality. Skip if we're below the threshold. Increment success count if all checks complete. Slice to only include successful points. Report partial writes back to shard. number dropped before deduping SeriesN returns the number of unique non-tombstoned series local to this shard. InitializeSeries is called during start-up. This works the same as CreateSeriesListIfNotExists except it ignore limit errors. CreateSeriesIfNotExists creates the provided series on the index if it is not already present. TagSets returns a list of tag sets based on series filtering. SeriesIDSet returns the bitset associated with the series ids. NewShardIndex returns a new index for a shard. seriesIDIterator emits series ids. Stats returns stats about the points processed. Next emits the next point in the iterator. Load next measurement's keys if there are no more remaining. Read the next key. nextKeys reads all keys for the next measurement. Ensure previous keys are cleared out. Read next measurement. Read all series keys. Sort series by key errMaxSeriesPerDatabaseExceeded is a marker error returned during series creation to indicate that a new series would exceed the limits of the database./Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/inmem/meta.govalueMaptagsAsKeyseriesNsortedTagsSetslfilterrfilterlfilterslidsrfiltersridsrhsTagValslongershorterinvalid operator"invalid operator" Measurement represents a collection of time series in a database. It also contains in memory structures for indexing tags. Exported functions are goroutine safe while un-exported functions assume the caller will use the appropriate locks. cached version as []byte in-memory index fields lookup table for series by their id map from tag key to value to sorted set of series ids lazily created sorted series IDs sorted list of series IDs in this measurement Indicates whether the seriesByTagKeyValueMap needs to be rebuilt as it contains deleted series that waste memory. newMeasurement allocates and initializes a new Measurement. bytes estimates the memory footprint of this measurement, in bytes. 24 bytes for m.mu RWMutex Do not count footprint of each series, to avoid double-counting in Index.bytes(). Authorized determines if this Measurement is authorized to be read, according to the provided Authorizer. A measurement is authorized to be read if at least one undeleted series from the measurement is authorized to be read. Note(edd): the cost of this check scales linearly with the number of series belonging to a measurement, which means it may become expensive when there are large numbers of series on a measurement. In the future we might want to push the set of series down into the authorizer, but that will require an API change. SeriesByID returns a series by identifier. SeriesByIDMap returns the internal seriesByID map. SeriesByIDSlice returns a list of series by identifiers. AppendSeriesKeysByID appends keys for a list of series ids to a buffer. SeriesKeysByID returns the a list of keys for a set of ids. SeriesKeys returns the keys of every series in this measurement HasTagKey returns true if at least one series in this measurement has written a value for the passed in tag key HasSeries returns true if there is at least 1 series under this measurement. CardinalityBytes returns the number of values associated with the given tag key. AddSeries adds a series to the measurement's index. It returns true if the series was added successfully or false if the series was already present. add this series id to the tag index on the measurement DropSeries removes a series from the measurement's index. Existence check before delete here to clean up the caching/indexing only when needed clear our lazily sorted set of ids Mark that this measurements tagValue map has stale entries that need to be rebuilt. Nothing needs to be rebuilt. Create a new measurement from the state of the existing measurement Re-add each series to allow the measurement indexes to get re-created.  If there were deletes, the existing measurement may have references to deleted series that need to be expunged.  Note: we're NOT using SeriesIDs which returns the series in sorted order because we need to do this under a write lock to prevent races.  The series are added in sorted order to prevent resorting them again after they are all re-added. Explicitly set the new measurement on the series. filters walks the where clause of a select statement and returns a map with all series ids matching the where clause and any filter expression that should be applied to each TagSets returns the unique tag sets that exist for the given tag keys. This is used to determine what composite series will be created by a group by. i.e. "group by region" should return: {"region":"uswest"}, {"region":"useast"} or region, service returns {"region": "uswest", "service": "redis"}, {"region": "uswest", "service": "mysql"}, etc... This will also populate the TagSet objects with the series IDs that match each tagset and any influx filter expression that goes with the series TODO: this shouldn't be exported. However, until tx.go and the engine get refactored into tsdb, we need it. get the unique set of series ids and the filters that should be applied to each For every series, get the tag values for the requested tag keys i.e. dimensions. This is the TagSet for that series. Series with the same TagSet are then grouped together, because for the purpose of GROUP BY they are part of the same composite series. This TagSet is new, create a new entry for it. Associate the series and filter with the Tagset. Release the lock while we sort all the tags Sort the series in each tag set. The TagSets have been created, as a map of TagSets. Just send the values back as a slice, sorting for consistency. intersectSeriesFilters performs an intersection for two sets of ids and filter expressions. We only want to allocate a slice and map of the smaller size. They're in sorted order so advance the counter as needed. This is, don't run comparisons against lower values that we've already passed. unionSeriesFilters performs a union for two sets of ids and filter expressions. Setup the filters with the smallest size since we will discard filters that do not have a match on the other side. If one side does not have a filter, then the series has been included on one side of the OR with no condition. Eliminate the filter in this case. Now append the remainder. SeriesIDsByTagKey returns a list of all series for a tag key. SeriesIDsByTagValue returns a list of all series for a tag value. IDsForExpr returns the series IDs that are candidates to match the given expression. idsForExpr returns a collection of series ids and a filter expression that should be used to filter points from those series. If this binary expression has another binary expression, then this is some expression math and we should just pass it to the underlying query. Retrieve the variable reference from the correct side of the expression. This is an expression we do not know how to evaluate. Let the query engine take care of this. For fields, return all series IDs from this measurement and return the expression passed in, as the filter. Check if the RHS is a variable and if it is a field. Retrieve list of series with this tag key. if we're looking for series with a specific tag value Special handling for "_name" to match measurement name. return series that have a tag of specific value. Make a copy of all series ids and mark the ones we need to evict. Go through each slice and mark the values we find as zero so they can be removed later. Make a new slice with only the remaining ids. if we're looking for series with a tag value that matches a regex Check if we match the empty string to see if we should include series that are missing the tag. Gather the series that match the regex. If we should include the empty string, start with the list of all series and reject series that don't match our condition. If we should not include the empty string, include series that match our condition. See comments above for EQ with a StringLiteral. compare tag values We do not know how to evaluate this expression so pass it on to the query engine. FilterExprs represents a map of series IDs to filter expressions. DeleteBoolLiteralTrues deletes all elements whose filter expression is a boolean literal true. Len returns the number of elements. WalkWhereForSeriesIds recursively walks the WHERE clause and returns an ordered set of series IDs and a map from those series IDs to filter expressions that should be used to limit points returned in the final query result. Get the series IDs and filter expression for the tag or field comparison. If the expression is a boolean literal that is true, ignore it. Get the series IDs and filter expressions for the LHS. Get the series IDs and filter expressions for the RHS. Combine the series IDs from the LHS and RHS. walk down the tree SeriesIDsAllOrByExpr walks an expressions for matching series IDs or, if no expressions is given, returns all series IDs for the measurement. If no expression given or the measurement has no series, we can take just return the ids or nil accordingly. tagKeysByExpr extracts the tag keys wanted by the expression. tagKeysByFilter will filter the tag keys for the measurement. Measurements represents a list of *Measurement. series belong to a Measurement and represent unique time series in a database. immutable newSeries returns an initialized series struct bytes estimates the memory footprint of this series, in bytes. RWMutex uses 24 bytes Do not count s.Measurement to prevent double-counting in Index.Bytes. Delete marks this series as deleted.  A deleted series should not be returned for queries. Deleted indicates if this was previously deleted. TagKeyValue provides goroutine-safe concurrent access to the set of series ids mapping to a set of tag values. bytes estimates the memory footprint of this tagKeyValue, in bytes. RWMutex is 24 bytes uint64 NewTagKeyValue initialises a new TagKeyValue. Cardinality returns the number of values in the TagKeyValue. Contains returns true if the TagKeyValue contains value. InsertSeriesIDByte adds a series id to the tag key value. Load returns the SeriesIDs for the provided tag value. TagKeyValue is a no-op. If f returns false then iteration over any remaining keys or values will cease. RangeAll calls f sequentially on each key and value. A call to RangeAll on a nil TagKeyValue is a no-op. series id set lazily sorted list of series. SeriesIDs is a convenience type for sorting, checking equality, and doing union and intersection of collections of series ids. Equals assumes that both are sorted. Intersect returns a new collection of series ids in sorted order that is the intersection of the two. The two collections must already be sorted. we want to iterate through the shortest one and stop they're in sorted order so advance the counter as needed. That is, don't run comparisons against lower values that we've already passed Union returns a new collection of series ids in sorted order that is the union of the two. now append the remainder Reject returns a new collection of series ids in sorted order with the passed in set removed from the original. This is useful for the NOT operator. The two collections must already be sorted. Append the remainder seriesID is a series id that may or may not have been evicted from the current id list. evictSeriesIDs is a slice of SeriesIDs with an extra field to mark if the field should be evicted or not. newEvictSeriesIDs copies the ids into a new slice that can be used for evicting series from the slice. mark marks all of the ids in the sorted slice to be evicted from the list of series ids. If an id to be evicted does not exist, it just gets ignored. Perform a binary search of the remaining slice if the first element does not match the value we're looking for. Skip over this series since it has been evicted and won't be encountered again. evict creates a new slice with only the series that have not been evicted. TagFilter represents a tag filter when looking up other tags or measurements. TagKeys returns a list of the measurement's tag names, in sorted order. TagValues returns all the values for the given tag key, in an arbitrary order. SetFieldName adds the field name to the measurement. SeriesByTagKeyValue returns the TagKeyValue for the provided tag key. stringSet represents a set of strings. newStringSet returns an empty stringSet. add adds strings to the set. list returns the current elements in the set, in sorted order. union returns the union of this set and another. intersect returns the intersection of this set and another./Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/tsi1/Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/tsi1/cache.goCompactingExtDefaultCompactionLevelsDefaultPartitionNErrCompactionInterruptedErrIncompatibleVersionErrInvalidIndexFileErrLogEntryChecksumMismatchErrMeasurementBlockSizeMismatchErrTagBlockSizeMismatchErrUnsupportedIndexFileVersionErrUnsupportedMeasurementBlockVersionErrUnsupportedTagBlockVersionFileSignatureFormatIndexFileNameFormatLogFileNameIndexFileExtIndexFileTrailerIndexFileTrailerSizeIndexFileVersionIndexFileVersionSizeIndexFilesInfoIsIndexDirIsPartitionDirLogEntryMeasurementTombstoneFlagLogEntrySeriesTombstoneFlagLogEntryTagKeyTombstoneFlagLogEntryTagValueTombstoneFlagLogFileExtManifestFileNameMaxIndexFileSizeMaxIndexMergeCountMeasurementBlockTrailerMeasurementBlockVersionMeasurementBlockWriterMeasurementElemsMeasurementFillSizeMeasurementNSizeMeasurementOffsetSizeMeasurementSeriesIDSetFlagMeasurementTombstoneFlagMeasurementTrailerSizeNewFileSetNewIndexFileNewLogFileNewManifestNewMeasurementBlockWriterNewPartitionNewTSDBMeasurementIteratorAdapterNewTSDBTagKeyIteratorAdapterNewTSDBTagValueIteratorAdapterNewTagBlockEncoderNewTagValueSeriesIDCacheParseFilenameReadIndexFileTrailerReadManifestFileReadMeasurementBlockTrailerReadTagBlockTrailerTagBlockEncoderTagBlockTrailerTagBlockTrailerSizeTagBlockVersionTagKeyNSizeTagKeyOffsetSizeTagKeyTombstoneFlagTagValueNSizeTagValueOffsetSizeTagValueSeriesIDSetFlagTagValueTombstoneFlagWithSeriesIDCacheSizeappendLogEntryblockMeasurementIteratordefaultLogFileBufferSizeencodeTagKeyFlagencodeTagValueFlagfileIDRegexfileSetMeasurementIteratorfileSetSeriesIDIteratorfileSetSeriesIDSetIteratorfileSetTagKeyIteratorfileSetTagValueIteratorindexCompactInfoindexFileBufferSizeindexTagSetPosintersectStringSetsjoinIntSlicelogMeasurementIteratorlogMeasurementSlicelogTagKeyIteratorlogTagKeySlicelogTagValueIteratorlogTagValueSlicemeasurementMergeElemnewFileSetMeasurementIteratornewFileSetSeriesIDIteratornewFileSetTagKeyIteratornewFileSetTagValueIteratornewLogFileCompactInfonewLogTagKeyIteratornewLogTagValueIteratorquoteSQLrawSeriesIDIteratorseriesIDCacheElementsqlReplacertagBlockKeyIteratortagBlockValueIteratortagKeyEncodeEntrytagKeyMergeElemtagValueMergeElemtoValidUTF8tsdbMeasurementIteratorAdaptertsdbTagKeyIteratorAdaptertsdbTagValueIteratorAdapterunionStringSetsuvarintwriteSketchTowriteUint16TowriteUint64TowriteUint8TowriteUvarintToeletkmaplistElementEVICT TagValueSeriesIDCache is an LRU cache for series id sets associated with name -> key -> value mappings. The purpose of the cache is to provide efficient means to get sets of series ids that would otherwise involve merging many individual bitmaps at query time. When initialising a TagValueSeriesIDCache a capacity must be provided. When more than c items are added to the cache, the least recently used item is evicted from the cache. A TagValueSeriesIDCache comprises a linked list implementation to track the order by which items should be evicted from the cache, and a hashmap implementation to provide constant time retrievals of items from the cache. NewTagValueSeriesIDCache returns a TagValueSeriesIDCache with capacity c. Get returns the SeriesIDSet associated with the {name, key, value} tuple if it This now becomes most recently used. exists returns true if the an item exists for the tuple {name, key, value}. addToSet adds x to the SeriesIDSet associated with the tuple {name, key, value} if it exists. This method takes a lock on the underlying SeriesIDSet. NB this does not count as an access on the setâtherefore the set is not promoted within the LRU cache. measurementContainsSets returns true if there are sets cached for the provided measurement. Put adds the SeriesIDSet to the cache under the tuple {name, key, value}. If the cache is at its limit, then the least recently used item is evicted. Check under the write lock if the relevant item is now in the cache. Ensure our SeriesIDSet is go heap backed. Create list item, and add to the front of the eviction list. Add the listElement to the set of items. Add the set to the map No series set map for the tag key - first tag value for the tag key. No map for the measurement - first tag key for the measurement. Delete removes x from the tuple {name, key, value} if it exists. This method takes a lock on the underlying SeriesIDSet. delete removes x from the tuple {name, key, value} if it exists. checkEviction checks if the cache is too big, and evicts the least recently used item if it is. Least recently used item. Remove from evictor Remove from hashmap of items. Check if there are no more tag values for the tag key. Check there are no more tag keys for the measurement. seriesIDCacheElement is an item stored within a cache.HashIndexTSketchSeriesSketchTombstoneSeriesSketchsketchtSketchwriteMeasurementToValueDataKeyDatabuildSeriesIDSetsprevValueEncodeKeyensureHeaderWrittenflushValueHashIndexencodeTagKeyBlock/Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/tsi1/doc.go

Package tsi1 provides a memory-mapped index implementation that supports
high cardinality series.

Overview

The top-level object in tsi1 is the Index. It is the primary access point from
the rest of the system. The Index is composed of LogFile and IndexFile objects.

Log files are small write-ahead log files that record new series immediately
in the order that they are received. The data within the file is indexed
in-memory so it can be quickly accessed. When the system is restarted, this log
file is replayed and the in-memory representation is rebuilt.

Index files also contain series information, however, they are highly indexed
so that reads can be performed quickly. Index files are built through a process
called compaction where a log file or multiple index files are merged together.


Operations

The index can perform many tasks related to series, measurement, & tag data.
All data is inserted by adding a series to the index. When adding a series,
the measurement, tag keys, and tag values are all extracted and indexed
separately.

Once a series has been added, it can be removed in several ways. First, the
individual series can be removed. Second, it can be removed as part of a bulk
operation by deleting the entire measurement.

The query engine needs to be able to look up series in a variety of ways such
as by measurement name, by tag value, or by using regular expressions. The
index provides an API to iterate over subsets of series and perform set
operations such as unions and intersections.


Log File Layout

The write-ahead file that series initially are inserted into simply appends
all new operations sequentially. It is simply composed of a series of log
entries. An entry contains a flag to specify the operation type, the measurement
name, the tag set, and a checksum.

	ââââââââââLogEntryââââââââââ
	â ââââââââââââââââââââââââ â
	â â         Flag         â â
	â ââââââââââââââââââââââââ¤ â
	â â     Measurement      â â
	â ââââââââââââââââââââââââ¤ â
	â â      Key/Value       â â
	â ââââââââââââââââââââââââ¤ â
	â â      Key/Value       â â
	â ââââââââââââââââââââââââ¤ â
	â â      Key/Value       â â
	â ââââââââââââââââââââââââ¤ â
	â â       Checksum       â â
	â ââââââââââââââââââââââââ â
	ââââââââââââââââââââââââââââ

When the log file is replayed, if the checksum is incorrect or the entry is
incomplete (because of a partially failed write) then the log is truncated.


Index File Layout

The index file is composed of 3 main block types: one series block, one or more
tag blocks, and one measurement block. At the end of the index file is a
trailer that records metadata such as the offsets to these blocks.


Series Block Layout

The series block stores raw series keys in sorted order. It also provides hash
indexes so that series can be looked up quickly. Hash indexes are inserted
periodically so that memory size is limited at write time. Once all the series
and hash indexes have been written then a list of index entries are written
so that hash indexes can be looked up via binary search.

The end of the block contains two HyperLogLog++ sketches which track the
estimated number of created series and deleted series. After the sketches is
a trailer which contains metadata about the block.

	ââââââââSeriesBlockâââââââââ
	â ââââââââââââââââââââââââ â
	â â      Series Key      â â
	â ââââââââââââââââââââââââ¤ â
	â â      Series Key      â â
	â ââââââââââââââââââââââââ¤ â
	â â      Series Key      â â
	â ââââââââââââââââââââââââ¤ â
	â â                      â â
	â â      Hash Index      â â
	â â                      â â
	â ââââââââââââââââââââââââ¤ â
	â â      Series Key      â â
	â ââââââââââââââââââââââââ¤ â
	â â      Series Key      â â
	â ââââââââââââââââââââââââ¤ â
	â â      Series Key      â â
	â ââââââââââââââââââââââââ¤ â
	â â                      â â
	â â      Hash Index      â â
	â â                      â â
	â ââââââââââââââââââââââââ¤ â
	â â    Index Entries     â â
	â ââââââââââââââââââââââââ¤ â
	â â     HLL Sketches     â â
	â ââââââââââââââââââââââââ¤ â
	â â       Trailer        â â
	â ââââââââââââââââââââââââ â
	ââââââââââââââââââââââââââââ


Tag Block Layout

After the series block is one or more tag blocks. One of these blocks exists
for every measurement in the index file. The block is structured as a sorted
list of values for each key and then a sorted list of keys. Each of these lists
has their own hash index for fast direct lookups.

	âââââââââTag Blockââââââââââ
	â ââââââââââââââââââââââââ â
	â â        Value         â â
	â ââââââââââââââââââââââââ¤ â
	â â        Value         â â
	â ââââââââââââââââââââââââ¤ â
	â â        Value         â â
	â ââââââââââââââââââââââââ¤ â
	â â                      â â
	â â      Hash Index      â â
	â â                      â â
	â ââââââââââââââââââââââââ â
	â ââââââââââââââââââââââââ â
	â â        Value         â â
	â ââââââââââââââââââââââââ¤ â
	â â        Value         â â
	â ââââââââââââââââââââââââ¤ â
	â â                      â â
	â â      Hash Index      â â
	â â                      â â
	â ââââââââââââââââââââââââ â
	â ââââââââââââââââââââââââ â
	â â         Key          â â
	â ââââââââââââââââââââââââ¤ â
	â â         Key          â â
	â ââââââââââââââââââââââââ¤ â
	â â                      â â
	â â      Hash Index      â â
	â â                      â â
	â ââââââââââââââââââââââââ â
	â ââââââââââââââââââââââââ â
	â â       Trailer        â â
	â ââââââââââââââââââââââââ â
	ââââââââââââââââââââââââââââ

Each entry for values contains a sorted list of offsets for series keys that use
that value. Series iterators can be built around a single tag key value or
multiple iterators can be merged with set operators such as union or
intersection.


Measurement block

The measurement block stores a sorted list of measurements, their associated
series offsets, and the offset to their tag block. This allows all series for
a measurement to be traversed quickly and it allows fast direct lookups of
measurements and their tags.

This block also contains HyperLogLog++ sketches for new and deleted
measurements.

	âââââMeasurement Blockââââââ
	â ââââââââââââââââââââââââ â
	â â     Measurement      â â
	â ââââââââââââââââââââââââ¤ â
	â â     Measurement      â â
	â ââââââââââââââââââââââââ¤ â
	â â     Measurement      â â
	â ââââââââââââââââââââââââ¤ â
	â â                      â â
	â â      Hash Index      â â
	â â                      â â
	â ââââââââââââââââââââââââ¤ â
	â â     HLL Sketches     â â
	â ââââââââââââââââââââââââ¤ â
	â â       Trailer        â â
	â ââââââââââââââââââââââââ â
	ââââââââââââââââââââââââââââ


Manifest file

The index is simply an ordered set of log and index files. These files can be
merged together or rewritten but their order must always be the same. This is
because series, measurements, & tags can be marked as deleted (aka tombstoned)
and this action needs to be tracked in time order.

Whenever the set of active files is changed, a manifest file is written to
track the set. The manifest specifies the ordering of files and, on startup,
all files not in the manifest are removed from the index directory.


Compacting index files

Compaction is the process of taking files and merging them together into a
single file. There are two stages of compaction within TSI.

First, once log files exceed a size threshold then they are compacted into an
index file. This threshold is relatively small because log files must maintain
their index in the heap which TSI tries to avoid. Small log files are also very
quick to convert into an index file so this is done aggressively.

Second, once a contiguous set of index files exceed a factor (e.g. 10x) then
they are all merged together into a single index file and the old files are
discarded. Because all blocks are written in sorted order, the new index file
can be streamed and minimize memory use.


Concurrency

Index files are immutable so they do not require fine grained locks, however,
compactions require that we track which files are in use so they are not
discarded too soon. This is done by using reference counting with file sets.

A file set is simply an ordered list of index files. When the current file set
is obtained from the index, a counter is incremented to track its usage. Once
the user is done with the file set, it is released and the counter is
decremented. A file cannot be removed from the file system until this counter
returns to zero.

Besides the reference counting, there are no other locking mechanisms when
reading or writing index files. Log files, however, do require a lock whenever
they are accessed. This is another reason to minimize log file size.


/Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/tsi1/file_set.gonewFilefssftsscannot replace empty files"cannot replace empty files"first replacement file not found"first replacement file not found"cannot replace non-contiguous files: subset=%+v, fileset=%+v"cannot replace non-contiguous files: subset=%+v, fileset=%+v" FileSet represents a collection of files. Size of the manifest file in bytes. NewFileSet returns a new instance of FileSet. bytes estimates the memory footprint of this FileSet, in bytes. Do not count SeriesFile because it belongs to the code that constructed this FileSet. Close closes all the files in the file set. Retain adds a reference count to all files. Release removes a reference count from all files. SeriesFile returns the attached series file. PrependLogFile returns a new file set with f added at the beginning. Filters do not need to be rebuilt because log files have no bloom filter. Size returns the on-disk size of the FileSet. MustReplace swaps a list of files for a single file and returns a new file set. The caller should always guarantee that the files exist and are contiguous. Find index of first old file. Ensure all old files are contiguous. Copy to new fileset. Build new fileset and rebuild changed filters. MaxID returns the highest file identifier. Files returns all files in the set. LogFiles returns all log files from the file set. IndexFiles returns all index files from the file set. LastContiguousIndexFilesByLevel returns the last contiguous files by level. These can be used by the compaction scheduler. Ignore files above level, stop on files below level. Measurement returns a measurement by name. TagKeyIterator returns an iterator over all tag keys for a measurement. MeasurementSeriesIDIterator returns a series iterator for a measurement. MeasurementTagKeysByExpr extracts the tag keys wanted by the expression. Return all keys if no condition was passed in. TagKeySeriesIDIterator returns a series iterator for all values across a single key. HasTagKey returns true if the tag key exists. HasTagValue returns true if the tag value exists. TagValueIterator returns a value iterator for a tag key. TagValueSeriesIDIterator returns a series iterator for a single tag value. Remove tombstones set in previous file. Fetch tag value series set for this file and merge into overall set. Fetch tombstone set to be processed on next file. MeasurementsSketches returns the merged measurement sketches for the FileSet. SeriesSketches returns the merged measurement sketches for the FileSet. File represents a log or index file. Series iteration. Sketches for cardinality estimation Bitmap series existence. Reference counting. Size of file on disk Estimated memory footprint fileSetSeriesIDIterator attaches a fileset to an iterator that is released on close. fileSetSeriesIDSetIterator attaches a fileset to an iterator that is released on close. fileSetMeasurementIterator attaches a fileset to an iterator that is released on close. fileSetTagKeyIterator attaches a fileset to an iterator that is released on close. fileSetTagValueIterator attaches a fileset to an iterator that is released on close./Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/tsi1/index.gopartitionNpidxpnamesupdateCachepNamespTagscascadessitrpfsgithub.com/influxdata/influxdb/v2/pkg/slices"github.com/influxdata/influxdb/v2/pkg/slices"tsi1: compaction interrupted"tsi1: compaction interrupted"INFLUXDB_EXP_TSI_PARTITIONS"INFLUXDB_EXP_TSI_PARTITIONS"tsi"tsi"index already open"index already open"tsi1_partition"tsi1_partition"index opened with %d partitions"index opened with %d partitions"LoadUint32StoreUint32names/tags length mismatch in index"names/tags length mismatch in index"Index is closing down"Index is closing down" IndexName is the name of the index. ErrCompactionInterrupted is returned if compactions are disabled or an index is closed while a compaction is occurring. DefaultPartitionN determines how many shards the index will be partitioned into. NOTE: Currently, this must not be change once a database is created. Further, it must also be a power of 2. An IndexOption is a functional option for changing the configuration of an Index. WithPath sets the root path of the Index DisableCompactions disables compactions on the Index. WithLogger sets the logger for the Index. WithMaximumLogFileSize sets the maximum size of LogFiles before they're compacted into IndexFiles. DisableFsync disables flushing and syncing of underlying files. Primarily this impacts the LogFiles. This option can be set when working with the index in an offline manner, for cases where a hard failure can be overcome by re-running the tooling. WithLogFileBufferSize sets the size of the buffer used within LogFiles. Typically appending an entry to a LogFile involves writing 11 or 12 bytes, so depending on how many new series are being created within a batch, it may be appropriate to set this. 128K 4K (runtime default) WithSeriesIDCacheSize sets the size of the series id set cache. If set to 0, then the cache is disabled. Index represents a collection of layered index files and WAL. The following may be set when initializing an Index. Root directory of the index partitions. Initially disables compactions on the index. Maximum size of a LogFile before it's compacted. The size of the buffer used by the LogFile. Disables flushing buffers and fsyning files. Used when working with indexes offline. Index's logger. The following must be set when initializing an Index. series lookup file Name of database. Cached sketches. Measurement sketches Series sketches Index's version. Number of partitions used by the index. NewIndex returns a new instance of Index. WithLogger sets the logger on the index after it's been created. It's not safe to call WithLogger after the index has been opened, or before it has been closed. Type returns the type of Index this is. SeriesFile returns the series file attached to the index. SeriesIDSet returns the set of series ids associated with series in this index. Any series IDs for series no longer present in the index are filtered out. Open opens the index. Ensure root exists. Initialize index partitions. Open all the Partitions in parallel. Store results. Run fn on each partition using a fixed number of goroutines. Index of maximum Partition being worked on. Refresh cached sketches. Mark opened. Compact requests a compaction of partitions. Wait blocks until all outstanding compactions have completed. Close closes the index. Lock index and close partitions. Mark index as closed. Path returns the path the index was opened with. PartitionAt returns the partition by index. partition returns the appropriate Partition for a provided series key. partitionIdx returns the index of the partition that key belongs in. availableThreads returns the minimum of GOMAXPROCS and the number of partitions in the Index. updateMeasurementSketches rebuilds the cached measurement sketches. updateSeriesSketches rebuilds the cached series sketches. ForEachMeasurementName iterates over all measurement names in the index, applying fn. It returns the first error encountered, if any. ForEachMeasurementName does not call fn on each partition concurrently so the call may provide a non-goroutine safe fn. Iterate over all measurements. MeasurementExists returns true if a measurement exists. Store errors Use this to signal we found the measurement. Check each partition for the measurement concurrently. Get next partition to check Check if the measurement has been found. If it has don't need to check this partition and can just move on. Check if we found the measurement. MeasurementHasSeries returns true if a measurement has non-tombstoned series. fetchByteValues is a helper for gathering values from each partition in the index, based on some criteria. fn is a function that works on partition idx and calls into some method on the partition that returns some ordered values. This is safe since there are no readers on names until all the writers are done. It's now safe to read from names. MeasurementIterator returns an iterator over all measurements. MeasurementSeriesIDIterator returns an iterator over all series in a measurement. MeasurementNamesByRegex returns measurement names for the provided regex. DropMeasurement deletes a measurement from the index. It returns the first error encountered, if any. Update sketches under lock. All slices must be of equal length. We need to move different series into collections for each partition to process. Determine partition for series using each series key. Process each subset of series on each partition. Store errors. Some cached bitset results may need to be updated. TODO(edd): It's not clear to me yet whether it will be better to take a lock on every series id set, or whether to gather them all up under the cache rlock and then take the cache lock and update them all at once (without invoking a lock on each series id set). Taking the cache lock will block all queries, but is one lock. Taking each series set lock might be many lock/unlocks but will only block a query that needs that particular set. Need to think on it, but I think taking a lock on each series id set is the way to go. One other option here is to take a lock on the series id set when we first encounter it and then keep it locked until we're done with all the ids. Note: this will only add `id` to the set if it exists. Takes a lock on the series id set CreateSeriesIfNotExists creates a series if it doesn't exist or is deleted. No new series, nothing further to update. If there are cached sets for any of the tag pairs, they will need to be updated with the series id. Note this will only add `id` to the set if it exists. InitializeSeries is a no-op. This only applies to the in-memory index. DropSeries drops the provided series from the index.  If cascade is true and this is the last series to the measurement, the measurement will also be dropped. Remove from partition. Add sketch tombstone. Extract measurement name & tags. Check if that was the last series for the measurement in the entire index. If no more series exist in the measurement then delete the measurement. DropSeriesGlobal is a no-op on the tsi1 index. MeasurementsSketches returns the two measurement sketches for the index. SeriesSketches returns the two series sketches for the index. Since indexes are not shared across shards, the count returned by SeriesN cannot be combined with other shard's results. If you need to count series across indexes then use either the database-wide series file, or merge the index-level bitsets or sketches. HasTagKey returns true if tag key exists. It returns the first error encountered if any. Use this to signal we found the tag key. Check each partition for the tag key concurrently. Check if the tag key has already been found. If it has, we don't need to check this partition and can just move on. Check if we found the tag key. TagKeyIterator returns an iterator for all keys across a single measurement. TagValueIterator returns an iterator for all values across a single key. Check series ID set cache... Return a clone because the set is mutable. Check if the iterator contains only series id sets. Cache them... This is safe since there are no readers on keys until all Merge into single map. DiskSizeBytes returns the size of the index on disk. Get MANIFEST sizes from each partition. TagKeyCardinality always returns zero. It is not possible to determine cardinality of tags across index files, and thus it cannot be done across partitions. RetainFileSet returns the set of all files across all partitions. This is only needed when all files need to be retained for an operation. SetFieldName is a no-op on this index. Rebuild rebuilds an index. It's a no-op for this index. IsIndexDir returns true if directory contains at least one partition directory./Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/tsi1/index_file.gotblkkevitrvalueElemtombstonedgithub.com/influxdata/influxdb/v2/pkg/mmap"github.com/influxdata/influxdb/v2/pkg/mmap"TSI1"TSI1"invalid index file"invalid index file"unsupported index file version"unsupported index file version"[Index file: %s] %v"[Index file: %s] %v""done"got type %T for iterator, expected %T"got type %T for iterator, expected %T"unread %d bytes left unread in trailer"unread %d bytes left unread in trailer"L%d-%08d%s"L%d-%08d%s".tsi IndexFileVersion is the current TSI1 index file version. FileSignature represents a magic number at the header of the index file. IndexFile field size constants. IndexFile trailer fields IndexFileTrailerSize is the size of the trailer. Currently 82 bytes. measurement block offset + size series id set offset + size tombstone series id set offset + size series sketch offset + size tombstone series sketch offset + size IndexFile errors. IndexFile represents a collection of measurement, tag, and series data. ref count Components tag blocks by measurement name Raw series set data. Series sketch data. Sortable identifier & filepath to the log file. Compaction tracking. Path to data file. NewIndexFile returns a new instance of IndexFile. bytes estimates the memory footprint of this IndexFile, in bytes. wg WaitGroup is 16 bytes Do not count f.data contents because it is mmap'd Do not count SeriesFile because it belongs to the code that constructed this IndexFile. Do not count TagBlock contents, they all reference f.data Do not count contents of seriesIDSetData or tombstoneSeriesIDSetData: references f.data Open memory maps the data file at the file's path. Extract identifier from path name. Close unmaps the data file. Wait until all references are released. ID returns the file sequence identifier. Path returns the file path. SetPath sets the file's path. Level returns the compaction level for the file. Retain adds a reference count to the file. Release removes a reference count from the file. Size returns the size of the index file, in bytes. Compacting returns true if the file is being compacted. UnmarshalBinary opens an index from data. The byte slice is retained so it must be kept open. Ensure magic number exists at the beginning. Read index file trailer. Slice series sketch data. Slice series set data. Unmarshal measurement block. Unmarshal each tag block. Slice measurement block data. Save reference to entire data block. Measurement returns a measurement element. MeasurementN returns the number of measurements in the file. MeasurementHasSeries returns true if a measurement has any non-tombstoned series. TagValueIterator returns a value iterator for a tag key and a flag indicating if a tombstone exists on the measurement or key. Find key element. Merge all value series iterators together. TagKeySeriesIDIterator returns a series iterator for a tag key and a flag TagValueSeriesIDSet returns a series id set for a tag value. Find value element. TagKey returns a tag key. TagValue returns a tag value. HasSeries returns flags indicating if the series exists and if it is tombstoned. TODO(benbjohnson): series tombstone TagValueElem returns an element for a measurement/tag/value. MeasurementSeriesIDIterator returns an iterator over a measurement's series. MeasurementsSketches returns existence and tombstone sketches for measurements. SeriesSketches returns existence and tombstone sketches for series. ReadIndexFileTrailer returns the index file trailer from data. Read version. Slice trailer data. Read measurement block info. Read series id set info. Read series tombstone id set info. Read series sketch set info. Read series tombstone sketch info. Version field still in buffer. IndexFileTrailer represents meta data written to the end of the index file. WriteTo writes the trailer to w. Write measurement block info. Write series id set info. Write tombstone series id set info. Write series sketch info. Write series tombstone sketch info. Write index file encoding version. FormatIndexFileName generates an index filename for the given index./Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/tsi1/index_files.gonnkitr IndexFiles represents a layered set of index files. IDs returns the ids for all index files. Files returns p as a list of File objects. Start with sets from last file. Build sets in reverse order. This assumes that bits in both sets are mutually exclusive. Add tombstones and remove from old series existence set. Add new series and remove from old series tombstone set. MeasurementNames returns a sorted list of all measurement names for all files. MeasurementIterator returns an iterator that merges measurements across all files. TagKeyIterator returns an iterator that merges tag keys across all files. MeasurementSeriesIDIterator returns an iterator that merges series across all files. TagValueSeriesIDSet returns an iterator that merges series across all files. CompactTo merges all index files and writes them to w. Check for cancellation. Wrap writer in buffered I/O. Setup context object to track shared data for this compaction. Write magic number. Flush buffer before re-mapping. Write tagset blocks in measurement order. Ensure block is word aligned. if offset := n % 8; offset != 0 { 	if err := writeTo(bw, make([]byte, 8-offset), &n); err != nil { 		return n, err Write measurement block. Build series sets. Generate sketches from series sets. Write series set. Write tombstone series set. Write series sketches. TODO(edd): Implement WriterTo on HLL++. Write trailer. Flush file. writeTagsetTo writes a single tagset to w and saves the tagset offset. if offset := (*n) % 8; offset != 0 { 	if err := writeTo(w, make([]byte, 8-offset), n); err != nil { 		return err Encode key. Iterate over tag values. Merge all series together. Save tagset offset to measurement. Flush data to writer. Save tagset size to measurement. Add measurement data & compute sketches. Look-up series ids. Check for cancellation periodically. Add measurement to writer. Stat returns the max index file size and the total file size for all index files. largest file size total file size last modified indexCompactInfo is a context object used for tracking position information during the compaction of index files. Tracks offset/size for each measurement's tagset. indexTagSetPos stores the offset/size of tagsets./Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/tsi1/log_file.gowriteRequiredtagNallSeriesSetsvalueNmmInfogithub.com/influxdata/influxdb/v2/pkg/bloom"github.com/influxdata/influxdb/v2/pkg/bloom"log entry checksum mismatch"log entry checksum mismatch"measurement info not found"measurement info not found"L0-%08d%s"L0-%08d%s".tsllint:file-ignore SA5011 we use assertions, which don't guard Log errors. Log entry flag constants. defaultLogFileBufferSize describes the size of the buffer that the LogFile's buffered writer uses. If the LogFile does not have an explicit buffer size set then this is the size of the buffer; it is equal to the default buffer size used by a bufio.Writer. indexFileBufferSize is the buffer size used when compacting the LogFile down into a .tsi file. LogFile represents an on-disk write-ahead log file. file sequence identifier mmap writer buffered writer The size of the buffer used by the buffered writer Disables buffer flushing and file syncing. Useful for offline tooling. marshaling buffer series lookup tracks current file size tracks last time write occurred In-memory series existence/tombstone sets. In-memory index. Filepath to the log file. NewLogFile returns a new instance of LogFile. bytes estimates the memory footprint of this LogFile, in bytes. Do not include f.data because it is mmap'd TODO(jacobmarble): Uncomment when we are using go >= 1.10.0b += int(unsafe.Sizeof(f.w)) + f.w.Size() Open reads the log from a file and validates all the checksums. Open file for appending. Finish opening if file is empty. Open a read-only memory map of the existing data. Read log entries from mmap. Read next entry. Truncate partial writes. Execute entry against in-memory index. Move buffer forward. Move to the end of the file. Close shuts down the file handle and mmap. Wait until the file has no more references. FlushAndSync flushes buffered data to disk and then fsyncs the underlying file. If the LogFile has disabled flushing and syncing then FlushAndSync is a no-op. SetPath sets the log file's path. Level returns the log level of the file. Filter returns the bloom filter for the file. Stat returns size and last modification time of the file. SeriesIDSet returns the series existence set. TombstoneSeriesIDSet returns the series tombstone set. Size returns the size of the file, in bytes. TODO(edd): if mm is using a seriesSet then this could be changed to do a fast intersection. MeasurementNames returns an ordered list of measurement names. DeleteMeasurement adds a tombstone for a measurement to the log file. Flush buffer and sync to disk. TagKeySeriesIDIterator returns a series iterator for a tag key. Combine iterators across all tag keys. TagKeyIterator returns a value iterator for a measurement. TagKey returns a tag key element. TagValue returns a tag value element. DeleteTagKey adds a tombstone for a tag key to the log file. TagValueSeriesIDSet returns a series iterator for a tag value. MeasurementN returns the total number of measurements. TagKeyN returns the total number of keys. TagValueN returns the total number of values. DeleteTagValue adds a tombstone for a tag value to the log file. AddSeriesList adds a list of series to the log file in bulk. We don't need to allocate anything for this series. Exit if all series already exist. NB - this doesn't evaluate all series ids returned from series file. DeleteSeriesID adds a tombstone for a series id. SeriesN returns the total number of series in the file. appendEntry adds a log entry to the end of the file. Marshal entry to the local buffer. Save the size of the record. Write record to file. Move position backwards over partial entry. Log should be reopened if seeking cannot be completed. Update in-memory file size & modification time. execEntry executes a log entry against the in-memory index. This is done after appending and on replay of the log. Series keys can be removed if the series has been deleted from the entire database and the server is restarted. This would cause the log to replay its insert but the key cannot be found. https://github.com/influxdata/influxdb/v2/issues/9444 Check if deleted. Read key size. Read measurement name. Read tag count. Save tags. Add/remove a reference to the series on the tag value. Add/remove from appropriate series id sets. SeriesIDIterator returns an iterator over all series in the log file. measurement is not using seriesSet to store series IDs. Fast merge all seriesSets. createMeasurementIfNotExists returns a measurement by name. MeasurementIterator returns an iterator over all the measurements in the file. MeasurementSeriesIDIterator returns an iterator over all series for a measurement. CompactTo compacts the log file and writes it to w. Wrap in bufferred writer with a buffer equivalent to the LogFile size. Setup compaction offset tracking data. Retreve measurement names in order. Flush buffer & mmap series block. Build series sketches. Write series sketches. Flush buffer. Encode tag. Skip values if tag is deleted. Sort tag values. Add each value. Flush tag block. Add measurement data. logFileCompactInfo is a context object to track compaction position info. newLogFileCompactInfo returns a new instance of logFileCompactInfo. MeasurementsSketches returns sketches for existing and tombstoned measurement names. SeriesSketches returns sketches for existing and tombstoned series. LogEntry represents a single log entry in the write-ahead log. flag series id tag key tag value checksum of flag/name/tags. total size of record, in bytes. Hint to LogFile that series data is already parsed series naem, this is a cached copy of the parsed measurement name series tags, this is a cached copied of the parsed tags position of entry in batch. UnmarshalBinary unmarshals data into e. Parse flag data. Parse series id. Parse name length. Read name data. Parse key length. Read key data. Parse value length. Read value data. Compute checksum. Parse checksum. Verify checksum. Save length of elem. appendLogEntry appends to dst and returns the new buffer. This updates the checksum on the entry. Append flag. Append series id. Append name. Append key. Append value. Calculate checksum. Append checksum. logMeasurements represents a map of measurement names to measurements. bytes estimates the memory footprint of this logMeasurements, in bytes. bytes estimates the memory footprint of this logMeasurement, in bytes. If the map is getting too big it can be converted into a roaring seriesSet. forEach applies fn to every series ID in the logMeasurement. seriesIDs returns a sorted set of seriesIDs. IDs are already sorted. seriesIDSet returns a copy of the logMeasurement's seriesSet, or creates a new one keys returns a sorted list of tag keys. logMeasurementSlice is a sortable list of log measurements. logMeasurementIterator represents an iterator over a slice of measurements. Next returns the next element in the iterator. bytes estimates the memory footprint of this logTagKey, in bytes. logTagKey is a sortable list of log tag keys. bytes estimates the memory footprint of this logTagValue, in bytes. logTagValue is a sortable list of log tag values. logTagKeyIterator represents an iterator over a slice of tag keys. newLogTagKeyIterator returns a new instance of logTagKeyIterator. logTagValueIterator represents an iterator over a slice of tag values. newLogTagValueIterator returns a new instance of logTagValueIterator. FormatLogFileName generates a log filename for the given index./Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/tsi1/measurement_block.gogithub.com/influxdata/influxdb/v2/pkg/rhh"github.com/influxdata/influxdb/v2/pkg/rhh"unsupported measurement block version"unsupported measurement block version"measurement block size mismatch"measurement block size mismatch"measurement sketch not set"measurement sketch not set"measurement tombstone sketch not set"measurement tombstone sketch not set" MeasurementBlockVersion is the version of the measurement block. Measurement flag constants. Measurement field size constants. 1 byte offset for the block to ensure non-zero offsets. Measurement trailer fields version data offset/size hash index offset/size measurement sketch offset/size tombstone measurement sketch offset/size Measurement key block fields. Measurement errors. MeasurementBlock represents a collection of all measurements in an index. Measurement sketch and tombstone sketch for cardinality estimation. block version bytes estimates the memory footprint of this MeasurementBlock, in bytes. Do not count contents of blk.data or blk.hashData because they reference into an external []byte Version returns the encoding version parsed from the data. Only valid after UnmarshalBinary() has been successfully invoked. Elem returns an element for a measurement. Track current distance Find offset of measurement. Evaluate name if offset is not empty. Parse into element. Return if name match. Check if we've exceeded the probe distance. Move position forward. UnmarshalBinary unpacks data into the block. Block is not copied so data should be retained and unchanged after being passed into this function. Read trailer. Save data section. Save hash index block. Initialise sketch data. Iterator returns an iterator over all measurements. SeriesIDIterator returns an iterator for all series ids in a measurement. Find measurement element. Sketches returns existence and tombstone measurement sketches. blockMeasurementIterator iterates over a list measurements in a block. Next returns the next measurement. Returns nil when iterator is complete. Return nil when we run out of data. Unmarshal the element at the current position. Move the data forward past the record. rawSeriesIterator iterates over a list of raw series data. Next returns the next decoded series. MeasurementBlockTrailer represents meta data at the end of a MeasurementBlock. Encoding version Offset & size of data section. Offset & size of hash map section. Offset and size of cardinality sketch for measurements. Offset and size of cardinality sketch for tombstoned measurements. ReadMeasurementBlockTrailer returns the block trailer from data. Read version (which is located in the last two bytes of the trailer). Read data section info. Read measurement sketch info. Read tombstone measurement sketch info. Write data section info. Write hash index section info. Write measurement sketch info. Write tombstone measurement sketch info. Write measurement block version. MeasurementBlockElem represents an internal measurement element. series count serialized series data size in bytes, set after unmarshalling. Name returns the measurement name. Deleted returns true if the tombstone flag is set. TagBlockOffset returns the offset of the measurement's tag block. TagBlockSize returns the size of the measurement's tag block. SeriesData returns the raw series data. SeriesN returns the number of series associated with the measurement. SeriesID returns series ID at an index. SeriesIDs returns a list of decoded series ids. NOTE: This should be used for testing and diagnostics purposes only. It requires loading the entire list of series in-memory. Read from roaring, if available. Read from uvarint encoded data, if available. Size returns the size of the element. Parse tag block offset. Parse name. Parse series count. Parse series data size. Parse series data (original uvarint encoded or roaring bitmap). data = memalign(data) MeasurementBlockWriter writes a measurement block. Measurement sketch and tombstoned measurement sketch. NewMeasurementBlockWriter returns a new MeasurementBlockWriter. Add adds a measurement with series and tag set offset/size. WriteTo encodes the measurements to w. The sketches must be set before calling WriteTo. Sort names. Begin data section. Write padding byte so no offsets are zero. Encode key list. Retrieve measurement and save offset. Write measurement Build key hash map Encode hash map length. Encode hash map offset entries. Write the sketches out. writeMeasurementTo encodes a single measurement entry into w. Write flag & tag block offset. Write measurement name. Write series data to buffer. Write series count. Write data size & buffer. Word align bitmap data. writeSketchTo writes an estimator.Sketch into w, updating the number of bytes written via n./Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/tsi1/partition.golocalErrmaxLevelminLevellogFile".tsl"".tsi".compacting".compacting"MANIFEST"MANIFEST"incompatible tsi1 index MANIFEST"incompatible tsi1 index MANIFEST"index partition already open"index partition already open"uneven batch, partition %s sent %d names and %d tags"uneven batch, partition %s sent %d names and %d tags"at least two index files are required for compaction"at least two index files are required for compaction"cannot compact level zero"cannot compact level zero"TSI level compaction"TSI level compaction"tsi1_compact_to_level"tsi1_compact_to_level"tsi1_level"tsi1_level"Cannot begin compaction"Cannot begin compaction"Cannot create compaction files"Cannot create compaction files"Performing full compaction"Performing full compaction""src""dst"Cannot compact index files"Cannot compact index files"Error closing index file"Error closing index file"Cannot open new index file"Cannot open new index file"Cannot write manifest"Cannot write manifest"Full compaction complete"Full compaction complete"kb_per_sec"kb_per_sec"Removing index file"Removing index file"Cannot close index file"Cannot close index file"Cannot remove index file"Cannot remove index file"cannot parse log file id: %s"cannot parse log file id: %s"TSI log compaction"TSI log compaction"tsi1_compact_log_file"tsi1_compact_log_file"tsi1_log_file_id"tsi1_log_file_id"Cannot create index file"Cannot create index file"Cannot compact log file"Cannot compact log file"Cannot close log file"Cannot close log file"Cannot open compacted index file"Cannot open compacted index file"Cannot update manifest"Cannot update manifest"Log file compacted"Log file compacted"Cannot remove log file"Cannot remove log file"^L(\d+)-(\d+)\..+$`^L(\d+)-(\d+)\..+$`json:"levels,omitempty"`json:"levels,omitempty"`json:"files,omitempty"`json:"files,omitempty"`json:"m,omitempty"`json:"m,omitempty"`json:"k,omitempty"`json:"k,omitempty"`33554432268435456536870912 Version is the current version of the TSI index. File extensions. ManifestFileName is the name of the index manifest file. Partition represents a collection of layered index files and WAL. current log file current file set file id sequence Fast series lookup of series IDs in the series file that have been present in this partition. This set tracks both insertions and deletions of a series. Compaction management compaction levels level compaction status Close management. closing is used to inform iterators the partition is closing. Fieldset shared with engine. counter of in-progress compactions Directory of the Partition's index files. id portion of path. Log file compaction thresholds. when true, flushing and syncing of LogFile will be disabled. the LogFile's buffer is set to this value. Frequency of compaction checks. Current size of MANIFEST. Used to determine partition size. NewPartition returns a new instance of Partition. compactionEnabled: true, bytes estimates the memory footprint of this Partition, in bytes. Do not count SeriesFile because it belongs to the code that constructed this Partition. once sync.Once is 12 bytes ErrIncompatibleVersion is returned when attempting to read from an incompatible tsi1 manifest file. Open opens the partition. Validate path is correct. Create directory if it doesn't exist. Read manifest file. Set manifest size on the partition Check to see if the MANIFEST file is compatible with the current Index. Copy compaction levels to the index. Set up flags to track whether a level is compacting. Open each file in the manifest. Make first log file active, if within threshold. Set initial sequence number. Delete any files not in the manifest. Ensure a log file exists. Build series existance set. Send a compaction request on start up. openLogFile opens a log file and appends it to the index. openIndexFile opens a log file and appends it to the index. deleteNonManifestFiles removes all files not in the manifest. Loop over all files and remove any not in the manifest. Read series sets from files in reverse. Delete anything that's been tombstoned. Add series created within the file. CurrentCompactionN returns the number of compactions currently running. Wait will block until all compactions are finished. Must only be called while they are disabled. Wait for goroutines to finish outstanding compactions. Lock index and close remaining Close log files. closing returns true if the partition is currently closing. It does not require a lock so will always return to callers. Path returns the path to the partition. NextSequence returns the next file identifier. ManifestPath returns the path to the index's manifest file. Manifest returns a manifest for the index. WithLogger sets the logger for the index. FieldSet returns the fieldset. RetainFileSet returns the current fileset and adds a reference count. FileN returns the active files in the file set. prependActiveLogFile adds a new log file so that the current log file can be compacted. Open file and insert it into the first position. Prepend and generate new fileset. Write new manifest. TODO: Close index if write fails. ForEachMeasurementName iterates over all measurement names in the index. MeasurementHasSeries returns true if a measurement has at least one non-tombstoned series. MeasurementIterator returns an iterator over all measurement names. Clone bytes since they will be used after the fileset is released. DropMeasurement deletes a measurement from the index. DropMeasurement does not remove any series from the index directly. Delete all keys and values. Delete key if not already deleted. Delete each value in key. Delete all series. Mark measurement as deleted. Check if the log file needs to be swapped. createSeriesListIfNotExists creates a list of series if they doesn't exist in bulk. Is there anything to do? The partition may have been sent an empty batch. Maintain reference count on files in file set. Ensure fileset cannot change during insert. Insert series into log file. Delete series from index. Swap log file, if necessary. MeasurementsSketches returns the two sketches for the partition by merging all instances of the type sketch types in all the index files. SeriesSketches returns the two sketches for the partition by merging all TODO(edd): this should probably return an error. TagValueSeriesIDIterator returns a series iterator for a single key value. ForEachMeasurementTagKey iterates over all tag keys in a measurement. It is not possible to determine cardinality of tags across index files. Compact requests a compaction of log files. Already enabled? compact compacts continguous groups of files that are not currently compacting. Iterate over each level we are going to compact. We skip the first level (0) because it is log files and they are compacted separately. We skip the last level because the files have no higher level to compact into. Skip level if it is currently compacting. Collect contiguous files from the end of the level. Retain files during compaction. Mark the level as compacting. Execute in closure to save reference to the group within the loop. Start compacting in a separate goroutine. Compact to a new level. Ensure compaction lock for the level is released. Check for new compactions compactToLevel compacts a set of files into a new file. Replaces old files with compacted file on successful completion. This runs in a separate goroutine. Build a logger for this compaction. Files have already been retained by caller. Ensure files are released only once. Track time to compact. Create new index file. Compact all index files to new index file. Close file. Reopen as an index file. Obtain lock to swap in index file and write manifest. Replace previous files with new index file. Release old files. Close and delete all old index files. Check log file size under read lock. If file size exceeded then recheck under write lock and swap files. Swap current log file. Open new log file and insert it into the first position. Begin compacting in a background goroutine. compaction is now complete check for new compactions compactLogFile compacts f into a tsi file. The new file will share the same identifier but will have a ".tsi" extension. Once the log file is compacted then the manifest is updated and the log file is discarded. Retrieve identifier from current path. Compact log file to new index file. Replace previous log file with index file. Closing the log file will automatically wait until the ref count is zero. unionStringSets returns the union of two sets intersectStringSets returns the intersection of two sets. ParseFilename extracts the numeric id from a log or index file path. Returns 0 if it cannot be parsed. Manifest represents the list of log & index files that make up the index. The files are listed in time order, not necessarily ID order. Version should be updated whenever the TSI format has changed. location on disk of the manifest. NewManifest returns a new instance of Manifest with default compaction levels. HasFile returns true if name is listed in the log files or index files. Validate checks if the Manifest's version is compatible with this version of the tsi1 index. If we don't have an explicit version in the manifest file then we know it's not compatible with the latest tsi1 Index. Write writes the manifest file to the provided path, returning the number of bytes written and an error, if any. ReadManifestFile reads a manifest from a file path and returns the Manifest, the size of the manifest on disk, and any error if appropriate. Decode manifest. Set the path of the manifest. CompactionLevel represents a grouping of index files based on bloom filter settings. By having the same bloom filter settings, the filters can be merged and evaluated at a higher level. Bloom filter bit size & hash count DefaultCompactionLevels is the default settings used by the index. L0: Log files, no filter. L1: Initial compaction L2 L3 L4 L5 L6 L7 MaxIndexMergeCount is the maximum number of files that can be merged together at once. MaxIndexFileSize is the maximum expected size of an index file. IsPartitionDir returns true if directory contains a MANIFEST file./Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/tsi1/sql_index_exporter.goBEGIN TRANSACTION;`BEGIN TRANSACTION;`COMMIT;"COMMIT;"INSERT INTO measurement_series (name, series_id) VALUES (%s, %d);
"INSERT INTO measurement_series (name, series_id) VALUES (%s, %d);\n"INSERT INTO tag_value_series (name, key, value, series_id) VALUES (%s, %s, %s, %d);
"INSERT INTO tag_value_series (name, key, value, series_id) VALUES (%s, %s, %s, %d);\n"
CREATE TABLE IF NOT EXISTS measurement_series (
	name      TEXT NOT NULL,
	series_id INTEGER NOT NULL
);

CREATE TABLE IF NOT EXISTS tag_value_series (
	name      TEXT NOT NULL,
	key       TEXT NOT NULL,
	value     TEXT NOT NULL,
	series_id INTEGER NOT NULL
);
`
CREATE TABLE IF NOT EXISTS measurement_series (
	name      TEXT NOT NULL,
	series_id INTEGER NOT NULL
);

CREATE TABLE IF NOT EXISTS tag_value_series (
	name      TEXT NOT NULL,
	key       TEXT NOT NULL,
	value     TEXT NOT NULL,
	series_id INTEGER NOT NULL
);
``'`''`''` SQLIndexExporter writes out all TSI data for an index to a SQL export. Logs non-fatal warnings. Write schema, if true. NewSQLIndexExporter returns a new instance of SQLIndexExporter. Close ends the export and writes final output. ExportIndex writes all blocks of the TSM file. Iterate over each measurement across all partitions. Replace special case keys for measurement & field./Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/tsi1/tag_block.gokeyNkeyElemunsupported tag block version"unsupported tag block version"tag block size mismatch"tag block size mismatch"invalid zero-length tag key"invalid zero-length tag key"invalid zero-length tag value"invalid zero-length tag value"tag key out of order: prev=%s, new=%s"tag key out of order: prev=%s, new=%s"tag key already encoded: %s"tag key already encoded: %s"tag key must be encoded before encoding values"tag key must be encoded before encoding values"zero length tag value not allowed"zero length tag value not allowed"tag value out of order: prev=%s, new=%s"tag value out of order: prev=%s, new=%s"tag value already encoded: %s"tag value already encoded: %s" TagBlockVersion is the version of the tag block. Tag key flag constants. Tag value flag constants. TagBlock variable size constants. TagBlock key block fields. TagBlock value block fields. TagBlock errors. TagBlock represents tag key/value block for a single measurement. tag block version UnmarshalBinary unpacks data into the tag block. Tag block is not copied so data Verify data size is correct. Save key data section. Save entire block. TagKeyElem returns an element for a tag key. Returns an element with a nil key if not found. Find offset of tag key. Return if keys match. TagValueElem returns an element for a tag value. DecodeTagValueElem returns an element for a tag value. Find key element, exit if not found. Slice hash index data. Find offset of tag value. Return if values match. TagKeyIterator returns an iterator over all the keys in the block. tagBlockKeyIterator represents an iterator over all keys in a TagBlock. Exit when there is no data left. Unmarshal next element & move data forward. tagBlockValueIterator represents an iterator over all values for a tag key. TagBlockKeyElem represents a tag key element in a TagBlock. Value data Value hash index data Deleted returns true if the key has been tombstoned. Key returns the key name of the element. TagValueIterator returns an iterator over the key's values. unmarshal unmarshals buf into e. The data argument represents the entire block data. Parse data offset/size. Slice data. Parse hash index offset/size. Parse key. TagBlockValueElem represents a tag value element. Legacy uvarint-encoded series data. Mutually exclusive with seriesIDSetData field. Series count Raw series data Roaring bitmap encoded series data. Mutually exclusive with series.data field. Deleted returns true if the element has been tombstoned. Value returns the value for the element. SeriesN returns the series count. SeriesIDs returns a list decoded series ids. SeriesIDSet returns a set of series ids. Read bitmap data directly from mmap, if available. Otherwise decode series ids from uvarint encoding. Parse value. Parse data block size. buf = memalign(buf) TagBlockTrailerSize is the total size of the on-disk trailer. value data offset/size key data offset/size size TagBlockTrailer represents meta data at the end of a TagBlock. Total size w/ trailer Offset & size of value data section. Offset & size of key data section. Write data info. Write key data info. Write hash index info. Write total size & encoding version. ReadTagBlockTrailer returns the tag block trailer from data. Read key section info. Read hash section info. Read total size. TagBlockEncoder encodes a tags to a TagBlock section. Track value offsets. Track bytes written, sections. Track tag keys. NewTagBlockEncoder returns a new TagBlockEncoder. N returns the number of bytes written. EncodeKey writes a tag key to the underlying writer. An initial empty byte must be written. Verify key is lexicographically after previous key. Flush values section for key. Append key on to the end of the key list. Clear previous value. EncodeValue writes a tag value to the underlying writer. The tag key must be lexicographical sorted after the previous encoded tag key. Validate that keys are in-order. Save offset to hash map. Write flag. Write value. Build series data in buffer. if offset := (enc.n) % 8; offset != 0 { 	if err := writeTo(enc.w, make([]byte, 8-offset), &enc.n); err != nil { Save previous value. Close flushes the trailer of the encoder to the writer. Flush last value set. Save ending position of entire data block. Write key block to point to value blocks. Compute total size w/ trailer. ensureHeaderWritten writes a single byte to offset the rest of the block. flushValueHashIndex builds writes the hash map at the end of a value set. Ignore if no keys have been written. Save size of data section. Clear offsets. encodeTagKeyBlock encodes the keys section to the writer. Encode key list in sorted order. Save current offset so we can use it in the hash index. Write value data offset & size. Write value hash index offset & size. Write key length and data./Users/austinjaybecker/projects/abeck-go-testing/tsdb/index/tsi1/tsi1.goassert failed: "assert failed: "parsing binary-encoded uint64 value failed; binary.Uvarint() returned %d"parsing binary-encoded uint64 value failed; binary.Uvarint() returned %d" LoadFactor is the fill percent for RHH indexes. MeasurementElem represents a generic measurement element. HasSeries() bool MeasurementElems represents a list of MeasurementElem. MeasurementIterator represents a iterator over a list of measurements. MergeMeasurementIterators returns an iterator that merges a set of iterators. Iterators that are first in the list take precedence and a deletion by those early iterators will invalidate elements by later iterators. Next returns the element with the next lowest name across the iterators. If multiple iterators contain the same name then the first is returned and the remaining ones are skipped. Find next lowest name amongst the buffers. Fill buffer if empty. Find next lowest name. Return nil if no elements remaining. Merge all elements together and clear buffers. measurementMergeElem represents a merged measurement element. Name returns the name of the first element. Deleted returns the deleted flag of the first element. tsdbMeasurementIteratorAdapter wraps MeasurementIterator to match the TSDB interface. This is needed because TSDB doesn't have a concept of "deleted" measurements. NewTSDBMeasurementIteratorAdapter return an iterator which implements tsdb.MeasurementIterator. TagKeyElem represents a generic tag key element. TagKeyIterator represents a iterator over a list of tag keys. tsdbTagKeyIteratorAdapter wraps TagKeyIterator to match the TSDB interface. This is needed because TSDB doesn't have a concept of "deleted" tag keys. NewTSDBTagKeyIteratorAdapter return an iterator which implements tsdb.TagKeyIterator. MergeTagKeyIterators returns an iterator that merges a set of iterators. Next returns the element with the next lowest key across the iterators. If multiple iterators contain the same key then the first is returned Find next lowest key amongst the buffers. Fill buffer. Find next lowest key. Merge elements together & clear buffer. tagKeyMergeElem represents a merged tag key element. Key returns the key of the first element. TagValueIterator returns a merge iterator for all elements until a tombstone occurs. TagValueElem represents a generic tag value element. TagValueIterator represents a iterator over a list of tag values. tsdbTagValueIteratorAdapter wraps TagValueIterator to match the TSDB interface. This is needed because TSDB doesn't have a concept of "deleted" tag values. NewTSDBTagValueIteratorAdapter return an iterator which implements tsdb.TagValueIterator. MergeTagValueIterators returns an iterator that merges a set of iterators. Next returns the element with the next lowest value across the iterators. If multiple iterators contain the same value then the first is returned Find next lowest value amongst the buffers. Find next lowest value. Merge elements and clear buffers. tagValueMergeElem represents a merged tag value element. Name returns the value of the first element.
type SeriesPointMergeIterator interface {
	Next() (*query.FloatPoint, error)
	Close() error
	Stats() query.IteratorStats
}

func MergeSeriesPointIterators(itrs ...*seriesPointIterator) SeriesPointMergeIterator {
	if n := len(itrs); n == 0 {
		return nil
	} else if n == 1 {
		return itrs[0]
	}

	return &seriesPointMergeIterator{
		buf:  make([]*query.FloatPoint, len(itrs)),
		itrs: itrs,
	}
}

type seriesPointMergeIterator struct {
	buf  []*query.FloatPoint
	itrs []*seriesPointIterator
}

func (itr *seriesPointMergeIterator) Close() error {
	for i := range itr.itrs {
		itr.itrs[i].Close()
	}
	return nil
}
func (itr *seriesPointMergeIterator) Stats() query.IteratorStats {
	return query.IteratorStats{}
}

func (itr *seriesPointMergeIterator) Next() (_ *query.FloatPoint, err error) {
	// Find next lowest point amongst the buffers.
	var key []byte
	for i, buf := range itr.buf {
		// Fill buffer.
		if buf == nil {
			if buf, err = itr.itrs[i].Next(); err != nil {
				return nil, err
			} else if buf != nil {
				itr.buf[i] = buf
			} else {
				continue
			}
		}

		// Find next lowest key.
		if key == nil || bytes.Compare(buf.Key(), key) == -1 {
			key = buf.Key()
		}
	}

	// Return nil if no elements remaining.
	if key == nil {
		return nil, nil
	}

	// Merge elements together & clear buffer.
	itr.e = itr.e[:0]
	for i, buf := range itr.buf {
		if buf == nil || !bytes.Equal(buf.Key(), key) {
			continue
		}
		itr.e = append(itr.e, buf)
		itr.buf[i] = nil
	}

	return itr.e, nil
}
 writeTo writes write v into w. Updates n. writeUint8To writes write v into w. Updates n. writeUint16To writes write v into w using big endian encoding. Updates n. writeUint64To writes write v into w using big endian encoding. Updates n. writeUvarintTo writes write v into w using variable length encoding. Updates n. assert will panic with a given formatted message if the given condition is false. uvarint is a wrapper around binary.Uvarint. Returns a non-nil error when binary.Uvarint returns n <= 0 or n > len(data)./Users/austinjaybecker/projects/abeck-go-testing/tsdb/index.gosetsitr0itr1expr0expr1sitruniqueIndexescheckMeasurementhasTagValuematchEmptylitrritrvalueExprmaxSeriesN"inmem""tsi1"index is closing"index is closing"keys %v are not in ascending order"keys %v are not in ascending order"163830x3fffindex already registered: "index already registered: "invalid index format: %q"invalid index format: %q" Available index types. ErrIndexClosing can be returned to from an Index method if the index is currently closing. Used to clean up series in inmem index that were dropped with a shard. InfluxQL system iterators Sets a shared fieldset from the engine. Size of the index on disk, if applicable. To be removed w/ tsi1. Returns a unique reference ID to the index instance. For inmem, returns a reference to the backing Index, not ShardIndex. SeriesElem represents a generic series element. InfluxQL expression associated with series during filtering. SeriesIterator represents a iterator over a list of series. NewSeriesIteratorAdapter returns an adapter for converting series ids to series. Skip if this key has been tombstoned. SeriesIDElem represents a single series and optional expression. SeriesIDElems represents a list of series id elements. SeriesIDIterator represents a iterator over a list of series ids. SeriesIDSetIterator represents an iterator that can produce a SeriesIDSet. NewSeriesIDSetIterators returns a slice of SeriesIDSetIterator if all itrs can be type casted. Otherwise returns nil. ReadAllSeriesIDIterator returns all ids from the iterator. NewSeriesIDSliceIterator returns a SeriesIDIterator that iterates over a slice. SeriesIDSliceIterator iterates over a slice of series ids. Next returns the next series id in the slice. SeriesIDSet returns a set of all remaining ids. seriesQueryAdapterIterator adapts SeriesIDIterator to an influxql.Iterator. reusable point NewSeriesQueryAdapterIterator returns a new instance of SeriesQueryAdapterIterator. Read next series element. Skip if key has been tombstoned. Convert to a key. Write auxiliary fields. filterUndeletedSeriesIDIterator returns all series which are not deleted. FilterUndeletedSeriesIDIterator returns an iterator which filters all deleted series. seriesIDExprIterator is an iterator that attaches an associated expression. newSeriesIDExprIterator returns a new instance of seriesIDExprIterator. MergeSeriesIDIterators returns an iterator that merges a set of iterators. Merge as series id sets, if available. seriesIDMergeIterator is an iterator that merges multiple iterators together. Next returns the element with the next lowest name/tags across the iterators. Find next lowest id amongst the buffers. Return EOF if no elements remaining. Clear matching buffers. IntersectSeriesIDIterators returns an iterator that only returns series which occur in both iterators. If both series have associated expressions then they are combined together. Create series id set, if available. seriesIDIntersectIterator is an iterator that merges two iterators together. Next returns the next element which occurs in both iterators. Fill buffers. Exit if either buffer is still empty. Skip if both series are not equal. Merge series together if equal. Attach expression. UnionSeriesIDIterators returns an iterator that returns series from both both iterators. If both series have associated expressions then they are combined together. Return other iterator if either one is nil. seriesIDUnionIterator is an iterator that unions two iterators together. Return non-zero or lesser series. Attach element. DifferenceSeriesIDIterators returns an iterator that only returns series which occur the first iterator but not the second iterator. seriesIDDifferenceIterator is an iterator that merges two iterators together. Next returns the next element which occurs only in the first iterator. Exit if first buffer is still empty. Return first series if it's less. If second series is less then skip it. If both series are equal then skip both. seriesPointIterator adapts SeriesIterator to an influxql.Iterator. newSeriesPointIterator returns a new instance of seriesPointIterator. Only equality operators are allowed. Read series keys for next measurement if no more keys remaining. Exit if there are no measurements remaining. TODO(edd): It seems to me like this authorisation check should be further down in the index. At this point we're going to be filtering series that have already been materialised in the LogFiles and IndexFiles. Slurp all series keys. Periodically check for interrupt. Sort keys. NewMeasurementSliceIterator returns an iterator over a slice of in-memory measurement names. NewTagKeySliceIterator returns a TagKeyIterator that iterates over a slice. tagKeySliceIterator iterates over a slice of tag keys. Next returns the next tag key in the slice. NewTagValueSliceIterator returns a TagValueIterator that iterates over a slice. tagValueSliceIterator iterates over a slice of tag values. Next returns the next tag value in the slice. IndexSet represents a list of indexes, all belonging to one database. The set of indexes comprising this IndexSet. The Series File associated with the db for this set. field sets for _all_ indexes in this set's DB. HasInmemIndex returns true if any in-memory index is in use. Database returns the database name of the first index. HasField determines if any of the field sets on the set of indexes in the IndexSet have the provided field for the provided measurement. field sets may not have been initialised yet. DedupeInmemIndexes returns an index set which removes duplicate indexes. Useful because inmem indexes are shared by shards per database. MeasurementNamesByExpr returns a slice of measurement names matching the provided condition. If no condition is provided then all names are returned. Return filtered list if expression exists. Iterate over all measurements if no condition exists. Determine if there exists at least one authorised series for the measurement name. Retrieve value or regex expression from RHS. measurementNamesByNameFilter returns matching measurement names in sorted order. MeasurementNamesByPredicate returns a slice of measurement names matching the This behaves differently from MeasurementNamesByExpr because it will return measurements using flux predicates. valEqual determines if the provided []byte is equal to the tag value to be filtered on. If the measurement doesn't have the tag key, then it won't be considered. When an authorizer is present, the measurement should be included only if one of it's series is authorized. Locate a series with this matching tag value that's authorized. The measurement can definitely be included or rejected. If there is an authorized series in this measurement and that series does not contain the tag key/value. measurementAuthorizedSeries determines if the measurement contains a series that is authorized to be read. End of iterator If the authorizer is open, return true. Any series that does not have a tag key has an empty tag value for that key. Iterate through all of the series to find one series that does not have the tag key. The tag key exists in this series. We need at least one series that does not have the tag keys. Verify that we can see this series. If the regex matches the empty string, do a special check to see if we have an empty tag value. Iterate over the tag values and find one that matches the value. The regex does not match this tag value. If the authorizer is open, then we have found a suitable tag value. When an authorizer is present, the measurement should only be included if one of the series is authorized. Locate an authorized series. HasTagKey returns true if the tag key exists in any index for the provided measurement. hasTagKey returns true if the tag key exists in any index for the provided measurement, and guarantees to never take a lock on the series file. HasTagValue returns true if the tag value exists in any index for the provided measurement and tag key. measurementIterator returns an iterator over all measurements in the index. It guarantees to never take any locks on the underlying series file. TagKeyIterator returns a key iterator for a measurement. tagKeyIterator returns a key iterator for a measurement. It guarantees to never take any locks on the underlying series file. tagValueIterator returns a value iterator for a tag key. It guarantees to never MeasurementSeriesIDIterator returns an iterator over all non-tombstoned series for the provided measurement. measurementSeriesIDIterator does not provide any locking on the Series file. See  MeasurementSeriesIDIterator for more details. ForEachMeasurementTagKey iterates over all tag keys in a measurement and applies the provided function. tagKeySeriesIDIterator returns a series iterator for all values across a single key. It guarantees to never take any locks on the series file. tagValueSeriesIDIterator does not provide any locking on the Series File. See TagValueSeriesIDIterator for more details. MeasurementSeriesByExprIterator returns a series iterator for a measurement that is filtered by expr. If expr only contains time expressions then this call is equivalent to MeasurementSeriesIDIterator(). measurementSeriesByExprIterator returns a series iterator for a measurement that is filtered by expr. See MeasurementSeriesByExprIterator for more details. measurementSeriesByExprIterator guarantees to never take any locks on the series file. Return all series for the measurement if there are no tag expressions. MeasurementSeriesKeysByExpr returns a list of series keys matching expr. Create iterator for all matching series. measurementSeriesByExprIterator filters deleted series; no need to do so here. Iterate over all series and generate keys. Intersect iterators if expression is "AND". Union iterators if expression is "OR". seriesByBinaryExprIterator returns a series iterator and a filtering expression. For fields, return all series from this measurement. Create iterator based on value type. Match a specific value. Return all measurement series that have no values from this tag key. Return all measurement series without this tag value. Return all series across all values of this tag key. MatchTagValueSeriesIDIterator returns a series iterator for tags which match value. If matches is false, returns iterators which do not match value. matchTagValueSeriesIDIterator returns a series iterator for tags which match value. See MatchTagValueSeriesIDIterator for more details. TagValuesByKeyAndExpr retrieves tag values for the provided tag keys. TagValuesByKeyAndExpr returns sets of values for each key, indexable by the position of the tag key in the keys argument. N.B tagValuesByKeyAndExpr relies on keys being sorted in ascending lexicographic order. tagValuesByKeyAndExpr retrieves tag values for the provided tag keys. See TagValuesByKeyAndExpr for more details. tagValuesByKeyAndExpr guarantees to never take any locks on the underlying Check that keys are in order. If the keys are not sorted, then sort them. No expression means that the values shouldn't be filtered; so fetch them all. If no authorizer present then return all values. Authorization is present â check all series with matching tag values and measurements for the presence of an authorized series. Convert result sets into []string TagSets returns an ordered list of tag sets for a measurement by dimension and filtered by an optional conditional expression. measurementSeriesByExprIterator filters deleted series IDs; no need to do so here. For every series, get the tag values for the requested tag keys i.e. dimensions. This is the TagSet for that series. Series with the same TagSet are then grouped together, because for the purpose of GROUP BY they are part of the same composite series. The tag sets require a string for each series key in the set, The series file formatted keys need to be parsed into models format. Since they will end up as strings we can re-use an intermediate buffer for this process. Buffer for tags. Tags are not needed outside of each loop iteration. Skip if the series has been tombstoned. check every 16384 series if the query has been canceled NOTE - must not escape this loop iteration. Ensure it's back in the map. IndexFormat represents the format for an index. InMemFormat is the format used by the original in-memory shared index. TSI1Format is the format used by the tsi1 index. NewIndexFunc creates a new index. newIndexFuncs is a lookup of index constructors by name. RegisterIndex registers a storage index initializer by name. RegisteredIndexes returns the slice of currently registered indexes. NewIndex returns an instance of an index based on its format. Use default format unless existing directory exists. nop, use default Lookup index by format./Users/austinjaybecker/projects/abeck-go-testing/tsdb/internal/Users/austinjaybecker/projects/abeck-go-testing/tsdb/internal/meta.pb.goErrIntOverflowMetaErrInvalidLengthMetaencodeVarintMetafileDescriptor_meta_3108ecf7b17f779eskipMetasovMetasozMetaxxx_messageInfo_Fieldxxx_messageInfo_MeasurementFieldSetxxx_messageInfo_MeasurementFieldsxxx_messageInfo_Seriesprotobuf:"bytes,1,opt,name=Key,proto3" json:"Key,omitempty"`protobuf:"bytes,1,opt,name=Key,proto3" json:"Key,omitempty"`protobuf:"bytes,2,rep,name=Tags" json:"Tags,omitempty"`protobuf:"bytes,2,rep,name=Tags" json:"Tags,omitempty"`protobuf:"bytes,2,rep,name=Fields" json:"Fields,omitempty"`protobuf:"bytes,2,rep,name=Fields" json:"Fields,omitempty"`protobuf:"varint,2,opt,name=Type,proto3" json:"Type,omitempty"`protobuf:"varint,2,opt,name=Type,proto3" json:"Type,omitempty"`protobuf:"bytes,1,rep,name=Measurements" json:"Measurements,omitempty"`protobuf:"bytes,1,rep,name=Measurements" json:"Measurements,omitempty"`tsdb.Series"tsdb.Series"tsdb.Tag"tsdb.Tag"tsdb.MeasurementFields"tsdb.MeasurementFields"tsdb.Field"tsdb.Field"tsdb.MeasurementFieldSet"tsdb.MeasurementFieldSet"proto: Series: wiretype end group for non-group"proto: Series: wiretype end group for non-group"proto: Series: illegal tag %d (wire type %d)"proto: Series: illegal tag %d (wire type %d)"proto: MeasurementFields: wiretype end group for non-group"proto: MeasurementFields: wiretype end group for non-group"proto: MeasurementFields: illegal tag %d (wire type %d)"proto: MeasurementFields: illegal tag %d (wire type %d)"proto: MeasurementFieldSet: wiretype end group for non-group"proto: MeasurementFieldSet: wiretype end group for non-group"proto: MeasurementFieldSet: illegal tag %d (wire type %d)"proto: MeasurementFieldSet: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Measurements"proto: wrong wireType = %d for field Measurements"internal/meta.proto"internal/meta.proto" source: internal/meta.proto 245 bytes of a gzipped FileDescriptorProto/Users/austinjaybecker/projects/abeck-go-testing/tsdb/meta.gogo:generate protoc --gogo_out=. internal/meta.proto MarshalTags converts a tag set to bytes for use as a lookup key. MakeTagsKey converts a tag set to bytes for use as a lookup key. precondition: keys is sorted precondition: models.Tags is sorted no tags matched the requested keys selected tags, add separators/Users/austinjaybecker/projects/abeck-go-testing/tsdb/series_cursor.go seriesCursor is an implementation of SeriesCursor over an IndexSet. newSeriesCursor returns a new instance of SeriesCursor.if itr.opt.Authorizer != nil && !itr.opt.Authorizer.AuthorizeSeriesRead(itr.indexSet.Database(), name, tags) {	continue/Users/austinjaybecker/projects/abeck-go-testing/tsdb/series_file.gomaxCompactionConcurrencykeyPartitionIDskeyPartitionpartitionIDsorigLentcBuftcSztotalSzsz64dstTagskey0value0value1name0tagN0tagN1github.com/influxdata/influxdb/v2/pkg/binaryutil"github.com/influxdata/influxdb/v2/pkg/binaryutil"tsdb: series file closed"tsdb: series file closed"tsdb: invalid series partition id"tsdb: invalid series partition id"Unable to open series file"Unable to open series file"%02x"%02x"series key encoding does not match calculated total length: actual=%d, exp=%d, key=%x"series key encoding does not match calculated total length: actual=%d, exp=%d, key=%x" SeriesIDSize is the size in bytes of a series key ID. SeriesFilePartitionN is the number of partitions a series file is split into. SeriesFile represents the section of the index that holds series data. RWMutex to track references to the SeriesFile that are in use. NewSeriesFile returns a new instance of SeriesFile. Wait for all references to be released and prevent new ones from being acquired. Create path if it doesn't exist. Limit concurrent series file compactions Open partitions. Path returns the path to the file. SeriesPartitionPath returns the path to a given partition. Partitions returns all partitions. Retain adds a reference count to the file.  It returns a release func. Return the RUnlock func as the release func to be called when done. EnableCompactions allows compactions to run. DisableCompactions prevents new compactions from running. Wait waits for all Retains to be released. FileSize returns the size of all partitions, in bytes. CreateSeriesListIfNotExists creates a list of series in bulk if they don't exist. The returned ids slice returns IDs for every name+tags, creating new series IDs as needed. DeleteSeriesID flags a series as permanently deleted. If the series is reintroduced later then it must create a new id. IsDeleted returns true if the ID has been deleted before. SeriesKey returns the series key for a given id. SeriesKeys returns a list of series keys from a list of ids. Series returns the parsed series name and tags for an offset. SeriesID return the series id for the series. HasSeries return true if the series exists. SeriesCount returns the number of series. SeriesIterator returns an iterator over all the series. AppendSeriesKey serializes name and tags to a byte slice. The total length is prepended as a uvarint. The tag count is variable encoded, so we need to know ahead of time what the size of the tag count value will be. Size of name/tags. Does not include total length. size of measurement measurement size of number of tags length of each tag key and value size of tag keys/values Variable encode length. If caller doesn't provide a buffer then pre-allocate an exact one. Append total length. Append tag count. Append tags. Verify that the total length equals the encoded byte count. ReadSeriesKey returns the series key from the beginning of the buffer. ParseSeriesKey extracts the name & tags from a series key. ParseSeriesKeyInto extracts the name and tags for data, parsing the tags into dstTags, which is then returned. The returned dstTags may have a different length and capacity. parseSeriesKey extracts the name and tags from data, attempting to re-use the provided tags value rather than allocating. The returned tags may have a different length and capacity to those provided. Grow dst to use full capacity Handle 'nil' keys. Read names. Compare names, return if not equal. Read tag counts. Compare each tag in order. Check for EOF. Read keys. Compare keys & values. GenerateSeriesKeys generates series keys for a list of names & tags using a single large memory block. SeriesKeysSize returns the number of bytes required to encode a list of name/tags. SeriesKeySize returns the number of bytes required to encode a series key./Users/austinjaybecker/projects/abeck-go-testing/tsdb/series_index.gominSegmentIDelemHashelemKeyelemOffsetelemIDSIDX"SIDX"invalid series index"invalid series index" offset + id rhh load factor magic + version max series + max offset count + capacity key/id map offset & size id/offset map offset & size SeriesIndex represents an index of key-to-id & id-to-offset mappings. mmap data key/id mmap data id/offset mmap data In-memory data since rebuild. Open memory-maps the index file. Map data file, if it exists. Close unmaps the index file. Recover rebuilds the in-memory index for all new entries. Allocate new in-memory maps. Process all entries since the maximum offset in the on-disk index. Count returns the number of series in the index. OnDiskCount returns the number of series in the on-disk index. InMemCount returns the number of series in the in-memory index. Delete marks the series id as deleted. IsDeleted returns true if series id has been deleted. Clone returns a copy of idx for use during compaction. In-memory maps are not cloned. SeriesIndexHeader represents the header of a series index. NewSeriesIndexHeader returns a new instance of SeriesIndexHeader. ReadSeriesIndexHeader returns the header from data. Read magic number. Read max offset. Read count & capacity. Read key/id map position. Read offset/id map position. WriteTo writes the header to w./Users/austinjaybecker/projects/abeck-go-testing/tsdb/series_partition.gokeyRangenewIDsnewKeyRangeserrDoneentryNtsdb: series partition closed"tsdb: series partition closed"tsdb: series partition compaction cancelled"tsdb: series partition compaction cancelled"tsdb: cannot reopen series partition"tsdb: cannot reopen series partition"0000"0000"Series partition compaction"Series partition compaction"series_partition_compaction"series_partition_compaction"series partition compaction failed"series partition compaction failed"%04x"%04x"unexpected series partition log entry flag: %d"unexpected series partition log entry flag: %d"key/id map full"key/id map full"id/offset map full"id/offset map full" DefaultSeriesPartitionCompactThreshold is the number of series IDs to hold in the in-memory series map before compacting and rebuilding the on-disk representation. SeriesPartition represents a subset of series file data. series id sequence NewSeriesPartition returns a new instance of SeriesPartition. Open memory maps the data file at the partition's path. Open components. Init last segment for writes. Find max series id by searching segments in reverse order. Reset our sequence num to the next one to assign Create initial segment if none exist. Close unmaps the data files. ID returns the partition id. IndexPath returns the path to the series index. Index returns the partition's index. Segments returns a list of partition segments. Used for testing. The ids parameter is modified to contain series IDs for all keys belonging to this partition. Exit if all series for this partition already exist. Obtain write lock to create new series. Track offsets of duplicate series. Skip series that don't belong to the partition or have already been created. Re-attempt lookup under write lock. Write to series log and save offset. Append new key to be added to hash map after flush. Flush active segment writes so we can access data in mmap. Add keys to hash map(s). Check if we've crossed the compaction threshold. Clear compaction flag. Compacting returns if the SeriesPartition is currently compacting. Already tombstoned, ignore. Write tombstone entry. Flush active segment write. Mark tombstone in memory. FindIDBySeriesKey return the series id for the series key. AppendSeriesIDs returns a list of all series ids. activeSegment returns the last segment. writeLogEntry appends an entry to the end of the active segment. If there is no more room in the segment then a new segment is added. createSegment appends a new segment Close writer for active segment, if one exists. Generate a new sequential segment identifier. Generate new empty segment. Allow segment to write. SeriesPartitionCompactor represents an object reindexes a series partition and optionally compacts segments. NewSeriesPartitionCompactor returns a new instance of SeriesPartitionCompactor. Compact rebuilds the series partition index. Snapshot the partitions and index so we can check tombstones and replay at the end under lock. Compact index to a temporary location. Swap compacted index under lock & replay since compaction. Reopen index with new file. Replay new entries. Allocate space for maps. Reindex all partitions. Make sure we don't go past the offset where the compaction began. Only process insert entries. fallthrough Save max series identifier processed. Ignore entry if tombstoned. Insert into maps. Open file handler. Calculate map positions. Write header. Write maps. Sync & close. If empty slot found or matching offset, insert and exit. Read key at position & hash. Insert current values. Swap with values in that position. If empty slot found or matching id, insert and exit. Hash key./Users/austinjaybecker/projects/abeck-go-testing/tsdb/series_segment.goSSEG"SSEG"invalid series segment"invalid series segment"invalid series segment version"invalid series segment version"series segment not writable"series segment not writable".initializing".initializing"[series id %d]: tombstone entry but exists in index"[series id %d]: tombstone entry but exists in index"0xFFFF0xFFFFFFFF^[0-9a-f]{4}$`^[0-9a-f]{4}$`unreachable: invalid flag: %d"unreachable: invalid flag: %d" Series entry constants. flag + id SeriesSegment represents a log of series entries. mmap file write file handle bufferred file handle current file size NewSeriesSegment returns a new instance of SeriesSegment. CreateSeriesSegment generates an empty segment at path. Generate segment in temp location. Write header to file and close. Swap with target path. Open segment at new location. Memory map file data. Read header. Path returns the file path to the segment. InitForWrite initializes a write handle for the segment. This is only used for the last segment in the series file. Only calculate segment data size if writing. Open file handler for writing & seek to end of data. Close unmaps the segment. Data returns the raw data. ID returns the id the segment was initialized with. Size returns the size of the data in the segment. This is only populated once InitForWrite() is called. Slice returns a byte slice starting at pos. WriteLogEntry writes entry data into the segment. Returns the offset of the beginning of the entry. CanWrite returns true if segment has space to write entry data. Flush flushes the buffer to disk. AppendSeriesIDs appends all the segments ids to a slice. Returns the new slice. MaxSeriesID returns the highest series id in the segment. ForEachEntry executes fn for every entry in the segment. Clone returns a copy of the segment. Excludes the write handler, if set. CompactToPath rewrites the segment to a new file and removes tombstoned entries. Iterate through the segment and write any entries to a new segment that exist in the index. series id has been deleted from index copy entry over to new segment Close the segment and truncate it to its maximum size. CloneSeriesSegments returns a copy of a slice of segments. FindSegment returns a segment by id. ReadSeriesKeyFromSegments returns a series key from an offset within a set of segments. JoinSeriesOffset returns an offset that combines the 2-byte segmentID and 4-byte pos. SplitSeriesOffset splits a offset into its 2-byte segmentID and 4-byte pos parts. IsValidSeriesSegmentFilename returns true if filename is a 4-character lowercase hexadecimal number. ParseSeriesSegmentFilename returns the id represented by the hexadecimal filename. SeriesSegmentSize returns the maximum size of the segment. The size goes up by powers of 2 starting from 4MB and reaching 256MB. 4MB 256MB SeriesSegmentHeader represents the header of a series segment. NewSeriesSegmentHeader returns a new instance of SeriesSegmentHeader. ReadSeriesSegmentHeader returns the header from data. If flag byte is zero then no more entries exist. IsValidSeriesEntryFlag returns true if flag is valid./Users/austinjaybecker/projects/abeck-go-testing/tsdb/series_set.goa32bmsroaring"github.com/RoaringBitmap/roaring"NewBitmapFastOr SeriesIDSet represents a lockable bitmap of series ids. NewSeriesIDSet returns a new instance of SeriesIDSet. Bytes estimates the memory footprint of this SeriesIDSet, in bytes. Add adds the series id to the set. AddNoLock adds the series id to the set. Add is not safe for use from multiple goroutines. Callers must manage synchronization. AddMany adds multiple ids to the SeriesIDSet. AddMany takes a lock, so may not be optimal to call many times with few ids. Contains returns true if the id exists in the set. ContainsNoLock returns true if the id exists in the set. ContainsNoLock is not safe for use from multiple goroutines. The caller must manage synchronization. Remove removes the id from the set. RemoveNoLock removes the id from the set. RemoveNoLock is not safe for use from multiple goroutines. The caller must manage synchronization. Cardinality returns the cardinality of the SeriesIDSet. Merge merged the contents of others into s. The caller does not need to provide s as an argument, and the contents of s will always be present in s after Merge returns. Add ourself. Add other bitsets. Hold until we have merged all the bitmaps MergeInPlace merges other into s, modifying s in the process. Equals returns true if other and s are the same set of ids. And returns a new SeriesIDSet containing elements that were present in s and other. AndNot returns a new SeriesIDSet containing elements that were present in s, but not present in other. ForEach calls f for each id in the set. The function is applied to the IDs ForEachNoLock calls f for each id in the set without taking a lock. Diff removes from s any elements also present in other. Clone returns a new SeriesIDSet with a deep copy of the underlying bitmap. CloneNoLock calls Clone without taking a lock. Iterator returns an iterator to the underlying bitmap. This iterator is not protected by a lock. UnmarshalBinary unmarshals data into the set. UnmarshalBinaryUnsafe unmarshals data into the set. References to the underlying data are used so data should not be reused by caller. WriteTo writes the set to w. Clear clears the underlying bitmap for re-use. Clear is safe for use by multiple goroutines. ClearNoLock clears the underlying bitmap for re-use without taking a lock. Slice returns a slice of series ids./Users/austinjaybecker/projects/abeck-go-testing/tsdb/shard.goipathfieldsToCreatewriteErrorvalidFieldvalidateKeysexpandedcosterrcostsfieldsUpdatefsetgithub.com/influxdata/influxdb/v2/tsdb/internal"github.com/influxdata/influxdb/v2/tsdb/internal"writeReq"writeReq"writeReqOk"writeReqOk"writeReqErr"writeReqErr"seriesCreate"seriesCreate"fieldsCreate"fieldsCreate"writePointsErr"writePointsErr"writePointsDropped"writePointsDropped"writePointsOk"writePointsOk""writeBytes"field overflow"field overflow"field type conflict"field type conflict"field not found"field not found"field ID not mapped"field ID not mapped"shard is disabled"shard is disabled"unknown field index format"unknown field index format"unknown field type"unknown field type"shard not idle"shard not idle"[shard %d] %s"[shard %d] %s"partial write: %s dropped=%d"partial write: %s dropped=%d""walPath""indexType"engine: %s"engine: %s"invalid tag key: input tag "%s" on measurement "%s" is invalid"invalid tag key: input tag \"%s\" on measurement \"%s\" is invalid"key contains invalid unicode: "%s""key contains invalid unicode: \"%s\""invalid field name: input field "%s" on measurement "%s" is invalid"invalid field name: input field \"%s\" on measurement \"%s\" is invalid"expandSources: unsupported source type: %T"expandSources: unsupported source type: %T"CreateSeriesCursor: no series file"CreateSeriesCursor: no series file"Store.ExpandSources: unsupported source type: %T"Store.ExpandSources: unsupported source type: %T"O_SYNC2690 ErrFieldOverflow is returned when too many fields are created on a measurement. ErrFieldTypeConflict is returned when a new field already exists with a different type. ErrFieldNotFound is returned when a field cannot be found. ErrFieldUnmappedID is returned when the system is presented, during decode, with a field ID there is no mapping for. ErrEngineClosed is returned when a caller attempts indirectly to access the shard's underlying engine. ErrShardDisabled is returned when a the shard is not available for queries or writes. ErrUnknownFieldsFormat is returned when the fields index file is not identifiable by the file's magic number. ErrUnknownFieldType is returned when the type of a field cannot be determined. ErrShardNotIdle is returned when an operation requiring the shard to be idle/cold is attempted on a hot shard. fieldsIndexMagicNumber is the file magic number for the fields index file. A ShardError implements the error interface, and contains extra context about the shard that generated the error. NewShardError returns a new ShardError. PartialWriteError indicates a write request could only write a portion of the requested values. A sorted slice of series keys that were dropped. Shard represents a self-contained time series database. An inverted index of the measurement and tag data is kept along with the raw time series data. Data can be split across many shards. The query engine in TSDB is responsible for combining the output of many shards into a single query result. expvar-based stats. CompactionDisabled specifies the shard should not schedule compactions. NewShard returns a new initialized Shard. walPath doesn't apply to the b1 type index WithLogger sets the logger on the shard. It must be called before Open. SetEnabled enables the shard for queries and write.  When disabled, all writes and queries return an error and compactions are stopped for the shard. Prevent writes and queries Disable background compactions and snapshotting ScheduleFullCompaction forces a full compaction to be schedule on the shard. ID returns the shards ID. Database returns the database of the shard. RetentionPolicy returns the retention policy of the shard. ShardStatistics maintains statistics for a shard. Refresh our disk size stat Set the index type on the tags.  N.B this needs to be checked since it's only set when the shard is opened. Add the index and engine statistics. Path returns the path set on the shard when it was created. Open initializes and opens the shard's store. Return if the shard is already open Initialize underlying index. Initialize underlying engine. Set log output on the engine. Disable compactions while loading the index Open engine. Load metadata index for the inmem index only. enable writes, queries and compactions Close shuts down the shard's store. close closes the shard an removes reference to the shard from associated indexes, unless clean is false. IndexType returns the index version being used for this shard. IndexType returns the empty string if it is called before the shard is opened, since it is only that point that the underlying index type is known. Shard not open yet. ready determines if the Shard is ready for queries or writes. It returns nil if ready, otherwise ErrShardClosed or ErrShardDisabled Index returns a reference to the underlying index. It returns an error if the index is nil. SeriesFile returns a reference the underlying series file. If return an error if the series file is nil. IsIdle return true if the shard is not receiving writes and is fully compacted. Disable compactions to stop background goroutines SetCompactionsEnabled enables or disable shard background compactions. DiskSize returns the size on disk of this shard. We don't use engine() because we still want to report the shard's disk size even if the shard has been disabled. FieldCreate holds information for a field to create on a measurement. WritePoints will write the raw data points and any new metadata to the index in the shard. There was a partial write (points dropped), hold onto the error to return to the caller, but continue on writing the remaining points. add any new fields and keep track of what needs to be saved Write to the engine. validateSeriesAndFields checks which series and fields are new and whose metadata should be saved and indexed. only first error reason is set unless returned from CreateSeriesListIfNotExists Create all series against the index in bulk. Check if keys should be unicode validated. Drop any series w/ a "time" tag, these are illegal Drop any series with invalid unicode characters in the key. Add new series. Check for partial writes. TODO(jmw): why is this a *PartialWriteError when everything else is not a pointer? Maybe we can just change it to be consistent if we change it also in all the places that construct it. Skip any points with only invalid fields. Skip any points whos keys have been dropped. Dropped has already been incremented for them. Check with the field validator. Create any fields that are missing. Skip fields named "time". They are illegal. add fields DeleteSeriesRange deletes all values from for seriesKeys between min and max (inclusive) DeleteSeriesRangeWithPredicate deletes all values from for seriesKeys between min and max (inclusive) for which predicate() returns true. If predicate() is nil, then all values in range are deleted. DeleteMeasurement deletes a measurement and all underlying series. SeriesN returns the unique number of series in the shard. SeriesSketches returns the measurement sketches for the shard. MeasurementsSketches returns the measurement sketches for the shard. MeasurementNamesByRegex returns names of measurements matching the regular expression. MeasurementTagKeysByExpr returns all the tag keys for the provided expression. MeasurementTagKeyValuesByExpr returns all the tag keys values for the provided expression. MeasurementNamesByPredicate returns fields for a measurement filtered by an expression. MeasurementFields returns fields for a measurement. MeasurementExists returns true if the shard contains name. TODO(edd): This method is currently only being called from tests; do we really need it? WriteTo writes the shard's data to w. CreateIterator returns an iterator for the data in the shard. TODO(benbjohnson): Move up to the Shards.CreateIterator(). FieldDimensions returns unique sets of fields and dimensions across a list of sources. Handle system sources. Unknown system source so default to looking for a measurement. Retrieve measurement. Append fields and dimensions. mapType returns the data type for the field within the measurement. Process system measurements. expandSources expands regex sources and removes duplicates. NOTE: sources must be normalized (db and rp set) before calling this function. Use a map as a set to prevent duplicates. Iterate all sources, expanding regexes when they're found. Add non-regex measurements directly to the set. Loop over matching measurements. Convert set to sorted slice. Convert set to a list of Sources. Backup backs up the shard by creating a tar archive of all TSM files that have been modified since the provided time. See Engine.Backup for more details. Restore restores data to the underlying engine for the shard. The shard is reopened after restore. Special case - we can still restore to a disabled shard, so we should only check if the engine is closed and not care if the shard is disabled. Restore to engine. Close shard. Reopen engine. Import imports data to the underlying engine for the shard. r should be a reader from a backup created by Backup. Special case - we can still import to a disabled shard, so we should Import to engine. CreateSnapshot will return a path to a temp directory containing hard links to the underlying shard files. ForEachMeasurementName iterates over each measurement in the shard. Digest returns a digest of the shard. Make sure the shard is idle/cold. (No use creating a digest of a hot shard that is rapidly changing.) engine safely (under an RLock) returns a reference to the shard's Engine, or an error if the Engine is closed, or the shard is currently disabled. The shard's Engine should always be accessed via a call to engine(), rather than directly referencing Shard.engine. If a caller needs an Engine reference but is already under a lock, then they should use engineNoLock(). engineNoLock is similar to calling engine(), but the caller must guarantee that they already hold an appropriate lock. Shards represents a sortable list of shards. MeasurementsByRegex returns the unique set of measurements matching the provided regex, for all the shards. Skip this shard's resultsâprevious behaviour. FieldKeysByMeasurement returns a de-duplicated, sorted, set of field keys for the provided measurement name. MeasurementNamesByPredicate returns the measurements that match the given predicate. FieldKeysByPredicate returns the field keys for series that match the given predicate. Iterate through every shard and expand the sources. MeasurementFields holds the fields of a measurement and their codec. map[string]*Field NewMeasurementFields returns an initialised *MeasurementFields value. bytes estimates the memory footprint of this MeasurementFields, in bytes. CreateFieldIfNotExists creates a new field with an autoincrementing ID. Returns an error if 255 fields have already been created on the measurement or the fields already exists with a different type. Ignore if the field already exists. Re-check field and type under write lock. Create and append a new field. Field returns the field for name, or nil if there is no field for name. FieldBytes returns the field for name, or nil if there is no field for name. FieldBytes should be preferred to Field when the caller has a []byte, because it avoids a string allocation, which can't be avoided if the caller converts the []byte to a string and calls Field. FieldSet returns the set of fields and their types for the measurement. MeasurementFieldSet represents a collection of fields by measurement. This safe for concurrent use. path is the location to persist field sets NewMeasurementFieldSet returns a new instance of MeasurementFieldSet. If there is a load error, return the error and an empty set so it can be rebuild manually. Bytes estimates the memory footprint of this MeasurementFieldSet, in bytes. Fields returns fields for a measurement by name. FieldsByString returns fields for a measurement by name. CreateFieldsIfNotExists returns fields for a measurement by name. Delete removes a field set for a measurement. DeleteWithLock executes fn and removes a field set from a measurement under lock. No fields left, remove the fields index file Write the new index to a temp file and rename when it's sync'dclose file handle before renaming to support Windows Field represents a series field. All of the fields must be hashable. NewFieldKeysIterator returns an iterator that can be iterated over to retrieve field keys. Retrieve measurements from shard. Filter if condition specified. FGA is currently not supported when retrieving field keys. fieldKeysIterator iterates over measurements and gets field keys from each measurement. remaining measurement names current measurement name current measurement's fields Next emits the next tag key name. If there are no more keys then move to the next measurements. Return next key. NewTagKeysIterator returns a new instance of TagKeysIterator. measurementKeyFunc is the function called by measurementKeysIterator. measurementKeysIterator iterates over measurements and gets keys from each measurement. current measurement's keys LimitError represents an error caused by a configurable limit./Users/austinjaybecker/projects/abeck-go-testing/tsdb/store.gorpPathshardDirsrpDirscompactionSettingsdbDirsthroughputthroughputBurstwaiterallShardsgetSketchessetMumeasumentsfinalKeystagKeySetmeasurementExprkeySetnextResultallResultsidxBufmaxMeasurementskeyCmptvsvalCmpvalueIdxspercfirstShardIndexdbLockt2shardOrWALPathstorePathshard not found"shard not found"store is closed"store is closed"shard is being deleted"shard is being deleted"cannot delete data. DB contains shards using both inmem and tsi1 indexes. Please convert all shards to use the same index type to delete data."cannot delete data. DB contains shards using both inmem and tsi1 indexes. Please convert all shards to use the same index type to delete data."numSeries"numSeries"numMeasurements"numMeasurements"Cannot retrieve series cardinality"Cannot retrieve series cardinality"Cannot retrieve measurement cardinality"Cannot retrieve measurement cardinality"Using data dir"Using data dir"max_concurrent_compactions"max_concurrent_compactions"throughput_bytes_per_second"throughput_bytes_per_second"throughput_bytes_per_second_burst"throughput_bytes_per_second_burst""unlimited"Compaction settings"Compaction settings"Open store"Open store"tsdb_open"tsdb_open"Skipping database dir"Skipping database dir"not a directory"not a directory"failed database filter"failed database filter"Skipping retention policy dir"Skipping retention policy dir"failed retention policy filter"failed retention policy filter"Skipping series file in retention policy dir"Skipping series file in retention policy dir"invalid shard ID found at path"invalid shard ID found at path"%s is not a valid ID. Skipping shard."%s is not a valid ID. Skipping shard."skipping shard"skipping shard"Failed to open shard"Failed to open shard"Failed to open shard: %d: %s"Failed to open shard: %d: %s"Opened shard"Opened shard"index_version"index_version"%s_count"%s_count"Mixed shard index types"Mixed shard index types"invalid database directory location for database '%s': %s"invalid database directory location for database '%s': %s"invalid path for database '%s', retention policy '%s': %s"invalid path for database '%s', retention policy '%s': %s"shard %d: %s"shard %d: %s"shard nil, can't get cardinality"shard nil, can't get cardinality"shard %d not found"shard %d not found"shard %d doesn't exist on this server"shard %d doesn't exist on this server"a condition is required"a condition is required"unexpected results returned engine. Got %d measurement sets for %d shards"unexpected results returned engine. Got %d measurement sets for %d shards"Error while freeing cold shard resources"Error while freeing cold shard resources"Cannot retrieve measurement names"Cannot retrieve measurement names"max-values-per-tag limit may be exceeded soon"max-values-per-tag limit may be exceeded soon""perc"%d%%"%d%%"store abs path: %s"store abs path: %s"file abs path: %s"file abs path: %s"file rel path: %s"file rel path: %s"lint:file-ignore ST1005 this is old code. we're not going to conform error messages import "github.com/influxdata/influxdb/v2/tsdb" ErrShardNotFound is returned when trying to get a non existing shard. ErrStoreClosed is returned when trying to use a closed Store. ErrShardDeletion is returned when trying to create a shard that is being deleted ErrMultipleIndexTypes is returned when trying to do deletes on a database with multiple index types. Statistics gathered by the store. number of series in a database number of measurements in a database SeriesFileDirectory is the name of the directory containing series files for a database. databaseState keeps track of the state of a database. addIndexType records that the database has a shard with the given index type. addIndexType records that the database no longer has a shard with the given index type. hasMultipleIndexTypes returns true if the database has multiple index types. Store manages shards and indexes for databases. Determines size of series file mmap. Can be altered in tests. shared per-database indexes, only if using "inmem". Maintains a set of shards that are in the process of deletion. This prevents new shards from being created while old ones are being deleted. Epoch tracker helps serialize writes and deletes that may conflict. It is stored by shard. NewStore returns a new store with the given path and a default configuration. The returned store must be initialized by calling Open before using it. WithLogger sets the logger for the store. Statistics returns statistics for period monitoring. Add all the series and measurements cardinality estimations. Gather allÂ statistics for all shards. Build index set to work on. Path returns the store's root path. Open initializes the store, creating all necessary directories, loading all shards as well as initializing periodic maintenance of them. Create directory. res holds the result from opening each shard in a goroutine Limit the number of concurrent TSM files to be opened to the number of cores. Setup a shared limiter for compactions Default to 50% of cores for compactions Don't allow more compactions to run than cores. Determine how many shards we need to open by checking the store path. Load series file. Retrieve database index. Load each retention policy within the database directory. The .series directory is not a retention policy. Series file should not be in a retention policy but skip just in case. Shard file names are numeric shardIDs Copy options and assign shared index. Provide an implementation of the ShardIDSets Existing shards should continue to use inmem index. Disable compactions, writes and queries until all shards are loaded Gather results of opening shards concurrently, keeping track of how many databases we are managing. Check if any databases are running multiple index types. Enable all shards Close closes the store and all associated shards. After calling Close accessing shards through the Store will result in ErrStoreClosed being returned. No other goroutines accessing the store, so no need for a Lock. Close all the shards in parallel. Close out the series files. Store may now be opened again. epochsForShards returns a copy of the epoch trackers only including what is necessary for the provided shards. Must be called under the lock. openSeriesFile either returns or creates a series file for the provided database. It must be called under a full lock. createIndexIfNotExists returns a shared index for a database, if the inmem index is being used. If the TSI index is being used, then this method is basically a no-op. Shard returns a shard by id. Shards returns a list of shards by id. ShardGroup returns a ShardGroup with a list of shards by id. ShardN returns the number of shards in the store. ShardDigest returns a digest of the shard with the specified ID. CreateShard creates a shard with the given id and retention policy on a database. Shard already exists. Shard may be undergoing a pending deletion. While the shard can be recreated, it must wait for the pending delete to finish. Create the db and retention policy directories if they don't exist. Create the WAL directory. Retrieve database series file. Retrieve shared index, if needed. Copy index options and pass in shared index. CreateShardSnapShot will create a hard link to the underlying shard and return a path. The caller is responsible for cleaning up (removing) the file path returned. SetShardEnabled enables or disables a shard for read and writes. DeleteShards removes all shards from disk. DeleteShard removes a shard from disk. Remove the shard from Store so it's not returned to callers requesting shards. Also mark that this shard is currently being deleted in a separate map so that we do not have to retain the global store lock while deleting files. We are already being deleted? This is possible if delete shard was called twice in sequence before the shard could be removed from the mapping. This is not an error because deleting a shard twice is not an error. Determine if the shard contained any series that are not present in any other shards in the database. Ensure the pending deletion flag is cleared on exit. Get the shard's local bitset of series IDs. Remove any remaining series in the set from the series file, as they don't exist in any of the database's remaining shards. If the inmem index is in use, then the series being removed from the series file will also need to be removed from the index. Series key buffer. Buffer for tags container. Series File series key Close the shard. Remove the on-disk shard data. DeleteDatabase will close all shards associated with a database and remove the directory and files from disk. no files locally, so nothing to do Close series file. extra sanity check to make sure that even if someone named their database "../.." that we don't delete everything because of it, they'll just have extra files forever Remove database from store list of databases Remove shared index for database if using inmem index. DeleteRetentionPolicy will close all shards associated with the provided retention policy, remove the retention policy directories on both the DB and WAL, and remove all shard files from disk. unknown database, nothing to do Close and delete all shards under the retention policy on the Remove the retention policy folder. ensure Store's path is the grandparent of the retention policy Remove the retention policy folder from the the WAL. DeleteMeasurement removes a measurement and all associated series from a database. Limit to 1 delete for each shard since expanding the measurement into the list of series keys can be very memory intensive if run concurrently. install our guard and wait for any prior deletes to finish. the guard ensures future deletes that could conflict wait for us. filterShards returns a slice of shards where fn returns true for the shard. If the provided predicate is nil then all shards are returned. filterShards should be called under a lock. byDatabase provides a predicate for filterShards that matches on the name of the database passed in. walkShards apply a function to each shard in parallel. fn must be safe for concurrent use. If any of the functions return an error, the first error is ShardIDs returns a slice of all ShardIDs under management. shardsSlice returns an ordered list of shards. Databases returns the names of all databases managed by the store. DiskSize returns the size of all the shard files in bytes. This size does not include the WAL size. sketchesForDatabase returns merged sketches for the provided database, by walking each shard in the database and merging the sketches found there. Sketch estimating number of items. Sketch estimating number of tombstoned items. Never return nil sketches. In the case that db exists but no data written return empty sketches. Iterate over all shards for the database and combine all of the sketches. SeriesCardinality returns the exact series cardinality for the provided Cardinality is calculated exactly by unioning all shards' bitsets of series IDs. The result of this method cannot be combined with any other results. SeriesSketches returns the sketches associated with the series data in all the shards in the provided database. The returned sketches can be combined with other sketches to provide an estimation across distributed databases. MeasurementsCardinality returns an estimation of the measurement cardinality for the provided database. Cardinality is calculated using a sketch-based estimation. The result of this method cannot be combined with any other results. MeasurementsSketches returns the sketches associated with the measurement data in all the shards in the provided database. BackupShard will get the shard and have the engine backup since the passed in time to the writer. RestoreShard restores a backup from r to a given shard. This will only overwrite files included in the backup. ImportShard imports the contents of r to a given shard. All files in the backup are added as new files which may cause duplicated data to occur requiring more expensive compactions. ShardRelativePath will return the relative path to the shard, i.e., <database>/<retention>/<id>. DeleteSeries loops through the local shards and deletes the series data for the passed in series keys. Expand regex expressions in the FROM clause. Determine deletion time range. No series file means nothing has been written to this DB and thus nothing to delete. Determine list of measurements from sources. Use all measurements if no FROM clause was provided. Find matching series keys for each measurement. ExpandSources expands sources against all local shards. WriteToShard writes a list of points to a shard identified by its ID. enter the epoch tracker wait for any guards before writing the points. Ensure snapshot compactions are enabled since the shard might have been cold and disabled by the monitor. MeasurementNames returns a slice of all measurements. Measurements accepts an optional condition expression. If cond is nil, then all measurements for the database will be returned. Build indexset. MeasurementSeriesCounts returns the number of measurements and series in all the shards' indices. TODO: implement me TagKeys returns the tag keys in the given database, matching the condition. Get all the shards we're interested in. Determine list of measurements. Iterate over each measurement. Build keyset over all indexes for measurement. If no tag value filter is present then all the tag keys can be returned If they have authorized series associated with them. Add to resultset. Tag filter provided so filter keys first. Sort the tag keys. Filter against tag values, skip if no values exist. Filter final tag keys using the matching values. If a key has one or more matching values then it will be included in the final set. Use same backing array as keys to save allocation. Tag key k has one or more matching tag values. tagValues is a temporary representation of a TagValues. Rather than allocating KeyValues as we build up a TagValues object, We hold off allocating KeyValues until we have merged multiple tagValues together. Is a slice of tagValues that can be sorted by measurement. TagValues returns the tag keys and values for the provided shards, where the tag values satisfy the provided condition. Stores each list of TagValues for each measurement. Hint as to lower bound on number of measurements. names will be sorted by MeasurementNamesByExpr. Authorisation can be done later on, when series may have been filtered out by other conditions. Assuming all series in all shards. Iterate over each matching measurement in the shard. For each measurement we'll get the matching tag keys (e.g., when a WITH KEYS) statement is used, and we'll then use those to fetch all the relevant values from matching series. Series may be filtered using a WHERE Determine a list of keys from condition. No matching tag keys for this measurement Add the keys to the tagValues and sort them. get all the tag values for each key in the keyset. Each slice in the results contains the sorted values associated associated with each tag key for the measurement from the key set. remove any tag keys that didn't have any authorized values only include result if there are keys with values We need to sort all results by measurement name. The next stage is to merge the tagValue results for each shard's measurements. Used as a temporary buffer in mergeTagValues. There can be at most len(shards) instances of tagValues for a given measurement. Gather all occurrences of the same measurement for merging. An invariant is that there can't be more than n instances of tag key value pairs for a given measurement, where n is the number of shards. mergeTagValues merges multiple sorted sets of temporary tagValues using a direct k-way merge whilst also removing duplicated entries. The result is a single TagValue type. TODO(edd): a Tournament based merge (see: Knuth's TAOCP 5.4.1) might be more appropriate at some point. TODO(edd): will be too small likely. Find a hint? Resize and reset to the number of TagValues we're merging. Which of the provided TagValue sets currently holds the smallest element. j is the candidate we're going to next pick for the result set. Find the smallest element We have completely drained all tag keys and values for this shard. There are no tag values for these keys. We haven't picked a best TagValues set yet. Pick this one. It this tag key is lower than the candidate's tag key Same tag key but this tag value is lower than the candidate. Duplicate tag key/value pair.... Remove and move onto the next value for shard i. Drained all these tag values, move onto next key. We could have drained all of the TagValue sets and be done... Append the smallest KeyValue Increment the indexes for the chosen TagValue. No inmem shards... Only process 1 shard from each database inmem shards share the same index instance so just use the first one to avoid allocating the same measurements repeatedly Log at 80, 85, 90-100% levels KeyValue holds a string key and a string value. KeyValues is a sortable slice of KeyValue. Less implements sort.Interface. Keys are compared before values. decodeStorePath extracts the database and retention policy names from a given shard or WAL path. shardOrWALPath format: /maybe/absolute/base/then/:database/:retentionPolicy/:nameOfShardOrWAL Discard the last part of the path (the shard name or the wal name). Extract the database and retention policy. relativePath will expand out the full paths passed in and return the relative shard path from the store/Users/austinjaybecker/projects/Abeck-Go-testing/ui/assets/index.html/Users/austinjaybecker/projects/Abeck-Go-testing/ui/assets/Users/austinjaybecker/projects/Abeck-Go-testing/ui/Users/austinjaybecker/projects/Abeck-Go-testinghtml
  
    
    <%= htmlWebpackPlugin.options.body  %>
  react-rootdata-basepath<%= htmlWebpackPlugin.options.base %>
    <%= htmlWebpackPlugin.options.header  %>
  InfluxDB 2.0viewportwidth=device-width, initial-scale=1charsetutf-8/Users/austinjaybecker/projects/Abeck-Go-testing/ui/src/index.html/Users/austinjaybecker/projects/Abeck-Go-testing/ui/src./index.tsxshortcut icon../assets/images/favicon.icohttp-equivContent-typetext/html; charset=utf-8 width=device-width, initial-scale=1/Users/austinjaybecker/projects/abeck-go-testing/usage.gousage_write_request_count"usage_write_request_count"usage_write_request_bytes"usage_write_request_bytes"usage_values"usage_values"usage_series"usage_series"usage_query_request_count"usage_query_request_count"usage_query_request_bytes"usage_query_request_bytes" UsageMetric used to track classes of usage. UsageWriteRequestCount is the name of the metrics for tracking write request count. UsageWriteRequestBytes is the name of the metrics for tracking the number of write bytes. UsageValues is the name of the metrics for tracking the number of values. UsageSeries is the name of the metrics for tracking the number of series written. UsageQueryRequestCount is the name of the metrics for tracking query request count. UsageQueryRequestBytes is the name of the metrics for tracking the number of query bytes. Usage is a metric associated with the utilization of a particular resource. UsageService is a service for accessing usage statistics. UsageFilter is used to filter usage. Timespan represents a range of time./Users/austinjaybecker/projects/abeck-go-testing/user.goInvalid user status"Invalid user status"json:"oauthID,omitempty"`json:"oauthID,omitempty"` UserStatus indicates whether a user is active or inactive Valid validates user status User is a user. ð Valid validates user Ops for user errors and op log. UserService represents a service for managing user data. FindPermissionForUser UserUpdate represents updates to a user. Valid validates UserUpdate UserFilter represents a set of filter that restrict the returned results./Users/austinjaybecker/projects/abeck-go-testing/user_resource_mapping.gounknown user type"unknown user type"unknown mapping type"unknown mapping type"user id is required"user id is required" ErrInvalidUserType notes that the provided UserType is invalid ErrInvalidMappingType notes that the provided MappingType is invalid ErrUserIDRequired notes that the ID was not provided ErrResourceIDRequired notes that the provided ID was not provided UserType can either be owner or member. Owner can read and write to a resource Member can read from a resource. Valid checks if the UserType is a member of the UserType enum UserResourceMappingService maps the relationships between users and resources. UserResourceMapping represents a mapping of a resource to its user. UserResourceMappingFilter represents a set of filters that restrict the returned results. TODO: Uncomment these once the URM system is no longer being used for find lookups for: 	Telegraf 	DashBoard 	notification rule 	notification endpoint Permission{ 	Action: ReadAction, 	Resource: Resource{ 		Type: m.ResourceType, 		ID:   &m.ResourceID, 	}, }, 	Action: WriteAction, ToPermissions converts a user resource mapping into a set of permissions./Users/austinjaybecker/projects/abeck-go-testing/uuid/Users/austinjaybecker/projects/abeck-go-testing/uuid/uuid.goFromTimeTimeUUIDclockSeqhardwareAddrhwAddrFunctimeBaseifacesaTimeutcTimehexString1582OctoberHardwareAddrMTUifiMulticastAddrsInterfaces0x0F0x3F0123456789abcdef"0123456789abcdef" Copyright (c) 2012 The gocql Authors. All rights reserved.    * Redistributions of source code must retain the above copyright    * Redistributions in binary form must reproduce the above    * Neither the name of Google Inc. nor the names of its The uuid package can be used to generate and parse universally unique identifiers, a standardized format in the form of a 128 bit number. http://tools.ietf.org/html/rfc4122 Package uuid provides functions to create time-based UUIDs. import "github.com/influxdata/influxdb/uuid" UUID - unique identifier type representing a 128 bit number TimeUUID generates a new time based UUID (version 1) using the current time as the timestamp. FromTime generates a new time based UUID (version 1) as described in RFC 4122. This UUID contains the MAC address of the node that generated the UUID, the given timestamp and a sequence number. set version to 1 (time based uuid) clear variant set to IETF variant String returns the UUID in it's canonical form, a 32 digit hexadecimal number in the form of xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx./Users/austinjaybecker/projects/abeck-go-testing/v1/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/authorizer.goErrUnsupportedSchemeSaltBytespasswordBucketpredicateFuncuserErrunsupported authorization scheme"unsupported authorization scheme" A type that is used to verify credentials. A service to find V1 tokens A service to find V2 tokens A service to compare passwords for V1 tokens A service to find users Authorize returns an influxdb.Authorization if c can be verified; otherwise, an error. influxdb.ErrCredentialsUnauthorized will be returned if the credentials are invalid. this represents a programmer error check the user is still active/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/caching_password_service.gohashedauhashercrandsha256crypto/sha256"crypto/sha256" An implementation of influxdb.PasswordsService that will perform ComparePassword requests at a reduced cost under certain conditions. See ComparePassword for further information. The cache is only valid for the duration of the process. protects concurrent access to authCache ComparePassword will attempt to perform the comparison using a lower cost hashing function if influxdb.ContextHasPasswordCacheOption returns true for ctx. verify the password using the cached salt and hash fall through to requiring a full bcrypt hash for invalid passwords NOTE(sgc): This caching implementation was lifted from the 1.x source   https://github.com/influxdata/influxdb/blob/c1e11e732e145fc1a356535ddf3dcb9fb732a22b/services/meta/client.go#L390-L406 SaltBytes is the number of bytes used for salts. hashWithSalt returns a salted hash of password using salt. saltedHash returns a salt and salted hash of password./Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/error.go/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/http_client.go/private/legacy/authorizations Client connects to Influx via HTTP using tokens to manage authorizations SetPassword sets the password for the authorization token id./Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/http_server.go"/private/legacy/authorizations"/%s"/%s"/private/legacy/authorizations/%stoken required for v1 user authorization type"token required for v1 user authorization type"invalid authorization ID provided in route"invalid authorization ID provided in route" handlePostAuthorization is the HTTP handler for the POST prefixAuthorization route. handleGetAuthorizations is the HTTP handler for the GET prefixAuthorization route. handleDeleteAuthorization is the HTTP handler for the DELETE prefixAuthorization/:id route. password APIs handlePutPassword is the HTTP handler for the PUT /private/legacy/authorizations/:id/password/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/middleware_auth_password_service.go AuthedPasswordService is middleware for authorizing requests to the inner PasswordService. NewAuthedPasswordService wraps an existing PasswordService with authorization middleware./Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/mock_tenant.go/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/mocks/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/mocks/auth_finder.goMockAuthFinderMockAuthFinderMockRecorderMockAuthTokenFinderMockAuthTokenFinderMockRecorderMockPasswordComparerMockPasswordComparerMockRecorderMockPasswordServiceMockPasswordServiceMockRecorderMockUserFinderMockUserFinderMockRecorderNewMockAuthFinderNewMockAuthTokenFinderNewMockPasswordComparerNewMockPasswordServiceNewMockUserFinder Source: github.com/influxdata/influxdb/v2/v1/authorization (interfaces: AuthFinder) MockAuthFinder is a mock of AuthFinder interface MockAuthFinderMockRecorder is the mock recorder for MockAuthFinder NewMockAuthFinder creates a new mock instance FindAuthorizationByID mocks base method FindAuthorizationByID indicates an expected call of FindAuthorizationByID/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/mocks/auth_token_finder.go Source: github.com/influxdata/influxdb/v2/v1/authorization (interfaces: AuthTokenFinder) MockAuthTokenFinder is a mock of AuthTokenFinder interface MockAuthTokenFinderMockRecorder is the mock recorder for MockAuthTokenFinder NewMockAuthTokenFinder creates a new mock instance FindAuthorizationByToken mocks base method FindAuthorizationByToken indicates an expected call of FindAuthorizationByToken/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/mocks/password_comparer.go Source: github.com/influxdata/influxdb/v2/v1/authorization (interfaces: PasswordComparer) MockPasswordComparer is a mock of PasswordComparer interface MockPasswordComparerMockRecorder is the mock recorder for MockPasswordComparer NewMockPasswordComparer creates a new mock instance/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/mocks/password_service.go Source: github.com/influxdata/influxdb/v2/v1/authorization (interfaces: PasswordService) MockPasswordService is a mock of PasswordService interface MockPasswordServiceMockRecorder is the mock recorder for MockPasswordService NewMockPasswordService creates a new mock instance/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/mocks/user_finder.go Source: github.com/influxdata/influxdb/v2/v1/authorization (interfaces: UserFinder) MockUserFinder is a mock of UserFinder interface MockUserFinderMockRecorder is the mock recorder for MockUserFinder NewMockUserFinder creates a new mock instance FindUserByID mocks base method FindUserByID indicates an expected call of FindUserByID/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/service.go FindAuthorizations retrives all authorizations that match an arbitrary authorization filter./Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/service_password.goinvalid bcrypt hash"invalid bcrypt hash" SetPasswordHash updates the password hash for id. If passHash is not a valid bcrypt hash, SetPasswordHash returns an error. This API is intended for upgrading 1.x users. verify passHash is a valid bcrypt hash/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/storage.go/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/storage_authorization.go/Users/austinjaybecker/projects/abeck-go-testing/v1/authorization/storage_password.gounable to access password bucket"unable to access password bucket"/Users/austinjaybecker/projects/abeck-go-testing/v1/coordinator/Users/austinjaybecker/projects/abeck-go-testing/v1/coordinator/config.goDefaultMaxConcurrentQueriesDefaultMaxSelectPointNDefaultMaxSelectSeriesNDefaultWriteTimeoutErrDatabaseNameRequiredErrPartialWriteErrTimeoutErrWriteFailedLocalTSDBStoreNewShardMappingsgListstatPointWriteReqstatPointWriteReqLocalstatSubWriteDropstatSubWriteOKstatWriteDropstatWriteOKstatWriteTimeouttoml:"write-timeout"`toml:"write-timeout"`toml:"max-concurrent-queries"`toml:"max-concurrent-queries"`toml:"log-queries-after"`toml:"log-queries-after"`toml:"max-select-point"`toml:"max-select-point"`toml:"max-select-series"`toml:"max-select-series"`toml:"max-select-buckets"`toml:"max-select-buckets"`write-timeout"write-timeout"max-concurrent-queries"max-concurrent-queries"log-queries-after"log-queries-after"max-select-point"max-select-point"max-select-series"max-select-series"max-select-buckets"max-select-buckets" Package coordinator contains abstractions for writing points, executing statements, and accessing meta data. DefaultWriteTimeout is the default timeout for a complete write to succeed. DefaultMaxConcurrentQueries is the maximum number of running queries. A value of zero will make the maximum query limit unlimited. DefaultMaxSelectPointN is the maximum number of points a SELECT can process. A value of zero will make the maximum point count unlimited. DefaultMaxSelectSeriesN is the maximum number of series a SELECT can run. A value of zero will make the maximum series count unlimited. Config represents the configuration for the coordinator service. NewConfig returns an instance of Config with defaults.CoversShardGroupAt/Users/austinjaybecker/projects/abeck-go-testing/v1/coordinator/meta_client.go MetaClient is an interface for accessing meta data./Users/austinjaybecker/projects/abeck-go-testing/v1/coordinator/points_writer.goshardInfowpshardMappingsgithub.com/influxdata/influxdb/v2/v1"github.com/influxdata/influxdb/v2/v1""req"pointReq"pointReq"pointReqLocal"pointReqLocal"writeDrop"writeDrop"writeTimeout"writeTimeout""writeError"subWriteOk"subWriteOk"subWriteDrop"subWriteDrop"partial write"partial write"write failed"write failed"ErrRetentionPolicyNotFoundnil shard group"nil shard group"ShardGroupInfosshard %d is pending deletion"shard %d is pending deletion"points beyond retention policy"points beyond retention policy"NewTimerWrite failed"Write failed" The keys for statistics generated by the "write" module. ErrTimeout is returned when a write times out. ErrPartialWrite is returned when a write partially succeeds but does not meet the requested consistency level. ErrWriteFailed is returned when no writes succeeded. PointsWriter handles writes across multiple local and remote data nodes. WritePointsRequest represents a request to write point data to the cluster. AddPoint adds a point to the WritePointRequest with field key 'value' NewPointsWriter returns a new instance of PointsWriter for a node. ShardMapping contains a mapping of shards to points. The points associated with a shard ID The shards that have been mapped, keyed by shard ID Points that were dropped NewShardMapping creates an empty ShardMapping. MapPoint adds the point to the ShardMapping, associated with the given shardInfo. Open opens the communication channel with the point writer. Close closes the communication channel with the point writer. 'nil' channels always block so this makes the select statement in WritePoints hit its default case dropping any in-flight writes. WithLogger sets the Logger on w. WriteStatistics keeps statistics related to the PointsWriter. MapShards maps the points contained in wp to a ShardMapping.  If a point maps to a shard group or shard that does not currently exist, it will be created before returning the mapping. Holds all the shard groups and shards that are required for writes. Either the point is outside the scope of the RP, or we already have a suitable shard group for the point. No shard groups overlap with the point's time, so we will create a new shard group for this point. We didn't create a shard group because the point was outside the scope of the RP. sgList is a wrapper around a meta.ShardGroupInfos where we can also check if a given time is covered by any of the shard groups in the list. ShardGroupAt attempts to find a shard group that could contain a point at the given time. Shard groups are sorted first according to end time, and then according to start time. Therefore, if there are multiple shard groups that match this point's time they will be preferred in this order:  - a shard group with the earliest end time;  - (assuming identical end times) the shard group with the earliest start time. We couldn't find a shard group the point falls into. Append appends a shard group to the list, and returns a sorted list. WritePoints writes the data to the underlying storage. consistencyLevel and user are only used for clustered scenarios WritePointsPrivileged writes the data to the underlying storage, consistencyLevel is only used for clustered scenarios Write each shard in it's own goroutine and return as soon as one fails. Send points to subscriptions if possible. We need to lock just in case the channel is about to be nil'ed return timeout error to caller writeToShards writes points to a shard. Except tsdb.ErrShardNotFound no error can be handled here If we've written to shard that should exist on the current node, but the store has not actually created this shard, tell it to create it and retry the write/Users/austinjaybecker/projects/abeck-go-testing/v1/coordinator/shard_mapper.gofinding DBRP mappings: %v"finding DBRP mappings: %v"retention policy not found: %s"retention policy not found: %s"finding DBRP mappings: expected 1, found %d"finding DBRP mappings: expected 1, found %d" IteratorCreator is an interface that combines mapping fields and creating iterators. LocalShardMapper implements a ShardMapper for local shards. MapShards maps the sources to the appropriate shards into an IteratorCreator. Retrieve the list of shards for this database. This list of shards is always the same regardless of which measurement we are using. lookup bucket and create info ShardMapper maps data sources to a list of shard information. MinTime is the minimum time that this shard mapper will allow. Any attempt to use a time before this one will automatically result in using this time instead. MaxTime is the maximum time that this shard mapper will allow. Any attempt to use a time after this one will automatically result in using Override the time constraints if they don't match each other. Create a Measurement for each returned matching measurement value from the regex. Set the name to this matching regex value. Close clears out the list of mapped shards. Source contains the database and retention policy source for data./Users/austinjaybecker/projects/abeck-go-testing/v1/coordinator/statement_executor.goiterTimetotalTimeemittedseenDbssgisallGroupsdefaultRetentionPolicyALTER RETENTION POLICY"ALTER RETENTION POLICY"CREATE CONTINUOUS QUERY"CREATE CONTINUOUS QUERY"CREATE DATABASE"CREATE DATABASE"CREATE RETENTION POLICY"CREATE RETENTION POLICY"CREATE SUBSCRIPTION"CREATE SUBSCRIPTION"CREATE USER"CREATE USER"DROP CONTINUOUS QUERY"DROP CONTINUOUS QUERY"DROP DATABASE"DROP DATABASE"DROP SERIES"DROP SERIES"DROP RETENTION POLICY"DROP RETENTION POLICY"DROP SHARD"DROP SHARD"DROP SUBSCRIPTION"DROP SUBSCRIPTION"DROP USER"DROP USER"GRANT ALL"GRANT ALL"REVOKE ALL"REVOKE ALL"SHOW CONTINUOUS QUERIES"SHOW CONTINUOUS QUERIES"SHOW DIAGNOSTICS"SHOW DIAGNOSTICS"SHOW GRANTS"SHOW GRANTS"SHOW MEASUREMENT CARDINALITY"SHOW MEASUREMENT CARDINALITY"SHOW SERIES CARDINALITY"SHOW SERIES CARDINALITY"SHOW SHARDS"SHOW SHARDS"SHOW SHARD GROUPS"SHOW SHARD GROUPS"SHOW STATS"SHOW STATS"SHOW SUBSCRIPTIONS"SHOW SUBSCRIPTIONS""SHOW USERS"SET PASSWORD"SET PASSWORD"SHOW QUERIES"SHOW QUERIES"QUERY PLAN"QUERY PLAN"select"select"CLEANUPtotal_time"total_time"execution_time"execution_time"EXPLAIN ANALYZE"EXPLAIN ANALYZE"SELECT INTO"SELECT INTO"default retention policy not set for: %s"default retention policy not set for: %s"168h0m0s"168h0m0s"invalid measurement"invalid measurement" ErrDatabaseNameRequired is returned when executing statements that require a database, when a database has not been provided. StatementExecutor executes a statement in the query. TSDB storage for local node. ShardMapper for mapping shards when executing a SELECT statement. Select statement limits ExecuteStatement executes the given statement with the given execution context. Select statements are handled separately so that they can be streamed. Prepare the query for execution, but do not actually execute it. This should perform any needed substitutions. Generate a row emitter from the iterator set. Emit rows to the results channel. Check if the query was interrupted while emitting. close auxiliary iterators deterministically to finalize any captured measurements SELECT INTO is unsupported Send results or exit if closing. Always emit at least one result. Create a set of iterators from a selection. Convert "now()" to current time. Determine shard set based on database and time range. SHOW TAG KEYS returns all tag keys for the default retention policy. Determine appropriate time range. If one or fewer time boundaries provided then min/max possible time should be used instead. Get all shards for all retention policies. Ensure at least one result is emitted. SHOW TAG VALUES returns all tag values for the default retention policy. NormalizeStatement adds a default database and policy to the measurements in statement. Parameter defaultRetentionPolicy can be "". DB and RP not supported by these statements so don't rewrite into invalid statements Targets (measurements in an INTO clause) can have blank names, which means it will be the same as the measurement name it came from in the FROM clause. Measurement does not have an explicit database? Insert default. The database must now be specified by this point. TODO(sgc): Validate database; fetch default RP If no retention policy was specified, use the default. TSDBStore is an interface for accessing the time series data store. LocalTSDBStore embeds a tsdb.Store and implements IteratorCreator to satisfy the TSDBStore interface./Users/austinjaybecker/projects/abeck-go-testing/v1/errors.goIsAuthorizationErrorIsClientErrorLoadNodeNewNodenodeFileoldNodeFilepeersFilenameupgradeNodeFileAuthorizationFailed ErrFieldTypeConflict is returned when a new field already exists with a different type. ErrDatabaseNotFound indicates that a database operation failed on the specified database because the specified database does not exist. ErrRetentionPolicyNotFound indicates that the named retention policy could not be found in the database. IsAuthorizationError indicates whether an error is due to an authorization failure IsClientError indicates whether an error is a known client error./Users/austinjaybecker/projects/abeck-go-testing/v1/monitor/Users/austinjaybecker/projects/abeck-go-testing/v1/monitor/build_info.goDefaultStoreDatabaseDefaultStoreEnabledDefaultStoreIntervalMonitorRetentionPolicyMonitorRetentionPolicyDurationMonitorRetentionPolicyReplicaNRemoteWriterConfiggoRuntimenetwork"Version""Commit""Branch"Build Time"Build Time" build holds information of the build of the current executable.StoreEnabledStoreDatabaseStoreIntervalBuildTimeglobalTagsdiagRegistrationsstoreCreatedstoreEnabledstoreDatabasestoreRetentionPolicystoreIntervalwritePointsSetGlobalTagSetPointsWriterRegisterDiagnosticsClientDeregisterDiagnosticsClientgatherStatisticscreateInternalStoragewaitUntilIntervalstoreStatisticsValueNames/Users/austinjaybecker/projects/abeck-go-testing/v1/monitor/config.gotoml:"store-enabled"`toml:"store-enabled"`toml:"store-database"`toml:"store-database"`toml:"store-interval"`toml:"store-interval"`monitor store interval must be positive"monitor store interval must be positive"monitor store database name must not be empty"monitor store database name must not be empty"store-enabled"store-enabled"store-database"store-database"store-interval"store-interval" DefaultStoreEnabled is whether the system writes gathered information in an InfluxDB system for historical analysis. DefaultStoreDatabase is the name of the database where gathered information is written. DefaultStoreInterval is the period between storing gathered information. Config represents the configuration for the monitor service. Validate validates that the configuration is acceptable./Users/austinjaybecker/projects/abeck-go-testing/v1/monitor/diagnostics/Users/austinjaybecker/projects/abeck-go-testing/v1/monitor/diagnostics/diagnostics.goClientFuncNewDiagnosticssortedKeys Package diagnostics provides the diagnostics type so that other packages can provide diagnostics without depending on the monitor package. import "github.com/influxdata/influxdb/v2/v1/monitor/diagnostics" Client is the interface modules implement if they register diagnostics with monitor. The ClientFunc type is an adapter to allow the use of ordinary functions as Diagnostics clients. Diagnostics calls f(). Diagnostics represents a table of diagnostic information. The first value is the name of the columns, the second is a slice of interface slices containing the values for each column, by row. This information is never written to an InfluxDB system and is display-only. An example showing, say, connections follows:     source_ip    source_port       dest_ip     dest_port     182.1.0.2    2890              127.0.0.1   38901     174.33.1.2   2924              127.0.0.1   38902 NewDiagnostics initialises a new Diagnostics with the specified columns. AddRow appends the provided row to the Diagnostics' rows. RowFromMap returns a new one-row Diagnostics from a map. Display columns in deterministic order./Users/austinjaybecker/projects/abeck-go-testing/v1/monitor/go_runtime.go"GOARCH""GOOS""GOMAXPROCS" goRuntime captures Go runtime diagnostics./Users/austinjaybecker/projects/abeck-go-testing/v1/monitor/network.gohostname"hostname" network captures network diagnostics./Users/austinjaybecker/projects/abeck-go-testing/v1/monitor/reporter.go Reporter is an interface for gathering internal statistics. Statistics returns the statistics for the reporter, with the given tags merged into the result./Users/austinjaybecker/projects/abeck-go-testing/v1/monitor/service.gosubKVstatisticdiagsexpvar"expvar"Monitor is already open"Monitor is already open"Starting monitor service"Starting monitor service""network"failed to store statistics"failed to store statistics"Monitor is already closed"Monitor is already closed"Shutting down monitor service"Shutting down monitor service"Registered diagnostics client"Registered diagnostics client"memstats"memstats"cmdline"cmdline"keysMuaddKeyAddFloatMallocsFreesAllocTotalAllocLookupsHeapAllocHeapSysHeapIdleHeapInuseHeapReleasedHeapObjectsStackInuseStackSysMSpanInuseMSpanSysMCacheInuseMCacheSysBuckHashSysGCSysOtherSysNextGCLastGCPauseTotalNsPauseNsPauseEndNumGCNumForcedGCGCCPUFractionEnableGCDebugGCBySizeReadMemStats"Alloc""TotalAlloc""Sys""Lookups""Mallocs""Frees""HeapAlloc""HeapSys""HeapIdle"HeapInUse"HeapInUse""HeapReleased""HeapObjects""PauseTotalNs""NumGC"NumGoroutine"NumGoroutine"Failed to create storage"Failed to create storage"interrupted"interrupted"Storing statistics"Storing statistics"Failed to retrieve registered statistics"Failed to retrieve registered statistics"5000Dropping point"Dropping point"Terminating storage of statistics"Terminating storage of statistics" Package monitor provides a service and associated functionality for InfluxDB to self-monitor internal statistics and diagnostics. import "github.com/influxdata/influxdb/v2/v1/monitor" Policy constants. Name of the retention policy used by the monitor service. Duration of the monitor retention policy. Default replication factor to set on the monitor retention policy. Monitor represents an instance of the monitor system. Build information for diagnostics. Writer for pushing stats back into the database. PointsWriter is a simplified interface for writing the points the monitor gathers. New returns a new instance of the monitor system. open returns whether the monitor service is open. Open opens the monitoring system, using the given clusterID, node ID, and hostname for identification purpose. Self-register various stats and diagnostics. If enabled, record stats in a InfluxDB system. Start periodic writes to system. Enabled returns true if any underlying Config is Enabled. WritePoints writes the points the monitor gathers. Close closes the monitor system. SetGlobalTag can be used to set tags that will appear on all points written by the Monitor. RemoteWriterConfig represents the configuration of a remote writer. SetPointsWriter can be used to set a writer for the monitoring points. not enabled, nothing to do Subsequent calls to an already open Monitor are just a no-op. WithLogger sets the logger for the Monitor. RegisterDiagnosticsClient registers a diagnostics client with the given name and tags. DeregisterDiagnosticsClient deregisters a diagnostics client by name. Statistics returns the combined statistics for all expvar data. The given tags are added to each of the returned statistics. Skip built-in expvar stats. Add any supplied tags. Every other top-level expvar value should be a map. straight to string name. string-string tags map. string-interface map. If a registered client has no field data, don't include it in the results Add Go memstats. Add any supplied tags to Go memstats Diagnostics fetches diagnostic information for each registered diagnostic client. It skips any clients that return an error when retrieving their diagnostics. createInternalStorage ensures the internal storage has been created. Mark storage creation complete. waitUntilInterval waits until we are on an even interval for the duration. storeStatistics writes the statistics to an InfluxDB system. Wait until an even interval to start recording monitor statistics. If we are interrupted before the interval for some reason, exit early. Write all stats in batches Write the last batch Statistic represents the information returned by a single monitor client. ValueNames returns a sorted list of the value names, if any. Statistics is a slice of sortable statistics./Users/austinjaybecker/projects/abeck-go-testing/v1/monitor/system.goPID"PID"Getpid"currentTime" system captures system-level diagnostics./Users/austinjaybecker/projects/abeck-go-testing/v1/node.gotmpFileoldFilepeersnode.json"node.json"peers.json"peers.json"to upgrade a cluster, please contact support at influxdata"to upgrade a cluster, please contact support at influxdata" LoadNode will load the node information from disk if present Always check to see if we are upgrading first NewNode will return a new node Save will save the node file to disk and replace the existing one if present We shouldn't have an empty ID file, but if we do, ignore it/Users/austinjaybecker/projects/abeck-go-testing/v1/services/Users/austinjaybecker/projects/abeck-go-testing/v1/services/meta/Users/austinjaybecker/projects/abeck-go-testing/v1/services/meta/client.goDefaultLeaseDurationDefaultLoggingEnabledDefaultRetentionPolicyDurationDefaultRetentionPolicyInfoDefaultRetentionPolicyReplicaNErrAuthenticateErrAuthorizeErrContinuousQueryExistsErrContinuousQueryNotFoundErrDatabaseExistsErrDatabaseNotExistsErrIncompatibleDurationsErrInvalidNameErrInvalidSubscriptionURLErrNameTooLongErrReplicationFactorTooLowErrRetentionPolicyConflictErrRetentionPolicyDefaultErrRetentionPolicyDurationTooLowErrRetentionPolicyExistsErrRetentionPolicyNameExistsErrRetentionPolicyNameRequiredErrRetentionPolicyRequiredErrServiceErrServiceUnavailableErrShardGroupExistsErrShardGroupNotFoundErrShardNotReplicatedErrStoreOpenErrSubscriptionExistsErrSubscriptionNotFoundErrUserExistsErrUsernameRequiredLeasesMarshalTimeMaxNameLenMinRetentionPolicyDurationNewContextWithUserNewLeasesNewQueryAuthorizerNewWriteAuthorizerNodeInfosQueryAuthorizerShardGroupDeletedExpirationUnmarshalTimeUserFromContextValidNameWriteAuthorizerbcryptCostcreateShardGroupnormalisedShardDurationuserKeyvalidateURLuserInforemainingShardGroupsexpirationnextShardGroupTime-2-14-336-1209600000000000v1_tsm1_metadata"v1_tsm1_metadata"meta service unavailable"meta service unavailable"meta service error"meta service error"CreateDatabaseWithRetentionPolicy called with nil spec"CreateDatabaseWithRetentionPolicy called with nil spec"retention policy deleted after shard group created"retention policy deleted after shard group created"Shard group already exists"Shard group already exists"Failed to precreate successive shard group"Failed to precreate successive shard group"group_id"group_id"New shard group successfully precreated"New shard group successfully precreated"metaclient"metaclient" Package meta provides control over meta data for InfluxDB, such as controlling databases, retention policies, users, etc. Filename specifies the default name of the metadata file. ShardGroupDeletedExpiration is the amount of time before a shard group info will be removed from cached data after it has been marked deleted (2 weeks). Name of the bucket to store TSM metadata ErrServiceUnavailable is returned when the meta service is unavailable. ErrService is returned when the meta service returns an error. Client is used to execute commands on and read data from a meta service cluster. Authentication cache. NewClient returns a new *Client. Open a connection to a meta service cluster. Try to load from disk If this is a brand new instance, persist to disk immediately. Close the meta service cluster connection. AcquireLease attempts to acquire the specified lease. TODO corylanou remove this for single node ClusterID returns the ID of the cluster it's connected to. Database returns info for the requested database. Databases returns a list of all database infos. CreateDatabase creates a database or returns it if it already exists. create default retention policy CreateDatabaseWithRetentionPolicy creates a database with the specified retention policy. When creating a database with a retention policy, the retention policy will always be set to default. Therefore if the caller provides a retention policy that already exists on the database, but that retention policy is not the default one, an error will be returned. This call is only idempotent when the caller provides the exact same retention policy, and that retention policy is already the default for the No existing retention policies, so we can create the provided policy as the new default policy. In this case we already have a retention policy on the database and the provided retention policy does not match it. Therefore, this call is not idempotent and we need to return an error. If a non-default retention policy was passed in that already exists then it's an error regardless of if the exact same retention policy is provided. CREATE DATABASE WITH RETENTION POLICY should only be used to create DEFAULT retention policies. Commit the changes. Refresh the database info. DropDatabase deletes a database. CreateRetentionPolicy creates a retention policy on the specified database. RetentionPolicy returns the requested retention policy info. DropRetentionPolicy drops a retention policy from a database. UpdateRetentionPolicy updates a retention policy. Users returns a slice of UserInfo representing the currently known users. User returns the user with the given name, or ErrUserNotFound. bcryptCost is the cost associated with generating password with bcrypt. This setting is lowered during testing to improve test suite performance. CreateUser adds a user with the given name and password and admin status. See if the user already exists. Hash the password before serializing it. UpdateUser updates the password of an existing user. DropUser removes the user with the given name. SetPrivilege sets a privilege for the given user on the given database. SetAdminPrivilege sets or unsets admin privilege to the given username. UserPrivileges returns the privileges for a user mapped by database name. UserPrivilege returns the privilege for the given user on the given database. AdminUserExists returns true if any user has admin privilege. Authenticate returns a UserInfo if the username and password match an existing entry. Find user. Check the local auth cache first. Compare password with user hash. generate a salt and hash of the password for the cache UserCount returns the number of users stored. ShardIDs returns a list of all shard ids. ShardGroupsByTimeRange returns a list of all shard groups on a database and policy that may contain data for the specified time range. Shard groups are sorted by start time. Find retention policy. ShardsByTimeRange returns a slice of shards that may contain data in the time range. DropShard deletes a shard by ID. TruncateShardGroups truncates any shard group that could contain timestamps beyond t. PruneShardGroups remove deleted shard groups from the data store. CreateShardGroupWithShards creates a shard group on a database and policy for a given timestamp and assign shards to the shard group Check under a read-lock Check again under the write lock It is the responsibility of the caller to check if it exists before calling this method. DeleteShardGroup removes a shard group from a database and retention policy by id. PrecreateShardGroups creates shard groups whose endtime is before the 'to' time passed in, but is yet to expire before 'from'. This is to avoid the need for these shards to be created when data for the corresponding time range arrives. Shard creation involves Raft consensus, and precreation avoids taking the hit at write-time. No data was ever written to this group, or all groups have been deleted. Get the last group in time. Group is not deleted, will end before the future time, but is still yet to expire. This last check is important, so the system doesn't create shards groups wholly in the past. Create successive shard group. if it already exists, continue ShardOwner returns the owning shard group info for a specific shard. CreateContinuousQuery saves a continuous query with the given name for the given database. DropContinuousQuery removes the continuous query with the given name on the given database. CreateSubscription creates a subscription against the given database and retention policy. DropSubscription removes the named subscription from the given database and retention policy. SetData overwrites the underlying data in the meta store. Data returns a clone of the underlying data in the meta store. WaitForDataChanged returns a channel that will get closed when the metastore data has changed. commit writes data to the underlying store. This method assumes c's mutex is already locked. try to write to disk before updating in memory update in memory close channels to signal changes MarshalBinary returns a binary representation of the underlying data. WithLogger sets the logger for the client. snapshot saves the current meta data to disk. Load loads the current meta data from disk.leases/Users/austinjaybecker/projects/abeck-go-testing/v1/services/meta/config.gotoml:"retention-autocreate"`toml:"retention-autocreate"`toml:"logging-enabled"`toml:"logging-enabled"`Meta.Dir must be specified"Meta.Dir must be specified" DefaultLeaseDuration is the default duration for leases. DefaultLoggingEnabled determines if log messages are printed for the meta service. Config represents the meta configuration. NewConfig builds a new configuration with default values. Validate returns an error if the config is invalid./Users/austinjaybecker/projects/abeck-go-testing/v1/services/meta/context.go NewContextWithUser returns a new context with user added. UserFromContext returns the User associated with ctx or nil if no user has been assigned./Users/austinjaybecker/projects/abeck-go-testing/v1/services/meta/data.gosidxsgidxrpidxdbidxwasAdminbackupDBNamebackupRPNamenewDBsrestoreDBNamerestoreRPNamerpImportrpPtrsgImportdbImportdbPtrinfossgDurationsgdiEndjEndnodeIDprivilegegithub.com/influxdata/influxdb/v2/v1/services/meta/internal"github.com/influxdata/influxdb/v2/v1/services/meta/internal"udp"udp"SplitHostPortNewPrivilegeimported metadata does not have datbase named %s"imported metadata does not have datbase named %s"database already exists"database already exists"retention Policy not found in meta backup: %s.%s"retention Policy not found in meta backup: %s.%s"432015552000000000000172800000000000json:"expiration"`json:"expiration"`json:"owner"`json:"owner"`another node has the lease"another node has the lease" DefaultRetentionPolicyReplicaN is the default value of RetentionPolicyInfo.ReplicaN. DefaultRetentionPolicyDuration is the default value of RetentionPolicyInfo.Duration. DefaultRetentionPolicyName is the default name for auto generated retention policies. MinRetentionPolicyDuration represents the minimum duration for a policy. MaxNameLen is the maximum length of a database or retention policy name. InfluxDB uses the name for the directory name on disk. Data represents the top level collection of all metadata. associated raft term associated raft index adminUserExists provides a constant time mechanism for determining if there is at least one admin user. Database returns a DatabaseInfo by the database name. CloneDatabases returns a copy of the DatabaseInfo. CreateDatabase creates a new database. It returns an error if name is blank or if a database with the same name already exists. Append new node. DropDatabase removes a database by name. It does not return an error if the database cannot be found. Remove all user privileges associated with this database. RetentionPolicy returns a retention policy for a database by name. CreateRetentionPolicy creates a new retention policy on a database. It returns an error if name is blank or if the database does not exist. Validate retention policy. Normalise ShardDuration before comparing to any existing retention policies. The client is supposed to do this, but do it again to verify input. Find database. RP with that name already exists. Make sure they're the same. if they want to make it default, and it's not the default, it's not an identical command so it's an error Append copy of new policy. Set the default if needed DropRetentionPolicy removes a retention policy from a database by name. no database? no problem Remove from list. RetentionPolicyUpdate represents retention policy fields to be updated. SetName sets the RetentionPolicyUpdate.Name. SetDuration sets the RetentionPolicyUpdate.Duration. SetReplicaN sets the RetentionPolicyUpdate.ReplicaN. SetShardGroupDuration sets the RetentionPolicyUpdate.ShardGroupDuration. UpdateRetentionPolicy updates an existing retention policy. Find policy. Ensure new policy doesn't match an existing policy. Enforce duration of at least MinRetentionPolicyDuration Enforce duration is at least the shard duration Update fields. DropShard removes a shard by ID. DropShard won't return an error if the shard can't be found, which allows the command to be re-run in the case that the meta store succeeds but a data node fails. We just deleted the last shard in the shard group. ShardGroups returns a list of all shard groups on a database and retention policy. ShardGroupByTimestamp returns the shard group on a database and policy for a given timestamp. CreateShardGroup creates a shard group on a database and policy for a given timestamp. Verify that shard group doesn't already exist for this timestamp. Create the shard group. Shard group range is [start, end) so add one to the max time. Retention policy has a new shard group, so update the policy. Shard Groups must be stored in sorted order, as other parts of the system assume this to be the case. Find shard group by ID and set its deletion timestamp. CreateContinuousQuery adds a named continuous query to a database. Ensure the name doesn't already exist. If the query string is the same, we'll silently return, otherwise we'll assume the user might be trying to overwrite an existing CQ with a different query.lint:ignore SA6005 this is old code so we should revisit the use of strings.EqualFold Append new query. DropContinuousQuery removes a continuous query. validateURL returns an error if the URL does not have a port or uses a scheme other than UDP or HTTP. CreateSubscription adds a named subscription to a database and retention policy. DropSubscription removes a subscription. User returns a user by username. prevent non-nil interface with nil pointer CreateUser creates a new user. Ensure the user doesn't already exist. Append new user. We know there is now at least one admin user. DropUser removes an existing user by name. Maybe we dropped the only admin user? UpdateUser updates the password hash of an existing user. CloneUsers returns a copy of the user infos. SetPrivilege sets a privilege for a user on a database. SetAdminPrivilege sets the admin privilege for a user. We could have promoted or revoked the only admin. Check if an admin user exists. AdminUserExists returns true if an admin user exists. UserPrivileges gets the privileges for a user. UserPrivilege gets the privilege for a user on a database. Clone returns a copy of data with a new version. marshal serializes data to a protobuf representation. Need this for reverse compatibility unmarshal deserializes from a protobuf representation. Exhaustively determine if there is an admin user. The marshalled cache value may not be correct. MarshalBinary encodes the metadata to a binary format. UnmarshalBinary decodes the object from a binary format. future shardgroup hasAdminUser exhaustively checks for the presence of at least one admin user. ImportData imports selected data into the current metadata. if non-empty, backupDBName, restoreDBName, backupRPName, restoreRPName can be used to select DB metadata from other, and to assign a new name to the imported data.  Returns a map of shard ID's in the old metadata to new shard ID's in the new metadata, along with a list of new databases created, both of which can assist in the import of existing shard data during a database restore. if no backupDBName then we'll try to import all the DB's.  If one of them fails, we'll mark the whole operation a failure and return an error. importOneDB imports a single database/rp from an external metadata object, renaming them if new names are provided. change the names if we want/need to import all RP's without renaming renumber the shard groups and shards for the new retention policy(ies) OSS doesn't use Owners but if we are importing this from Enterprise, we'll want to clear it out to avoid any issues if they ever export this DB again to bring back to Enterprise. NodeInfo represents information about a single node in the cluster. NodeInfos is a slice of NodeInfo used for sorting DatabaseInfo represents information about a database in the system. RetentionPolicy returns a retention policy by name. ShardInfos returns a list of all shards' info for the database. Skip deleted shard groups clone returns a deep copy of di. Copy continuous queries. MarshalBinary encodes dbi to a binary format. UnmarshalBinary decodes dbi from a binary format. marshal serializes to a protobuf representation. RetentionPolicySpec represents the specification for a new retention policy. NewRetentionPolicyInfo creates a new retention policy info from the specification. Matches checks if this retention policy specification matches an existing retention policy. Normalise ShardDuration before comparing to any existing retention policies. Normalize with the retention policy info's duration instead of the spec since they should be the same and we're performing a comparison. MarshalBinary encodes RetentionPolicySpec to a binary format. UnmarshalBinary decodes RetentionPolicySpec from a binary format. RetentionPolicyInfo represents metadata about a retention policy. NewRetentionPolicyInfo returns a new instance of RetentionPolicyInfo with default replication and duration. DefaultRetentionPolicyInfo returns a new instance of RetentionPolicyInfo with default name, replication, and duration. ToSpec returns RetentionPolicySpec instance with the same data as in RetentionPolicyInfo Apply applies a specification to the retention policy info. ShardGroupByTimestamp returns the shard group in the policy that contains the timestamp, or nil if no shard group matches. ExpiredShardGroups returns the Shard Groups which are considered expired, for the given time. DeletedShardGroups returns the Shard Groups which are marked as deleted. clone returns a deep copy of rpi. MarshalBinary encodes rpi to a binary format. UnmarshalBinary decodes rpi from a binary format. shardGroupDuration returns the default duration for a shard group based on a policy duration. 6 months or 0 2 days normalisedShardDuration returns normalised shard duration based on a policy duration. If it is zero, it likely wasn't specified, so we default to the shard group duration If it was specified, but it's less than the MinRetentionPolicyDuration, then normalize to the MinRetentionPolicyDuration ShardGroupInfo represents metadata about a shard group. The DeletedAt field is important because it makes it clear that a ShardGroup has been marked as deleted, and allow the system to be sure that a ShardGroup is not simply missing. If the DeletedAt is set, the system can safely delete any associated shards. ShardGroupInfos implements sort.Interface on []ShardGroupInfo, based on the StartTime field. Contains returns true iif StartTime â¤ t < EndTime. Overlaps returns whether the shard group contains data for the time range between min and max Deleted returns whether this ShardGroup has been deleted. Truncated returns true if this ShardGroup has been truncated (no new writes). clone returns a deep copy of sgi. ShardFor returns the ShardInfo for a Point hash. ShardInfo represents metadata about a shard. OwnedBy determines whether the shard's owner IDs includes nodeID. clone returns a deep copy of si. If deprecated "OwnerIDs" exists then convert it to "Owners" format. SubscriptionInfo holds the subscription information. ShardOwner represents a node that owns a shard. clone returns a deep copy of so. ContinuousQueryInfo represents metadata about a continuous query. clone returns a deep copy of cqi. UserInfo represents metadata about a user in the system. User's name. Hashed password. Whether the user is an admin, i.e. allowed to do everything. Map of database name to granted privilege. AuthorizeDatabase returns true if the user is authorized for the given privilege on the given database. AuthorizeSeriesRead is used to limit access per-series (enterprise only) AuthorizeSeriesWrite is used to limit access per-series (enterprise only) AuthorizeUnrestricted allows admins to shortcut access checks. Lease represents a lease held on a resource. Leases is a concurrency-safe collection of leases keyed by name. NewLeases returns a new instance of Leases. Acquire acquires a lease with the given name for the given nodeID. If the lease doesn't exist or exists but is expired, a valid lease is returned. If nodeID already owns the named and unexpired lease, the lease expiration is extended. If a different node owns the lease, an error is returned. MarshalTime converts t to nanoseconds since epoch. A zero time returns 0. UnmarshalTime converts nanoseconds since epoch to time. A zero value returns a zero time. ValidName checks to see if the given name can would be valid for DB/RP name/Users/austinjaybecker/projects/abeck-go-testing/v1/services/meta/errors.gostore already open"store already open"raft store already closed"raft store already closed"database does not exist"database does not exist"name too long"name too long"invalid name"invalid name"retention policy already exists"retention policy already exists"retention policy not found"retention policy not found"retention policy is default"retention policy is default"retention policy required"retention policy required"retention policy name required"retention policy name required"retention policy name already exists"retention policy name already exists"retention policy duration must be at least %s"retention policy duration must be at least %s"retention policy conflicts with an existing policy"retention policy conflicts with an existing policy"retention policy duration must be greater than the shard duration"retention policy duration must be greater than the shard duration"replication factor must be greater than 0"replication factor must be greater than 0"shard group already exists"shard group already exists"shard group not found"shard group not found"shard not replicated"shard not replicated"continuous query already exists"continuous query already exists"continuous query not found"continuous query not found"subscription already exists"subscription already exists"subscription not found"subscription not found"invalid subscription URL: %s"invalid subscription URL: %s"username required"username required"authentication failed"authentication failed" ErrStoreOpen is returned when opening an already open store. ErrStoreClosed is returned when closing an already closed store. ErrDatabaseExists is returned when creating an already existing database. ErrDatabaseNotExists is returned when operating on a not existing database. ErrDatabaseNameRequired is returned when creating a database without a name. ErrNameTooLong is returned when attempting to create a database or retention policy with a name that is too long. ErrInvalidName is returned when attempting to create a database or retention policy with an invalid name ErrRetentionPolicyExists is returned when creating an already existing policy. ErrRetentionPolicyNotFound is returned when an expected policy wasn't found. ErrRetentionPolicyDefault is returned when attempting a prohibited operation on a default retention policy. ErrRetentionPolicyRequired is returned when a retention policy is required by an operation, but a nil policy was passed. ErrRetentionPolicyNameRequired is returned when creating a policy without a name. ErrRetentionPolicyNameExists is returned when renaming a policy to the same name as another existing policy. ErrRetentionPolicyDurationTooLow is returned when updating a retention policy that has a duration lower than the allowed minimum. ErrRetentionPolicyConflict is returned when creating a retention policy conflicts with an existing policy. ErrIncompatibleDurations is returned when creating or updating a retention policy that has a duration lower than the current shard duration. ErrReplicationFactorTooLow is returned when the replication factor is not in an acceptable range. ErrShardGroupExists is returned when creating an already existing shard group. ErrShardGroupNotFound is returned when mutating a shard group that doesn't exist. ErrShardNotReplicated is returned if the node requested to be dropped has the last copy of a shard present and the force keyword was not used ErrContinuousQueryExists is returned when creating an already existing continuous query. ErrContinuousQueryNotFound is returned when removing a continuous query that doesn't exist. ErrSubscriptionExists is returned when creating an already existing subscription. ErrSubscriptionNotFound is returned when removing a subscription that doesn't exist. ErrInvalidSubscriptionURL is returned when the subscription's destination URL is invalid. ErrUserExists is returned when creating an already existing user. ErrUserNotFound is returned when mutating a user that doesn't exist. ErrUsernameRequired is returned when creating a user without a username. ErrAuthenticate is returned when authentication fails./Users/austinjaybecker/projects/abeck-go-testing/v1/services/meta/filestore/Users/austinjaybecker/projects/abeck-go-testing/v1/services/meta/filestore/kv.go root directory where file will be stored the name of the bucket the name of the file region: kv.Bucket implementation close file handle before renaming to support Windows endregion/Users/austinjaybecker/projects/abeck-go-testing/v1/services/meta/internal/Users/austinjaybecker/projects/abeck-go-testing/v1/services/meta/internal/meta.pb.goCommand_CreateContinuousQueryCommandCommand_CreateDataNodeCommandCommand_CreateDatabaseCommandCommand_CreateMetaNodeCommandCommand_CreateNodeCommandCommand_CreateRetentionPolicyCommandCommand_CreateShardGroupCommandCommand_CreateSubscriptionCommandCommand_CreateUserCommandCommand_DeleteDataNodeCommandCommand_DeleteMetaNodeCommandCommand_DeleteNodeCommandCommand_DeleteShardGroupCommandCommand_DropContinuousQueryCommandCommand_DropDatabaseCommandCommand_DropRetentionPolicyCommandCommand_DropShardCommandCommand_DropSubscriptionCommandCommand_DropUserCommandCommand_RemovePeerCommandCommand_SetAdminPrivilegeCommandCommand_SetDataCommandCommand_SetDefaultRetentionPolicyCommandCommand_SetMetaNodeCommandCommand_SetPrivilegeCommandCommand_TypeCommand_Type_nameCommand_Type_valueCommand_UpdateDataNodeCommandCommand_UpdateNodeCommandCommand_UpdateRetentionPolicyCommandCommand_UpdateUserCommandCreateContinuousQueryCommandCreateDataNodeCommandCreateDatabaseCommandCreateMetaNodeCommandCreateNodeCommandCreateRetentionPolicyCommandCreateShardGroupCommandCreateSubscriptionCommandCreateUserCommandDeleteDataNodeCommandDeleteMetaNodeCommandDeleteNodeCommandDeleteShardGroupCommandDropContinuousQueryCommandDropDatabaseCommandDropRetentionPolicyCommandDropShardCommandDropSubscriptionCommandDropUserCommandE_CreateContinuousQueryCommand_CommandE_CreateDataNodeCommand_CommandE_CreateDatabaseCommand_CommandE_CreateMetaNodeCommand_CommandE_CreateNodeCommand_CommandE_CreateRetentionPolicyCommand_CommandE_CreateShardGroupCommand_CommandE_CreateSubscriptionCommand_CommandE_CreateUserCommand_CommandE_DeleteDataNodeCommand_CommandE_DeleteMetaNodeCommand_CommandE_DeleteNodeCommand_CommandE_DeleteShardGroupCommand_CommandE_DropContinuousQueryCommand_CommandE_DropDatabaseCommand_CommandE_DropRetentionPolicyCommand_CommandE_DropShardCommand_CommandE_DropSubscriptionCommand_CommandE_DropUserCommand_CommandE_RemovePeerCommand_CommandE_SetAdminPrivilegeCommand_CommandE_SetDataCommand_CommandE_SetDefaultRetentionPolicyCommand_CommandE_SetMetaNodeCommand_CommandE_SetPrivilegeCommand_CommandE_UpdateDataNodeCommand_CommandE_UpdateNodeCommand_CommandE_UpdateRetentionPolicyCommand_CommandE_UpdateUserCommand_CommandRemovePeerCommandSetAdminPrivilegeCommandSetDataCommandSetDefaultRetentionPolicyCommandSetMetaNodeCommandSetPrivilegeCommandUpdateDataNodeCommandUpdateNodeCommandUpdateRetentionPolicyCommandUpdateUserCommandextRange_CommandfileDescriptorMetaExtensionRangeArrayGetRandGetForceGetNewNamePolicyShardGroupIDGetShardGroupIDGetAddrGetHTTPAddrGetTCPAddrGetOK"CreateNodeCommand""DeleteNodeCommand""CreateDatabaseCommand""DropDatabaseCommand""CreateRetentionPolicyCommand""DropRetentionPolicyCommand""SetDefaultRetentionPolicyCommand""UpdateRetentionPolicyCommand""CreateShardGroupCommand""DeleteShardGroupCommand""CreateContinuousQueryCommand""DropContinuousQueryCommand""CreateUserCommand""DropUserCommand""UpdateUserCommand""SetPrivilegeCommand""SetDataCommand""SetAdminPrivilegeCommand""UpdateNodeCommand""CreateSubscriptionCommand""DropSubscriptionCommand""RemovePeerCommand""CreateMetaNodeCommand""CreateDataNodeCommand""UpdateDataNodeCommand""DeleteMetaNodeCommand""DeleteDataNodeCommand""SetMetaNodeCommand""DropShardCommand""Command_Type"protobuf:"varint,1,req,name=Term" json:"Term,omitempty"`protobuf:"varint,1,req,name=Term" json:"Term,omitempty"`protobuf:"varint,2,req,name=Index" json:"Index,omitempty"`protobuf:"varint,2,req,name=Index" json:"Index,omitempty"`protobuf:"varint,3,req,name=ClusterID" json:"ClusterID,omitempty"`protobuf:"varint,3,req,name=ClusterID" json:"ClusterID,omitempty"`protobuf:"bytes,4,rep,name=Nodes" json:"Nodes,omitempty"`protobuf:"bytes,4,rep,name=Nodes" json:"Nodes,omitempty"`protobuf:"bytes,5,rep,name=Databases" json:"Databases,omitempty"`protobuf:"bytes,5,rep,name=Databases" json:"Databases,omitempty"`protobuf:"bytes,6,rep,name=Users" json:"Users,omitempty"`protobuf:"bytes,6,rep,name=Users" json:"Users,omitempty"`protobuf:"varint,7,req,name=MaxNodeID" json:"MaxNodeID,omitempty"`protobuf:"varint,7,req,name=MaxNodeID" json:"MaxNodeID,omitempty"`protobuf:"varint,8,req,name=MaxShardGroupID" json:"MaxShardGroupID,omitempty"`protobuf:"varint,8,req,name=MaxShardGroupID" json:"MaxShardGroupID,omitempty"`protobuf:"varint,9,req,name=MaxShardID" json:"MaxShardID,omitempty"`protobuf:"varint,9,req,name=MaxShardID" json:"MaxShardID,omitempty"`protobuf:"bytes,10,rep,name=DataNodes" json:"DataNodes,omitempty"`protobuf:"bytes,10,rep,name=DataNodes" json:"DataNodes,omitempty"`protobuf:"bytes,11,rep,name=MetaNodes" json:"MetaNodes,omitempty"`protobuf:"bytes,11,rep,name=MetaNodes" json:"MetaNodes,omitempty"`protobuf:"varint,1,req,name=ID" json:"ID,omitempty"`protobuf:"varint,1,req,name=ID" json:"ID,omitempty"`protobuf:"bytes,2,req,name=Host" json:"Host,omitempty"`protobuf:"bytes,2,req,name=Host" json:"Host,omitempty"`protobuf:"bytes,3,opt,name=TCPHost" json:"TCPHost,omitempty"`protobuf:"bytes,3,opt,name=TCPHost" json:"TCPHost,omitempty"`protobuf:"bytes,2,req,name=DefaultRetentionPolicy" json:"DefaultRetentionPolicy,omitempty"`protobuf:"bytes,2,req,name=DefaultRetentionPolicy" json:"DefaultRetentionPolicy,omitempty"`protobuf:"bytes,3,rep,name=RetentionPolicies" json:"RetentionPolicies,omitempty"`protobuf:"bytes,3,rep,name=RetentionPolicies" json:"RetentionPolicies,omitempty"`protobuf:"bytes,4,rep,name=ContinuousQueries" json:"ContinuousQueries,omitempty"`protobuf:"bytes,4,rep,name=ContinuousQueries" json:"ContinuousQueries,omitempty"`protobuf:"bytes,1,opt,name=Name" json:"Name,omitempty"`protobuf:"bytes,1,opt,name=Name" json:"Name,omitempty"`protobuf:"varint,2,opt,name=Duration" json:"Duration,omitempty"`protobuf:"varint,2,opt,name=Duration" json:"Duration,omitempty"`protobuf:"varint,3,opt,name=ShardGroupDuration" json:"ShardGroupDuration,omitempty"`protobuf:"varint,3,opt,name=ShardGroupDuration" json:"ShardGroupDuration,omitempty"`protobuf:"varint,4,opt,name=ReplicaN" json:"ReplicaN,omitempty"`protobuf:"varint,4,opt,name=ReplicaN" json:"ReplicaN,omitempty"`protobuf:"varint,2,req,name=Duration" json:"Duration,omitempty"`protobuf:"varint,2,req,name=Duration" json:"Duration,omitempty"`protobuf:"varint,3,req,name=ShardGroupDuration" json:"ShardGroupDuration,omitempty"`protobuf:"varint,3,req,name=ShardGroupDuration" json:"ShardGroupDuration,omitempty"`protobuf:"varint,4,req,name=ReplicaN" json:"ReplicaN,omitempty"`protobuf:"varint,4,req,name=ReplicaN" json:"ReplicaN,omitempty"`protobuf:"bytes,5,rep,name=ShardGroups" json:"ShardGroups,omitempty"`protobuf:"bytes,5,rep,name=ShardGroups" json:"ShardGroups,omitempty"`protobuf:"bytes,6,rep,name=Subscriptions" json:"Subscriptions,omitempty"`protobuf:"bytes,6,rep,name=Subscriptions" json:"Subscriptions,omitempty"`protobuf:"varint,2,req,name=StartTime" json:"StartTime,omitempty"`protobuf:"varint,2,req,name=StartTime" json:"StartTime,omitempty"`protobuf:"varint,3,req,name=EndTime" json:"EndTime,omitempty"`protobuf:"varint,3,req,name=EndTime" json:"EndTime,omitempty"`protobuf:"varint,4,req,name=DeletedAt" json:"DeletedAt,omitempty"`protobuf:"varint,4,req,name=DeletedAt" json:"DeletedAt,omitempty"`protobuf:"bytes,5,rep,name=Shards" json:"Shards,omitempty"`protobuf:"bytes,5,rep,name=Shards" json:"Shards,omitempty"`protobuf:"varint,6,opt,name=TruncatedAt" json:"TruncatedAt,omitempty"`protobuf:"varint,6,opt,name=TruncatedAt" json:"TruncatedAt,omitempty"`protobuf:"varint,2,rep,name=OwnerIDs" json:"OwnerIDs,omitempty"`protobuf:"varint,2,rep,name=OwnerIDs" json:"OwnerIDs,omitempty"`protobuf:"bytes,3,rep,name=Owners" json:"Owners,omitempty"`protobuf:"bytes,3,rep,name=Owners" json:"Owners,omitempty"`protobuf:"bytes,2,req,name=Mode" json:"Mode,omitempty"`protobuf:"bytes,2,req,name=Mode" json:"Mode,omitempty"`protobuf:"bytes,3,rep,name=Destinations" json:"Destinations,omitempty"`protobuf:"bytes,3,rep,name=Destinations" json:"Destinations,omitempty"`protobuf:"varint,1,req,name=NodeID" json:"NodeID,omitempty"`protobuf:"varint,1,req,name=NodeID" json:"NodeID,omitempty"`protobuf:"bytes,2,req,name=Query" json:"Query,omitempty"`protobuf:"bytes,2,req,name=Query" json:"Query,omitempty"`protobuf:"bytes,2,req,name=Hash" json:"Hash,omitempty"`protobuf:"bytes,2,req,name=Hash" json:"Hash,omitempty"`protobuf:"varint,3,req,name=Admin" json:"Admin,omitempty"`protobuf:"varint,3,req,name=Admin" json:"Admin,omitempty"`protobuf:"bytes,4,rep,name=Privileges" json:"Privileges,omitempty"`protobuf:"bytes,4,rep,name=Privileges" json:"Privileges,omitempty"`protobuf:"bytes,1,req,name=Database" json:"Database,omitempty"`protobuf:"bytes,1,req,name=Database" json:"Database,omitempty"`protobuf:"varint,2,req,name=Privilege" json:"Privilege,omitempty"`protobuf:"varint,2,req,name=Privilege" json:"Privilege,omitempty"`protobuf:"varint,1,req,name=type,enum=meta.Command_Type" json:"type,omitempty"`protobuf:"varint,1,req,name=type,enum=meta.Command_Type" json:"type,omitempty"`536870911protobuf:"bytes,1,req,name=Host" json:"Host,omitempty"`protobuf:"bytes,1,req,name=Host" json:"Host,omitempty"`protobuf:"varint,2,req,name=Rand" json:"Rand,omitempty"`protobuf:"varint,2,req,name=Rand" json:"Rand,omitempty"`meta.CreateNodeCommand.command"meta.CreateNodeCommand.command"bytes,101,opt,name=command"bytes,101,opt,name=command"protobuf:"varint,2,req,name=Force" json:"Force,omitempty"`protobuf:"varint,2,req,name=Force" json:"Force,omitempty"`meta.DeleteNodeCommand.command"meta.DeleteNodeCommand.command"bytes,102,opt,name=command"bytes,102,opt,name=command"meta.CreateDatabaseCommand.command"meta.CreateDatabaseCommand.command"bytes,103,opt,name=command"bytes,103,opt,name=command"meta.DropDatabaseCommand.command"meta.DropDatabaseCommand.command"bytes,104,opt,name=command"bytes,104,opt,name=command"protobuf:"bytes,2,req,name=RetentionPolicy" json:"RetentionPolicy,omitempty"`protobuf:"bytes,2,req,name=RetentionPolicy" json:"RetentionPolicy,omitempty"`meta.CreateRetentionPolicyCommand.command"meta.CreateRetentionPolicyCommand.command"bytes,105,opt,name=command"bytes,105,opt,name=command"protobuf:"bytes,2,req,name=Name" json:"Name,omitempty"`protobuf:"bytes,2,req,name=Name" json:"Name,omitempty"`meta.DropRetentionPolicyCommand.command"meta.DropRetentionPolicyCommand.command"bytes,106,opt,name=command"bytes,106,opt,name=command"meta.SetDefaultRetentionPolicyCommand.command"meta.SetDefaultRetentionPolicyCommand.command"bytes,107,opt,name=command"bytes,107,opt,name=command"protobuf:"bytes,3,opt,name=NewName" json:"NewName,omitempty"`protobuf:"bytes,3,opt,name=NewName" json:"NewName,omitempty"`protobuf:"varint,4,opt,name=Duration" json:"Duration,omitempty"`protobuf:"varint,4,opt,name=Duration" json:"Duration,omitempty"`protobuf:"varint,5,opt,name=ReplicaN" json:"ReplicaN,omitempty"`protobuf:"varint,5,opt,name=ReplicaN" json:"ReplicaN,omitempty"`meta.UpdateRetentionPolicyCommand.command"meta.UpdateRetentionPolicyCommand.command"bytes,108,opt,name=command"bytes,108,opt,name=command"protobuf:"bytes,2,req,name=Policy" json:"Policy,omitempty"`protobuf:"bytes,2,req,name=Policy" json:"Policy,omitempty"`protobuf:"varint,3,req,name=Timestamp" json:"Timestamp,omitempty"`protobuf:"varint,3,req,name=Timestamp" json:"Timestamp,omitempty"`meta.CreateShardGroupCommand.command"meta.CreateShardGroupCommand.command"bytes,109,opt,name=command"bytes,109,opt,name=command"protobuf:"varint,3,req,name=ShardGroupID" json:"ShardGroupID,omitempty"`protobuf:"varint,3,req,name=ShardGroupID" json:"ShardGroupID,omitempty"`meta.DeleteShardGroupCommand.command"meta.DeleteShardGroupCommand.command"bytes,110,opt,name=command"bytes,110,opt,name=command"protobuf:"bytes,3,req,name=Query" json:"Query,omitempty"`protobuf:"bytes,3,req,name=Query" json:"Query,omitempty"`meta.CreateContinuousQueryCommand.command"meta.CreateContinuousQueryCommand.command"bytes,111,opt,name=command"bytes,111,opt,name=command"meta.DropContinuousQueryCommand.command"meta.DropContinuousQueryCommand.command"bytes,112,opt,name=command"bytes,112,opt,name=command"meta.CreateUserCommand.command"meta.CreateUserCommand.command"bytes,113,opt,name=command"bytes,113,opt,name=command"meta.DropUserCommand.command"meta.DropUserCommand.command"bytes,114,opt,name=command"bytes,114,opt,name=command"meta.UpdateUserCommand.command"meta.UpdateUserCommand.command"bytes,115,opt,name=command"bytes,115,opt,name=command"protobuf:"bytes,1,req,name=Username" json:"Username,omitempty"`protobuf:"bytes,1,req,name=Username" json:"Username,omitempty"`protobuf:"bytes,2,req,name=Database" json:"Database,omitempty"`protobuf:"bytes,2,req,name=Database" json:"Database,omitempty"`protobuf:"varint,3,req,name=Privilege" json:"Privilege,omitempty"`protobuf:"varint,3,req,name=Privilege" json:"Privilege,omitempty"`meta.SetPrivilegeCommand.command"meta.SetPrivilegeCommand.command"bytes,116,opt,name=command"bytes,116,opt,name=command"protobuf:"bytes,1,req,name=Data" json:"Data,omitempty"`protobuf:"bytes,1,req,name=Data" json:"Data,omitempty"`meta.SetDataCommand.command"meta.SetDataCommand.command"bytes,117,opt,name=command"bytes,117,opt,name=command"protobuf:"varint,2,req,name=Admin" json:"Admin,omitempty"`protobuf:"varint,2,req,name=Admin" json:"Admin,omitempty"`meta.SetAdminPrivilegeCommand.command"meta.SetAdminPrivilegeCommand.command"bytes,118,opt,name=command"bytes,118,opt,name=command"meta.UpdateNodeCommand.command"meta.UpdateNodeCommand.command"bytes,119,opt,name=command"bytes,119,opt,name=command"protobuf:"bytes,3,req,name=RetentionPolicy" json:"RetentionPolicy,omitempty"`protobuf:"bytes,3,req,name=RetentionPolicy" json:"RetentionPolicy,omitempty"`protobuf:"bytes,4,req,name=Mode" json:"Mode,omitempty"`protobuf:"bytes,4,req,name=Mode" json:"Mode,omitempty"`protobuf:"bytes,5,rep,name=Destinations" json:"Destinations,omitempty"`protobuf:"bytes,5,rep,name=Destinations" json:"Destinations,omitempty"`meta.CreateSubscriptionCommand.command"meta.CreateSubscriptionCommand.command"bytes,121,opt,name=command"bytes,121,opt,name=command"meta.DropSubscriptionCommand.command"meta.DropSubscriptionCommand.command"bytes,122,opt,name=command"bytes,122,opt,name=command"protobuf:"varint,1,opt,name=ID" json:"ID,omitempty"`protobuf:"varint,1,opt,name=ID" json:"ID,omitempty"`protobuf:"bytes,2,req,name=Addr" json:"Addr,omitempty"`protobuf:"bytes,2,req,name=Addr" json:"Addr,omitempty"`meta.RemovePeerCommand.command"meta.RemovePeerCommand.command"bytes,123,opt,name=command"bytes,123,opt,name=command"protobuf:"bytes,1,req,name=HTTPAddr" json:"HTTPAddr,omitempty"`protobuf:"bytes,1,req,name=HTTPAddr" json:"HTTPAddr,omitempty"`protobuf:"bytes,2,req,name=TCPAddr" json:"TCPAddr,omitempty"`protobuf:"bytes,2,req,name=TCPAddr" json:"TCPAddr,omitempty"`protobuf:"varint,3,req,name=Rand" json:"Rand,omitempty"`protobuf:"varint,3,req,name=Rand" json:"Rand,omitempty"`meta.CreateMetaNodeCommand.command"meta.CreateMetaNodeCommand.command"bytes,124,opt,name=command"bytes,124,opt,name=command"meta.CreateDataNodeCommand.command"meta.CreateDataNodeCommand.command"bytes,125,opt,name=command"bytes,125,opt,name=command"protobuf:"bytes,3,req,name=TCPHost" json:"TCPHost,omitempty"`protobuf:"bytes,3,req,name=TCPHost" json:"TCPHost,omitempty"`meta.UpdateDataNodeCommand.command"meta.UpdateDataNodeCommand.command"bytes,126,opt,name=command"bytes,126,opt,name=command"meta.DeleteMetaNodeCommand.command"meta.DeleteMetaNodeCommand.command"bytes,127,opt,name=command"bytes,127,opt,name=command"meta.DeleteDataNodeCommand.command"meta.DeleteDataNodeCommand.command"bytes,128,opt,name=command"bytes,128,opt,name=command"protobuf:"varint,1,req,name=OK" json:"OK,omitempty"`protobuf:"varint,1,req,name=OK" json:"OK,omitempty"`protobuf:"bytes,2,opt,name=Error" json:"Error,omitempty"`protobuf:"bytes,2,opt,name=Error" json:"Error,omitempty"`protobuf:"varint,3,opt,name=Index" json:"Index,omitempty"`protobuf:"varint,3,opt,name=Index" json:"Index,omitempty"`meta.SetMetaNodeCommand.command"meta.SetMetaNodeCommand.command"bytes,129,opt,name=command"bytes,129,opt,name=command"meta.DropShardCommand.command"meta.DropShardCommand.command"bytes,130,opt,name=command"bytes,130,opt,name=command"meta.Data"meta.Data"meta.NodeInfo"meta.NodeInfo"meta.DatabaseInfo"meta.DatabaseInfo"meta.RetentionPolicySpec"meta.RetentionPolicySpec"meta.RetentionPolicyInfo"meta.RetentionPolicyInfo"meta.ShardGroupInfo"meta.ShardGroupInfo"meta.ShardInfo"meta.ShardInfo"meta.SubscriptionInfo"meta.SubscriptionInfo"meta.ShardOwner"meta.ShardOwner"meta.ContinuousQueryInfo"meta.ContinuousQueryInfo"meta.UserInfo"meta.UserInfo"meta.UserPrivilege"meta.UserPrivilege"meta.Command"meta.Command"meta.CreateNodeCommand"meta.CreateNodeCommand"meta.DeleteNodeCommand"meta.DeleteNodeCommand"meta.CreateDatabaseCommand"meta.CreateDatabaseCommand"meta.DropDatabaseCommand"meta.DropDatabaseCommand"meta.CreateRetentionPolicyCommand"meta.CreateRetentionPolicyCommand"meta.DropRetentionPolicyCommand"meta.DropRetentionPolicyCommand"meta.SetDefaultRetentionPolicyCommand"meta.SetDefaultRetentionPolicyCommand"meta.UpdateRetentionPolicyCommand"meta.UpdateRetentionPolicyCommand"meta.CreateShardGroupCommand"meta.CreateShardGroupCommand"meta.DeleteShardGroupCommand"meta.DeleteShardGroupCommand"meta.CreateContinuousQueryCommand"meta.CreateContinuousQueryCommand"meta.DropContinuousQueryCommand"meta.DropContinuousQueryCommand"meta.CreateUserCommand"meta.CreateUserCommand"meta.DropUserCommand"meta.DropUserCommand"meta.UpdateUserCommand"meta.UpdateUserCommand"meta.SetPrivilegeCommand"meta.SetPrivilegeCommand"meta.SetDataCommand"meta.SetDataCommand"meta.SetAdminPrivilegeCommand"meta.SetAdminPrivilegeCommand"meta.UpdateNodeCommand"meta.UpdateNodeCommand"meta.CreateSubscriptionCommand"meta.CreateSubscriptionCommand"meta.DropSubscriptionCommand"meta.DropSubscriptionCommand"meta.RemovePeerCommand"meta.RemovePeerCommand"meta.CreateMetaNodeCommand"meta.CreateMetaNodeCommand"meta.CreateDataNodeCommand"meta.CreateDataNodeCommand"meta.UpdateDataNodeCommand"meta.UpdateDataNodeCommand"meta.DeleteMetaNodeCommand"meta.DeleteMetaNodeCommand"meta.DeleteDataNodeCommand"meta.DeleteDataNodeCommand"meta.Response"meta.Response"meta.SetMetaNodeCommand"meta.SetMetaNodeCommand"meta.DropShardCommand"meta.DropShardCommand"meta.Command_Type"meta.Command_Type"RegisterExtension Code generated by protoc-gen-gogo.
Package meta is a generated protocol buffer package.

It is generated from these files:
	internal/meta.proto

It has these top-level messages:
	Data
	NodeInfo
	DatabaseInfo
	RetentionPolicySpec
	RetentionPolicyInfo
	ShardGroupInfo
	ShardInfo
	SubscriptionInfo
	ShardOwner
	ContinuousQueryInfo
	UserInfo
	UserPrivilege
	Command
	CreateNodeCommand
	DeleteNodeCommand
	CreateDatabaseCommand
	DropDatabaseCommand
	CreateRetentionPolicyCommand
	DropRetentionPolicyCommand
	SetDefaultRetentionPolicyCommand
	UpdateRetentionPolicyCommand
	CreateShardGroupCommand
	DeleteShardGroupCommand
	CreateContinuousQueryCommand
	DropContinuousQueryCommand
	CreateUserCommand
	DropUserCommand
	UpdateUserCommand
	SetPrivilegeCommand
	SetDataCommand
	SetAdminPrivilegeCommand
	UpdateNodeCommand
	CreateSubscriptionCommand
	DropSubscriptionCommand
	RemovePeerCommand
	CreateMetaNodeCommand
	CreateDataNodeCommand
	UpdateDataNodeCommand
	DeleteMetaNodeCommand
	DeleteDataNodeCommand
	Response
	SetMetaNodeCommand
	DropShardCommand
 added for 0.10.0 This isn't used in >= 0.10.0. Kept around for upgrade purposes. Instead look at CreateDataNodeCommand and CreateMetaNodeCommand SetMetaNodeCommand is for the initial metanode in a cluster or if the single host restarts and its hostname changes, this will update it 1808 bytes of a gzipped FileDescriptorProto/Users/austinjaybecker/projects/abeck-go-testing/v1/services/meta/query_authorizer.goprivscreate admin user first or disable authentication"create admin user first or disable authentication"no user provided"no user provided"user %q, requires %s for database %q"user %q, requires %s for database %q"statement '%s', requires admin privilege"statement '%s', requires admin privilege"statement '%s', requires %s on %s"statement '%s', requires %s on %s"%s not authorized to execute %s"%s not authorized to execute %s" QueryAuthorizer determines whether a user is authorized to execute a given query. NewQueryAuthorizer returns a new instance of QueryAuthorizer. AuthorizeQuery authorizes u to execute q on database. Database can be "" for queries that do not require a database. If no user is provided it will return an error unless the query's first statement is to create a root user. Special case if no users exist. Ensure there is at least one statement. First statement in the query must create a user with admin privilege. Admin privilege allows the user to execute all statements. Check each statement in the query. Get the privileges required to execute the statement. Make sure the user has the privileges required to execute each statement. Admin privilege already checked so statement requiring admin privilege cannot be run. Use the db name specified by the statement or the db name passed by the caller if one wasn't specified by the statement. ErrAuthorize represents an authorization error. Error returns the text of the error./Users/austinjaybecker/projects/abeck-go-testing/v1/services/meta/write_authorizer.go%s not authorized to write to %s"%s not authorized to write to %s" WriteAuthorizer determines whether a user is authorized to write to a given database. NewWriteAuthorizer returns a new instance of WriteAuthorizer. AuthorizeWrite returns nil if the user has permission to write to the database./Users/austinjaybecker/projects/abeck-go-testing/v1/services/precreator/Users/austinjaybecker/projects/abeck-go-testing/v1/services/precreator/config.goDefaultAdvancePeriodDefaultCheckInterval1800000000000toml:"enabled"`toml:"enabled"`toml:"check-interval"`toml:"check-interval"`toml:"advance-period"`toml:"advance-period"`check-interval must be positive"check-interval must be positive"advance-period must be positive"advance-period must be positive""enabled"check-interval"check-interval"advance-period"advance-period" DefaultCheckInterval is the shard precreation check time if none is specified. DefaultAdvancePeriod is the default period ahead of the endtime of a shard group that its successor group is created. Config represents the configuration for shard precreation. NewConfig returns a new Config with defaults. Validate returns an error if the Config is invalid./Users/austinjaybecker/projects/abeck-go-testing/v1/services/precreator/service.goshard-precreation"shard-precreation"Starting precreation service"Starting precreation service"check_interval"check_interval"advance_period"advance_period"Failed to precreate shards"Failed to precreate shards"Terminating precreation service"Terminating precreation service" Package precreator provides the shard precreation service. import "github.com/influxdata/influxdb/v2/v1/services/precreator" Service manages the shard precreation service. NewService returns an instance of the precreation service. Open starts the precreation service. Close stops the precreation service. runPrecreation continually checks if resources need precreation. precreate performs actual resource precreation./Users/austinjaybecker/projects/abeck-go-testing/v1/services/retention/Users/austinjaybecker/projects/abeck-go-testing/v1/services/retention/config.go Config represents the configuration for the retention service./Users/austinjaybecker/projects/abeck-go-testing/v1/services/retention/service.godeletedShardIDsdeletionInforetryNeededStarting retention policy enforcement service"Starting retention policy enforcement service"Closing retention policy enforcement service"Closing retention policy enforcement service"Retention policy deletion check"Retention policy deletion check"retention_delete_check"retention_delete_check"Failed to delete shard group"Failed to delete shard group"Deleted shard group"Deleted shard group"Failed to delete shard"Failed to delete shard"Deleted shard"Deleted shard"Problem pruning shard groups"Problem pruning shard groups"One or more errors occurred during shard deletion and will be retried on the next check"One or more errors occurred during shard deletion and will be retried on the next check" Package retention provides the retention policy enforcement service. import "github.com/influxdata/influxdb/services/retention" Service represents the retention policy enforcement service. NewService returns a configured retention policy enforcement service. Open starts retention policy enforcement. Close stops retention policy enforcement. WithLogger sets the logger on the service. Mark down if an error occurred during this function so we can inform the user that we will try again on the next interval. Without the message, they may see the error message and assume they have to do it manually. Build list of already deleted shards. Determine all shards that have expired and need to be deleted. Store all the shard IDs that may possibly need to be removed locally. Remove shards if we store them locally/Users/austinjaybecker/projects/abeck-go-testing/v1/services/storage/Users/austinjaybecker/projects/abeck-go-testing/v1/services/storage/context.goErrIntOverflowSourceErrInvalidLengthSourceErrMissingReadSourceGetReadSourceHasFieldKeyOrValueHasSingleMeasurementNoORNewContextWithReadOptionsReadOptionsReadOptionsFromContextRewriteExprRemoveFieldKeyAndValuecursorHasDataencodeFixed32SourceencodeFixed64SourceencodeVarintSourcefieldKeyBytesfileDescriptorSourcegetReadSourcehasAnyTagKeysindexSeriesCursormeasurementKeymeasurementKeyBytesmeasurementRemapnewIndexSeriesCursornewIndexSeriesCursorInfluxQLPredreadOptionsKeyreadSourceskipSourcesovSourcesozSource ReadOptions are additional options that may be passed with context.Context to configure the behavior of a storage read request. NewContextWithRequestOptions returns a new Context with nodeID added. ReadOptionsFromContext returns the ReadOptions associated with the context or nil if no additional options have been specified.GetBucketIDsqrymeasurementCondhasFieldExprhasValueExpr/Users/austinjaybecker/projects/abeck-go-testing/v1/services/storage/gen.gogo:generate protoc -I$GOPATH/src/github.com/influxdata/influxdb/vendor -I. --gogofaster_out=. source.proto/Users/austinjaybecker/projects/abeck-go-testing/v1/services/storage/predicate_influxql.gofoundOnceinvalidOPlastMeasurement HasSingleMeasurementNoOR determines if an index optimisation is available. Typically the read service will use the query engine to retrieve all field keys for all measurements that match the expression, which can be very inefficient if it can be proved that only one measurement matches the expression. This condition is determined when the following is true:		* there is only one occurrence of the tag key `_measurement`.		* there are no OR operators in the expression tree.		* the operator for the `_measurement` binary expression is ==. Check that RHS is a literal string/Users/austinjaybecker/projects/abeck-go-testing/v1/services/storage/series_cursor.gofkeysmfkeyssingleMeasurementindex_cursor.create"index_cursor.create" Optimisation to check if request is only interested in results for a single measurement. In this case we can efficiently produce all known field keys from the collection of shards without having to go via the query engine. next series key c.nf may be nil if there are no fields TODO(sgc): lazily evaluate valueCond we've reduced the expression to "true"/Users/austinjaybecker/projects/abeck-go-testing/v1/services/storage/source.goprotobuf:"varint,1,opt,name=bucket_id,proto3"`protobuf:"varint,1,opt,name=bucket_id,proto3"`protobuf:"varint,2,opt,name=organization_id,proto3"`protobuf:"varint,2,opt,name=organization_id,proto3"`"readSource"readSource{}"readSource{}"UnmarshalAny this is easier than fooling around with .proto files./Users/austinjaybecker/projects/abeck-go-testing/v1/services/storage/source.pb.goprotobuf:"bytes,1,opt,name=database,proto3" json:"database,omitempty"`protobuf:"bytes,1,opt,name=database,proto3" json:"database,omitempty"`protobuf:"bytes,2,opt,name=retention_policy,json=retentionPolicy,proto3" json:"retention_policy,omitempty"`protobuf:"bytes,2,opt,name=retention_policy,json=retentionPolicy,proto3" json:"retention_policy,omitempty"`com.github.influxdata.influxdb.services.storage.ReadSource"com.github.influxdata.influxdb.services.storage.ReadSource"proto: ReadSource: wiretype end group for non-group"proto: ReadSource: wiretype end group for non-group"proto: ReadSource: illegal tag %d (wire type %d)"proto: ReadSource: illegal tag %d (wire type %d)"proto: wrong wireType = %d for field Database"proto: wrong wireType = %d for field Database"proto: wrong wireType = %d for field RetentionPolicy"proto: wrong wireType = %d for field RetentionPolicy"source.proto"source.proto" source: source.proto
	Package storage is a generated protocol buffer package.

	It is generated from these files:
		source.proto

	It has these top-level messages:
		ReadSource
 Database identifies which database to query. RetentionPolicy identifies which retention policy to query. 210 bytes of a gzipped FileDescriptorProto/Users/austinjaybecker/projects/abeck-go-testing/v1/services/storage/store.gonewCursormqAttrsinfluxqlPredhasFieldKeytagKeyExprfitriamissing ReadSource"missing ReadSource"missing read source"missing read source"no database"no database"invalid retention policy"invalid retention policy"field values unsupported"field values unsupported" getReadSource will attempt to unmarshal a ReadSource from the ReadRequest or return an error if no valid resource is present. TODO(jeff): this was a typed nil TODO(jsternberg): Use a real authorizer. Getting values of _measurement or _field are handled specially If there are any references to _field, we need to use the slow path since we cannot rely on the index alone. If there is a predicate on _field, we cannot use the index to filter out unwanted measurement names. Use a slower block scan instead. If there predicates on anything besides _measurement, we can't use the index and need to use the slow path. tagValuesSlow will determine the tag values for the given tagKey. It's generally faster to use tagValues, measurementFields or MeasurementNames, but those methods will only use the index and metadata stored in the shard. Because fields are not themselves indexed, we have no way of correlating fields to tag values, so we sometimes need to consult tsm to provide an accurate answer. no data for series key + field combination? It seems that even when there is no data for this series key + field combo that the cursor may be not nil. We need to request invoke an array cursor to be sure. This is the reason for the call to cursorHasData below./Users/austinjaybecker/projects/abeck-go-testing/variable.goinValidNamesvalidTypesvariableValuesprs"variable not found"json:"language"`json:"language"`missing variable name"missing variable name"^[a-zA-Z_].*`^[a-zA-Z_].*`variable name must start with a letter"variable name must start with a letter"invalid arguments type"invalid arguments type""and"import"import""not"return"return""option""test"package"package""builtin"%q is a protected variable name"%q is a protected variable name"no fields supplied in update"no fields supplied in update"error parsing %v as VariableConstantArguments"error parsing %v as VariableConstantArguments"expected variable constant value to be string but received %T"expected variable constant value to be string but received %T"error parsing %v as VariableMapArguments"error parsing %v as VariableMapArguments"expected variable map value to be string but received %T"expected variable map value to be string but received %T"error parsing %v as VariableQueryArguments"error parsing %v as VariableQueryArguments""query" key not present in VariableQueryArguments"\"query\" key not present in VariableQueryArguments"expected "query" to be string but received %T"expected \"query\" to be string but received %T""language" key not present in VariableQueryArguments"\"language\" key not present in VariableQueryArguments"expected "language" to be string but received %T"expected \"language\" to be string but received %T"unknown VariableArguments type %s"unknown VariableArguments type %s" ErrVariableNotFound is the error msg for a missing variable. ops for variable error. VariableService describes a service for managing Variables A Variable describes a keyword that can be expanded into several possible values when used in an InfluxQL or Flux query DefaultVariableFindOptions are the default find options for variables. VariableFilter represents a set of filter that restrict the returned results. QueryParams implements PagingFilter. It converts VariableFilter fields to url query params. A VariableUpdate describes a set of changes that can be applied to a Variable A VariableArguments contains arguments used when expanding a Variable "constant", "map", or "query" either VariableQueryValues, VariableConstantValues, VariableMapValues VariableQueryValues contains a query used when expanding a query-based Variable "influxql" or "flux" VariableConstantValues are the data for expanding a constants-based Variable VariableMapValues are the data for expanding a map-based Variable Valid returns an error if a Variable contains invalid data todo(leodido) > check it org ID validity? variable name must start with a letter to be a valid identifier in Flux Valid returns an error if a Variable changeset is not valid Apply applies non-zero fields from a VariableUpdate to a Variable UnmarshalJSON unmarshals json into a VariableArguments struct, using the `Type` field to assign the approriate struct to the `Values` field Decode the polymorphic VariableArguments.Values field into the appropriate struct/Users/austinjaybecker/projects/abeck-go-testing/vault/Users/austinjaybecker/projects/abeck-go-testing/vault/secret.goWithTLSConfigverapiCFGtlsCFGcfgOptsexplicitConfig"github.com/hashicorp/vault/api"/secret/data/%s"/secret/data/%s"value found in secret data is not map[string]interface{}"value found in secret data is not map[string]interface{}"value found in secret metadata is not map[string]interface{}"value found in secret metadata is not map[string]interface{}"version provided is not a valid integer: %v"version provided is not a valid integer: %v"version provided is %T not a string or int"version provided is %T not a string or int""options"cas"cas" SecretService is service for storing user secrets Config may setup the vault client configuration. If any field is a zero value, it will be ignored and the default used. TLSConfig is the configuration for TLS. ConfigOptFn is a functional input option to configure a vault service. WithConfig provides a configuration to the service constructor. WithTLSConfig allows one to set the TLS config only. NewSecretService creates an instance of a SecretService. The service is configured using the standard vault environment variables. loadSecrets retrieves a map of secrets for an organization and the version of the secrets retrieved. The version is used to ensure that concurrent updates will not overwrite one another. TODO(desa): update url construction putSecrets will set all provided data values for the organization orgID. If version is negative, the write will overwrite all specified values. If version is 0, the write will only be allowed if the keys do not exists. If version is non-zero, the write will only be allowed if the keys current version in vault matches the version specified./Users/austinjaybecker/projects/abeck-go-testing/write/Users/austinjaybecker/projects/abeck-go-testing/write/batcher.goDefaultIntervalErrLineTooLongwriteFnmaxLineLength500000batcher: line too long"batcher: line too long"destination write service required"destination write service required"MaxScanTokenSizeErrTooLong DefaultMaxBytes is 500KB; this is typically 250 to 500 lines. DefaultInterval will flush every 10 seconds. ErrLineTooLong is the error returned when reading a line that exceeds MaxLineLength. batcher is a write service that batches for another write service. Batcher batches line protocol for sends to output. MaxFlushBytes is the maximum number of bytes to buffer before flushing MaxFlushInterval is the maximum amount of time to wait before flushing MaxLineLength specifies the maximum length of a single line Service receives batches flushed from Batcher. Write reads r in batches and writes to a target specified by org and bucket. WriteTo reads r in batches and writes to a target specified by filter. we loop twice to check if both read and write have an error. if read exits cleanly, then we still want to wait for write. only if there is any error, exit immediately. read will close the line channel when there is no more data, or an error occurs. it is possible for an io.Reader to block forever; Write's context can be used to cancel, but, it's possible there will be dangling read go routines. exit early if the context is done finishes when the lines channel is closed or context is done. if an error occurs while writing data to the write service, the error is send in the errC channel and the function returns. if read closes the channel normally, exit the loop write if we exceed the max lines OR read routine has finished ScanLines is used in bufio.Scanner.Split to split lines of line protocol. We have a full newline-terminated line. If we're at EOF, we have a final, non-terminated line. Return it. Request more data./Users/austinjaybecker/projects/abeck-go-testing/write.go/Users/austinjaybecker/projects/abeck-go-testing/zap/Users/austinjaybecker/projects/abeck-go-testing/zap/auth_service.goconvertFieldextractTextMapReaderinjectTextMapWriterlogChildOfKeylogDurationKeylogFollowsFromKeylogSpanIDKeylogStartKeylogStopKeylogTraceIDKeynewSpanContexttraceHTTPHeaderError finding authorization by id"Error finding authorization by id"Error finding authorization by token"Error finding authorization by token"Error finding authorizations"Error finding authorizations"Error creating authorization"Error creating authorization"Error deleting authorization"Error deleting authorization"Error updating authorization"Error updating authorization" FindAuthorizationByID returns an authorization given an id, and logs any errors. FindAuthorizationByToken returns an authorization given a token, and logs any errors. FindAuthorizations returns authorizations given a filter, and logs any errors. CreateAuthorization creates an authorization, and logs any errors. DeleteAuthorization deletes an authorization, and logs any errors. UpdateAuthorization updates an authorization's status, description and logs any errors.TextMapReaderTextMapWriteropName/Users/austinjaybecker/projects/abeck-go-testing/zap/proxy_query_service.go"Query" ProxyQueryService logs the request but does not write to the writer. NewProxyQueryService creates a new proxy query service with a log. If the logger is nil, then it will use a noop logger. Query logs the query request./Users/austinjaybecker/projects/abeck-go-testing/zap/tracer.gorefCtxstartOptscarrierrestrictedKeypayloadZap-Trace-Span"Zap-Trace-Span"ot_span_id"ot_span_id"ot_start"ot_start"ot_stop"ot_stop"ot_duration"ot_duration"ot_child_of"ot_child_of"ot_follows_from"ot_follows_from"unsupported span context %T"unsupported span context %T"carrier must be an io.Writer for binary format, got %T"carrier must be an io.Writer for binary format, got %T"TextMapcarrier must be an opentracing.TextMapWriter for text map format, got %T"carrier must be an opentracing.TextMapWriter for text map format, got %T"carrier must be an opentracing.TextMapWriter for http header format, got %T"carrier must be an opentracing.TextMapWriter for http header format, got %T"unsupported format %v"unsupported format %v"carrier must be an io.Reader for binary format, got %T"carrier must be an io.Reader for binary format, got %T"carrier must be an opentracing.TextMapReader for text map format, got %T"carrier must be an opentracing.TextMapReader for text map format, got %T"carrier must be an opentracing.TextMapReader for http header format, got %T"carrier must be an opentracing.TextMapReader for http header format, got %T"no span ID found in carrier"no span ID found in carrier"CanonicalHeaderKeyChildOfRefFollowsFromRefnon-even keyValues len: %v"non-even keyValues len: %v"InterleavedKVToFields"LogKV"use of deprecated LogEvent: not implemented"use of deprecated LogEvent: not implemented"use of deprecated LogEventWithPayload: not implemented"use of deprecated LogEventWithPayload: not implemented"use of deprecated Log: not implemented"use of deprecated Log: not implemented"Baggagejson:"trace_id"`json:"trace_id"`json:"span_id"`json:"span_id"`json:"baggage"`json:"baggage"` Tracer implements opentracing.Tracer and logs each span as its own log. Span implements opentracing.Span, all Spans must be created using  the Tracer. LogEvent is deprecated, as such it is not implemented. LogEventWithPayload is deprecated, as such it is not implemented. Log is deprecated, as such it is not implemented. SpanContext implements opentracing.SpanContext, all span contexts must be created using the Tracer.ErrFieldTooLongErrInsecurePathErrWriteAfterCloseErrWriteTooLongFormatGNUFormatPAXFormatUSTARFormatUnknownTypeBlockTypeCharTypeContTypeFifoTypeGNULongLinkTypeGNULongNameTypeGNUSparseTypeLinkTypeRegTypeRegATypeSymlinkTypeXGlobalHeaderTypeXHeaderalignSparseEntriesbasicKeysblockPaddingblockSizec_ISBLKc_ISCHRc_ISDIRc_ISFIFOc_ISGIDc_ISLNKc_ISREGc_ISSOCKc_ISUIDc_ISVTXensureEOFerrMissDataerrUnrefDataerrWriteHolefitsInBase256fitsInOctalformatMaxformatNamesformatPAXRecordformatPAXTimeformatSTARformatV7groupMaphasNULheaderErrorheaderFileInfoinvertSparseEntriesisASCIIisHeaderOnlyTypemagicGNUmagicUSTARmaxSpecialFileSizemergePAXmustReadFullnameSizeparsePAXparsePAXRecordparsePAXTimepaxAtimepaxCharsetpaxCommentpaxCtimepaxGNUSparsepaxGNUSparseMajorpaxGNUSparseMappaxGNUSparseMinorpaxGNUSparseNamepaxGNUSparseNumBlockspaxGNUSparseNumBytespaxGNUSparseOffsetpaxGNUSparseRealSizepaxGNUSparseSizepaxGidpaxGnamepaxLinkpathpaxMtimepaxNonepaxPathpaxSchilyXattrpaxSizepaxUidpaxUnameprefixSizereadGNUSparseMap0x1readGNUSparseMap1x0readSpecialFileregFileReaderregFileWritersparseFileReadersparseFileWritersparseHolessplitUSTARPathstatAtimestatCtimestatUnixsysStattarinsecurepathtoASCIItrailerSTARtryReadFulluserMapvalidPAXRecordvalidateSparseEntriesversionGNUversionUSTARzeroBlockzeroReaderzeroWriterformatNumericformatOctalparseNumericparseOctalDeflateErrAlgorithmErrFormatRegisterCompressorchecksumReadercompressorscountWritercreatorFATcreatorMacOSXcreatorNTFScreatorUnixcreatorVFATdataDescriptor64LendataDescriptorLendataDescriptorSignaturedetectUTF8dirReaderdirWriterdirectory64EndLendirectory64EndSignaturedirectory64LocLendirectory64LocSignaturedirectoryEnddirectoryEndLendirectoryEndSignaturedirectoryHeaderLendirectoryHeaderSignaturedotFileerrLongExtraerrLongNameextTimeExtraIDfileEntryLessfileHeaderLenfileHeaderSignaturefileModeToUnixModefindDirectory64EndfindSignatureInBlockflateReaderPoolflateWriterPoolinfoZipUnixExtraIDmin64msDosTimeToTimemsdosDirmsdosModeToFileModemsdosReadOnlynewFlateReadernewFlateWriternopCloserntfsExtraIDopenDirpooledFlateReaderpooledFlateWriterreadDataDescriptorreadDirectory64EndreadDirectoryEndreadDirectoryHeaders_IFBLKs_IFCHRs_IFDIRs_IFIFOs_IFLNKs_IFMTs_IFREGs_IFSOCKs_ISGIDs_ISUIDs_ISVTXtimeToMsDosTimetoValidNameuint16maxuint32maxunixExtraIDunixModeToFileModezip64ExtraIDzipVersion20zipVersion45zipinsecurepathHash32Sum32zipwrawCountcompCountwriteDataDescriptortestHookCloseSizeOffsetSetOffsetCreateHeaderCreateRawdiskNbrdirDiskNbrdirRecordsThisDiskdirectoryRecordsdirectorySizedirectoryOffsetcommentLennreaddesrErrAdvanceTooFarErrBadReadCountErrBufferFullErrFinalTokenErrInvalidUnreadByteErrInvalidUnreadRuneErrNegativeAdvanceErrNegativeCountNewReadWriterScanRunesScanWordsdropCRerrNegativeReaderrNegativeWriteerrorRuneisSpacemaxConsecutiveEmptyReadsminReadBufferSizestartBufSizeContainsRuneCutCutPrefixCutSuffixErrTooLargeFieldsFuncIndexAnyIndexFuncIndexRuneLastIndexAnyLastIndexFuncMinReadNewBufferStringRunesSplitAfterSplitAfterNToLowerSpecialToTitleToTitleSpecialToUpperSpecialToValidUTF8TrimFuncTrimLeftFuncTrimRightTrimRightFuncasciiSetasciiSpacecontainsRuneerrUnreadByteexplodegenSplitgrowSliceindexBytePortableindexFuncisSeparatorlastIndexFuncmakeASCIISetmaxIntopInvalidopReadopReadRune1opReadRune2opReadRune3opReadRune4smallBufferSizetrimLeftASCIItrimLeftBytetrimLeftUnicodetrimRightASCIItrimRightBytetrimRightUnicodeSpecialCaseCaseRangeAccessEntryAccessRoleAvroBatchPriorityBigtableBigtableColumnBigtableColumnFamilyBigtableOptionsBooleanFieldTypeBytesFieldTypeCSVOptionsCivilDateTimeStringCivilTimeStringCopierCopyConfigCreateIfNeededCreateNeverDatasetIteratorDatasetMetadataDatasetMetadataToUpdateDatastoreBackupDateFieldTypeDateTimeFieldTypeDomainEntityEncryptionConfigEntityTypeExplainQueryStageExplainQueryStepExternalDataExternalDataConfigExternalDataConfigOptionsExternalTableExtractConfigExtractStatisticsFieldSchemaFloatFieldTypeGCSReferenceGeographyFieldTypeGoogleSheetsGoogleSheetsOptionsGroupEmailEntityIAMMemberEntityISO_8859_1InferSchemaInserterIntegerFieldTypeInteractivePriorityJobJobConfigJobIDConfigJobIteratorJobStatisticsJobStatusLoadConfigLoadSourceLoadStatisticsLoaderModelModelIteratorModelMetadataModelMetadataToUpdateMultiErrorNeverExpireNewGCSReferenceNewReaderSourceNoDedupeIDNullBoolNullDateNullDateTimeNullFloat64NullGeographyNullInt64NullStringNullTimeNullTimestampNumericFieldTypeNumericPrecisionDigitsNumericScaleDigitsNumericStringORCOwnerRoleParquetPutMultiErrorQueryParameterQueryPriorityQueryStatisticsQueryTimelineSampleRangePartitioningRangePartitioningRangeReaderRoleReaderSourceRecordFieldTypeRegularTableRoutineRoutineArgumentRoutineIteratorRoutineMetadataRoutineMetadataToUpdateRowInsertionErrorRowIteratorSchemaFromJSONScriptStackFrameScriptStatisticsSnappySpecialGroupEntityStandardSQLDataTypeStandardSQLFieldStandardSQLStructTypeStateUnspecifiedStreamingBufferStringFieldTypeStructSaverTableCreateDispositionTableMetadataTableMetadataToUpdateTableTypeTableWriteDispositionTimeFieldTypeTimePartitioningTimestampFieldTypeTrainingRunUTF_8UserEmailEntityValueLoaderValueSaverValuesSaverViewEntityViewTableWriteAppendWriteEmptyWriteTruncateWriterRoleaccessListToBQalphanumbadNullableErrorbigQueryJSONFieldboolParamTypebqPopulateFileConfigbqTagParserbqToAccessEntrybqToArgsbqToBigtableColumnbqToBigtableColumnFamilybqToBigtableOptionsbqToCSVOptionsbqToClusteringbqToCopyConfigbqToDatasetMetadatabqToEncryptionConfigbqToErrorbqToExternalDataConfigbqToExtractConfigbqToFieldSchemabqToGoogleSheetsOptionsbqToJobbqToJob2bqToJobConfigbqToLoadConfigbqToModelbqToModelColsbqToModelMetadatabqToQueryConfigbqToQueryParameterbqToRangePartitioningbqToRangePartitioningRangebqToRoutinebqToRoutineArgumentbqToRoutineMetadatabqToSchemabqToScriptStackFramebqToScriptStatisticsbqToStandardSQLDataTypebqToStandardSQLFieldbqToStandardSQLStructTypebqToTablebqToTableMetadatabqToTimePartitioningbytesParamTypecacheValcompileToOpsconvertBasicTypeconvertListedJobconvertNestedRecordconvertParamArrayconvertParamStructconvertParamValueconvertRepeatedRecordconvertRowconvertRowsconvertSchemaFromJSONconvertValuedateParamTypedateTimeParamTypedetermineSetFuncerrEmptyJSONSchemaerrNoNullsfetchPagefetchPageResultfieldCachefloat64ParamTypeformatUploadValuehandleInsertErrorshasRecursiveTypeinferFieldSchemainferFieldsinferSchemaReflectinferSchemaReflectCachedinferStructint64ParamTypeinvalidFieldNameErrorinvalidTimeErrorisStructPtrisSupportedIntTypeisSupportedUintTypejsonNulllabelUpdaterlistDatasetslistModelslistRoutineslistTablesloadMapnewRowIteratornoStructErrornullableFieldTypenullableTagOptionnulljsonnullstrnumericParamTypepageFetcherparamTypeparamTypeToFieldTypeparamValueparseCivilDateTimequeryPlanFromProtorandomIDFnrandomIDLenretryableErrorrngMuroutineArgumentsToBQrunOpsrunWithRetryschemaCachesetAnysetBoolsetClientHeadersetFloatsetFuncsetGeographysetIntsetNestedsetNullsetRepeatedsetStringsetUintstandardSQLStructFieldsToBQstateMapstringParamTypestructFieldToUploadValuestructLoaderstructLoaderOpstructToMaptimeParamTypetimelineFromPrototimestampFormattimestampParamTypetoUploadValuetoUploadValueReflecttoValueSavertypeListtypeOfByteSlicetypeOfDatetypeOfDateTimetypeOfGoTimetypeOfNullBooltypeOfNullDatetypeOfNullDateTimetypeOfNullFloat64typeOfNullGeographytypeOfNullInt64typeOfNullStringtypeOfNullTimetypeOfNullTimestamptypeOfRattypeOfTimeunixMillisToTimeunsupportedFieldTypeErroruserAgentPrefixvalidExpirationvalidFieldNamevalueListvalueSaversvaluesToMapxGoogHeaderTableRowTableCellRepeatedtoBQRelaxDatasetsServiceJobsServiceGetQueryResultsModelsServiceGetServiceAccountRoutinesServiceTabledataServiceInsertAllTablesServiceDatasetsJobsModelsRoutinesTabledataprojectIDbqsinsertJobDatasetInProjectDatasetsInProjectJobFromIDJobFromIDLocationgetJobInternalatEndfetchbufLentakeBufnextCallednextPageCalledpiJobConfigurationJobConfigurationTableCopyEncryptionConfigurationKmsKeyNameTableReferenceDatasetIdProjectIdTableIdCreateDispositionDestinationEncryptionConfigurationDestinationTableSourceTableSourceTablesWriteDispositionJobConfigurationExtractModelReferenceModelIdDestinationFormatDestinationUriDestinationUrisFieldDelimiterPrintHeaderSourceModelUseAvroLogicalTypesJobConfigurationLoadDestinationTablePropertiesFriendlyNameHivePartitioningOptionsSourceUriPrefixTableSchemaTableFieldSchemaTableFieldSchemaCategoriesTableFieldSchemaPolicyTagsPolicyTagsExpirationMsRequirePartitionFilterAllowJaggedRowsAllowQuotedNewlinesAutodetectHivePartitioningModeIgnoreUnknownValuesMaxBadRecordsNullMarkerProjectionFieldsSchemaInlineSchemaInlineFormatSchemaUpdateOptionsSkipLeadingRowsSourceFormatSourceUrisJobConfigurationQueryDatasetReferenceQueryParameterTypeQueryParameterTypeStructTypesArrayTypeStructTypesQueryParameterValueArrayValuesStructValuesParameterTypeParameterValueExternalDataConfigurationOnlyReadLatestQualifierEncodedQualifierStringFamilyIdColumnFamiliesIgnoreUnspecifiedColumnFamiliesReadRowkeyAsStringCsvOptionsUserDefinedFunctionResourceInlineCodeResourceUriAllowLargeResultsDefaultDatasetFlattenResultsMaximumBillingTierMaximumBytesBilledParameterModePreserveNullsQueryParametersTableDefinitionsUseLegacySqlUseQueryCacheUserDefinedFunctionResourcesJobTimeoutMsJobTypeimplementsStatisticsEndColumnProcedureIDEvaluationKindStackFramesTotalBytesProcessedNumChildJobsParentJobIDjobIDlastStatusLastStatussetConfigisQuerysetStatussetStatisticsMinCreationTimeMaxCreationTimepageInfonextFuncTableIDCopierFromExtractorToLoaderFromFullyQualifiedNameimplicitTableDatasetAccessGroupByEmailIamMemberSpecialGroupUserByEmailfieldIndexvalueIndexpopulateExternalDataConfigSourceURIsAutoDetectDeleteWithContentsdeleteInternalModelIDListModelsResponseStandardSqlFieldStandardSqlDataTypeStandardSqlStructTypeArrayElementTypeStructTypeTypeKindDataSplitResultEvaluationTableTrainingTableEvaluationMetricsBinaryClassificationMetricsAggregateClassificationMetricsAccuracyF1ScoreLogLossRecallRocAucBinaryConfusionMatrixFalseNegativesFalsePositivesPositiveClassThresholdTrueNegativesTruePositivesBinaryConfusionMatrixListNegativeLabelPositiveLabelClusteringMetricsFeatureValueCategoricalValueCategoryCountCategoryCategoryCountsFeatureColumnNumericalValueCentroidIdFeatureValuesClustersDaviesBouldinIndexMeanSquaredDistanceMultiClassClassificationMetricsConfusionMatrixItemCountPredictedLabelActualLabelConfidenceThresholdConfusionMatrixListRegressionMetricsMeanAbsoluteErrorMeanSquaredErrorMeanSquaredLogErrorMedianAbsoluteErrorRSquaredIterationResultArimaResultArimaModelInfoArimaCoefficientsAutoRegressiveCoefficientsInterceptCoefficientMovingAverageCoefficientsArimaFittingMetricsAicLogLikelihoodVarianceArimaOrderNonSeasonalOrderSeasonalPeriodsClusterInfoClusterRadiusClusterSizeClusterInfosDurationMsEvalLossLearnRateTrainingLossTrainingOptionsDataSplitColumnDataSplitEvalFractionDataSplitMethodDistanceTypeEarlyStopInitialLearnRateInputLabelColumnsKmeansInitializationColumnKmeansInitializationMethodL1RegularizationL2RegularizationLabelClassWeightsLearnRateStrategyLossTypeMinRelativeProgressModelUriNumClustersOptimizationStrategyWarmStartEtagExpirationTimeFeatureColumnsLabelColumnsLastModifiedTimeModelTypeTrainingRunsNextPageTokenJobIDAddJobIDSuffixcreateJobRefrprKMSKeyNameDstDefaultProjectIDDefaultDatasetIDDisableQueryCacheDisableFlattenedResultsMaxBillingTierMaxBytesBilledUseStandardSQLUseLegacySQLDestinationEncryptionConfigisJobConfignewJobssdtssfssstQualifierFamilyIDTableDataInsertAllResponseInsertErrorsErrorProtoDebugInfoTableDataInsertAllRequestRowsJsonValueInsertIdJsonschemapageTokentotalRowsForceZeroQuotequotesetQuoteArgumentKindRoutineReferenceRoutineIdDefinitionBodyImportedLibrariesRoutineTypeListHiddenDatasetListDatasetListDatasetspopulateLoadConfiggcsRoutineIDroutinesListRoutinesResponsevstructpActiveUnitsCompletedUnitsPendingUnitsSlotMillissetLabelsdeleteLabelsStartIndexTotalRowsSubstepsCompletedParallelInputsComputeAvgComputeMaxComputeRatioAvgComputeRatioMaxInputStagesParallelInputsReadAvgReadMaxReadRatioAvgReadRatioMaxRecordsReadRecordsWrittenShuffleOutputBytesShuffleOutputBytesSpilledStepsWaitAvgWaitMaxWaitRatioAvgWaitRatioMaxWriteAvgWriteMaxWriteRatioAvgWriteRatioMaxBillingTierCacheHitTotalBytesBilledTotalBytesProcessedAccuracyQueryPlanNumDMLAffectedRowsReferencedTablesUndeclaredQueryParameterNamesDDLTargetTableDDLOperationPerformedDDLTargetRoutinefeatureColumnslabelColumnstrainingRunsRawTrainingRunsRawLabelColumnsRawFeatureColumnsInsertIDRowIndexDefaultTableExpirationAccessDefaultEncryptionConfigFullIDdmSrcsProcedureIdMaterializedViewDefinitionEnableRefreshLastRefreshTimeRefreshIntervalMsModelDefinitionModelDefinitionModelOptionsBqmlTrainingRunBqmlIterationResultBqmlTrainingRunTrainingOptionsL1RegL2RegLineSearchInitLearnRateMaxIterationMinRelProgressIterationResultsModelOptionsStreamingbufferEstimatedBytesEstimatedRowsOldestEntryTimeViewDefinitionMaterializedViewNumBytesNumLongTermBytesNumPhysicalBytesNumRowsSelfLinkViewQueryInputFileBytesInputFilesOutputBytesOutputRowsJobReferenceJobIdErrorResultJobStatistics4Int64sDestinationUriFileCountsInputBytesJobStatistics3BadRecordsJobStatistics2BigQueryModelTrainingCurrentIterationExpectedTotalIterationsComputeMsAvgComputeMsMaxEndMsReadMsAvgReadMsMaxSlotMsStartMsWaitMsAvgWaitMsMaxWriteMsAvgWriteMsMaxJobStatistics2ReservationUsageElapsedMsTotalSlotMsDdlOperationPerformedDdlTargetRoutineDdlTargetTableEstimatedBytesProcessedModelTrainingModelTrainingCurrentIterationModelTrainingExpectedTotalIterationNumDmlAffectedRowsReferencedRoutinesReservationUsageTotalPartitionsProcessedUndeclaredQueryParametersJobStatisticsReservationUsageCompletionRatioParentJobIdQuotaDefermentsReservationIdDestinationURIFileCountspmeSrcDisableHeaderJobListJobsDateTimeAddDaysDaysSincedt1vlsDefaultEncryptionConfigurationDefaultPartitionExpirationMsDefaultTableExpirationMsRatSetFloat64SetFracSetFrac64InvDenomRatStringFloatStringSkipInvalidRowsTableTemplateSuffixputMultinewInsertRequestTableListTableListTablesTableListTablesViewTotalItemsGeographyValParseTagFuncLeafTypesFuncparseTagleafTypescachedTypeFieldstypeFieldslistFieldsDatasetsUpdateCallprojectIddatasetIdTablesPatchCalltableIdNameFromTagParsedTagnameBytesequalFoldTablesInsertCallMaxResultsTimeoutMsJobsQueryCallqueryrequestMatchBytesDatasetsDeleteCallDeleteContentsDatasetsListCallPageTokenJobsCancelCalljobIdModelsListCallProjectsListCallTabledataListCallSelectedFieldsRoutinesGetCallroutineIdReadMaskRoutinesInsertCallroutineTableDataInsertAllRequestTemplateSuffixRoutinesListCallDatasetsInsertCallJobsListCallProjectionStateFilterModelsDeleteCallmodelIdJobsGetQueryResultsCallJobsInsertCallMediaInfoMediaBuffermediaChunkloadChunkProgressUpdatersingleChunkmTypeprogressUpdaterSetProgressUpdaterUploadTypeUploadRequestResumableUploadmediaInfo_MediaResumableMediaTablesUpdateCallProjectsGetServiceAccountCallModelsGetCallRoutinesUpdateCallTablesListCallJobsGetCallTabledataInsertAllCalltabledatainsertallrequestDatasetsGetCallDatasetsPatchCallTablesGetCallModelsPatchCallmodelRoutinesDeleteCallTablesDeleteCallGetQueryResultsResponseJobCompleteJobCancelResponseTableDataListURIMediaTypeprogressrxdoUploadRequestreportProgresstransferChunkUploadMediaOptionMediaOptionsForceEmptyContentTypeProjectListProjectListProjectsProjectReferenceNumericIdJobListGetServiceAccountResponseTableDataInsertAllResponseInsertErrorsbigquerycloud.google.com/go/bigqueryClientInterceptorOptionsDefaultClientOptionsmergeOutgoingMetadatastreamInterceptorunaryInterceptorwithGoogleClientInfocloud.google.com/go/bigtable/internal/optionAdminClientAdminScopeApplyOptionBlockAllFilterCellsPerRowLimitFilterCellsPerRowOffsetFilterChainFiltersClusterConfigColumnFilterColumnRangeFilterConditionFilterDEVELOPMENTDefaultSnapshotDurationFamilyFilterFamilyInfoGCPolicyGCRuleToStringGetCondMutationResultHDDInfiniteRangeInstanceAdminClientInstanceAdminScopeInstanceConfInstanceInfoInstanceTypeInstanceWithClustersConfigInterleaveFiltersIntersectionPolicyLatestNFilterLimitRowsMaxAgePolicyMaxVersionsPolicyMultiClusterRoutingMutationNewAdminClientNewClientWithConfigNewCondMutationNewInstanceAdminClientNewMutationNewRangeNewReadModifyWriteNoGcPolicyPRODUCTIONPassAllFilterPrefixRangeProfileAttrsToUpdateProfileConfProfileIteratorReadItemReadModifyWriteReadOptionReadonlyScopeRowFilterRowKeyFilterRowListRowRangeRowRangeListRowSampleFilterRowSetSSDServerTimeSingleClusterRoutingSingleRowSnapshotInfoSnapshotIteratorStorageTypeStripValueFilterTableConfTableInfoTimestampRangeFilterTimestampRangeFilterMicrosUNSPECIFIEDUnionPolicyUpdateInstanceAndSyncClustersUpdateInstanceResultsValueFilterValueRangeFilteradminAddrapplyAfterFuncblockAllFiltercellInProgresscellsPerRowLimitFiltercellsPerRowOffsetFilterchainFilterchunkReaderclientUserAgentcolumnFiltercolumnRangeFilterconditionFilterdecodeFamilyProtoentryErrfamilyFiltergroupEntriesidempotentRetryCodesinstanceAdminAddrinstanceNameRegexpinterleaveFilterintersectionPolicyisIdempotentRetryCodejoinRuleslatestNFilterlimitRowsmaxAgePolicymaxMutationsmaxVersionsPolicymutationsAreRetryablenewChunkReadernewSnapshotInfonoGCPolicypassAllFilterprefixSuccessorprodAddrresourcePrefixHeaderretryOptionsrowFilterrowInProgressrowKeyFilterrowSampleFilterrrStatestorageTypeFromProtostripValueFiltertimestampRangeFilterunionPolicyvalueFiltervalueRangeFilterisRowFilter_FilterGetChainGetInterleaveGetSinkGetPassAllFilterGetBlockAllFilterGetRowKeyRegexFilterGetRowSampleFilterGetFamilyNameRegexFilterGetColumnQualifierRegexFilterGetColumnRangeFilterGetTimestampRangeFilterGetValueRegexFilterGetValueRangeFilterGetCellsPerRowOffsetFilterGetCellsPerRowLimitFilterGetCellsPerColumnLimitFilterGetStripValueTransformerGetApplyLabelTransformerrfAppProfileTruncateToMillisecondsisMutation_MutationGetMutationGetSetCellGetDeleteFromColumnGetDeleteFromFamilyGetDeleteFromRowisAppProfile_RoutingPolicyRoutingPolicyGetEtagGetRoutingPolicyGetMultiClusterRoutingUseAnyGetSingleClusterRoutingBigtableTableAdminClientCheckConsistencyRequestConsistencyTokenGetConsistencyTokenCheckConsistencyResponseConsistentGetConsistentCreateTableRequestTable_ClusterStateTable_ClusterState_ReplicationStateReplicationStateGetReplicationStateColumnFamilyGcRuleisGcRule_RuleGetRuleGetMaxNumVersionsGetMaxAgeGetIntersectionGetUnionGetGcRuleTable_TimestampGranularityClusterStatesGranularityGetClusterStatesGetColumnFamiliesGetGranularityCreateTableRequest_SplitInitialSplitsGetTableIdGetTableGetInitialSplitsCreateTableFromSnapshotRequestSourceSnapshotGetSourceSnapshotisOperation_ResultGetDoneGetResponseDeleteSnapshotRequestDeleteTableRequestDropRowRangeRequestisDropRowRangeRequest_TargetGetTargetGetRowKeyPrefixGetDeleteAllDataFromTableGenerateConsistencyTokenRequestGenerateConsistencyTokenResponseGetIamPolicyRequestGetPolicyOptionsRequestedPolicyVersionGetRequestedPolicyVersionGetResourceGetOptionsBindingGetExpressionGetMembersBindingsGetBindingsGetSnapshotRequestSnapshot_StateDataSizeBytesCreateTimeDeleteTimeGetSourceTableGetDataSizeBytesGetCreateTimeGetDeleteTimeGetTableRequestTable_ViewGetViewListSnapshotsRequestGetPageSizeGetPageTokenListSnapshotsResponseSnapshotsGetSnapshotsGetNextPageTokenListTablesRequestListTablesResponseGetTablesModifyColumnFamiliesRequestModifyColumnFamiliesRequest_ModificationisModifyColumnFamiliesRequest_Modification_ModGetIdGetModGetCreateGetUpdateGetDropModificationsGetModificationsSetIamPolicyRequestSnapshotTableRequestSnapshotIdTtlGetClusterGetSnapshotIdGetTtlTestIamPermissionsRequestTestIamPermissionsResponseCheckConsistencyCreateTableCreateTableFromSnapshotDeleteSnapshotDeleteTableDropRowRangeGenerateConsistencyTokenGetIamPolicyGetSnapshotListSnapshotsListTablesModifyColumnFamiliesSetIamPolicySnapshotTableTestIamPermissionsOperationsClientCancelOperationRequestDeleteOperationRequestGetOperationRequestListOperationsRequestListOperationsResponseGetOperationsWaitOperationRequestGetTimeoutCancelOperationDeleteOperationGetOperationListOperationsWaitOperationOperationsCallOptionsCallSettingsRetryerGRPCResolveoperationsClientCallOptionsxGoogMetadataSetGoogleClientInfotClientlroClientprojectinstanceinstancePrefixCreatePresplitTableCreateTableFromConfCreateColumnFamilyDeleteColumnFamilySetGCPolicyDropAllRowsgetConsistencyTokenisConsistentWaitForReplicationTableIAMretainRowsAfterServeNodescurKeycurFamcurQualcurTScurValcurRowlastKeyhandleCellValuefinishCellcommitRowresetToNewRowvalidateNewRowvalidateRowInProgressvalidateCellInProgressisAnyKeyPresentvalidateRowStatusTimestampMicrosGetTimestampMicrosGetQualifierBigtableClientCheckAndMutateRowRequestTableNameAppProfileIdPredicateFilterTrueMutationsFalseMutationsGetTableNameGetAppProfileIdGetRowKeyGetPredicateFilterGetTrueMutationsGetFalseMutationsCheckAndMutateRowResponsePredicateMatchedGetPredicateMatchedMutateRowRequestGetMutationsMutateRowResponseMutateRowsRequestMutateRowsRequest_EntryGetEntriesBigtable_MutateRowsClientMutateRowsResponseMutateRowsResponse_EntryReadModifyWriteRowRequestReadModifyWriteRuleisReadModifyWriteRule_RuleColumnQualifierGetFamilyNameGetColumnQualifierGetAppendValueGetIncrementAmountRulesGetRulesReadModifyWriteRowResponseFamiliesGetFamiliesReadRowsRequestisRowRange_StartKeyisRowRange_EndKeyStartKeyEndKeyGetStartKeyGetStartKeyClosedGetStartKeyOpenGetEndKeyGetEndKeyOpenGetEndKeyClosedRowKeysRowRangesGetRowKeysGetRowRangesRowsLimitGetRowsGetRowsLimitBigtable_ReadRowsClientReadRowsResponseReadRowsResponse_CellChunkBytesValueisReadRowsResponse_CellChunk_RowStatusValueSizeRowStatusGetValueSizeGetRowStatusGetResetRowGetCommitRowChunksLastScannedRowKeyGetChunksGetLastScannedRowKeySampleRowKeysRequestBigtable_SampleRowKeysClientSampleRowKeysResponseOffsetBytesGetOffsetBytesCheckAndMutateRowMutateRowMutateRowsReadModifyWriteRowReadRowsSampleRowKeysappProfilefullTableNameReadRowApplyBulkgetApplyBulkRetriesdoApplyBulkApplyReadModifyWritecrfDataSizemvptrfInstanceIdClusterIdNumNodesBigtableInstanceAdminClientCreateAppProfileRequestIgnoreWarningsGetAppProfileGetIgnoreWarningsCreateClusterRequestCluster_StateDefaultStorageTypeGetServeNodesGetDefaultStorageTypeGetClusterIdCreateInstanceRequestInstanceInstance_StateInstance_TypeGetInstanceIdGetInstanceGetClustersDeleteAppProfileRequestDeleteClusterRequestDeleteInstanceRequestGetAppProfileRequestGetClusterRequestGetInstanceRequestListAppProfilesRequestListAppProfilesResponseAppProfilesFailedLocationsGetAppProfilesGetFailedLocationsListClustersRequestListClustersResponseListInstancesRequestListInstancesResponseInstancesGetInstancesPartialUpdateInstanceRequestFieldMaskPathsGetPathsUpdateMaskGetUpdateMaskUpdateAppProfileRequestCreateAppProfileCreateClusterCreateInstanceDeleteAppProfileDeleteClusterDeleteInstanceListAppProfilesListClustersListInstancesPartialUpdateInstanceUpdateAppProfileUpdateClusterUpdateInstanceiClientiacCreateInstanceWithClustersupdateInstanceUpdateInstanceWithClustersInstanceIAMcofInstanceUpdatedCreatedClustersDeletedClustersUpdatedClusterspafmtruemfalseDeleteCellsInColumnDeleteTimestampRangeDeleteCellsInFamilyDeleteRowlnfInstanceIDclfvfilfAllowTransactionalWritesGetFieldMaskPathSplitKeysFamilyInfosvrfbafpredicateFiltertrueFilterfalseFilterrkfProfileIDrsfSetPolicyTestPermissionsAppProfile_MultiClusterRoutingUseAnyGcRule_UnionRowFilter_ConditionTrueFilterFalseFilterGetTrueFilterGetFalseFilterMutation_SetCellValueRangeisValueRange_StartValueisValueRange_EndValueStartValueEndValueGetStartValueGetStartValueClosedGetStartValueOpenGetEndValueGetEndValueClosedGetEndValueOpenColumnRangeisColumnRange_StartQualifierisColumnRange_EndQualifierStartQualifierEndQualifierGetStartQualifierGetStartQualifierClosedGetStartQualifierOpenGetEndQualifierGetEndQualifierClosedGetEndQualifierOpenRowFilter_ChainGetFiltersMutation_DeleteFromFamilyGcRule_IntersectionMutation_DeleteFromColumnStartTimestampMicrosEndTimestampMicrosGetStartTimestampMicrosGetEndTimestampMicrosGetTimeRangeOperationIteratorInternalFetchRowFilter_InterleaveAppProfile_SingleClusterRoutingGetAllowTransactionalWritesMutation_DeleteFromRowInternalProtoHasRolebindingbindingIndexbigtablecloud.google.com/go/bigtableDateOfDateTimeOfParseDateParseDateTimeTimeOfcivilcloud.google.com/go/civilExternalIPInstanceAttributeValueInstanceAttributesInstanceNameInstanceTagsInternalIPNotDefinedErrorNumericProjectIDOnGCEProjectAttributeValueProjectAttributescachedValueinitOnGCEinstIDmetadataHostEnvmetadataIPonGCEonGCEOnceprojIDprojNumstrsContainssubscribeClientsystemInfoSuggestsGCEtestOnGCEgetETaggetTrimmedtrimcloud.google.com/go/compute/metadataAllAuthenticatedUsersEditorInternalNewHandleInternalNewHandleClientInternalNewHandleGRPCClientViewergrpcClientinsertMetadatamemberIndexwithRetryIAMPolicyClientiamcloud.google.com/go/iamParseStandardTagasciiEqualFoldbyIndexcacheValuecaseMaskdominantFieldequalFoldRightfieldScanfoldFunckelvinsimpleLetterEqualFoldsmallLongEsscloud.google.com/go/internal/fieldsToBoolToFloat64ToIntToStringToUintdoPaniccloud.google.com/go/internal/optionalEndSpanTracePrintfhttpStatusCodeToOCCodetoStatuscloud.google.com/go/internal/tracedevelPrefixgoVergoVersionnotSemverRunecloud.google.com/go/internal/versionAnnotatefMultiplierPausebocloud.google.com/go/internalDefaultAuthScopesInternalFromConnNewOperationsClientdefaultOperationsCallOptionsdefaultOperationsClientOptionsversionClientversionGolongrunningcloud.google.com/go/longrunning/autogenDefaultWaitIntervalErrNoMetadataInternalNewOperationsleeperPollWaitWithIntervalcloud.google.com/go/longrunning/opt/homebrew/Caskroom/codeql-bundle/2.12.5/codeql/go/tools/osx64/go-extractor--./...@listerrorgo build github.com/influxdata/flux/libflux/go/libflux:
# pkg-config --cflags  -- flux
pkg-config: exec: "pkg-config": executable file not found in $PATH@typeerrorcould not import C (no metadata for C)BestCompressionBestSpeedCorruptInputErrorDefaultCompressionHuffmanOnlyNewReaderDictNewWriterDictNoCompressionReadErrorResetterWriteErrorbadCodebaseMatchLengthbaseMatchOffsetbufferFlushSizebufferResetbulkHash4codeOrdercodegenCodeCountcodegenOrderdictDecoderdictWriteremitLiteralendBlockMarkererrWriterClosedfixedHuffmanDecoderfixedHuffmanDecoderInitfixedLiteralEncodingfixedOffsetEncodingfixedOncegenerateFixedLiteralEncodinggenerateFixedOffsetEncodinghash4hashBitshashMaskhashSizehashmulhuffOffsethuffmanChunkBitshuffmanCountMaskhuffmanDecoderhuffmanNumChunkshuffmanValueShiftinputMarginlengthBaselengthCodelengthCodeslengthCodesStartlengthExtraBitslengthShiftlevelInfoliteralTokenliteralTypeload32load64logWindowSizemakeReadermatchTokenmatchTypemaxBitsLimitmaxCodeLenmaxFlateBlockTokensmaxHashOffsetmaxMatchLengthmaxMatchOffsetmaxNodemaxNumDistmaxNumLitmaxStoreBlockSizeminMatchLengthminNonLiteralBlockSizenewDeflateFastnewHuffmanBitWriternewHuffmanEncodernumCodesoffsetBaseoffsetCodeoffsetCodeCountoffsetCodesoffsetExtraBitsoffsetMaskreverseBitsskipNevertableBitstableMasktableShifttableSizetypeMaskwindowMaskwindowSizelinkMask316wrPosrdPoshistSizeavailReadavailWritewriteMarkwriteCopytryWriteCopyreadFlushroffseth1h2codebitsstepStatefinaltoReadhlcopyLencopyDistnextBlockreadHuffmanhuffmanBlockdataBlockcopyDatafinishBlockmoreBitshuffSymlastFreqnextCharFreqnextPairFreqneededflatecompress/flateNewWriterLevelflagCommentflagExtraflagHdrCrcflagNameflagTextgzipDeflategzipID1gzipID2leLSBMSBdecoderInvalidCodeerrClosederrOutOfCodesflushBufferinvalidCodeinvalidEntrymaxCodenewWriter8192nBitslitWidthreadLSBreadMSBByteWritersavedCodewriteLSBwriteMSBincHilzwcompress/lzwCancelCauseFuncCauseWithCancelCauseWithDeadlinecancelCtxKeycancelerclosedchancontextNamedeadlineExceededErroremptyCtxgoroutinesnewCancelCtxparentCancelCtxpropagateCancelstringertimerCtxtodovalueCtxwithCancelcauseKeySizeErrorNewCipheraesCipheraesCipherAsmaesCipherGCMcbcDecAblecbcEncAblectrAbledecryptBlockAsmdecryptBlockGoencryptBlockAsmencryptBlockGoerrOpenexpandKeyexpandKeyAsmexpandKeyGogcmAblegcmAesDatagcmAesDecgcmAesEncgcmAesFinishgcmAesInitgcmAsmgcmBlockSizegcmMinimumTagSizegcmStandardNonceSizegcmTagSizenewCiphernewCipherGenericpolypowxrotwsbox0sbox1sliceForAppendsubwsupportsAESsupportsGFMULtd0td1td2td3te0te1te2te3DecryptEncryptXORKeyStreamNewCTRNewGCMBlockModeCryptBlocksNewCBCEncrypterproductTablenonceSizetagSizeNewCBCDecrypteraescrypto/aesNewCFBDecrypterNewCFBEncrypterNewGCMWithNonceSizeNewGCMWithTagSizeNewOFBStreamReaderStreamWritercbccbcDecryptercbcEncryptercfbctrgcmgcmAddgcmDoublegcmFieldElementgcmInc32gcmReductionTablenewCBCnewCBCGenericDecrypternewCBCGenericEncrypternewCFBnewGCMWithNonceAndTagSizeofbstreamBufferSizeoutUsedSetIVupdateBlockscounterCryptderiveCountercrypto/cipherNewTripleDESCiphercryptBlockdecryptBlockdesCipherencryptBlockexpansionFunctionfeistelBoxfeistelBoxOncefinalPermutationinitFeistelBoxinitialPermutationksRotateksRotationspermutationFunctionpermuteBlockpermuteFinalBlockpermuteInitialBlockpermutedChoice1permutedChoice2sBoxestripleDESCiphersubkeysgenerateSubkeyscipher1cipher2cipher3descrypto/desErrInvalidPublicKeyGenerateParametersL1024N160L2048N224L2048N256L3072N256ParameterSizesfermatInversenumMRTestsdsacrypto/dsaP256P384P521X25519errInvalidPrivateKeyisLessnewBoringPrivateKeynistCurvenistPointp256p256Orderp384p384Orderp521p521Orderx25519x25519Curvex25519PrivateKeySizex25519PublicKeySizex25519ScalarMultx25519SharedSecretSizeBytesXScalarBaseMultScalarMultnewPointscalarOrdercrypto/ecdhSignASN1VerifyASN1_p224_p256_p384_p521addASN1IntBytesbigIntEqualboringPrivateKeyboringPublicKeycurveToECDHencodeSignatureerrNoAsmerrZeroParamgenerateLegacygenerateNISTEChashToInthashToNatinversemixedCSPRNGp224p224Oncep256Oncep384Oncep521OnceparseSignatureprecomputeParamsrandFieldElementrandomPointsignAsmsignLegacysignNISTECtestingOnlyRejectionSamplingLoopedverifyAsmverifyLegacyverifyNISTECzrCurveParamsGxGyBitSizepolynomialIsOnCurveaffineFromJacobianaddJacobianDoubledoubleJacobianModulusNatlimbssetBigSetOverflowingBytescmpGeqshiftInExpandForresetFormontgomeryRepresentationmontgomeryReductionmontgomeryMulm0invnMinus2pointFromAffinepointToAffinependingLenLenpendingIsASN1inContinuationAddASN1Int64AddASN1Int64WithTagAddASN1EnumaddASN1SignedAddASN1Uint64AddASN1BigIntAddASN1OctetStringAddASN1GeneralizedTimeAddASN1UTCTimeAddASN1BitStringaddBase128IntAddASN1ObjectIdentifierAddASN1BooleanAddASN1NULLMarshalASN1AddASN1SetErrorBytesOrPanicAddUint24AddBytesAddUint8LengthPrefixedAddUint16LengthPrefixedAddUint24LengthPrefixedAddUint32LengthPrefixedcallContinuationaddLengthPrefixedflushChildUnwriteAddValuePrivateKeyECDSAPublicKeyECDSAchoiceBuilderContinuationSignerOptsConstructedContextSpecificMarshalingValueecdsacrypto/ecdsaNewKeyFromSeedPrivateKeySizePublicKeySizeSeedSizeSignatureSizeVerifyWithOptionsdomPrefixCtxdomPrefixPhdomPrefixPurenewKeyFromSeeded25519crypto/ed25519MarshalCompressedP224UnmarshalCompressedbigFromDecimalbigFromHexinitAllinitP224initP256initP384initP521initoncematchesSpecificCurvep256CurvepanicIfNotOnCurvezForAffinenormalizeScalarCombinedMultInverseellipticcrypto/elliptichmacmarshalableopadipadmarshaledcrypto/hmacAnyOverlapInexactOverlapcrypto/internal/aliasNewModulusFromBigNewNat_MASK_WctEqctGeqctSelectminusInverseModWmontgomeryLoopmontgomeryLoopGenericpreallocLimbspreallocTargetbigmodcrypto/internal/bigmodEncBigIntbbigcrypto/internal/boring/bbigBoringCryptoFIPSOnlyStandardCryptocrypto/internal/boring/sigDecryptRSANoPaddingDecryptRSAOAEPDecryptRSAPKCS1EncryptRSANoPaddingEncryptRSAOAEPEncryptRSAPKCS1GenerateKeyECDHGenerateKeyECDSAGenerateKeyRSANewAESCipherNewGCMTLSNewHMACNewPrivateKeyECDHNewPrivateKeyECDSANewPrivateKeyRSANewPublicKeyECDHNewPublicKeyECDSANewPublicKeyRSANewSHA1NewSHA224NewSHA256NewSHA384NewSHA512PrivateKeyRSAPublicKeyRSARandReaderSHA1SHA224SHA384SHA512SignMarshalECDSASignRSAPKCS1v15SignRSAPSSUnreachableUnreachableExceptTestsVerifyECDSAVerifyRSAPKCS1v15VerifyRSAPSSrandReadercrypto/internal/boringaddMul64carryPropagatefeMulfeMulGenericfeOnefeSquarefeSquareGenericfeZeromask64BitsmaskLow51Bitsmul51mul64shiftRightBy51sqrtM1l1l2l3l4NegateInvertMultiplySquareMult32Pow22523SqrtRatiocarryPropagateGenericcrypto/internal/edwards25519/fieldNewGeneratorPointNewIdentityPointNewScalarScalaraffineCachedaffineLookupTablebasepointNafTablebasepointNafTablePrecompbasepointTablebasepointTablePrecompcheckInitializedcopyFieldElementfiatScalarAddfiatScalarCmovznzU64fiatScalarFromBytesfiatScalarFromMontgomeryfiatScalarInt1fiatScalarMontgomeryDomainFieldElementfiatScalarMulfiatScalarNonMontgomeryDomainFieldElementfiatScalarNonzerofiatScalarOppfiatScalarSubfiatScalarToBytesfiatScalarToMontgomeryfiatScalarUint1generatorisReducednafLookupTable5nafLookupTable8projCachedprojLookupTableprojP1xP1projP2scalarMinusOneBytesscalarTwo168scalarTwo336MultiplyAddSetUniformBytessetShortBytesSetCanonicalBytesSetBytesWithClampingnonAdjacentFormsignedRadix16YplusXYminusXT2dFromP3CondNegSelectIntofromP1xP1fromP2VarTimeDoubleScalarBaseMultFromP1xP1initOnceAddAffineSubAffineedwards25519crypto/internal/edwards25519P224ElementP256ElementP384ElementP521Elementp224Addp224CmovznzU64p224ElementLenp224FromBytesp224FromMontgomeryp224Int1p224InvertEndiannessp224MontgomeryDomainFieldElementp224Mulp224NonMontgomeryDomainFieldElementp224Selectznzp224SetOnep224Squarep224Subp224ToBytesp224ToMontgomeryp224Uint1p224UntypedFieldElementp256Addp256CmovznzU64p256ElementLenp256FromBytesp256FromMontgomeryp256Int1p256InvertEndiannessp256MontgomeryDomainFieldElementp256Mulp256NonMontgomeryDomainFieldElementp256Selectznzp256SetOnep256Squarep256Subp256ToBytesp256ToMontgomeryp256Uint1p256UntypedFieldElementp384Addp384CmovznzU64p384ElementLenp384FromBytesp384FromMontgomeryp384Int1p384InvertEndiannessp384MontgomeryDomainFieldElementp384Mulp384NonMontgomeryDomainFieldElementp384Selectznzp384SetOnep384Squarep384Subp384ToBytesp384ToMontgomeryp384Uint1p384UntypedFieldElementp521Addp521CmovznzU64p521ElementLenp521FromBytesp521FromMontgomeryp521Int1p521InvertEndiannessp521MontgomeryDomainFieldElementp521Mulp521NonMontgomeryDomainFieldElementp521Selectznzp521SetOnep521Squarep521Subp521ToBytesp521ToMontgomeryp521Uint1p521UntypedFieldElementfiatcrypto/internal/nistec/fiatNewP224PointNewP256PointNewP384PointNewP521PointP224PointP256OrdInverseP256PointP384PointP521Point_p224B_p224BOnce_p384B_p384BOnce_p521B_p521BOnceboothW5boothW6p224Bp224CheckOnCurvep224ElementLengthp224GGp224GGOncep224GeneratorTablep224GeneratorTableOncep224Polynomialp224Sqrtp224SqrtCandidatep224Tablep256AffinePointp256AffineTablep256BigToLittlep256CheckOnCurvep256CompressedLengthp256Elementp256ElementLengthp256Equalp256FromMontp256Inversep256LessThanPp256LittleToBigp256MovCondp256NegCondp256Onep256OrdBigToLittlep256OrdElementp256OrdLittleToBigp256OrdMulp256OrdReducep256OrdSqrp256Pp256PointAddAffineAsmp256PointAddAsmp256PointDoubleAsmp256Polynomialp256Precomputedp256PrecomputedEmbedp256Selectp256SelectAffinep256Sqrp256Sqrtp256Tablep256UncompressedLengthp256Zerop384Bp384CheckOnCurvep384ElementLengthp384GeneratorTablep384GeneratorTableOncep384Polynomialp384Sqrtp384SqrtCandidatep384Tablep521Bp521CheckOnCurvep521ElementLengthp521GeneratorTablep521GeneratorTableOncep521Polynomialp521Sqrtp521SqrtCandidatep521Tableuint64IsZeroSetGeneratorbytesXBytesCompressedbytesCompressedgeneratorTableisInfinityaffineFromMontp256BaseMultp256ScalarMultnisteccrypto/internal/nistecMaybeReadByteclosedChanclosedChanOncerandutilcrypto/internal/randutilblockGenericconsumeUint32consumeUint64haveAsminit0init1init2init3marshaledSizenxcheckSummd5crypto/md5PrimealtGetRandombatchedhideAgainReaderurandomDevicewarnBlockedCipherrc4crypto/rc4CRTValueDecryptOAEPDecryptPKCS1v15DecryptPKCS1v15SessionKeyEncryptOAEPEncryptPKCS1v15ErrDecryptionErrMessageTooLongErrVerificationGenerateMultiPrimeKeyOAEPOptionsPKCS1v15DecryptOptionsPSSOptionsPSSSaltLengthAutoPSSSaltLengthEqualsHashPrecomputedValuesSignPKCS1v15SignPSSVerifyPKCS1v15VerifyPSSbigOnecheckPubdecryptOAEPdecryptPKCS1v15emsaPSSEncodeemsaPSSVerifyerrPublicExponentLargeerrPublicExponentSmallerrPublicModulushashPrefixesincCounterintToBytesinvalidSaltLenErrmgf1XORnoChecknonZeroRandomBytespkcs1v15HashInfosignPSSWithSaltwithCheckCoeffDpDqQinvCRTValuesPrimesPrecomputedPrecomputeSaltLengthsaltLengthSessionKeyLenMGFHashDecrypterOptsrsacrypto/rsa_K0_K1_K2_K3boringEnabledboringNewSHA1boringSHA1boringUnreachableinit4sha1blockConstantTimeSumconstSumNew224Size224Sum224Sum256_Kinit0_224init1_224init2_224init3_224init4_224init5init5_224init6init6_224init7init7_224magic224magic256sha256blockis224New384New512_224New512_256Size256Size384Sum384Sum512Sum512_224Sum512_256blockAsminit0_256init0_384init1_256init1_384init2_256init2_384init3_256init3_384init4_256init4_384init5_256init5_384init6_256init6_384init7_256init7_384magic384magic512magic512_224magic512_256sha512crypto/sha512ConstantTimeByteEqConstantTimeCompareConstantTimeCopyConstantTimeEqConstantTimeLessOrEqConstantTimeSelectXORBytesxorBytessubtlecrypto/subtleCertificateVerificationErrorCipherSuiteNameDialWithDialerECDSAWithP256AndSHA256ECDSAWithP384AndSHA384ECDSAWithP521AndSHA512ECDSAWithSHA1Ed25519InsecureCipherSuitesNewLRUClientSessionCacheNoClientCertPKCS1WithSHA1PKCS1WithSHA256PKCS1WithSHA384PKCS1WithSHA512PSSWithSHA256PSSWithSHA384PSSWithSHA512RenegotiateFreelyAsClientRenegotiateNeverRenegotiateOnceAsClientRequestClientCertRequireAndVerifyClientCertRequireAnyClientCertTLS_AES_128_GCM_SHA256TLS_AES_256_GCM_SHA384TLS_CHACHA20_POLY1305_SHA256TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHATLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHATLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256TLS_ECDHE_ECDSA_WITH_RC4_128_SHATLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHATLS_ECDHE_RSA_WITH_AES_128_CBC_SHATLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256TLS_ECDHE_RSA_WITH_RC4_128_SHATLS_FALLBACK_SCSVTLS_RSA_WITH_3DES_EDE_CBC_SHATLS_RSA_WITH_AES_128_CBC_SHATLS_RSA_WITH_AES_128_CBC_SHA256TLS_RSA_WITH_AES_128_GCM_SHA256TLS_RSA_WITH_RC4_128_SHAVerifyClientCertIfGivenVersionSSL30X509KeyPair_ClientAuthType_index_ClientAuthType_name_CurveID_index_0_CurveID_name_0_CurveID_name_1_SignatureScheme_index_8_SignatureScheme_name_0_SignatureScheme_name_1_SignatureScheme_name_2_SignatureScheme_name_3_SignatureScheme_name_4_SignatureScheme_name_5_SignatureScheme_name_6_SignatureScheme_name_7_SignatureScheme_name_8addBytesWithLengthaddUint64aeadAESGCMaeadAESGCMTLS13aeadChaCha20Poly1305aeadNonceLengthaesgcmCiphersaesgcmPreferredalertAccessDeniedalertBadCertificatealertBadCertificateHashValuealertBadCertificateStatusResponsealertBadRecordMACalertCertificateExpiredalertCertificateRequiredalertCertificateRevokedalertCertificateUnknownalertCertificateUnobtainablealertCloseNotifyalertDecodeErroralertDecompressionFailurealertDecryptErroralertDecryptionFailedalertExportRestrictionalertHandshakeFailurealertIllegalParameteralertInappropriateFallbackalertInsufficientSecurityalertInternalErroralertLevelErroralertLevelWarningalertMissingExtensionalertNoApplicationProtocolalertNoRenegotiationalertProtocolVersionalertRecordOverflowalertTextalertUnexpectedMessagealertUnknownCAalertUnknownPSKIdentityalertUnrecognizedNamealertUnsupportedCertificatealertUnsupportedExtensionalertUserCanceledatLeastReadercacheEntrycbcModecertCachecertTypeECDSASigncertTypeRSASigncertificateMsgcertificateMsgTLS13certificateRequestInfoFromMsgcertificateRequestMsgcertificateRequestMsgTLS13certificateStatusMsgcertificateVerifyMsgcheckALPNcipher3DEScipherAEScipherRC4cipherSuiteByIDcipherSuiteTLS13ByIDcipherSuitesPreferenceOrdercipherSuitesPreferenceOrderNoAEScipherSuitesTLS13clientApplicationTrafficLabelclientCertCacheclientFinishedLabelclientHandshakeStateclientHandshakeStateTLS13clientHandshakeTrafficLabelclientHelloInfoclientKeyExchangeMsgclientSessionCacheKeyclientSignatureContextcloneHashcompressionNoneconstantTimeHashcthWrappercurveForCurveIDcurveIDForCurvedefaultCipherSuitesdefaultCipherSuitesLendefaultCipherSuitesTLS13defaultCipherSuitesTLS13NoAESdefaultConfigdefaultCurvePreferencesdefaultSupportedSignatureAlgorithmsdeprecatedSessionTicketKeydirectSigningdisabledCipherSuitesdowngradeCanaryTLS11downgradeCanaryTLS12ecdheECDSAKAecdheKeyAgreementecdheRSAKAekmFromMasterSecretemptyConfigencryptedExtensionsMsgendOfEarlyDataMsgerrClientKeyExchangeerrEarlyCloseWriteerrNoCertificateserrServerKeyExchangeerrShutdownexporterLabelextensionALPNextensionCertificateAuthoritiesextensionCookieextensionEarlyDataextensionKeyShareextensionPSKModesextensionPreSharedKeyextensionRenegotiationInfoextensionSCTextensionServerNameextensionSessionTicketextensionSignatureAlgorithmsextensionSignatureAlgorithmsCertextensionStatusRequestextensionSupportedCurvesextensionSupportedPointsextensionSupportedVersionsextractPaddingfinishedMsgfinishedVerifyLengthfipsCipherSuitesfipsCurvePreferencesfipsMaxVersionfipsMinVersionfipsSupportedSignatureAlgorithmsgenerateECDHEKeyhasAESGCMHardwareSupporthasGCMAsmAMD64hasGCMAsmARM64hasGCMAsmS390XhashForServerKeyExchangehelloRequestMsghelloRetryRequestRandomhostnameInSNIillegalClientHelloChangeisSupportedSignatureAlgorithmkeyAgreementkeyExpansionLabelkeyLogLabelClientHandshakekeyLogLabelClientTraffickeyLogLabelServerHandshakekeyLogLabelServerTraffickeyLogLabelTLS12keysFromMasterSecretlegacyTypeAndHashFromPublicKeylruSessionCachelruSessionCacheEntrymacSHA1macSHA256marshalCertificatemarshalingFunctionmasterFromPreMasterSecretmasterSecretLabelmasterSecretLengthmaxCiphertextmaxCiphertextTLS13maxClientPSKIdentitiesmaxHandshakemaxPlaintextmaxSessionTicketLifetimemaxUselessRecordsmd5SHA1HashmutualCipherSuitemutualCipherSuiteTLS13needFIPSnegotiateALPNnewConstantTimeHashnewFinishedHashnewSessionTicketMsgnoExportedKeyingMaterialnonAESGCMAEADCiphersnoncePrefixLengthoutBufPoolpHashparsePrivateKeypermanentErrorpointFormatUncompressedprefixNonceAEADprf10prf12prfAndHashForVersionprfForVersionpskModeDHEpskModePlainreadUint16LengthPrefixedreadUint24LengthPrefixedreadUint8LengthPrefixedrecordHeaderLenrecordSizeBoostThresholdrecordTypeAlertrecordTypeApplicationDatarecordTypeChangeCipherSpecrecordTypeHandshakerequiresClientCertresumptionBinderLabelresumptionLabelroleClientroleServerroundUprsaKArsaKeyAgreementrsaSignatureSchemesscsvRenegotiationselectCipherSuiteselectSignatureSchemeserverApplicationTrafficLabelserverFinishedLabelserverHandshakeStateserverHandshakeStateTLS13serverHandshakeTrafficLabelserverHelloDoneMsgserverKeyExchangeMsgserverSignatureContextsessionStatesessionStateTLS13sha1HashsignatureECDSAsignatureEd25519signaturePKCS1v15signaturePaddingsignatureRSAPSSsignatureSchemesForCertificatesignedMessagesplitPreMasterSecretstatusTypeOCSPsuiteECDHEsuiteECSignsuiteSHA384suiteTLS12supportedOnlyTLS12supportedOnlyTLS13supportedUpToTLS12supportedVersionsFromMaxsupportsECDHEtcpMSSEstimatetestingOnlyForceClientHelloSignatureAlgorithmstestingOnlyForceDowngradeCanaryticketKeyLifetimeticketKeyNameLenticketKeyRotationtimeoutErrortls10MACtrafficUpdateLabeltranscriptMsgtypeAndHashFromSignatureSchemetypeCertificatetypeCertificateRequesttypeCertificateStatustypeCertificateVerifytypeClientHellotypeClientKeyExchangetypeEncryptedExtensionstypeEndOfEarlyDatatypeFinishedtypeHelloRequesttypeKeyUpdatetypeMessageHashtypeNewSessionTickettypeNextProtocoltypeServerHellotypeServerHelloDonetypeServerKeyExchangeunexpectedMessageErrorunmarshalCertificateunsupportedCertificateErrorverifyHandshakeSignaturewriterMutexxorNonceAEADReadASN1BooleanReadASN1IntegerreadASN1BigIntreadASN1BytesreadASN1Int64readASN1Uint64ReadASN1Int64WithTagReadASN1EnumreadBase128IntReadASN1ObjectIdentifierReadASN1GeneralizedTimeReadASN1UTCTimeReadASN1BitStringReadASN1BitStringAsBytesReadASN1BytesReadASN1ReadASN1ElementReadAnyASN1ReadAnyASN1ElementPeekASN1TagSkipASN1ReadOptionalASN1SkipOptionalASN1ReadOptionalASN1IntegerReadOptionalASN1OctetStringReadOptionalASN1BooleanreadASN1ReadUint24readUnsignedreadLengthPrefixedReadUint8LengthPrefixedReadUint16LengthPrefixedReadUint24LengthPrefixedCopyBytesciphertextgenerateClientKeyExchangegenerateServerKeyExchangeprocessClientKeyExchangeprocessServerKeyExchangemacLenivLenkanonceMaskNetDialernetDialercertificateAuthoritiesUnverifiedCertificatesnewCertclientMD5serverMD5prfclientSumserverSumhashForClientCertificatediscardHandshakeBufferverifyDataminModulusBytesmaxVersionticketcertificatesusedOldKeyclientHellosuiteecdheOkecSignOkrsaDecryptOkrsaSignOkhandshakehsprocessClientHellopickCipherSuitecipherSuiteOkcheckForResumptiondoResumeHandshakedoFullHandshakeestablishKeysreadFinishedsendSessionTicketsendFinishedhasSignatureAlgorithmcertificateTypessentDummyCCSusingPSKsigAlgearlySecretsharedKeyhandshakeSecrettranscriptpickCertificatesendDummyChangeCipherSpecdoHelloRetryRequestsendServerParametersrequestClientCertsendServerCertificatesendServerFinishedshouldSendSessionTicketssendSessionTicketsreadClientCertificatereadClientFinishedsignatureAlgorithmsignatureserverHelloecdheKeybinderKeycertReqcheckServerHelloOrHRRprocessHelloRetryRequestprocessServerHelloestablishHandshakeKeysreadServerParametersreadServerCertificatereadServerFinishedsendClientCertificatesendClientFinishedisRSAckxpreMasterSecretsessionKeyserverResumedSessionreadSessionTicketBytesToCFDataCFArrayAppendValueCFArrayCreateMutableCFArrayGetCountCFArrayGetValueAtIndexCFDataGetBytePtrCFDataGetLengthCFDataToSliceCFDateCreateCFDictionaryGetValueIfPresentCFEqualCFErrorCopyDescriptionCFErrorGetCodeCFNumberGetValueCFRefCFReleaseCFStringCFStringCreateExternalRepresentationCFStringToStringErrNoTrustSettingsErrSecCertificateExpiredErrSecHostNameMismatchErrSecNotTrustedOSStatusReleaseCFArraySecCertificateCopyDataSecCertificateCreateWithDataSecPolicyAppleSSLSecPolicyCreateSSLSecPolicyOidSecTrustCreateWithCertificatesSecTrustEvaluateSecTrustEvaluateWithErrorSecTrustGetCertificateAtIndexSecTrustGetCertificateCountSecTrustGetResultSecTrustResultConfirmSecTrustResultDenySecTrustResultFatalTrustFailureSecTrustResultInvalidSecTrustResultOtherErrorSecTrustResultProceedSecTrustResultRecoverableTrustFailureSecTrustResultTypeSecTrustResultUnspecifiedSecTrustSetVerifyDateSecTrustSettingsCopyCertificatesSecTrustSettingsCopyTrustSettingsSecTrustSettingsDomainSecTrustSettingsDomainAdminSecTrustSettingsDomainSystemSecTrustSettingsDomainUserSecTrustSettingsPolicySecTrustSettingsPolicyStringSecTrustSettingsResultSecTrustSettingsResultDenySecTrustSettingsResultInvalidSecTrustSettingsResultKeySecTrustSettingsResultTrustAsRootSecTrustSettingsResultTrustRootSecTrustSettingsResultUnspecifiedStringToCFStringTimeToCFDateReferrSecItemNotFounderrSecNoTrustSettingskCFAllocatorDefaultkCFNumberSInt32TypekCFStringEncodingUTF8x509_CFArrayAppendValue_trampolinex509_CFArrayCreateMutable_trampolinex509_CFArrayGetCount_trampolinex509_CFArrayGetValueAtIndex_trampolinex509_CFDataCreate_trampolinex509_CFDataGetBytePtr_trampolinex509_CFDataGetLength_trampolinex509_CFDateCreate_trampolinex509_CFDictionaryGetValueIfPresent_trampolinex509_CFEqual_trampolinex509_CFErrorCopyDescription_trampolinex509_CFErrorGetCode_trampolinex509_CFNumberGetValue_trampolinex509_CFRelease_trampolinex509_CFStringCreateExternalRepresentation_trampolinex509_CFStringCreateWithBytes_trampolinex509_SecCertificateCopyData_trampolinex509_SecCertificateCreateWithData_trampolinex509_SecPolicyCreateSSL_trampolinex509_SecTrustCreateWithCertificates_trampolinex509_SecTrustEvaluateWithError_trampolinex509_SecTrustEvaluate_trampolinex509_SecTrustGetCertificateAtIndex_trampolinex509_SecTrustGetCertificateCount_trampolinex509_SecTrustGetResult_trampolinex509_SecTrustSetVerifyDate_trampolinex509_SecTrustSettingsCopyCertificates_trampolinex509_SecTrustSettingsCopyTrustSettings_trampolinemacOScrypto/x509/internal/macosAttributeTypeAndValueSETattributeTypeNamesoidCommonNameoidCountryoidInAttributeTypeAndValueoidLocalityoidOrganizationoidOrganizationalUnitoidPostalCodeoidProvinceoidSerialNumberoidStreetAddresspkixcrypto/x509/pkixCANotAuthorizedForExtKeyUsageCANotAuthorizedForThisNameCertificateInvalidErrorCertificateRequestConstraintViolationErrorCreateCertificateCreateCertificateRequestCreateRevocationListDSADSAWithSHA1DSAWithSHA256DecryptPEMBlockECDSAECDSAWithSHA256ECDSAWithSHA384ECDSAWithSHA512EncryptPEMBlockErrUnsupportedAlgorithmExtKeyUsageAnyExtKeyUsageClientAuthExtKeyUsageCodeSigningExtKeyUsageEmailProtectionExtKeyUsageIPSECEndSystemExtKeyUsageIPSECTunnelExtKeyUsageIPSECUserExtKeyUsageMicrosoftCommercialCodeSigningExtKeyUsageMicrosoftKernelCodeSigningExtKeyUsageMicrosoftServerGatedCryptoExtKeyUsageNetscapeServerGatedCryptoExtKeyUsageOCSPSigningExtKeyUsageServerAuthExtKeyUsageTimeStampingHostnameErrorIncompatibleUsageIncorrectPasswordErrorInsecureAlgorithmErrorInvalidReasonIsEncryptedPEMBlockKeyUsageCRLSignKeyUsageCertSignKeyUsageContentCommitmentKeyUsageDataEnciphermentKeyUsageDecipherOnlyKeyUsageDigitalSignatureKeyUsageEncipherOnlyKeyUsageKeyAgreementKeyUsageKeyEnciphermentMD2WithRSAMD5WithRSAMarshalECPrivateKeyMarshalPKCS1PrivateKeyMarshalPKCS1PublicKeyMarshalPKCS8PrivateKeyMarshalPKIXPublicKeyNameConstraintsWithoutSANsNameMismatchNewCertPoolNotAuthorizedToSignPEMCipherPEMCipher3DESPEMCipherAES128PEMCipherAES192PEMCipherAES256PEMCipherDESParseCRLParseCertificateRequestParseCertificatesParseDERCRLParseECPrivateKeyParsePKCS1PrivateKeyParsePKCS1PublicKeyParsePKCS8PrivateKeyParsePKIXPublicKeyParseRevocationListPureEd25519RSARevocationListSHA1WithRSASHA256WithRSASHA256WithRSAPSSSHA384WithRSASHA384WithRSAPSSSHA512WithRSASHA512WithRSAPSSSetFallbackRootsSystemCertPoolSystemRootsErrorTooManyConstraintsTooManyIntermediatesUnconstrainedNameUnhandledCriticalExtensionUnknownAuthorityErrorUnknownPublicKeyAlgorithmUnknownSignatureAlgorithmalreadyInChainappendToFreshChainasn1BitLengthauthKeyIdauthorityInfoAccessbasicConstraintsboringAllowCertbuildCSRExtensionsbuildCertExtensionscertificateListcertificateRequestcheckChainForKeyUsagecheckSignaturecipherByKeycipherByNamedistributionPointdistributionPointNamedomainToReverseLabelsdsaAlgorithmParametersecPrivKeyVersionecPrivateKeyemptyASN1SubjecterrNotParsedexportCertificateextKeyUsageFromOIDextKeyUsageOIDsfallbacksSetforEachSANforceFallbackgetPublicKeyAlgorithmFromOIDgetSignatureAlgorithmFromAIhashToPSSParametersinitSystemRootsintermediateCertificateisIA5StringisPrintableisValidIPMaskleafCertificateloadSystemRootsmarshalBasicConstraintsmarshalCertificatePoliciesmarshalECDHPrivateKeymarshalECPrivateKeyWithOIDmarshalExtKeyUsagemarshalKeyUsagemarshalPublicKeymarshalSANsmatchDomainConstraintmatchEmailConstraintmatchExactlymatchHostnamesmatchIPConstraintmatchURIConstraintmaxChainSignatureChecksnameTypeDNSnameTypeEmailnameTypeIPnameTypeURInamedCurveFromOIDnewRawAttributesoidAuthorityInfoAccessIssuersoidAuthorityInfoAccessOcspoidExtKeyUsageAnyoidExtKeyUsageClientAuthoidExtKeyUsageCodeSigningoidExtKeyUsageEmailProtectionoidExtKeyUsageIPSECEndSystemoidExtKeyUsageIPSECTunneloidExtKeyUsageIPSECUseroidExtKeyUsageMicrosoftCommercialCodeSigningoidExtKeyUsageMicrosoftKernelCodeSigningoidExtKeyUsageMicrosoftServerGatedCryptooidExtKeyUsageNetscapeServerGatedCryptooidExtKeyUsageOCSPSigningoidExtKeyUsageServerAuthoidExtKeyUsageTimeStampingoidExtensionAuthorityInfoAccessoidExtensionAuthorityKeyIdoidExtensionBasicConstraintsoidExtensionCRLDistributionPointsoidExtensionCRLNumberoidExtensionCertificatePoliciesoidExtensionExtendedKeyUsageoidExtensionKeyUsageoidExtensionNameConstraintsoidExtensionRequestoidExtensionSubjectAltNameoidExtensionSubjectKeyIdoidFromECDHCurveoidFromExtKeyUsageoidFromNamedCurveoidISOSignatureSHA1WithRSAoidInExtensionsoidMGF1oidNamedCurveP224oidNamedCurveP256oidNamedCurveP384oidNamedCurveP521oidPublicKeyDSAoidPublicKeyECDSAoidPublicKeyEd25519oidPublicKeyRSAoidPublicKeyX25519oidSHA256oidSHA384oidSHA512oidSignatureDSAWithSHA1oidSignatureDSAWithSHA256oidSignatureECDSAWithSHA1oidSignatureECDSAWithSHA256oidSignatureECDSAWithSHA384oidSignatureECDSAWithSHA512oidSignatureEd25519oidSignatureMD2WithRSAoidSignatureMD5WithRSAoidSignatureRSAPSSoidSignatureSHA1WithRSAoidSignatureSHA256WithRSAoidSignatureSHA384WithRSAoidSignatureSHA512WithRSAparseAIparseASN1StringparseBasicConstraintsExtensionparseCSRExtensionsparseCertificateparseCertificatePoliciesExtensionparseCertificateRequestparseECPrivateKeyparseExtKeyUsageExtensionparseExtensionparseKeyUsageExtensionparseNameparseNameConstraintsExtensionparsePublicKeyparseRFC2821MailboxparseRawAttributesparseSANExtensionparseTimeparseValiditypemCRLPrefixpemTypepkcs1AdditionalRSAPrimepkcs1PrivateKeypkcs1PublicKeypkcs8pkixPublicKeypolicyInformationprocessExtensionspssParameterspublicKeyAlgoNamepublicKeyInforeverseBitsInAByterfc1423Algorfc1423Algosrfc2821MailboxrootCertificatesignatureAlgorithmDetailssignaturePublicKeyAlgoMismatchErrorsigningParamsForPublicKeysubjectBytessystemRootssystemRootsErrsystemRootsMusystemRootsPooltbsCertificatetbsCertificateListtbsCertificateRequesttoLowerCaseASCIIvalidHostnamevalidHostnameInputvalidHostnamePatternvalidityx509sha1x509v2VersioncipherFunckeySizederiveKeyRelativeNameRawAttributesTBSCSRRawTBSRevocationListrlextKeyUsageSignerpubKeyAlgoRawTBSCertificateRequestValidityUniqueIdSubjectUniqueIdTBSCertificateAdditionalPrimesDetailAlgohintErrhintCertDistributionPointCRLIssuerMGFTrailerFieldNamedCurveOIDBLAKE2b_256BLAKE2b_384BLAKE2b_512BLAKE2s_256DecrypterMD4MD5MD5SHA1RIPEMD160RegisterHashSHA3_224SHA3_256SHA3_384SHA3_512SHA512_224SHA512_256digestSizesmaxHashcryptoColumnConverterConnBeginTxConnPrepareContextConnectorDefaultParameterConverterDriverContextErrBadConnErrRemoveArgumentErrSkipExecerExecerContextIsScanValueIsValueIsolationLevelNamedValueNamedValueCheckerNotNullNullPingerQueryerQueryerContextResultNoRowsRowsAffectedRowsColumnTypeDatabaseTypeNameRowsColumnTypeLengthRowsColumnTypeNullableRowsColumnTypePrecisionScaleRowsColumnTypeScanTypeRowsNextResultSetSessionResetterStmtStmtExecContextStmtQueryContextTxOptionsValueConvertercallValuerValuedecimalDecomposedefaultConverterint32TypenoRowsvaluerReflectTypeOrdinalCheckNamedValueResetSessionLastInsertIdNumInputPrepareContextConvertValueConverterQueryContextColumnTypePrecisionScaleExecContextHasNextResultSetNextResultSetColumnTypeLengthIsolationBeginTxColumnTypeNullableOpenConnectorColumnTypeDatabaseTypeNameColumnTypeScanTypeDecomposedatabase/sql/driverColumnTypeDBStatsDriversErrConnDoneErrNoRowsErrTxDoneLevelDefaultLevelLinearizableLevelReadCommittedLevelReadUncommittedLevelRepeatableReadLevelSerializableLevelSnapshotLevelWriteCommittedNamedArgNullByteNullInt16NullInt32OpenDBRawBytesalwaysNewConnasBytesasStringbypassRowsAwaitDonecachedOrNewConnccCheckerconnRequestconnReuseStrategyconnStmtconnectionRequestQueueSizeconvertAssignconvertAssignRowsctxDriverBeginctxDriverExecctxDriverPreparectxDriverQueryctxDriverStmtExecctxDriverStmtQuerydebugGetPutdecimalComposedefaultCheckNamedValuedefaultMaxIdleConnsdepSetdescribeNamedValuedriverArgsConnLockeddriverConndriverResultdriverStmtdriversdriversMudsnConnectorerrDBClosederrNilPtrerrNoRowserrRowsClosedfinalCloserhookTxGrabConnmaxBadConnRetriesnamedValueToValuenowFuncputConnHookreleaseConnresultFromStatementrollbackHookrowsCloseHookrowsColumnInfoSetupConnLockedrowsiFromStatementstmtConnGrabberstrconvErrunregisterAllDriversvalidateNamedValueNamewithLock_NamedFieldsRequiredMaxOpenConnectionsOpenConnectionsWaitCountWaitDurationMaxIdleClosedMaxIdleTimeClosedMaxLifetimeClosedfinalClosewaitDurationconnectornumClosedfreeConnconnRequestsnextRequestnumOpenopenerChlastPutmaxIdleCountmaxOpenmaxLifetimemaxIdleTimecleanerChwaitCountmaxIdleClosedmaxIdleTimeClosedmaxLifetimeClosedaddDepaddDepLockedremoveDepremoveDepLockedpingDCPingContextmaxIdleConnsLockedshortestIdleTimeLockedSetMaxIdleConnsSetMaxOpenConnsSetConnMaxLifetimeSetConnMaxIdleTimestartCleanerLockedconnectionCleanerconnectionCleanerRunLockedmaybeOpenNewConnectionsconnectionOpeneropenNewConnectionnextRequestKeyLockednoteUnusedDriverStatementputConnputConnDBLockedprepareDCexecDCqueryDCQueryRowContextQueryRowbeginbeginDCneedResetfinalClosedopenStmtinUsereturnedAtonPutdbmuClosedremoveOpenStmtresetSessionvalidateConnectionprepareLockedcloseDBLockedgrabConntxCtxComposerowsicloseStmtclosemulasterrlastcolslasterrOrErrLockedinitContextCloseawaitDonenextLockedColumnTypeshasNullablehasLengthhasPrecisionScalenullabledatabaseTypeDecimalSizeScanTypeNullableDatabaseTypeNameBytestickyErrcgcgdsparentStmtcsslastNumClosedremoveClosedStmtLockedprepareOnConnLockedtxikeepConnOnRollbackisDoneclosemuRUnlockReleaseclosePreparedStmtContextresiclosemuRUnlockCondReleaseConnDestdsnccisqldatabase/sqlsortSearchtrimSlashreadDirembedClassApplicationClassContextSpecificClassPrivateClassUniversalEnumeratedMarshalWithParamsNullBytesNullRawValueStructuralErrorSyntaxErrorTagBMPStringTagBitStringTagBooleanTagEnumTagGeneralStringTagGeneralizedTimeTagIA5StringTagIntegerTagNullTagNumericStringTagOIDTagOctetStringTagPrintableStringTagSequenceTagT61StringTagUTCTimeTagUTF8StringUnmarshalWithParamsallowAmpersandallowAsteriskampersandFlagappendBase128IntappendFourDigitsappendGeneralizedTimeappendLengthappendTagAndLengthappendTimeCommonappendTwoDigitsappendUTCTimeasteriskFlagbase128IntLengthbigIntTypebitStringEncoderbitStringTypebyte00EncoderbyteEncoderbyteFFEncoderbytesEncodercanHaveDefaultValuecheckIntegerenumeratedTypefieldParametersflagTypegetUniversalTypeint64EncoderinvalidLengthinvalidUnmarshalErrorlengthLengthmakeBigIntmakeBodymakeFieldmakeGeneralizedTimemakeIA5StringmakeNumericStringmakeObjectIdentifiermakePrintableStringmakeUTCTimemakeUTF8StringmultiEncoderobjectIdentifierTypeoidEncoderoutsideUTCRangeparseBMPStringparseBase128IntparseBigIntparseBitStringparseBoolparseFieldParametersparseGeneralizedTimeparseIA5StringparseInt32parseInt64parseNumericStringparseObjectIdentifierparsePrintableStringparseSequenceOfparseT61StringparseTagAndLengthparseUTCTimeparseUTF8StringrawContentsTyperawValueTyperejectAmpersandrejectAsterisksetDefaultValuesetEncoderstringEncoderstripTagAndLengthtagAndLengthtaggedEncodertimeTypeexplicitapplicationisCompoundasn1encoding/asn1HexEncodingNewEncodingNoPaddingStdPaddingdecodeMapInitializeencodeHexencodeStdnewlineFilteringReaderreadEncodedDatastripNewlines640nbufoutbufbase32encoding/base32RawStdEncodingRawURLEncodingassemble32assemble64encodeURL768AppendByteOrderAppendUvarintAppendVarintMaxVarintLen16PutVarintReadUvarintReadVarintVarintcoderdataSizeintDataSizesizeofstructSizeErrBareQuoteErrQuoteErrTrailingCommaerrInvalidDelimlengthNLnextRunevalidDelimGobDecoderGobEncoderRegisterNameallocValuebinaryMarshalerInterfaceTypebinaryUnmarshalerInterfaceTypebootstrapTypebuildEncEnginebuildTypeInfobuiltinIdToTypecatchErrorcheckIdcompileEncconcreteTypeToNamedebugFuncdecAllocdecArrayHelperdecBooldecBoolArraydecBoolSlicedecComplex128decComplex128ArraydecComplex128SlicedecComplex64decComplex64ArraydecComplex64SlicedecFloat32decFloat32ArraydecFloat32SlicedecFloat64decFloat64ArraydecFloat64SlicedecIgnoreOpMapdecInt16decInt16ArraydecInt16SlicedecInt32decInt32ArraydecInt32SlicedecInt64decInt64ArraydecInt64SlicedecInt8decInt8ArraydecInt8SlicedecIntArraydecIntSlicedecOpTabledecSliceHelperdecStringdecStringArraydecStringSlicedecUint16decUint16ArraydecUint16SlicedecUint32decUint32ArraydecUint32SlicedecUint64decUint64ArraydecUint64SlicedecUint8decUint8SlicedecUintArraydecUintSlicedecUintptrArraydecUintptrSlicedecodeIntoValuedecodeUintReaderemptyStructemptyStructTypeencArrayHelperencBoolencBoolArrayencBoolSliceencBufferPoolencComplexencComplex128ArrayencComplex128SliceencComplex64ArrayencComplex64SliceencFloatencFloat32ArrayencFloat32SliceencFloat64ArrayencFloat64SliceencIndirectencIntencInt16ArrayencInt16SliceencInt32ArrayencInt32SliceencInt64ArrayencInt64SliceencInt8ArrayencInt8SliceencIntArrayencIntSliceencOpForencOpTableencSliceHelperencStringencStringArrayencStringSliceencStructTerminatorencUintencUint16ArrayencUint16SliceencUint32ArrayencUint32SliceencUint64ArrayencUint64SliceencUint8ArrayencUintArrayencUintSliceencUintptrArrayencUintptrSliceencodeReflectValueerrBadCounterrBadTypeerrBadUinterrRangeerror_firstUserIdfloat32FromBitsfloat64FromBitsfloatBitsgetBaseTypegetEncEnginegetTypeInfogobDecoderInterfaceTypegobEncodeOpForgobEncoderInterfaceTypegobErroridToTypeignoreTwoUintsignoreUintignoreUint8ArrayimplementsInterfaceintBitslookupTypeInfomaxIgnoreNestingDepthmustGetTypeInfonameToConcreteTypenewArrayTypenewGobEncoderTypenewMapTypenewSliceTypenewStructTypenewTypeObjectnextIdnoValueregisterBasicssetTypeIdsingletonFieldspaceForLengthtBooltBytestComplextFloattInttInterfacetReserved1tReserved2tReserved3tReserved4tReserved5tReserved6tReserved7tStringtUinttWireTypetextMarshalerInterfaceTypetextUnmarshalerInterfaceTypetoInttooBigtypeInfoMaptypeLockuint64SizeuintptrBitsuserTypeCachevalidUserTypewireTypeUserInfoxBinaryxGobxTextencInitDumperErrLengthInvalidByteErrordumperhextablereverseHexTabletoCharrightCharsDelimHTMLEscapeInvalidUTF8ErrorInvalidUnmarshalErrorMarshalerErrorUnmarshalFieldErrorUnmarshalTypeErrorUnsupportedTypeErrorUnsupportedValueErroraddrMarshalerEncoderaddrTextMarshalerEncoderarrayEncoderboolEncodercondAddrEncoderencOptsencodeByteSliceencodeStateencodeStatePoolencoderCacheencoderFuncfloat32Encoderfloat64EncoderfloatEncoderfreeScannergetu4htmlSafeSetindirectintEncoderinterfaceEncoderinvalidValueEncoderisEmptyValueisValidNumberisValidTagjsonErrormapEncodermarshalerEncodermarshalerTypemaxNestingDepthnewArrayEncodernewCondAddrEncodernewEncodeStatenewMapEncodernewPtrEncodernewScannernewSliceEncodernewStructEncodernewTypeEncodernonSpacenullLiteralnumberTypeparseArrayValueparseObjectKeyparseObjectValuephasePanicMsgptrEncoderquoteCharreflectWithStringsafeSetscanArrayValuescanBeginArrayscanBeginLiteralscanBeginObjectscanContinuescanEndscanEndArrayscanEndObjectscanErrorscanObjectKeyscanObjectValuescanSkipSpacescannerPoolsliceEncoderstartDetectingCyclesAfterstate0state1stateBeginStringstateBeginStringOrEmptystateBeginValuestateBeginValueOrEmptystateDotstateDot0stateEstateE0stateESignstateEndTopstateEndValuestateErrorstateFstateFastateFalstateFalsstateInStringstateInStringEscstateInStringEscUstateInStringEscU1stateInStringEscU12stateInStringEscU123stateNstateNegstateNustateNulstateTstateTrstateTrustructEncoderstructFieldstagOptionstextMarshalerEncodertextMarshalerTypetextUnmarshalerTypetokenArrayCommatokenArrayStarttokenArrayValuetokenObjectColontokenObjectCommatokenObjectKeytokenObjectStarttokenObjectValuetokenTopValuetypeByIndextypeEncoderuintEncoderunquoteunquoteBytesunquotedValueunsupportedTypeEncodervalueEncoderptrLevelptrSeenreflectValuestringByteselemEncnameNonEscnameEscHTMLarrayEncnameIndexsourceFuncresolvecanAddrEncelseEncEncodeToMemorygetLinelineBreakerpemEndpemEndOfLinepemLineLengthpemStartremoveSpacesAndTabspemencoding/pemAttrCharDataCopyTokenDirectiveEndElementEscapeEscapeTextHTMLAutoCloseHTMLEntityMarshalerAttrNewTokenDecoderProcInstStartElementTagPathErrorTokenReaderUnmarshalErrorUnmarshalerAttraddFieldInfoattrTypebegCommentcdataEndcdataEscapecdataStartcopyValueddBytesdefaultStartdontInitNilPointersemitCDATAendCommentendProcInsterrRawTokenerrUnmarshalDepthescAmpescAposescCRescFFFDescGTescLTescNLescQuotescTabfAnyfAttrfCDATAfCharDatafCommentfElementfInnerXMLfModefOmitEmptyfieldFlagsfieldInfohtmlAutoClosehtmlEntityinitNilPointersisInCharacterRangeisNameisNameByteisNameStringisValidDirectivelookupXMLNamemarshalerAttrTypemaxUnmarshalDepthmaxUnmarshalDepthWasmnameTypeparentStackprocInstreceiverTypestkEOFstkNsstkStarttinfoMapunmarshalerAttrTypeunmarshalerTypexmlNamexmlPrefixxmlURLxmlnsPrefixxmlnsfinfoindentedInputNewlineattrNSattrPrefixprefixescreateAttrPrefixdeleteAttrPrefixmarkPrefixpopPrefixmarshalValuemarshalAttrmarshalInterfacemarshalTextInterfacewriteStartwriteEndmarshalSimplemarshalStructcachedWriteErrorwriteIndentEscapeStringEncodeElementEncodeTokenAutoCloseCharsetReaderDefaultSpacesavedstkneedClosetoClosenextTokennextBytelinestartunmarshalDepthDecodeElementunmarshalInterfaceunmarshalTextInterfaceunmarshalAttrunmarshalPathtranslateswitchToReaderpushEOFpopEOFpushElementpushNssyntaxErrorpopElementautoCloseRawTokenrawTokenattrvalspacegetcInputPossavedOffsetmustgetcungetcnsnamereadNameUnmarshalXMLxmlnameMarshalXMLAttrUnmarshalXMLAttrMarshalXMLField1Field2xmlencoding/xmlerrorStringjoinErrorchanDirNewMapexpvarHandlervarKeysvarKeysMuCommandLineContinueOnErrorGetterPanicOnErrorUnquoteUsageboolFlagcommandLineUsagedurationValueerrParsefloat64ValuefuncValueint64ValueisZeroValuenewBoolValuenewDurationValuenewFloat64ValuenewInt64ValuenewIntValuenewStringValuenewTextValuenewUint64ValuenewUintValuenumErrorsortFlagsstringValuetextValueuint64ValueuintValueIsBoolFlagAppendfAppendlnFormatStringFormatterFscanFscanfFscanlnGoStringerScanfSscanSscanlnbadIndexStringbadPrecStringbadWidthStringbinaryDigitsboolErrorcommaSpaceStringcomplexErrordecimalDigitsexponentextraStringfloatVerbsfmtFlagsgetFieldhasXhexDigithexadecimalDigitshugeWidindexRuneintFromArginvReflectStringldigitsmapStringmissingStringnewPrinternewScanStatenilAngleStringnilParenStringnilStringnoVerbStringnotSpaceoctalDigitspanicStringparseArgNumberparsenumpercentBangStringppFreereadRunesignedssFreessavetooLargeudigitswrapErrorwrapErrorswidPresentprecPresentminusplusplusVsharpVpendBufreadBytevalidSavenlIsEndnlIsSpaceargLimitmaxWidwriteRunegetRunemustReadRunenotEOFokVerbscanBoolgetBasescanRunescanBasePrefixscanIntscanUintfloatTokencomplexTokensconvertFloatscanComplexconvertStringhexBytescanPercentscanOnedoScandoScanfwidprecintbufclearflagspadStringfmtBooleanfmtUnicodefmtIntegertruncateStringfmtSfmtBsfmtSbxfmtSxfmtBxfmtQfmtCfmtQcfmtFloatreorderedgoodArgNumpanickingerroringwrapErrswrappedErrsunknownTypebadVerbfmtBoolfmt0x64fmtComplexfmtStringfmtBytesfmtPointercatchPanichandleMethodsprintArgprintValueargNumberbadArgNummissingArgdoPrintfdoPrintdoPrintlnAddToUserAgentCheckForUserCompletionCheckForUserCompletionWithContextCreateSenderDecorateSenderDeviceCodeErrDeviceAccessDeniedErrDeviceAuthorizationPendingErrDeviceCodeEmptyErrDeviceCodeExpiredErrDeviceGenericErrDeviceSlowDownErrOAuthTokenEmptyGetMSIAppServiceEndpointGetMSIEndpointGetMSIVMEndpointInitiateDeviceAuthInitiateDeviceAuthWithContextLoadTokenMultiTenantOAuthConfigMultiTenantServicePrincipalTokenMultitenantOAuthTokenProviderNewMultiTenantOAuthConfigNewMultiTenantServicePrincipalTokenNewOAuthConfigNewOAuthConfigWithAPIVersionNewServicePrincipalTokenNewServicePrincipalTokenFromAuthorizationCodeNewServicePrincipalTokenFromCertificateNewServicePrincipalTokenFromMSINewServicePrincipalTokenFromMSIWithUserAssignedIDNewServicePrincipalTokenFromManualTokenNewServicePrincipalTokenFromManualTokenSecretNewServicePrincipalTokenFromUsernamePasswordNewServicePrincipalTokenWithSecretOAuthConfigOAuthGrantTypeAuthorizationCodeOAuthGrantTypeClientCredentialsOAuthGrantTypeDeviceCodeOAuthGrantTypeRefreshTokenOAuthGrantTypeUserPassOAuthOptionsOAuthTokenProviderRefresherRefresherWithContextSaveTokenSendDecoratorSenderSenderFuncServicePrincipalAuthorizationCodeSecretServicePrincipalCertificateSecretServicePrincipalMSISecretServicePrincipalNoSecretServicePrincipalSecretServicePrincipalTokenServicePrincipalTokenSecretServicePrincipalUsernamePasswordSecretTokenErrorTokenRefreshTokenRefreshCallbackTokenRefreshErrorWaitForUserCompletionWaitForUserCompletionWithContextactiveDirectoryEndpointTemplateasMSIEndpointEnvasMSISecretEnvdefaultMaxMSIRefreshAttemptsdefaultRefreshdefaultSenderdefaultSenderInitdeviceTokenerrCodeHandlingFailserrCodeSendingFailserrStatusNotOKerrTokenHandlingFailserrTokenSendingFailsisAppServiceisIMDSlogPrefixmetadataHeadermimeTypeFormPostmsiEndpointmultiTenantOAuthConfignewServicePrincipalTokenFromMSInewTokennewTokenRefreshErrorresponseHasStatusCoderetryForIMDSsenderservicePrincipalTokentokenRefreshErrorvalidateOAuthConfigvalidateStringParamExpiresOnIsExpiredWillExpireInOAuthTokenSetAuthenticationValuesAuthorityEndpointAuthorizeEndpointTokenEndpointDeviceCodeEndpointoacOauthConfigAutoRefreshRefreshWithinrefreshLockcustomRefreshFuncrefreshCallbacksMaxMSIRefreshAttemptsMarshalTokenJSONsptSetRefreshCallbacksSetCustomRefreshFuncEnsureFreshEnsureFreshWithContextInvokeRefreshCallbacksRefreshWithContextRefreshExchangeRefreshExchangeWithContextgetGrantTyperefreshInternalSetAutoRefreshSetRefreshWithinSetSenderSignJwtErrorCodesErrorDescriptionAuxiliaryOAuthTokensPrimaryOAuthTokenUserCodeVerificationURLAuthorizationCodeRedirectURInoSecretmsiSecrettokenSecretPrimaryTenantAuxiliaryTenantsPrimaryTokenAuxiliaryTokenstreadalgithub.com/Azure/go-autorest/autorest/adalActiveDirectoryEndpointAuthorizerConfigAuxiliaryTenantIDsCertificatePasswordCertificatePathClientCertificateConfigClientCredentialsConfigDeviceFlowConfigEnvironmentNameEnvironmentSettingsFileSettingsGalleryEndpointGetSettingsFromEnvironmentGetSettingsFromFileGraphResourceIDMSIConfigManagementEndpointNewAuthorizerFromCLINewAuthorizerFromCLIWithResourceNewAuthorizerFromEnvironmentNewAuthorizerFromEnvironmentWithResourceNewAuthorizerFromFileNewAuthorizerFromFileWithResourceNewClientCertificateConfigNewClientCredentialsConfigNewDeviceFlowConfigNewMSIConfigNewUsernamePasswordConfigResourceManagerEndpointSQLManagementEndpointSubscriptionIDTenantIDUsernamePasswordConfigdecodePkcs12ResourceIdentifierGraphKeyVaultDatalakeOperationalInsightsManagementPortalURLPublishSettingsURLServiceManagementEndpointKeyVaultEndpointGraphEndpointServiceBusEndpointBatchManagementEndpointStorageEndpointSuffixSQLDatabaseDNSSuffixTrafficManagerDNSSuffixKeyVaultDNSSuffixServiceBusEndpointSuffixServiceManagementVMDNSSuffixResourceManagerVMDNSSuffixContainerRegistryDNSSuffixCosmosDBDNSSuffixTokenAudienceResourceIdentifiersGetSubscriptionIDsettingsgetClientAndTenantGetClientCredentialsGetUsernamePasswordGetMSIGetDeviceFlowPrepareDecoratorPreparerWithAuthorizationAADEndpointcccAuxTenantsupsdfcsetKeyValuegetAADEndpointServicePrincipalTokenFromClientCredentialsClientCredentialsAuthorizerServicePrincipalTokenFromClientCredentialsWithResourceclientCertificateConfigWithResourceClientCredentialsAuthorizerWithResourceServicePrincipalTokenFromClientCertificateClientCertificateAuthorizerServicePrincipalTokenFromClientCertificateWithResourceClientCertificateAuthorizerWithResourcegetResourceForTokengithub.com/Azure/go-autorest/autorest/azure/authAccessTokensPathGetTokenFromCLILoadProfileLoadTokensParseExpirationDateProfilePathaccessTokensJSONazureProfileJSONconfigDirIdentityProviderIsMRRTToADALTokenIsDefaultInstallationIDgithub.com/Azure/go-autorest/autorest/azure/cliAsyncOpIncompleteErrorChinaCloudDoRetryWithRegistrationEnvironmentActiveDirectoryEndpointEnvironmentBatchManagementEndpointEnvironmentContainerRegistryDNSSuffixEnvironmentFilepathNameEnvironmentFromFileEnvironmentFromNameEnvironmentFromURLEnvironmentGalleryEndpointEnvironmentGraphEndpointEnvironmentKeyVaultDNSSuffixEnvironmentKeyVaultEndpointEnvironmentManagementPortalURLEnvironmentPropertyEnvironmentPublishSettingsURLEnvironmentResourceManagerEndpointEnvironmentResourceManagerVMDNSSuffixEnvironmentSQLDatabaseDNSSuffixEnvironmentServiceBusEndpointEnvironmentServiceBusEndpointSuffixEnvironmentServiceManagementEndpointEnvironmentServiceManagementVMDNSSuffixEnvironmentStorageEndpointSuffixEnvironmentTokenAudienceEnvironmentTrafficManagerDNSSuffixExtractClientIDExtractRequestIDGermanCloudHeaderClientIDHeaderRequestIDHeaderReturnClientIDIsAzureErrorNewAsyncOpIncompleteErrorNewErrorWithErrorNewFutureFromResponseNotAvailableOverridePropertyParseResourceIDPollingAsyncOperationPollingLocationPollingMethodTypePollingRequestURIPollingUnknownPublicCloudRequestErrorServiceErrorSetEnvironmentUSGovernmentCloudWithClientIDWithErrorUnlessStatusCodeWithReturnClientIDWithReturningClientIDaudienceauthenticationcreatePollingTrackerenvironmentMetadataInfoenvironmentsgetProvidergetSubscriptiongetURLFromAsyncOpHeadergetURLFromLocationHeaderheaderAsyncOperationisValidURLoperationCanceledoperationFailedoperationInProgressoperationSucceededoverridePropertiespollingCodespollingTrackerpollingTrackerBasepollingTrackerDeletepollingTrackerPatchpollingTrackerPostpollingTrackerPutretrieveMetadataEnvironmentRespondDecoratorResponderDetailedErrorOriginalPackageTypeInnerErrorAdditionalInfopopulaterawBodyPmFinalGetURIinitializeStategetProvisioningStateupdateRawBodypollForStatusupdateErrorFromResponseupdatePollingStatepollingErrorpollingMethodpollingStatuspollingURLfinalGetURLhasTerminatedhasFailedhasSucceededlatestResponsebaseCheckForErrorsinitPollingMethodupdatePollingMethodcheckForErrorsprovisioningStateApplicableRequestInspectorResponseInspectorPollingDelayPollingDurationRetryAttemptsRetryDurationSkipResourceProviderRegistrationSendDecoratorsWithInspectionByInspectingFutureTypePollingMethodDoneWithContextGetPollingDelayWaitForCompletionRefPollingURLResourceGroupLoginEndpointPortalEndpointazuregithub.com/Azure/go-autorest/autorest/azureNewUnixTimeFromDurationNewUnixTimeFromNanosecondsNewUnixTimeFromSecondsTimeRFC1123UnixEpochUnixTimeazureUtcFormatazureUtcFormatJSONdateFormatfullDatefullDateJSONjsonFormatparseDaterfc1123rfc1123JSONrfc3339JSONtzOffsetRegexunixEpochToTimegithub.com/Azure/go-autorest/autorest/dateAPIKeyAuthorizerAfterDelayAsContentTypeAsDeleteAsFormURLEncodedAsGetAsHeadAsIsAsJSONAsMergeAsOctetStreamAsOptionsAsPatchAsPostAsPutAsStringSliceBasicAuthorizerBearerAuthorizerBearerAuthorizerCallbackBearerAuthorizerCallbackFuncByClosingByClosingIfErrorByCopyingByDiscardingBodyByIgnoringByUnmarshallingBytesByUnmarshallingJSONByUnmarshallingXMLChangeToGetClientOptionsCognitiveServicesAuthorizerCopyAndDecodeCount429AsRetryCreatePreparerCreateResponderDecoratePreparerDecorateResponderDefaultPollingDelayDefaultPollingDurationDefaultRetryAttemptsDefaultRetryDurationDelayForBackoffDelayForBackoffWithCapDelayWithRetryAfterDoCloseIfErrorDoErrorIfStatusCodeDoErrorUnlessStatusCodeDoPollForStatusCodesDoRetryForAttemptsDoRetryForDurationDoRetryForStatusCodesDoRetryForStatusCodesWithCapDrainResponseBodyEncodedAsEncodedAsJSONEncodedAsXMLEventGridKeyAuthorizerExtractHeaderExtractHeaderValueGetPrepareDecoratorsGetRetryAfterGetSendDecoratorsHeaderLocationHeaderRetryAfterIsTemporaryNetworkErrorIsTokenRefreshErrorLoggingInspectorMapToValuesMax429DelayMultiTenantServicePrincipalTokenAuthorizerNewAPIKeyAuthorizerNewAPIKeyAuthorizerWithHeadersNewAPIKeyAuthorizerWithQueryParametersNewBasicAuthorizerNewBearerAuthorizerNewBearerAuthorizerCallbackNewClientWithOptionsNewClientWithUserAgentNewCognitiveServicesAuthorizerNewErrorWithResponseNewEventGridKeyAuthorizerNewMultiTenantServicePrincipalTokenAuthorizerNewPollingRequestNewPollingRequestWithContextNewRetriableRequestNewSASTokenAuthorizerNewSharedKeyAuthorizerNullAuthorizerPreparerFuncResponderFuncResponseHasStatusCodeRetriableRequestSASTokenAuthorizerSendWithSenderSharedKeySharedKeyAuthorizerSharedKeyForTableSharedKeyLiteSharedKeyLiteForTableSharedKeyTypeStatusCodesForRetryTeeReadCloserUndefinedStatusCodeWithBaseURLWithBearerAuthorizationWithBoolWithBytesWithCustomBaseURLWithErrorUnlessOKWithEscapedPathParametersWithFileWithFloat32WithFloat64WithFormDataWithHeadersWithInt32WithInt64WithJSONWithLoggingWithMethodWithMultiPartFormDataWithNothingWithPathParametersWithPrepareDecoratorsWithQueryParametersWithSendDecoratorsWithStringWithUserAgentWithXMLapiKeyAuthorizerHeaderbearerChallengebearerChallengeHeaderbingAPISdkHeaderbuildCanonicalizedHeaderbuildCanonicalizedResourcebuildCanonicalizedStringbuildSharedKeycontainsIntcreateAuthorizationHeaderctxPrepareDecoratorsctxSendDecoratorsdoRetryForStatusCodesImplensureValueStringensureValueStringsescapeValueStringsgetCanonicalizedAccountNamegolangBingAPISdkHeaderValuehasBearerChallengeheaderAcceptheaderAcceptCharsetheaderAuthorizationheaderAuxAuthorizationheaderContentLanguageheaderContentLengthheaderContentMD5headerDateheaderIfMatchheaderIfModifiedSinceheaderIfNoneMatchheaderIfUnmodifiedSinceheaderRangeheaderUserAgentheaderXMSDateheaderXMSVersionmimeTypeJSONmimeTypeOctetStreammultiTenantSPTAuthorizernewBearerChallengeparseURLpathEscapequeryEscaperemoveRequestBodyrequestFormatresponseFormatstorageEmulatorAccountNameteeReadClosertenantIDbatokenProviderIsHTTPStatusHasHTTPStatusprepareFromByteReaderbacbsubscriptionKeycsaqueryParametersakaaccountNameaccountKeykeyTypesklitopicKeyegtasasTokensasautorestgithub.com/Azure/go-autorest/autorestLevelTypeLogDebugLogFatalLogInfoLogNoneLogPanicLogWarningParseLevelentryHeaderfileLoggerinitDefaultLoggerlogDebuglogErrorlogFatallogInfologNonelogPaniclogUnknownlogWarningnilLoggerprocessURLprocessHeaderprocessBodyWriteRequestWritefWritelnshouldLogBodygithub.com/Azure/go-autorest/loggerIsEnabledNewTransportgithub.com/Azure/go-autorest/tracingarrayEndarrayStartarrayTableEndarrayTableStartbadtypecommacommentStarteindirectencPanicerrAnonNonStructerrAnythingerrArrayMixedElementTypeserrArrayNilElementerrArrayNoTableerrNoKeyerrNonStringfloatAddDecimalgetOptionsinlineTableEndinlineTableStartisBareKeyCharisHexadecimalisNLisStringTypeisUnifiableisValidKeyNameisWhitespaceitemArrayitemArrayEnditemArrayTableEnditemArrayTableStartitemBoolitemCommentStartitemDatetimeitemEOFitemErroritemFloatitemInlineTableEnditemInlineTableStartitemIntegeritemKeyStartitemMultilineStringitemNILitemRawMultilineStringitemRawStringitemStringitemTableEnditemTableStartitemTextkeySeplexArrayEndlexArrayTableEndlexArrayValuelexArrayValueEndlexBareKeylexBareTableNamelexBoollexCommentlexCommentStartlexDatetimelexFloatlexInlineTableEndlexInlineTableValuelexInlineTableValueEndlexKeyEndlexKeyStartlexLongUnicodeEscapelexMultilineRawStringlexMultilineStringlexMultilineStringEscapelexNumberlexNumberOrDatelexNumberOrDateStartlexNumberStartlexRawStringlexShortUnicodeEscapelexSkiplexStringlexStringEscapelexTableEndlexTableNameEndlexTableNameStartlexTableStartlexToplexTopEndlexValuenumPeriodsOKnumUnderscoresOKpanicIfInvalidKeyquotedReplacerrawStringEndrawStringStartrvaluestringEndstringStartstripEscapedWhitespacestripFirstNewlinetableEndtableSeptableStarttomlArraytomlArrayHashtomlArrayTypetomlBaseTypetomlBooltomlDatetimetomlEncodeErrortomlFloattomlHashtomlIntegertomlStringtomlTypeOfGotypeEqualtypeIsHashitypeprevWidthsnprevlxemitTrimbtypecurrentKeyapproxLineimplicitspanicfbugassertEqualtopLevelkeyStringestablishContextsetTypeaddImplicitremoveImplicitreplaceEscapesasciiEscapeToUnicodetypeOfPrimitivetypeOfArrayomitemptyomitzeroAnyArgCSVColumnParserErrCancelledExpectedBeginExpectedCloseExpectedCommitExpectedExecExpectedPingExpectedPrepareExpectedQueryExpectedRollbackMonitorPingsOptionNewErrorResultNewResultNewRowsNewWithDSNQueryMatcherQueryMatcherEqualQueryMatcherFuncQueryMatcherOptionQueryMatcherRegexpSqlmockValueConverterOptionanyArgumentcommonExpectationconvertValueToNamedValueexpectationmockDriverqueryBasedExpectationrawBytesrowSetssqlmockstatementstripQuerytriggeredfulfilleddelayWillDelayForWillReturnErrordrvconverterqueryMatchermonitorPingsExpectCloseMatchExpectationsInOrderExpectationsWereMetExpectBeginExpectExecExpectPrepareExpectQueryExpectCommitExpectRollbackExpectPingexpectSQLargsMatchesattemptArgMatchWithArgsWillReturnResultmustBeClosedwasClosedWillReturnCloseErrorWillBeClosedrowsMustBeClosedrowsWereClosedRowsWillBeClosedWillReturnRowsnextErrCloseErrorRowErrorFromCSVStringinsertIDrowsAffectedinvalidateRawgithub.com/DATA-DOG/go-sqlmockCompressionLevelContentTypesDefaultMinSizeDefaultQValueGzipHandlerWithOptsGzipResponseWriterGzipResponseWriterWithCloseNotifyMinSizeMustNewGzipLevelHandlerNewGzipLevelAndMinSizeNewGzipLevelHandleracceptEncodingacceptsGzipaddLevelPoolcodingsgzipWriterPoolshandleContentTypeparseCodingparseEncodingspoolIndexsetAcceptEncodingForPushOptionsvaryPushOptionscontentTypesstartGzipAddOffsetBitmapOfBoundSerializedSizeInBytesFastAndHeapOrHeapXorMaxRangeMaxUint16ParAndParHeapOrParOr_logS_maddHelper16appenderRoutinearrayContainerarrayContainerSizeInBytesarrayContypearrayDefaultMaxSizearrayLazyLowerBoundbaseDiskRc16SizebaseRc16SizebcBaseBytesbitmapContainerbitmapContainerHeapbitmapContainerKeybitmapContainerManyIteratorbitmapContainerShortIteratorbitmapContainerSizeInBytesbitmapContypebitmapEqualsbyteSliceAsInterval16SlicebyteSliceAsUint16SlicebyteSliceAsUint64SlicecanMerge16clzcontainerPriorityQueuecontaineritemcopyOfcountLeadingZeroscountTrailingZerosdefaultWorkerCounterrCorruptedStreamexclusiveUnion2by2fillArrayANDfillArrayANDNOTfillArrayXORfillRangeflipBitmapRangeflipBitmapRangeAndCardinalityChangegetRandomPermutationgetSizeInBytesFromCardinalityhave4Overlap16haveOverlap16highbitsintIteratorintReverseIteratorintersectInterval16sintersectWithLeftover16intersection2by2intersection2by2Cardinalityintersects2by2interval16invalidCardinalityivalString16keyedContainerlazyIOrOnRangelazyOrOnRangelocalintersect2by2localintersect2by2CardinalitylowbitsmanyIntIteratormanyIteratormanyRunIterator16maxCapacitymaxLowBitmaxOfIntmaxOfUint16maxUint16maxWordmergeInterval16sminOfIntminOfUint16minUint16multipleContainersnewArrayContainernewArrayContainerCapacitynewArrayContainerFromBitmapnewArrayContainerRangenewArrayContainerSizenewBitmapContainernewBitmapContainerFromRunnewBitmapContainerHeapnewBitmapContainerManyIteratornewBitmapContainerShortIteratornewBitmapContainerwithRangenewIntIteratornewIntReverseIteratornewInterval16RangenewManyIntIteratornewReverseBitmapContainerShortIteratornewRoaringArraynewRunContainer16newRunContainer16CopyIvnewRunContainer16FromArraynewRunContainer16FromBitmapContainernewRunContainer16FromContainernewRunContainer16FromValsnewRunContainer16RangenewRunContainer16TakeOwnershipnoOffsetThresholdonesidedgallopingintersect2by2onesidedgallopingintersect2by2CardinalitypanicOnparChunkparChunkSpecparNaiveStartAtperIntervalRc16SizephphapopcntAndSlicepopcntAndSliceGopopcntMaskSlicepopcntMaskSliceGopopcntOrSlicepopcntOrSliceGopopcntSlicepopcntSliceGopopcntXorSlicepopcntXorSliceGopopcountrangeOfOnesresetBitmapRangeresetBitmapRangeAndCardinalityChangereverseBitmapContainerShortIteratorreverseIteratorrun16Contyperun32ContyperunContainer16runContainer16SerializedSizeInBytesrunIterator16runReverseIterator16searchOptionsselectBitPositionserialCookieserialCookieNoRunContainersetBitmapRangesetBitmapRangeAndCardinalityChangeshortIteratorsliceToString16toBitmapContaineruint16Sliceuint64SliceAsByteSliceunion2by2union2by2CardinalitywordCardinalityForBitmapRangewordSizeInBitswordSizeInBytesnotCloseiorArrayiorBitmapiorRun16lazyIorArraylazyIorBitmaplazyIorRun16orArrayorArrayCardinalitylazyorArrayiandBitmapxorArrayandNotRun16iandNotRun16andNotArrayiandNotArrayandNotBitmapandBitmapiandNotBitmapinotClosenegateRangeandArrayandArrayCardinalityintersectsArrayiandArrayloadDatabcorBitmaporBitmapCardinalityandBitmapCardinalitycomputeCardinalitylazyIORArraylazyORArraylazyIORBitmaplazyORBitmapxorBitmapiandRun16getCardinalityInRangeintersectsBitmapiandNotBitmapSurelybitValuetoArrayContainerfillArrayNextSetBitPrevSetBitasLittleEndianByteSlicebcsirunlenisSuperSetOfsubtractIntervalpopIncrementingkeyindexendxIndexmyOptsunionCardinalityindexOfIntervalAtOrAfterintersectCardinalitynumIntervalsnewRunIterator16newRunReverseIterator16newManyRunIterator16removeKeydeleteAtfindNextIntervalThatIntersectsStartingFromselectInt16invertlastIntervalinvertisubtractAndNotRunContainer16equals16andBitmapContainerinplaceIntersectiandBitmapContainerorBitmapContainerandBitmapContainerCardinalityorBitmapContainerCardinalityinplaceUnioniorBitmapContainerandNotRunContainer16iandNotRunContainer16xorRunContainer16curIndexcurPosInIndexcurSeqbitsetbcmipqrunstartactuallyAddedstoreIvalSetOnTracelnnewSQLTracesqlTracesqltracegithub.com/SAP/go-hdb/driver/sqltraceDSNDSNFetchSizeDSNLocaleDSNTLSInsecureSkipVerifyDSNTLSRootCAFileDSNTLSServerNameDSNTimeoutDecimalDefaultFetchSizeDriverNameDriverVersionErrDecimalOutOfRangeErrFloatOutOfRangeErrIntegerOutOfRangeErrNestedTransactionErrUnsupportedIsolationLevelHdbErrorHdbFatalErrorHdbWarningLobNewBasicAuthConnectorNewDSNConnectorNewLobNoFlushNullDecimalNullLobRandomIdentifierSessionVariables_SaccessModeStmtargsPoolbigIntFreebigRatFreebulkbulkInsertStmtcallResultStorecheckBulkInsertcheckCallProcedurecheckNamedValueconvertNamedValueconvertNvBytesconvertNvDecimalconvertNvFloatconvertNvIntegerconvertNvLobconvertNvStringconvertNvTimeconvertRatToDecimaldec128Biasdec128Digitsdec128MaxExpdec128MinExpdecFlagsdecimalSizedecodeDecimaldecodeTableQuerydfNotExactdfOverflowdfUnderflowdigits10driverDataFormatVersionencodeDecimalencodeTableQueryexp10flushTokhdbDrvisolationLevelisolationLevelStmtlg10maxBigintmaxDecimalmaxDoublemaxIntegermaxRealmaxSmallintmaxTinyintminBigintminFetchSizeminIntegerminSmallintminTimeoutminTinyintmodeReadOnlymodeReadWritenatOnenatTennatZeronewBulkInsertStmtnewConnectornewProcedureCallResultnewTxnoColumnsnoFlushToknoResultnoResultTypepingQueryprocedureCallResultprocedureCallResultStorequeryResultreBulkreCallreSimplescanTypeBigintscanTypeBytesscanTypeDecimalscanTypeDoublescanTypeIntegerscanTypeLobscanTypeRealscanTypeSmallintscanTypeStringscanTypeTimescanTypeTinyintscanTypeUnknownsessionVariabletableQueryPrefixtypeOfByteswriterSettersessionPrmFetchSizesessionConnisBadbadErrorinTxResetCntCntReadBReadCesu8WriteZeroesWriteBWriteCesu8WriteStringCesu8messageHeaderpacketCountvarPartLengthvarPartSizenoOfSegmsegmentHeadersegmentKindmessageTypecommandOptionsfunctionCodequeryTypesegmentLengthsegmentOfsnoOfPartssegmentNopartHeaderpartKindpartAttributesResultsetClosedLastPacketNoRowsargumentCountbigArgumentCountbufferLengthscramsha256InitialRequestclientChallengenumArgscramsha256InitialReplyserverChallengesetNumArgscramsha256FinalRequestclientProofscramsha256FinalReplyserverProoftopologyInformationmultiLineOptionsplainOptionsmlo_numArgconnectOptionsporesultMetadataResultFieldSetResultFieldsetNamesortOffsetscolumnOptionsTypeCodeisLobisCharBasedisVariableLengthisDecimalTypeTypeNamefractionTypeLengthTypePrecisionScaleresultFieldSetresultsetIDresultsetFieldValuesNumRowparameterMetadataParameterFieldSetParameterFieldparameterOptionsparameterModelobChunkReaderlocatorIDlobLocatorIDSetLobReader_inputFields_outputFieldsinputFieldsoutputFieldsNumInputFieldNumOutputFieldOutputFieldprmFieldSetoutputParameterswriteLobRequestlobPrmFieldsreadLobRequestlobChunkWriterSetWriterreadOfsLenwriteLobReplyreadLobReplystatementContexttransactionFlagshdbErrorshdbErrorerrorLevelsqlStateerrorPositionerrorTextLengthstmtNoerrorTextNumErrorSetIdxStmtNoIsWarningIsErrorIsFatalsetStmtNoisWarningsprmstmtCtxtxFlagsInTxSetInTxIsBadBadErrauthenticateScramsha256QueryDirectExecDirectDropStatementIDFetchNextCloseResultsetIDreadLobStreamwriteLobStreaminitRequestreadReplyTableResultattrsSetReader_tableRowstableRowsPartAttributeslocalefetchSizetlsConfigsessionVariablesSetLocaleSetFetchSizeSetTimeoutSetTLSConfigSetSessionVariablesBasicAuthDSNexecFlushexecBufferQueryTypeqtnoFlushdefaultQueryprocedureCalllastErrbeforeReadreplyPartrequestPartconnectOptiongithub.com/SAP/go-hdb/driverwriterBufferSizegithub.com/SAP/go-hdb/internal/bufioDtBigintDtBytesDtDecimalDtDoubleDtIntegerDtLobDtRealDtSmallintDtStringDtTimeDtTinyintDtUnknownNewSessionNewSnifferQtNoneQtProcedureCallQtSelectSniffer_DataType_index_DataType_name_QueryType_index_QueryType_name_TypeCode_index_0_TypeCode_index_1_TypeCode_index_2_TypeCode_index_4_TypeCode_index_6_TypeCode_name_0_TypeCode_name_1_TypeCode_name_2_TypeCode_name_3_TypeCode_name_4_TypeCode_name_5_TypeCode_name_6_connectOption_index_0_connectOption_index_1_connectOption_index_2_connectOption_index_3_connectOption_name_0_connectOption_name_1_connectOption_name_2_connectOption_name_3_connectOption_name_4_endianess_index_endianess_name_functionCode_index_functionCode_name_hmac_messageType_index_1_messageType_index_3_messageType_index_4_messageType_index_5_messageType_name_0_messageType_name_1_messageType_name_2_messageType_name_3_messageType_name_4_messageType_name_5_partKind_map_partKind_name_segmentKind_index_0_segmentKind_name_0_segmentKind_name_1_sha256_statementContextType_index_statementContextType_name_topologyOption_index_topologyOption_name_transactionFlagType_index_transactionFlagType_nameauthFieldSizebigintFieldSizebigintTypebinaryLobChunkReaderbinaryLobChunkWriterbinaryStringTypebooleanTypebytesLenIndBigbytesLenIndMediumbytesLenIndNullValuebytesLenIndSmallbytesSizecdmConnectioncdmConnectionStatementcdmOffcdmStatementcharLobChunkReadercharLobChunkWriterclientChallengeSizeclientProofDataSizeclientProofSizecoAbapVarcharModecoAssociatedConnectionIDcoClientDistributionModecoClientInfoNullValueSupportedcoClientLocalecoColumnarResultsetcoCompleteArrayExecutioncoConnectionIDcoDataFormatVersion2coDescribeTableOutputParametercoDistributionProtocolVersioncoEndianesscoEngineDataFormatVersioncoExecuteLocallycoFDAEnabledcoHoldCursorOverCommtitcoIgnoreUnknownPartscoImplicitLobStreamingcoItabParametercoLargeNumberOfParameterSupportcoMandatorycoNilcoNoResultsetCloseNeededcoNoTransactionalPreparecoOSUsercoOptionalcoRowslotImageResultcoScrollablResultSetcoScrollableCursorOncoSelectForUpdateSupportedcoSelfetchOffcoSplitBatchCommandscoSupportsLargeBulkOperationscoSystemIDcoTableOutputParametercoUseTransactionFlagsOnlycolumnDisplayNamecolumnOptionsTextcommandOptionsTextconvertDaydateToTimeconvertLongdateToTimeconvertSeconddateToTimeconvertSecondtimeToTimeconvertTimeToDayDateconvertTimeToLongdateconvertTimeToSeconddateconvertTimeToSecondtimedateFieldSizedaydateFieldSizedecimalFieldSizedfvBINTEXTdfvBaselinedfvDoNotUsedfvSPS06doubleFieldSizedoubleNullValuedoubleTypedpvBaselinedpvClientHandlesStatementSequenceendianesserrLoggererrorLevelErrorerrorLevelFatalErrorerrorLevelWarningfcAbapStreamfcCloseCursorfcCommitfcConnectfcDBProcedureCallfcDBProcedureCallWithResultfcDDLfcDeletefcDisconnectfcExplainfcFetchfcFindLobfcInsertfcNilfcPingfcReadLobfcRollbackfcSavepointfcSelectfcSelectForUpdatefcUpdatefcWriteLobfcXAJoinfcXAStartfetchsizefieldSizefixLengthfragmentgregorianDategregorianDayinitReplyinitRequestFillerinitRequestFillerSizeinputParametersintFieldSizeintTypejulianDayToTimejulianHdbloDataincludedloLastdataloNullindicatorlobChunkSizelobInputDescriptorSizelobOptionslobOptionsTextlocatorIDSizelongdateFieldSizemaxBinarySizemaxNamesmessageHeaderSizemnGSSmnSAMLmnSCRAMSHA256mtAbapStreammtAuthenticatemtCloseResultsetmtCommitmtConnectmtDisconnectmtDropStatementIDmtExecutemtExecuteDirectmtExecuteITabmtFetchAbsolutemtFetchFirstmtFetchLastmtFetchNextmtFetchNextITabmtFetchRelativemtFindLobmtInsertNextITabmtNilmtPreparemtReadLobmtRollbackmtWriteLobmtXAJoinmtXAStartnewClientIDnewConnectOptionsnewFieldNamesnewFieldValuesnewInitReplynewInitRequestnewInputParametersnewLobChunkReadernewLobChunkWriternewParameterFieldnewParameterFieldSetnewResultFieldnewResultFieldSetnewScramsha256FinalReplynewScramsha256FinalRequestnewSessionConnnewStatementContextnewTableResultnewTopologyInformationnewTransactionFlagsnoFieldNameokEndianessoutLoggerpaFirstPacketpaLastPacketpaNextPacketpaResultsetClosedpaRowNotFoundpadBytesparameterModeTextparameterOptionsTextpartAttributesTextpartHeaderSizepkAbapIStreampkAbapOStreampkAuthenticationpkBatchExecutepkBatchPreparepkClientContextpkClientIDpkClientInfopkCommandpkCommandInfopkCommitOptionspkConnectOptionspkDBConnectInfopkErrorpkFDAReplyMetadatapkFDARequestMetadatapkFetchOptionspkFetchSizepkFindLobReplypkFindLobRequestpkItabChunkMetadatapkItabMetadatapkItabResultChunkpkItabSHMpkLobFlagspkNilpkOStreamResultpkOutputParameterspkParameterMetadatapkParameterspkPartitionInformationpkProfilepkReadLobReplypkReadLobRequestpkResultMetadatapkResultsetpkResultsetIDpkResultsetOptionspkRowSlotImageParamMetadatapkRowSlotImageResultsetpkRowsAffectedpkSQLReplyOptionspkSessionContextpkSessionVariablepkStatementContextpkStatementIDpkStreamDatapkTableLocationpkTopologyInformationpkTransactionFlagspkTransactionIDpkWorkLoadReplayContextpkWriteLobReplypkWriteLobRequestpkXATransactionInfopmInpmInoutpmOutpoDefaultpoMandatorypoOptionalproductVersionraExecutionFailedraSuccessNoInforeadBytesSizereadDatereadDaydatereadDecimalreadFieldreadLobreadLobRequestSizereadLongdatereadMethodNamereadSeconddatereadSecondtimereadShortUtf8readTimereadUtf8realFieldSizerealNullValueresizeBufferresultsetIDSizescServerExecutionTimescStatementSequenceInfoschemaNameseconddateFieldSizesecondtimeFieldSizesegmentHeaderSizeserverChallengeDataSizeskErrorskInvalidskReplyskRequestsmallintFieldSizesqlStateSizestatementContextTypestatementIDSizetcAlphanumtcArraytcBiginttcBinarytcBlobtcBlocatortcBooleantcBstringtcChartcClobtcDatetcDaydatetcDecimaltcDoubletcIntegertcLongdatetcNchartcNclobtcNlocatortcNstringtcNulltcNvarchartcNvarchar3tcRealtcSeconddatetcSecondtimetcShorttexttcSmalldecimaltcSmallinttcStringtcTexttcTimetcTimestamptcTinyinttcVarbinarytcVarbinary3tcVarchartcVarchar2tcVarchar3tfCommitedtfDDLCommitmodeChangedtfNewIsolationLeveltfNowriteTransactionStartedtfRolledbacktfSessionClosingTransactionErrortfWriteTransactionStartedtimeFieldSizetimeToJulianDaytimestampFieldSizetinyintFieldSizetoAllHostNamestoAllIPAddressestoHostNametoHostPortnumbertoIsCurrentSessiontoIsMastertoIsStandbytoLoadfactortoNetworkDomaintoServiceTypetoTenantNametoVolumeIDtopologyOptiontransactionFlagTypeuint32SlicewriteAuthFieldwriteBytesSizewriteDatewriteDaydatewriteFieldwriteLobwriteLobRequestHeaderSizewriteLongdatewriteSeconddatewriteSecondtimewriteTimewriteUtf8ByteswriteUtf8StringzeroTimeproductprotocol_idcharLenreadOfs_eofruneCountnumOptionsdbAddrdbConnclRdclWrdbRddbWrgetBufferstreamPartstreamBinarystreamFragment_size_donegithub.com/SAP/go-hdb/internal/protocolCESUMaxEncodeRuneFullRuneRuneLendecodeRuneencodeRunemask2mask3mask4maskxrune1Maxrune2Maxrune3MaxsurrogateMaxsurrogateMint1t3t4t5cesu8github.com/SAP/go-hdb/internal/unicode/cesu8Cesu8ToUtf8TransformerErrInvalidCesu8ErrInvalidUtf8Utf8ToCesu8Transformercesu8ToUtf8Transformerutf8ToCesu8TransformerNopResettergithub.com/SAP/go-hdb/internal/unicodeCharacterDiffLineDiffAsLinesTrimLinesTrimLinesInStringdiffsToPatchLinesdiffsToStringpatchBuilderoldLinesnewLinesnewLineBufferoldLineBufferAddCharactersAddNewlineFlushChunkArrayApproxEqualArrayEqualArraySliceEqualDate32Date32BuilderDate64Date64BuilderDayTimeIntervalDayTimeIntervalBuilderDecimal128Decimal128BuilderDurationBuilderEqualOptionFixedSizeBinaryFixedSizeBinaryBuilderFixedSizeListFixedSizeListBuilderFloat16Float16BuilderFloat32BuilderInt16BuilderInt32BuilderInt8BuilderListBuilderMakeFromDataMonthIntervalMonthIntervalBuilderNewBooleanNewBooleanBuilderNewBooleanDataNewBuilderNewChunkedNewColumnNewDataNewDate32BuilderNewDate32DataNewDate64BuilderNewDate64DataNewDayTimeIntervalBuilderNewDayTimeIntervalDataNewDecimal128BuilderNewDecimal128DataNewDurationBuilderNewDurationDataNewFixedSizeBinaryBuilderNewFixedSizeBinaryDataNewFixedSizeListBuilderNewFixedSizeListDataNewFloat16BuilderNewFloat16DataNewFloat32BuilderNewFloat32DataNewFloat64BuilderNewFloat64DataNewInt16BuilderNewInt16DataNewInt32BuilderNewInt32DataNewInt8BuilderNewInt8DataNewIntervalDataNewListBuilderNewListDataNewMonthIntervalBuilderNewMonthIntervalDataNewNullNewNullBuilderNewNullDataNewRecordNewRecordBuilderNewRecordReaderNewSliceNewStringDataNewStructBuilderNewStructDataNewTableNewTableFromRecordsNewTableReaderNewTime32BuilderNewTime32DataNewTime64BuilderNewTime64DataNewTimestampBuilderNewTimestampDataNewUint16BuilderNewUint16DataNewUint32BuilderNewUint32DataNewUint64BuilderNewUint64DataNewUint8BuilderNewUint8DataNullBuilderRecordApproxEqualRecordBuilderRecordEqualRecordReaderStringBuilderStructBuilderTableReaderTime32Time32BuilderTime64Time64BuilderTimestampBuilderUint16BuilderUint32BuilderUint8BuilderUnknownNullCountWithAbsToleranceWithNaNsEqualarrayApproxEqualarrayApproxEqualFixedSizeListarrayApproxEqualFloat16arrayApproxEqualFloat32arrayApproxEqualFloat64arrayApproxEqualListarrayApproxEqualStructarrayConstructorFnarrayEqualBinaryarrayEqualBooleanarrayEqualDate32arrayEqualDate64arrayEqualDayTimeIntervalarrayEqualDecimal128arrayEqualDurationarrayEqualFixedSizeBinaryarrayEqualFixedSizeListarrayEqualFloat16arrayEqualFloat32arrayEqualFloat64arrayEqualInt16arrayEqualInt32arrayEqualInt64arrayEqualInt8arrayEqualListarrayEqualMonthIntervalarrayEqualStringarrayEqualStructarrayEqualTime32arrayEqualTime64arrayEqualTimestamparrayEqualUint16arrayEqualUint32arrayEqualUint64arrayEqualUint8baseArrayEqualbinaryArrayMaximumCapacitydefaultAbsoluteToleranceequalOptionimin64invalidDataTypemakeArrayFnminBuilderCapacitynewByteBufferBuildernewEqualOptionnewInt32BufferBuildersimpleRecordsimpleRecordssimpleTablestringArrayMaximumCapacityunsupportedArrayTypevalidityBitmapEqualFindKeyHasMetadataFieldsByNameFieldIndicesTimestampTypeTimeUnitBitWidthNewTimestampArrayDayTimeIntervalValuesNewFloat32ArrayDecimal128TypeLowBitsHighBitsNewDecimal128ArrayatolnansEqf16f32f64Int32ValuesListValuesnewListValueNewInt32ArrayFixedSizeBinaryTypeByteWidthNewFixedSizeBinaryArrayInt16ValuesOffsetsDate32ValuesTime64ValuesNewNullArrayTime32ValuesnewStructFieldWithParentValidityMaskDurationTypeNewDurationArrayetypeunsafeAppendBoolToBitmapValueBuilderNewListArrayColumnNameNumColsMonthIntervalValuesTimestampValuesDate64ValuesNewFloat16ArrayNewStringArrayFloat32ValuesrecsresizeHelperFieldBuilderNewStructArrayDurationValuesbytewidthNewUint16ArrayNewDayTimeIntervalArrayInt8ValuesUint32ValuesUint16ValuesUint8ValuesNewInt16ArrayNewDate64ArraychkszslotsTime64TypeNewTime64ArrayNewInt8ArrayTime32TypeNewTime32ArrayNewDate32ArrayNewMonthIntervalArrayNewUint8ArrayNewUint32ArrayBitIsNotSetBitIsSetBitMaskBytesForBitsCeilByteCeilByte64ClearBitCountSetBitsFlippedBitMaskIsMultipleOf8NextPowerOf2SetBitTobytesToUint64countSetBitsWithOffsetuint64SizeBitsuint64SizeBytesbitutilgithub.com/apache/arrow/go/arrow/bitutilFromI64FromU64MaxDecimal128decimal128github.com/apache/arrow/go/arrow/decimal128float16github.com/apache/arrow/go/arrow/float16Assertgithub.com/apache/arrow/go/arrow/internal/debugFloat64FuncsInt64FuncsUint64Funcssum_float64_gosum_int64_gosum_uint64_gogithub.com/apache/arrow/go/arrow/mathCheckedAllocatorCheckedAllocatorScopeDefaultAllocatorGoAllocatorNewBufferBytesNewCheckedAllocatorNewCheckedAllocatorScopeNewGoAllocatorNewResizableBufferaddressOfalignmentisMultipleOfPowerOf2memory_memset_gomemsetroundToPowerOf2roundUpToMultipleOf64AssertSizeCheckSizeBINARYBinaryTypeBooleanTraitsBooleanTypeCheckMetadataDATE32DATE64DECIMALDICTIONARYDate32SizeBytesDate32TraitsDate32TypeDate64SizeBytesDate64TraitsDate64TypeDayTimeIntervalSizeBytesDayTimeIntervalTraitsDayTimeIntervalTypeDecimal128SizeBytesDecimal128TraitsDurationSizeBytesDurationTraitsEXTENSIONFIXED_SIZE_BINARYFIXED_SIZE_LISTFLOAT16FLOAT32FLOAT64FixedSizeListOfFixedSizeListTypeFixedWidthDataTypeFixedWidthTypesFloat16SizeBytesFloat16TraitsFloat16TypeFloat32SizeBytesFloat32TraitsFloat32TypeFloat64SizeBytesFloat64TraitsFloat64TypeINT16INT32INT64INT8INTERVALInt16SizeBytesInt16TraitsInt16TypeInt32SizeBytesInt32TraitsInt32TypeInt64SizeBytesInt64TraitsInt64TypeInt8SizeBytesInt8TraitsInt8TypeLISTListOfListTypeMAPMetadataFromMonthIntervalSizeBytesMonthIntervalTraitsMonthIntervalTypeNULLNewMetadataNewSchemaNullTypePrimitiveTypesSTRUCTStringTypeStructOfTIME32TIME64TIMESTAMPTime32SizeBytesTime32TraitsTime64SizeBytesTime64TraitsTimestampSizeBytesTimestampTraitsTypeEqualTypeEqualOptionUINT16UINT32UINT64UINT8UNIONUint16SizeBytesUint16TraitsUint16TypeUint32SizeBytesUint32TraitsUint32TypeUint64SizeBytesUint64TraitsUint64TypeUint8SizeBytesUint8TraitsUint8Type_Type_index_Type_namebooleanTraitsdate32Traitsdate64TraitsdaytimeTraitsdecimal128TraitsdurationTraitsfloat16Traitsfloat32Traitsfloat64Traitsint16Traitsint32Traitsint64Traitsint8TraitsmonthTraitstime32Traitstime64TraitstimestampTraitstypeEqualsConfiguint16Traitsuint32Traitsuint64Traitsuint8TraitsBytesRequiredPutValueCastFromBytesCastToBytesDuration_sDuration_msDuration_usDuration_nsTime32sTime32msTime64usTime64nsTimestamp_sTimestamp_msTimestamp_usTimestamp_nsAddSampleAddSampleWithLabelsAggregateSampleBlackholeSinkDefaultInmemSignalDefaultSignalEmitKeyFanoutSinkIncrCounterIncrCounterWithLabelsInmemSignalInmemSinkIntervalMetricsMeasureSinceMeasureSinceWithLabelsMetricSinkMetricsSummaryNewGlobalNewInmemSignalNewInmemSinkNewInmemSinkFromURLNewIntervalMetricsNewMetricSinkFromURLNewStatsdSinkNewStatsdSinkFromURLNewStatsiteSinkNewStatsiteSinkFromURLPointValueSampledValueSetGaugeSetGaugeWithLabelsStatsdSinkStatsiteSinkUpdateFilterUpdateFilterAndLabelsformatSamplesglobalMetricssinkRegistrysinkURLFactoryFuncstatsdMaxLenmetricQueueflattenKeyflattenKeyLabelspushMetricflushMetricsHostNameEnableHostnameEnableHostnameLabelEnableServiceLabelEnableRuntimeMetricsEnableTypePrefixTimerGranularityProfileIntervalAllowedPrefixesBlockedPrefixesAllowedLabelsBlockedLabelsFilterDefaultmutateChdelEdgeGetWatchLongestPrefixrawIteratorWalkPrefixWalkPathTxnlastNumGCsinkallowedLabelsblockedLabelsfilterLocklabelIsAllowedfilterLabelsallowMetriccollectStatsemitRuntimeStatsDisplayLabelsSumSqLastUpdatedStddevMeanIngestGaugesCountersSamplesretainmaxIntervalsintervalLockrateDenomgetExistingIntervalcreateIntervalgetIntervalDisplayMetricsinmdumpStatsflattenLabelsWalkFnSeekPrefixWatchSeekPrefixrawStackEntryLRUEvictCallbackevictListonEvictPurgeRemoveOldestGetOldestremoveElementsnaptrackChannelstrackOverflowtrackMutateTrackMutatetrackChannelwriteNodetrackChannelsAndCountCommitOnlyslowNotifygithub.com/armon/go-metricsBatchErrorBatchedErrorsNewBatchErrorNewRequestFailureNewUnmarshalErrorRequestFailureSprintErrorawsErrorbaseErrorerrorListnewBaseErrornewRequestErrorrequestErrorunmarshalErrorOrigErrOrigErrsrequestIDawserrgithub.com/aws/aws-sdk-go/aws/awserrCopyOfPrettifySetValueAtPathValuesAtPathindexReprettifyrValuesAtPathrcopyawsutilgithub.com/aws/aws-sdk-go/aws/awsutilPartitionIDSigningNameSigningRegionJSONVersionTargetPrefixgithub.com/aws/aws-sdk-go/aws/client/metadataConfigNoResolveEndpointProviderConfigProviderDefaultRetryerDefaultRetryerMaxNumRetriesDefaultRetryerMaxRetryDelayDefaultRetryerMaxThrottleDelayDefaultRetryerMinRetryDelayDefaultRetryerMinThrottleDelayLogHTTPRequestHandlerLogHTTPRequestHeaderHandlerLogHTTPResponseHandlerLogHTTPResponseHeaderHandlerNoOpRetryercanUseRetryAfterHeadergetJitterDelaygetRetryAfterDelaylogReqErrMsglogReqMsglogRequestlogRequestHeaderlogRespErrMsglogRespMsglogResponselogResponseHeaderlogWriterteeReaderCloserforgottenForgetAccessKeyIDSecretAccessKeySessionTokenProviderNameHasKeysRetrieveGetWithContextsingleRetrieveisExpiredSTSRegionalEndpointS3UsEast1RegionalEndpointDisableSSLUseDualStackStrictMatchingResolveUnknownServiceResolvedEndpointSigningNameDerivedEndpointForLogLevelTypeAtLeastRequestRetryerCredentialsChainVerboseErrorsEndpointResolverEnforceShouldRetryCheckRegionDisableParamValidationDisableComputeChecksumsS3ForcePathStyleS3Disable100ContinueS3UseAccelerateS3DisableContentMD5ValidationS3UseARNRegionLowerCaseHeaderMapsEC2MetadataDisableTimeoutOverrideSleepDelayDisableRestProtocolURICleaningEnableEndpointDiscoveryDisableEndpointHostPrefixWithCredentialsChainVerboseErrorsWithCredentialsWithEndpointWithEndpointResolverWithRegionWithDisableSSLWithMaxRetriesWithDisableParamValidationWithDisableComputeChecksumsWithLogLevelWithS3ForcePathStyleWithS3Disable100ContinueWithS3UseAccelerateWithS3DisableContentMD5ValidationWithS3UseARNRegionWithUseDualStackWithEC2MetadataDisableTimeoutOverrideWithSleepDelayWithEndpointDiscoveryWithDisableEndpointHostPrefixMergeInWithSTSRegionalEndpointWithS3UsEast1RegionalEndpointHandlerListNamedHandlerRetryRulesShouldRetryPaginatorInputTokensOutputTokensLimitTokenTruncationTokenHTTPMethodHTTPPathBeforePresignFnoffsetReaderCloseAndCopyAttemptTimeHTTPRequestHTTPResponsestreamingBodyBodyStartRetryCountRetryableRetryDelayNotHoistSignedHeaderValsLastSignedAtDisableFollowRedirectsRetryErrorCodesThrottleErrorCodesExpireTimebuiltsafeBodyWillRetryParamsFilledDataFilledSetBufferBodySetStringBodySetReaderBodySetStreamingBodyPresignPresignRequestIsPresignedgetNextRequestBodyprepareRetrysendRequestResetBodynextPageTokensHasNextPageEachPageIsErrorRetryableIsErrorThrottleIsErrorExpiredHandlerListRunItemAfterEachFnPushBackNamedPushFrontNamedRemoveByNameSwapNamedSetBackNamedSetFrontNamedBuildStreamValidateResponseUnmarshalStreamUnmarshalMetaAfterRetryCompleteAttemptClientConfigNoResolveEndpointAddDebugHandlersNumMaxRetriesMinRetryDelayMinThrottleDelayMaxRetryDelayMaxThrottleDelaysetRetryerDefaultsgithub.com/aws/aws-sdk-go/aws/clientAddHostExecEnvUserAgentHanderAfterRetryHandlerBuildContentLengthHandlerSDKVersionUserAgentHandlerSendHandlerValidateEndpointHandlerValidateParametersHandlerValidateReqSigHandlerValidateResponseHandlerexecEnvUAKeyexecEnvVarhandleSendErrorlenerreStatusCodesendFollowRedirectssendWithoutFollowRedirectscorehandlersgithub.com/aws/aws-sdk-go/aws/corehandlersEC2RoleProviderNewCredentialsNewCredentialsWithClientec2RoleCredRespBodyiamSecurityCredsPathrequestCredrequestCredListEC2MetadatagetTokenGetUserDataGetDynamicDataGetInstanceIdentityDocumentIAMInfoSetExpirationExpiryWindowEC2IAMInfoInstanceProfileArnInstanceProfileIDEC2InstanceIdentityDocumentDevpayProductCodesMarketplaceProductCodesAvailabilityZonePrivateIPBillingProductsAccountIDPendingTimeImageIDKernelIDRamdiskIDArchitecturetokenOutputec2rolecredsgithub.com/aws/aws-sdk-go/aws/credentials/ec2rolecredsNewCredentialsClientNewProviderClientgetCredentialsOutputunmarshalHandlervalidateEndpointHandlerstaticCredsAuthorizationTokengetCredentialsendpointcredsgithub.com/aws/aws-sdk-go/aws/credentials/endpointcredsDefaultBufSizeDefaultDurationErrCodeProcessProviderExecutionErrCodeProcessProviderParseErrCodeProcessProviderRequiredErrCodeProcessProviderVersionNewCredentialsCommandNewCredentialsTimeoutProcessProviderappendErrorcredentialProcessResponseerrMsgProcessProviderEmptyCmderrMsgProcessProviderMissKeyerrMsgProcessProviderMissSecreterrMsgProcessProviderParseerrMsgProcessProviderPipeerrMsgProcessProviderPrepareCmderrMsgProcessProviderProcesserrMsgProcessProviderTimeouterrMsgProcessProviderVersionexecuteCommandreadInputoriginalCommandMaxBufSizeprepareCommandexecuteCredentialProcessprocesscredsgithub.com/aws/aws-sdk-go/aws/credentials/processcredsAssumeRoleProviderAssumeRolerErrCodeWebIdentityNewWebIdentityCredentialsNewWebIdentityRoleProviderStdinTokenProviderWebIdentityProviderNameWebIdentityRoleProviderAssumeRoleInputPolicyDescriptorTypeArnSetArnSetKeyDurationSecondsExternalIdPolicyArnsRoleArnRoleSessionNameTokenCodeTransitiveTagKeysSetDurationSecondsSetExternalIdSetPolicyArnsSetRoleArnSetRoleSessionNameSetSerialNumberSetTokenCodeSetTransitiveTagKeysAssumeRoleOutputAssumedRoleUserAssumedRoleIdSetAssumedRoleIdAccessKeyIdSetAccessKeyIdSetSecretAccessKeySetSessionTokenPackedPolicySizeSetAssumedRoleUserSetCredentialsSetPackedPolicySizeAssumeRoleRoleARNTokenProviderMaxJitterFracSTSAPIAssumeRoleWithSAMLInputPrincipalArnSAMLAssertionSetPrincipalArnSetSAMLAssertionAssumeRoleWithSAMLOutputNameQualifierSetAudienceSetIssuerSetNameQualifierSetSubjectSetSubjectTypeAssumeRoleWithWebIdentityInputProviderIdWebIdentityTokenSetProviderIdSetWebIdentityTokenAssumeRoleWithWebIdentityOutputSubjectFromWebIdentityTokenSetProviderSetSubjectFromWebIdentityTokenDecodeAuthorizationMessageInputEncodedMessageSetEncodedMessageDecodeAuthorizationMessageOutputDecodedMessageSetDecodedMessageGetAccessKeyInfoInputGetAccessKeyInfoOutputSetAccountGetCallerIdentityInputGetCallerIdentityOutputSetUserIdGetFederationTokenInputGetFederationTokenOutputFederatedUserFederatedUserIdSetFederatedUserIdSetFederatedUserGetSessionTokenInputGetSessionTokenOutputAssumeRoleRequestAssumeRoleWithContextAssumeRoleWithSAMLAssumeRoleWithSAMLRequestAssumeRoleWithSAMLWithContextAssumeRoleWithWebIdentityAssumeRoleWithWebIdentityRequestAssumeRoleWithWebIdentityWithContextDecodeAuthorizationMessageDecodeAuthorizationMessageRequestDecodeAuthorizationMessageWithContextGetAccessKeyInfoGetAccessKeyInfoRequestGetAccessKeyInfoWithContextGetCallerIdentityGetCallerIdentityRequestGetCallerIdentityWithContextGetFederationTokenGetFederationTokenRequestGetFederationTokenWithContextGetSessionTokenGetSessionTokenRequestGetSessionTokenWithContexttokenFilePathroleARNroleSessionNamestscredsgithub.com/aws/aws-sdk-go/aws/credentials/stscredsAnonymousCredentialsChainProviderEnvProviderEnvProviderNameErrAccessKeyIDNotFoundErrNoValidProvidersFoundInChainErrSecretAccessKeyNotFoundErrSharedCredentialsHomeNotFoundErrStaticCredentialsEmptyErrorProviderExpirerNewChainCredentialsNewEnvCredentialsNewSharedCredentialsNewStaticCredentialsNewStaticCredentialsFromCredsSharedCredentialsProviderSharedCredsProviderNameStaticProviderStaticProviderNamebackgroundContextloadProfileretrievedProvidersVerboseErrorsgithub.com/aws/aws-sdk-go/aws/credentialsAPICallAttemptMetricHandlerNameAPICallMetricHandlerNameAddressWithDefaultsDefaultHostDefaultPortMetricsChannelSizeawsExceptionboolIntValuegetMetricExceptionmetricChanmetricExceptionmetricTimenewMetricChannewReporterpausedEnumrequestExceptionrunningEnumsdkExceptionAttemptCountLatencyFqdnAttemptLatencyAccessKeyXAmzID2XAmzRequestIDAWSExceptionAWSExceptionMessageSDKExceptionSDKExceptionMessageFinalHTTPStatusCodeFinalAWSExceptionFinalAWSExceptionMessageFinalSDKExceptionFinalSDKExceptionMessageDestinationIPConnectionReusedAcquireConnectionLatencyConnectLatencyRequestLatencyDNSLatencyTCPLatencySSLLatencyMaxRetriesExceededTruncateFieldsSetExceptionSetFinalExceptionpausedContinueIsPausedmetricsChsendAPICallAttemptMetricrepsendAPICallMetricInjectHandlersExceptionexceptiongithub.com/aws/aws-sdk-go/aws/csmCredChainCredProvidersRemoteCredProviderSharedConfigFilenameSharedCredentialsFilenameec2RoleProviderhttpCredProviderhttpProviderAuthorizationEnvVarhttpProviderEnvVarisLoopbackHostlocalHTTPCredProviderlookupHostFngithub.com/aws/aws-sdk-go/aws/defaultsdefaultTTLdisableServiceEnvVarec2TokenenableTokenProviderHandlerNamefetchTokenHandlerNamehttpClientZerometadataOutputnewTokenProvidertokenHeaderttlExpirationWindowttlHeaderunmarshalMetadataHandlerNameunmarshalTokenHandlerunmarshalTokenHandlerNameconfiguredTTLdisabledfetchTokenHandlerenableTokenProviderHandlerec2metadatagithub.com/aws/aws-sdk-go/aws/ec2metadataA4bServiceIDAcmPcaServiceIDAcmServiceIDAddSchemeApEast1RegionIDApNortheast1RegionIDApNortheast2RegionIDApSouth1RegionIDApSoutheast1RegionIDApSoutheast2RegionIDApiMediatailorServiceIDApiPricingServiceIDApiSagemakerServiceIDApigatewayServiceIDApplicationAutoscalingServiceIDAppstream2ServiceIDAppsyncServiceIDAthenaServiceIDAutoscalingPlansServiceIDAutoscalingServiceIDAwsCnPartitionAwsCnPartitionIDAwsIsoBPartitionAwsIsoBPartitionIDAwsIsoPartitionAwsIsoPartitionIDAwsPartitionAwsPartitionIDAwsUsGovPartitionAwsUsGovPartitionIDBatchServiceIDBudgetsServiceIDCaCentral1RegionIDCeServiceIDChimeServiceIDCloud9ServiceIDClouddirectoryServiceIDCloudformationServiceIDCloudfrontServiceIDCloudhsmServiceIDCloudhsmv2ServiceIDCloudsearchServiceIDCloudtrailServiceIDCnNorth1RegionIDCnNorthwest1RegionIDCodebuildServiceIDCodecommitServiceIDCodedeployServiceIDCodepipelineServiceIDCodestarServiceIDCognitoIdentityServiceIDCognitoIdpServiceIDCognitoSyncServiceIDComprehendServiceIDConfigServiceIDCurServiceIDDatapipelineServiceIDDaxServiceIDDecodeModelDecodeModelOptionsDefaultPartitionsDefaultResolverDevicefarmServiceIDDirectconnectServiceIDDisableSSLOptionDiscoveryServiceIDDmsServiceIDDsServiceIDDynamodbServiceIDEc2ServiceIDEc2metadataServiceIDEcrServiceIDEcsServiceIDElasticacheServiceIDElasticbeanstalkServiceIDElasticfilesystemServiceIDElasticloadbalancingServiceIDElasticmapreduceServiceIDElastictranscoderServiceIDEmailServiceIDEndpointNotFoundErrorEntitlementMarketplaceServiceIDEnumPartitionsEsServiceIDEuCentral1RegionIDEuNorth1RegionIDEuWest1RegionIDEuWest2RegionIDEuWest3RegionIDEventsServiceIDFirehoseServiceIDFmsServiceIDGameliftServiceIDGetS3UsEast1RegionalEndpointGetSTSRegionalEndpointGlacierServiceIDGlueServiceIDGreengrassServiceIDGuarddutyServiceIDHealthServiceIDIamServiceIDImportexportServiceIDInspectorServiceIDIotServiceIDIotanalyticsServiceIDKinesisServiceIDKinesisanalyticsServiceIDKinesisvideoServiceIDKmsServiceIDLambdaServiceIDLegacyS3UsEast1EndpointLegacySTSEndpointLightsailServiceIDLogsServiceIDMachinelearningServiceIDMarketplacecommerceanalyticsServiceIDMeSouth1RegionIDMediaconvertServiceIDMedialiveServiceIDMediapackageServiceIDMediastoreServiceIDMeteringMarketplaceServiceIDMghServiceIDMobileanalyticsServiceIDModelsLexServiceIDMonitoringServiceIDMturkRequesterServiceIDNeptuneServiceIDNewUnknownEndpointErrorNewUnknownServiceErrorOpsworksCmServiceIDOpsworksServiceIDOrganizationsServiceIDPartitionForRegionPinpointServiceIDPollyServiceIDRdsServiceIDRedshiftServiceIDRegionalS3UsEast1EndpointRegionalSTSEndpointRegionsForServiceRekognitionServiceIDResolveUnknownServiceOptionResolverFuncResourceGroupsServiceIDRoute53ServiceIDRoute53domainsServiceIDRuntimeLexServiceIDRuntimeSagemakerServiceIDS3ControlServiceIDS3ServiceIDSTSRegionalEndpointOptionSaEast1RegionIDSagemakerServiceIDSdbServiceIDSecretsmanagerServiceIDServerlessrepoServiceIDServicecatalogServiceIDServicediscoveryServiceIDShieldServiceIDSmsServiceIDSnowballServiceIDSnsServiceIDSqsServiceIDSsmServiceIDStatesServiceIDStoragegatewayServiceIDStreamsDynamodbServiceIDStrictMatchingOptionStsServiceIDSupportServiceIDSwfServiceIDTaggingServiceIDTransferServiceIDTranslateServiceIDUnknownEndpointErrorUnknownServiceErrorUnsetS3UsEast1EndpointUnsetSTSEndpointUsEast1RegionIDUsEast2RegionIDUsGovEast1RegionIDUsGovWest1RegionIDUsIsoEast1RegionIDUsIsobEast1RegionIDUsWest1RegionIDUsWest2RegionIDUseDualStackOptionWafRegionalServiceIDWafServiceIDWorkdocsServiceIDWorkmailServiceIDWorkspacesServiceIDXrayServiceIDallowLegacyEmptyRegionawsPartitionawscnPartitionawsisoPartitionawsisobPartitionawsusgovPartitionboxedBoolboxedBoolUnsetboxedFalseboxedTruecredentialScopecustAddDualstackcustAddEC2MetadatacustAddS3DualStackcustFixAppAutoscalingChinacustFixAppAutoscalingUsGovcustRegionalS3custRmIotDataServicedecodeModelErrordecodeV3EndpointsdefaultPartitionsdefaultProtocoldefaultSignerendpointListgetByPrioritygetEndpointSchemelegacyGlobalRegionsmodelDefinitionnewDecodeModelErrorprotocolPriorityregionregionRegexregionsschemeREserviceListsignerPriorityProtocolsCredentialScopeHasDualStackDualStackHostnameSignatureVersionsSSLCommonNamemergeInPartitionEndpointIsRegionalizedendpointForRegionDNSSuffixRegionRegexRegionscanResolveEndpointdnsSuffixKnownSkipCustomizationsResolveEndpointserviceIDgithub.com/aws/aws-sdk-go/aws/endpointsCanceledErrorCodeConstantWaiterDelayErrCodeInvalidPresignExpireErrCodeReadErrCodeRequestErrorErrCodeResponseTimeoutErrCodeSerializationErrInvalidParamErrInvalidParamsErrParamFormatErrParamMaxLenErrParamMinLenErrParamMinValueErrParamRequiredErrorWaiterMatchFailureWaiterStateHandlerListLogItemHandlerListStopOnErrorHandlerResponseTimeoutInvalidParameterErrCodeIsErrorExpiredCredsMakeAddToUserAgentFreeFormHandlerMakeAddToUserAgentHandlerNewErrParamFormatNewErrParamMaxLenNewErrParamMinLenNewErrParamMinValueNewErrParamRequiredNoBodyPaginationParamFormatErrCodeParamMaxLenErrCodeParamMinLenErrCodeParamMinValueErrCodeParamRequiredErrCodePathAllWaiterMatchPathAnyWaiterMatchPathListWaiterMatchPathWaiterMatchRetryWaiterStateSanitizeHostForHeaderStatusWaiterMatchSuccessWaiterStateWaiterWaiterAcceptorWaiterDelayWaiterMatchModeWaiterOptionWaiterResourceNotReadyErrorCodeWaiterStateWithAppendUserAgentWithGetResponseHeaderWithGetResponseHeadersWithResponseReadTimeoutWithRetryerWithSetRequestHeadersWithWaiterDelayWithWaiterLoggerWithWaiterMaxAttemptsWithWaiterRequestOptionsadaptToResponseTimeoutErrorcopyHTTPRequestcredsExpiredCodesdebugLogReqErrorerrInvalidParamfmtAttemptCountgetHostgetPresignedURLisCodeExpiredCredsisCodeRetryableisCodeThrottleisDefaultPortisErrCodeisErrConnectionResetisNestedErrorRetryablelogDeprecatedEachPagelogDeprecatedHasNextPagelogDeprecatedNextPagelogDeprecatedfnewOffsetReadernoOpRetryernotRetryingportOnlyreadResultretryableCodessetRequestContextshouldRetryErrorstripPorttemporarythrottleCodestimeoutErrtimeoutReadCloservalidParentCodeswaiterLogfwithRequestHeadernestedContextAddNestedContextMaxLenExpectedAcceptorsMaxAttemptsRequestOptionsSleepWithContextWaitWithContextSetRequestHeadersMinValueEndPageOnSameTokenprevTokensnextTokenscurPageAddNestedMinLennoBodygithub.com/aws/aws-sdk-go/aws/requestAssumeRoleTokenProviderNotSetErrorCredentialRequiresARNErrorDefaultSharedConfigProfileErrCodeSharedConfigErrSharedConfigECSContainerEnvVarEmptyErrSharedConfigInvalidCredSourceErrSharedConfigSourceCollisionNewSessionWithOptionsSharedConfigAssumeRoleErrorSharedConfigDisableSharedConfigEnableSharedConfigLoadErrorSharedConfigProfileNotExistsErrorSharedConfigStateSharedConfigStateFromEnvWebIdentityEmptyRoleARNErrWebIdentityEmptyTokenFilePathErraccessKeyIDKeyassumeWebIdentitycredAccessEnvKeycredProviderErrorcredSecretEnvKeycredSessionEnvKeycredSourceECSContainercredSourceEc2MetadatacredSourceEnvironmentcredentialProcessKeycredentialSourceKeycredsFromAssumeRolecsmClientIDEnvKeycsmClientIDKeycsmConfigcsmEnabledEnvKeycsmEnabledKeycsmHostEnvKeycsmHostKeycsmPortEnvKeycsmPortKeycsmProfileNamedeprecatedNewSessionenableCSMenableEndpointDiscoveryEnvKeyenableEndpointDiscoveryKeyenvConfigenvConfigLoadexternalIDKeygetCABundleTransportinitHandlersloadCSMConfigloadCertPoolloadCustomCABundleloadEnvConfigloadSharedConfigloadSharedConfigIniFilesloadSharedEnvConfigmergeConfigSrcsmergeS3UsEast1RegionalEndpointConfigmergeSTSRegionalEndpointConfigmfaSerialKeynewSessiononeOrNoneprofileEnvKeysregionEnvKeysregionKeyresolveCredentialsresolveCredsFromProfileresolveCredsFromSourceroleARNEnvKeyroleArnKeyroleSessionNameEnvKeyroleSessionNameKeys3UsEast1RegionalEndpoints3UsEast1RegionalSharedKeys3UseARNRegionEnvKeys3UseARNRegionKeysecretAccessKeysessionTokenKeysetFromEnvValsharedConfigsharedConfigFilesharedConfigFileEnvKeysharedCredsFileEnvKeysourceProfileKeystsRegionalEndpointKeystsRegionalEndpointSharedKeyupdateBoolupdateBoolPtrupdateStringwebIdentityTokenFileKeywebIdentityTokenFilePathEnvKeySectionsSectionIntValueBoolValueGetSectionIniDataresolveEndpointlogDeprecatedNewSessionErrorEnableSharedConfigSharedCredentialsFileSharedConfigFileCustomCABundlecsmEnabledCSMEnabledCSMPortCSMHostCSMClientIDenableEndpointDiscoveryWebIdentityTokenFilePathCredentialSourceCredentialProcessWebIdentityTokenFileMFASerialSourceProfileNameSourceProfilesetFromIniFilessetFromIniFilevalidateCredentialsRequireARNvalidateCredentialTypehasCredentialsclearCredentialOptionsclearAssumeRoleOptionsSharedConfigFilesAssumeRoleTokenProviderAssumeRoleDurationgithub.com/aws/aws-sdk-go/aws/sessionBuildNamedHandlerGetSignedRequestSignatureNewSignerNewStreamSignerSignRequestHandlerSignSDKRequestSignSDKRequestWithCurrentTimeStreamSignerWithUnsignedPayloadallowedQueryHoistingauthHeaderPrefixauthHeaderSignatureElemauthorizationHeaderawsV4RequestblacklistbuildEventStreamStringToSignbuildQuerybuildSigningScopecredentialValueProviderderiveSigningKeydoubleSpaceemptyStringSHA256formatShortTimeformatTimegetURIPathhashSHA256hmacSHA256ignoredHeadersinclusiveRuleslogSignInfoMsglogSignedURLMsgmakeSha256ReadermapRulepatternsrequestContextrequiredSignedHeadersshortTimeFormatsignatureQueryKeysigningCtxstripExcessSpaceswhitelistprevSigDisableHeaderHoistingDisableURIPathEscapingDisableRequestBodyOverwritecurrentTimeFnUnsignedPayloadsignWithBodylogSigningInfocredValuesisPresignunsignedPayloadbodyDigestsignedHeaderscanonicalHeaderscanonicalStringcredentialStringstringToSignsanitizeHostForHeaderhandlePresignRemovalassignAmzQueryValuesbuildTimebuildCredentialStringbuildCanonicalHeadersbuildCanonicalStringbuildStringToSignbuildSignaturebuildBodyDigestisRequestSignedremovePresigngithub.com/aws/aws-sdk-go/aws/signer/v4BackgroundContextBoolMapBoolValueMapBoolValueSliceErrMissingEndpointErrMissingRegionFloat32MapFloat32ValueFloat32ValueMapFloat32ValueSliceFloat64MapFloat64ValueFloat64ValueMapFloat64ValueSliceInt16MapInt16SliceInt16ValueInt16ValueMapInt16ValueSliceInt32MapInt32ValueInt32ValueMapInt32ValueSliceInt64MapInt64ValueInt64ValueMapInt64ValueSliceInt8MapInt8SliceInt8ValueInt8ValueMapInt8ValueSliceIntMapIntValueMapIntValueSliceIsReaderSeekableJSONValueLogDebugWithEventStreamBodyLogDebugWithHTTPBodyLogDebugWithRequestErrorsLogDebugWithRequestRetriesLogDebugWithSigningLogOffLoggerFuncMillisecondsTimeValueNewDefaultLoggerNewWriteAtBufferReadSeekCloserReaderSeekerCloserSDKNameSDKVersionSecondsTimeValueSeekerLenStringMapStringValueMapStringValueSliceTimeMapTimeSliceTimeUnixMilliTimeValueTimeValueMapTimeValueSliceURLHostnameUint16MapUint16SliceUint16ValueUint16ValueMapUint16ValueSliceUint32MapUint32SliceUint32ValueUint32ValueMapUint32ValueSliceUint64MapUint64SliceUint64ValueUint64ValueMapUint64ValueSliceUint8MapUint8SliceUint8ValueUint8ValueMapUint8ValueSliceUintMapUintValueUintValueMapUintValueSliceUseServiceDefaultRetriesWriteAtBufferdefaultLoggermergeInConfigseekerLenIsSeekerHasLenGetLenGrowthCoeffawsgithub.com/aws/aws-sdk-go/awsASTKindASTKindCommentStatementASTKindCompletedNestedSectionStatementASTKindCompletedSectionStatementASTKindEqualExprASTKindExprASTKindExprStatementASTKindNestedSectionStatementASTKindNoneASTKindSectionStatementASTKindSkipStatementASTKindStartASTKindStatementBoolTypeCloseScopeStateCommentStateDecimalTypeDefaultVisitorEqualExprKeyErrCodeParseErrorErrCodeUnableToReadFileIntegerTypeInvalidStateMarkCompleteStateNewDefaultVisitorNoneTypeOpenScopeStateParseASTParseASTBytesParseStackQuotedStringTypeSectionStateSkipStateSkipTokenStateStatementPrimeStateStatementStateTerminalStateTokenCommaTokenCommentTokenLitTokenNLTokenNoneTokenOpTokenSepTokenWSValueStatecloseBracecommaRunescountTokensemptyRunesemptyTokenequalColonOpequalOpgetBoolValuegetEscapedBytegetNegativeNumbergetNumericalValuegetStringValuehasExponentiniLexerisBinaryByteisBoolValueisCommaisCommentisEscapedisHexByteisLitValueisNewlineisNumberValueisOctalByteisOpisSepisTrimmableisValidRuneliteralValuesnewASTnewASTWithRootTokennewCommaTokennewCommentStatementnewCommentTokennewCompletedSectionStatementnewEqualExprnewExprStatementnewExpressionnewLitTokennewNewlineTokennewOpTokennewParseStacknewSectionStatementnewSepTokennewSkipStatementnewSkippernewStatementnewValuenewWSTokennumberFormatnumberHelperoctalopenBraceparseTableremoveEscapedCharactersruneComparerunesFalserunesTrueskippertrimSpacesAppendChildSetChildrenTokenSetprevTokShouldSkipTokenizetokenizeMarkCompleteVisitExprVisitStatementOrigErrornegativeExponentDetermineCorrectByteinigithub.com/aws/aws-sdk-go/internal/iniGibiByteKibiByteMebiBytesdkiogithub.com/aws/aws-sdk-go/internal/sdkiosdkmathgithub.com/aws/aws-sdk-go/internal/sdkmathSeededRandlockedSourcelksdkrandgithub.com/aws/aws-sdk-go/internal/sdkrandPathJoinsdkurigithub.com/aws/aws-sdk-go/internal/sdkuriECSContainerCredentialsURIECSCredsProviderEnvVarUserHomeDirshareddefaultsgithub.com/aws/aws-sdk-go/internal/shareddefaultsHasPrefixFoldgithub.com/aws/aws-sdk-go/internal/stringssingleflightgithub.com/aws/aws-sdk-go/internal/sync/singleflightBuildJSONUnmarshalJSONCaseInsensitiveUnmarshalJSONErrorbuildAnybuildListbuildMapbuildScalarbuildStructbyteSliceTypeelemOfsortedValuescaseInsensitiveunmarshalAnyunmarshalStructunmarshalListunmarshalMapunmarshalScalarjsonutilgithub.com/aws/aws-sdk-go/private/protocol/json/jsonutilBuildHandlerNewUnmarshalTypedErrorUnmarshalErrorHandlerUnmarshalHandlerUnmarshalMetaHandlerUnmarshalTypedErroremptyJSONjsonErrorResponseResponseMetadataexceptionsjsonrpcgithub.com/aws/aws-sdk-go/private/protocol/jsonrpcqueryParserisEC2parseValueparseStructparseListparseMapparseScalarqueryutilgithub.com/aws/aws-sdk-go/private/protocol/query/queryutilxmlErrorResponsexmlResponseErrorgithub.com/aws/aws-sdk-go/private/protocol/queryBuildAsGETEscapePathPayloadMemberPayloadTypeUnmarshalResponsebuildBodybuildHeaderbuildHeaderMapbuildLocationElementsbuildQueryStringbuildURIconvertTypeerrValueNotSetnoEscapeunmarshalBodyunmarshalHeaderunmarshalHeaderMapunmarshalLocationElementsunmarshalStatusCodegithub.com/aws/aws-sdk-go/private/protocol/restBuildXMLNewXMLElementStructToXMLUnmarshalXMLErrorXMLNodeXMLToStructbuildXMLparseMapEntryxmlAttrSlicexmlBuildernamespacesAddChildfindNamespacesfindElembuildValuexmlutilgithub.com/aws/aws-sdk-go/private/protocol/xml/xmlutilBase64EscapeCanSetIdempotencyTokenDecodeJSONValueEncodeJSONValueErrCodeMinimumHTTPProtocolErrorErrorUnmarshalerEscapeModeFormatTimeGetIdempotencyTokenHandlerPayloadMarshalHandlerPayloadUnmarshalHostPrefixBuilderHostPrefixHandlerNameISO8601OutputTimeFormatISO8601TimeFormatISO8601TimeFormatNameIsKnownTimestampFormatNewHostPrefixHandlerNewUnmarshalErrorHandlerNoEscapePayloadMarshalerPayloadUnmarshalerQuotedEscapeRFC822OutputTimeFormatRFC822TimeFormatRFC822TimeFormatNameRequireHTTPMinProtocolSetIdempotencyTokenUUIDVersion4UnixTimeFormatNameUnmarshalDiscardBodyUnmarshalDiscardBodyHandlerUnmarshalErrorHandlerNameValidHostLabelValidateEndpointHostValidateEndpointHostHandleridempotencyTokenFillTagnewMinHTTPProtoErrorMarshalersMarshalPayloadUnmarshalersUnmarshalPayloadLabelsFnMajorMinorgithub.com/aws/aws-sdk-go/private/protocolAthenaAPIBatchGetNamedQueryInputNamedQueryIdsSetNamedQueryIdsBatchGetNamedQueryOutputNamedQueryNamedQueryIdQueryStringWorkGroupSetDatabaseSetNamedQueryIdSetQueryStringSetWorkGroupUnprocessedNamedQueryIdSetErrorCodeSetErrorMessageNamedQueriesUnprocessedNamedQueryIdsSetNamedQueriesSetUnprocessedNamedQueryIdsBatchGetQueryExecutionInputQueryExecutionIdsSetQueryExecutionIdsBatchGetQueryExecutionOutputQueryExecutionQueryExecutionContextResultConfigurationEncryptionOptionKmsKeySetEncryptionOptionSetKmsKeyOutputLocationSetEncryptionConfigurationSetOutputLocationQueryExecutionStatisticsDataManifestLocationDataScannedInBytesEngineExecutionTimeInMillisQueryPlanningTimeInMillisQueryQueueTimeInMillisServiceProcessingTimeInMillisTotalExecutionTimeInMillisSetDataManifestLocationSetDataScannedInBytesSetEngineExecutionTimeInMillisSetQueryPlanningTimeInMillisSetQueryQueueTimeInMillisSetServiceProcessingTimeInMillisSetTotalExecutionTimeInMillisQueryExecutionStatusCompletionDateTimeStateChangeReasonSubmissionDateTimeSetCompletionDateTimeSetStateSetStateChangeReasonSetSubmissionDateTimeQueryExecutionIdSetQuerySetQueryExecutionContextSetQueryExecutionIdSetResultConfigurationSetStatementTypeSetStatisticsUnprocessedQueryExecutionIdQueryExecutionsUnprocessedQueryExecutionIdsSetQueryExecutionsSetUnprocessedQueryExecutionIdsCreateNamedQueryInputClientRequestTokenSetClientRequestTokenCreateNamedQueryOutputCreateWorkGroupInputWorkGroupConfigurationBytesScannedCutoffPerQueryEnforceWorkGroupConfigurationPublishCloudWatchMetricsEnabledRequesterPaysEnabledSetBytesScannedCutoffPerQuerySetEnforceWorkGroupConfigurationSetPublishCloudWatchMetricsEnabledSetRequesterPaysEnabledSetConfigurationCreateWorkGroupOutputDeleteNamedQueryInputDeleteNamedQueryOutputDeleteWorkGroupInputRecursiveDeleteOptionSetRecursiveDeleteOptionDeleteWorkGroupOutputGetNamedQueryInputGetNamedQueryOutputSetNamedQueryGetQueryExecutionInputGetQueryExecutionOutputSetQueryExecutionGetQueryResultsInputNextTokenSetMaxResultsSetNextTokenGetQueryResultsOutputResultSetMetadataColumnInfoCaseSensitiveCatalogNameSchemaNameSetCaseSensitiveSetCatalogNameSetNullableSetScaleSetSchemaNameSetTableNameSetTypeSetColumnInfoDatumVarCharValueSetVarCharValueSetResultSetMetadataSetRowsUpdateCountSetResultSetSetUpdateCountGetWorkGroupInputGetWorkGroupOutputSetCreationTimeListNamedQueriesInputListNamedQueriesOutputListQueryExecutionsInputListQueryExecutionsOutputListTagsForResourceInputResourceARNSetResourceARNListTagsForResourceOutputListWorkGroupsInputListWorkGroupsOutputWorkGroupSummaryWorkGroupsSetWorkGroupsStartQueryExecutionInputStartQueryExecutionOutputStopQueryExecutionInputStopQueryExecutionOutputTagResourceInputTagResourceOutputUntagResourceInputSetTagKeysUntagResourceOutputUpdateWorkGroupInputWorkGroupConfigurationUpdatesResultConfigurationUpdatesRemoveEncryptionConfigurationRemoveOutputLocationSetRemoveEncryptionConfigurationSetRemoveOutputLocationRemoveBytesScannedCutoffPerQuerySetRemoveBytesScannedCutoffPerQuerySetResultConfigurationUpdatesConfigurationUpdatesSetConfigurationUpdatesUpdateWorkGroupOutputBatchGetNamedQueryBatchGetNamedQueryRequestBatchGetNamedQueryWithContextBatchGetQueryExecutionBatchGetQueryExecutionRequestBatchGetQueryExecutionWithContextCreateNamedQueryCreateNamedQueryRequestCreateNamedQueryWithContextCreateWorkGroupCreateWorkGroupRequestCreateWorkGroupWithContextDeleteNamedQueryDeleteNamedQueryRequestDeleteNamedQueryWithContextDeleteWorkGroupDeleteWorkGroupRequestDeleteWorkGroupWithContextGetNamedQueryGetNamedQueryRequestGetNamedQueryWithContextGetQueryExecutionGetQueryExecutionRequestGetQueryExecutionWithContextGetQueryResultsPagesGetQueryResultsPagesWithContextGetQueryResultsRequestGetQueryResultsWithContextGetWorkGroupGetWorkGroupRequestGetWorkGroupWithContextListNamedQueriesListNamedQueriesPagesListNamedQueriesPagesWithContextListNamedQueriesRequestListNamedQueriesWithContextListQueryExecutionsListQueryExecutionsPagesListQueryExecutionsPagesWithContextListQueryExecutionsRequestListQueryExecutionsWithContextListTagsForResourceListTagsForResourceRequestListTagsForResourceWithContextListWorkGroupsListWorkGroupsPagesListWorkGroupsPagesWithContextListWorkGroupsRequestListWorkGroupsWithContextStartQueryExecutionStartQueryExecutionRequestStartQueryExecutionWithContextStopQueryExecutionStopQueryExecutionRequestStopQueryExecutionWithContextTagResourceTagResourceRequestTagResourceWithContextUntagResourceUntagResourceRequestUntagResourceWithContextUpdateWorkGroupUpdateWorkGroupRequestUpdateWorkGroupWithContextathenaifacegithub.com/aws/aws-sdk-go/service/athena/athenaifaceAthenaColumnNullableNotNullColumnNullableNullableColumnNullableUnknownEncryptionOptionCseKmsEncryptionOptionSseKmsEncryptionOptionSseS3EndpointsIDErrCodeInternalServerExceptionErrCodeInvalidRequestExceptionErrCodeResourceNotFoundExceptionErrCodeTooManyRequestsExceptionInternalServerExceptionInvalidRequestExceptionQueryExecutionStateCancelledQueryExecutionStateFailedQueryExecutionStateQueuedQueryExecutionStateRunningQueryExecutionStateSucceededResourceNotFoundExceptionStatementTypeDdlStatementTypeDmlStatementTypeUtilityThrottleReasonConcurrentQueryLimitExceededTooManyRequestsExceptionWorkGroupStateDisabledWorkGroupStateEnabledexceptionFromCodenewErrorInternalServerExceptionnewErrorInvalidRequestExceptionnewErrorResourceNotFoundExceptionnewErrorTooManyRequestsExceptionopBatchGetNamedQueryopBatchGetQueryExecutionopCreateNamedQueryopCreateWorkGroupopDeleteNamedQueryopDeleteWorkGroupopGetNamedQueryopGetQueryExecutionopGetQueryResultsopGetWorkGroupopListNamedQueriesopListQueryExecutionsopListTagsForResourceopListWorkGroupsopStartQueryExecutionopStopQueryExecutionopTagResourceopUntagResourceopUpdateWorkGrouprespMetadataMessage_newRequestAthenaErrorCodeathenagithub.com/aws/aws-sdk-go/service/athenastsifacegithub.com/aws/aws-sdk-go/service/sts/stsifaceErrCodeExpiredTokenExceptionErrCodeIDPCommunicationErrorExceptionErrCodeIDPRejectedClaimExceptionErrCodeInvalidAuthorizationMessageExceptionErrCodeInvalidIdentityTokenExceptionErrCodeMalformedPolicyDocumentExceptionErrCodePackedPolicyTooLargeExceptionErrCodeRegionDisabledExceptionSTScustomizeRequestopAssumeRoleopAssumeRoleWithSAMLopAssumeRoleWithWebIdentityopDecodeAuthorizationMessageopGetAccessKeyInfoopGetCallerIdentityopGetFederationTokenopGetSessionTokengithub.com/aws/aws-sdk-go/service/stsNewMockgoschedinternalTickerinternalTimerNewHighBiasedNewLowBiasedNewTargetedinvarianttargetMapToSliceÆcompresssamplesmaybeSortflushedgithub.com/beorn7/perks/quantileDefaultAllocSizeDefaultFillPercentDefaultMaxBatchDelayDefaultMaxBatchSizeErrBucketExistsErrBucketNameRequiredErrDatabaseNotOpenErrDatabaseOpenErrDatabaseReadOnlyErrIncompatibleValueErrInvalidErrKeyRequiredErrKeyTooLargeErrTxClosedErrValueTooLargeErrVersionMismatchIgnoreNoSyncMaxKeySizeMaxValueSize_assertbranchPageElementSizebranchPageFlagbrokenUnalignedbucketHeaderSizebucketLeafFlagcloneBytesdefaultPageSizefdatasyncflockfreelistPageFlagfunlockleafPageElementSizeleafPageFlagmaxAllocSizemaxFillPercentmaxMapSizemaxMmapStepmaxUintmergepgidsmetaPageFlagminFillPercentminIntminKeysPerPageminUintnewFreelistpageHeaderSizeprintstacksafelyCalltrySolowarnfConfigFromConnStringConnectionStringEnvKeyNewConnNewConnectorNewStmtbqRowsprepareQueryApiKeyexecContextqueryContextconnectionStringlastInsertIDgithub.com/bonitoo-io/go-sql-bigqueryCleanPathGetParamGetParamsGetParamsFromContextWithParamsbufAppcatchAllcountParamsparamsContextKeyshiftNRuneBytesArrayEachEachKeyGetBooleanGetUnsafeStringKeyPathNotFoundErrorMalformedArrayErrorMalformedJsonErrorMalformedObjectErrorMalformedStringErrorMalformedStringEscapeErrorMalformedValueErrorObjectEachOverflowIntegerErrorParseBooleanParseStringStringToBytesUnknownValueTypeErrorbackslashCharEscapeTablebadHexbasicMultilingualPlaneOffsetbasicMultilingualPlaneReservedOffsetbitwiseFlagsblockEndbytesToStringcombineUTF16SurrogatescreateInsertComponentdecodeSingleUnicodeEscapedecodeUnicodeEscapeequalStrfalseLiteralfindKeyStartfindTokenStarth2IhighSurrogateOffsetinternalGetisUTF16EncodedRunelastTokenlowSurrogateOffsetminInt64parseFloatparseIntsameTreesearchKeyssupplementalPlanesOffsettokenEndtokenStarttrueLiteralunescapeStackBufSizeunescapeToUTF8Sum64StringappendUint64mergeRoundprime1prime1vprime2prime2vprime3prime3vprime4prime4vprime5prime5vrol1rol11rol12rol18rol23rol27rol31rol7u32u64writeBlocksv3github.com/cespare/xxhash/v2xxhHash64ConfigStateFdumpNewDefaultConfigNewFormatterSdumpUnsafeDisabledasteriskBytescCharREcUint8tCharREcUnsignedCharREcanSortSimplycapEqualsBytescircularBytescircularShortBytescloseAngleBytescloseBraceBytescloseBracketBytescloseMapBytescloseParenBytescolonBytescolonSpaceBytescommaNewlineBytesconvertArgsdumpStatefalseBytesfdumpflagAddrflagFieldflagKindMaskflagROflagValOffsetformatStatehexDigitsiBytesinterfaceBytesinvalidAngleByteslenEqualsBytesmaxNewlineBytesmaxShortBytesnewValuesSorternewlineBytesnilAngleBytesokFlagsopenAngleBytesopenBraceBytesopenBraceNewlineBytesopenBracketBytesopenMapBytesopenParenBytespanicBytespercentBytesplusBytespointerChainBytesprecisionBytesprintBoolprintComplexprintFloatprintHexPtrprintIntprintUintsortValuesspaceBytessupportedFlagstrueBytesuint8TypeunsafeReflectValuevalueSortLessvaluesSorterMaxDepthDisableMethodsDisablePointerMethodsDisablePointerAddressesDisableCapacitiesContinueOnMethodSortKeysSpewKeysignoreNextTypeignoreNextIndentunpackValuedumpPtrdumpSlicebuildDefaultFormatconstructOrigFormatformatPtrspewgithub.com/davecgh/go-spew/spewCharsetToUTF8CollationcharsetMapcollation2charsetcp1250cp1251cp1252cp1253cp1254cp1255cp1256cp1257cp1258cp437cp850cp874cp932cp936cp949cp950LcidAndFlagsSortIdgetLcidgetFlagsgetVersiongithub.com/denisenkom/go-mssqldb/internal/cpFloat64ToDecimalFloat64ToDecimalScaleInt64ToDecimalScaleScaleBytesStringToDecimalScaleautoScaleint10int1e5scaletblflt64positiveSetIntegerSetPositiveSetPrecUnscaledBytesgithub.com/denisenkom/go-mssqldb/internal/decimalParseParamsparseBracketparseCommentparseDoubleQuoteparseLineCommentparseNamedParameterparseNormalparseOrdinalParameterparseQuotestateFuncparamCountparamMaxnamedParamsquerytextgithub.com/denisenkom/go-mssqldb/internal/querytextBulkBulkOptionsCopyInDataValueDateTime1DateTimeOffsetErrorEmptyTVPTypeNameErrorObjectNameErrorSkipErrorTypeSliceErrorTypeSliceIsEmptyErrorWrongTypingIsSkipFieldMssqlBulkMssqlBulkOptionsMssqlConnMssqlDriverMssqlResultMssqlRowsMssqlStmtNVarCharMaxNewAccessTokenConnectorReturnStatusStreamErrorTVPUniqueIdentifierVarCharVarCharMax_AUTHENTICATE_MESSAGE_CHALLENGE_MESSAGE_NEGOTIATE_128_NEGOTIATE_56_NEGOTIATE_ALWAYS_SIGN_NEGOTIATE_ANONYMOUS_NEGOTIATE_DATAGRAM_NEGOTIATE_EXTENDED_SESSIONSECURITY_NEGOTIATE_FLAGS_NEGOTIATE_IDENTIFY_NEGOTIATE_KEY_EXCH_NEGOTIATE_LMKEY_NEGOTIATE_MESSAGE_NEGOTIATE_NTLM_NEGOTIATE_OEM_NEGOTIATE_OEM_DOMAIN_SUPPLIED_NEGOTIATE_OEM_WORKSTATION_SUPPLIED_NEGOTIATE_SEAL_NEGOTIATE_SIGN_NEGOTIATE_TARGET_NEGOTIATE_TARGET_INFO_NEGOTIATE_TARGET_TYPE_DOMAIN_NEGOTIATE_TARGET_TYPE_SERVER_NEGOTIATE_UNICODE_NEGOTIATE_VERSION_PLP_NULL_PLP_TERMINATOR_REQUEST_NON_NT_SESSION_KEY_TVP_END_TOKEN_TVP_ROW_TOKEN_UNKNOWN_PLP_LEN_token_index_0_token_index_1_token_index_2_token_index_3_token_index_4_token_index_5_token_index_6_token_index_7_token_name_0_token_name_1_token_name_2_token_name_3_token_name_4_token_name_5_token_name_6_token_name_7accessTokenConnectorbadStreamPanicbadStreamPanicfbuildNTLMResponsePayloadcalcTimeSizecolFlagNullablecolumnStructconnectParamsconvertInputParameterconvertOldArgscopyincreateDesKeycreateDialerdataStmHdrQueryNotifdataStmHdrTraceActivitydataStmHdrTransDescrdateTime2decodeChardecodeDatedecodeDateIntdecodeDateTim4decodeDateTimedecodeDateTime2decodeDateTimeOffsetdecodeGuiddecodeMoneydecodeMoney4decodeNChardecodeTimedecodeTimeIntdecodeUcs2decodeUdtdecodeXmldefaultServerPortdialConnectiondoneAttndoneCountdoneErrordoneFinaldoneFlags2StrdoneFlags2strdoneInProcStructdoneInxactdoneMoredoneSrvErrordoneStructdriverInstancedriverInstanceNoProcessencodeDateencodeDateTim4encodeDateTimeencodeDateTime2encodeDateTimeOffsetencodeTimeencodeTimeIntencryptDesencryptNotSupencryptOffencryptOnencryptReqenvDatabaseMirrorPartnerenvDefectTranenvEnlistDTCenvPromoteTranenvResetConnAckenvRoutingenvSortFlagsenvSortIdenvSqlCollationenvStartedInstanceNameenvTranEndedenvTranMgrAddrenvTypBeginTranenvTypCharsetenvTypCommitTranenvTypDatabaseenvTypLanguageenvTypPacketSizeenvTypRollbackTranerrorNTLMfBeginXactfByRevValuefCacheConnectfDefaultValuefExtensionfIntSecurityfLanguageFatalfNoMetaDatafODBCfReadOnlyIntentfReuseMetaDatafTransBoundaryfWithRecompfeatureExtfeatureExtFedAuthSTSfeatureExtsgenerateSpngetAuthgetCountSQLSeparatorsgetInstancesgetNTLMv2AndLMv2ResponsePayloadsgetNTLMv2TargetInfoFieldsgetSchemeAndNamegregorianDaysheaderStructhmacMD5isProcisoLevelisolationReadCommitedisolationReadUncommitedisolationRepeatableReadisolationSerializableisolationSnapshotisolationUseCurrentjsonTagkeySlicelmHashlmResponselogErrorslogMessageslogParamslogRowslogSQLlogTransactionloginAckStructloginHeadermakeDeclmakeGoLangScanTypemakeGoLangTypeLengthmakeGoLangTypeNamemakeGoLangTypePrecisionScalemakeStrParammanglePasswordnamedValuenegotiateExtendedSessionSecuritynewTdsBuffernewTimeoutConnnormalizeOdbcKeyntResponsentlmAuthntlmHashntlmHashNoPaddingntlmSessionResponseoddParityoptionalLoggerorderStructpackAttentionpackBulkLoadBCPpackLogin7packNormalpackPreloginpackRPCRequestpackReplypackSQLBatchpackSSPIMessagepackTransMgrReqpacketTypeparseColMetadata72parseConnectParamsparseDoneparseDoneInProcparseError72parseFeatureExtAckparseInfoparseInstancesparseLoginAckparseNbcRowparseOrderparseRespparseRespIterparseRespIterContinueparseRespIterDoneparseRespIterNextparseRespStateparseRespStateCancelparseRespStateClosingparseRespStateNormalparseReturnStatusparseReturnValueparseRowparseSSPIMsgpassthroughConnpreloginENCRYPTIONpreloginFEDAUTHREQUIREDpreloginINSTOPTpreloginMARSpreloginNONCEOPTpreloginTERMINATORpreloginTHREADIDpreloginTRACEIDpreloginVERSIONprocIdprocessEnvChgprocessResponseprocessSingleResponsequeryNotifHdrqueryNotifSubreadBVarBytereadBVarCharreadBVarCharOrPanicreadByteLenTypereadCollationreadFixedTypereadLongLenTypereadPLPTypereadPreloginreadShortLenTypereadTypeInforeadUcs2readUsVarCharreadUsVarCharOrPanicreadUshortreadVarLenreadVariantTyperesolveServerPortscanIntoOutsendAttentionsendBeginXactsendCommitXactsendLoginsendRollbackXactsendRpcsendSqlBatch72serializableBulkConfigskipTagValuesp_Cursorsp_CursorClosesp_CursorExecutesp_CursorFetchsp_CursorOpensp_CursorOptionsp_CursorPrepExecsp_CursorPreparesp_CursorUnpreparesp_ExecuteSqlsp_PrepExecsp_PrepExecRpcsp_Preparesp_UnpreparesplitConnectionStringsplitConnectionStringOdbcsplitConnectionStringURLsqlDateFormatsqlSeparatorsqlTimeFormatsspiMsgstr2ucs2streamErrorftdsBuffertdsSessiontimeoutConntlsHandshakeConntmBeginXacttmCommitXacttmGetDtcAddrtmPromoteXacttmPropagateXacttmRollbackXacttmSaveXacttokenColMetadatatokenDonetokenDoneInProctokenDoneProctokenEnvChangetokenFeatureExtAcktokenInfotokenLoginAcktokenNbcRowtokenOrdertokenReturnStatustokenReturnValuetokenRowtokenSSPItokenStructtransDescrHdrtvpTagtypeBigBinarytypeBigChartypeBigVarBintypeBigVarChartypeBinarytypeBittypeBitNtypeChartypeDateNtypeDateTim4typeDateTimetypeDateTime2NtypeDateTimeNtypeDateTimeOffsetNtypeDecimaltypeDecimalNtypeFlt4typeFlt8typeFltNtypeGuidtypeImagetypeInt1typeInt2typeInt4typeInt8typeIntNtypeMoneytypeMoney4typeMoneyNtypeNChartypeNTexttypeNVarChartypeNulltypeNumerictypeNumericNtypeTexttypeTimeNtypeTvptypeUdttypeVarBinarytypeVarChartypeVarianttypeXmlucs22strudtInfoutf16leverTDS70verTDS71verTDS71rev1verTDS72verTDS73verTDS73AverTDS73BverTDS74writeAllHeaderswriteBVarCharwriteByteLenTypewriteCollationwriteFixedTypewriteLongLenTypewritePLPTypewritePreloginwriteShortLenTypewriteTypeInfowriteUsVarCharwriteVarLenxmlInfoCheckConstraintsFireTriggersKeepNullsKilobytesPerBatchRowsPerBatchTablockhdrtypeprocessQueryTextOpenConnectionDBNameAssemblyQualifiedNameSchemaPresentOwningSchemaXmlSchemaCollectionpacketSizewposwPacketSeqwPacketTyperbufrposrsizerPacketTypeafterFirstResizeBufferPackageSizeBeginPacketFinishPacketreadNextPacketBeginReadBVarCharUsVarCharTypeIdPrecUdtInfoXmlInfoColNameColIdsfeatureIDfeatureslogFlagsdial_timeoutconn_timeoutkeepAlivedisableEncryptiontrustServerCertificatehostInCertificatehostInCertificateProvidedserverSPNworkstationtypeFlagsfailOverPartnerfailOverPortfedAuthAccessTokenSessionInitSQLgetDialerTDSVersionProgNameProgVerloginAckpartnertranidroutedServerroutedPorttransactionCtxconnectionGoodoutsreturnStatusCreateBulkCreateBulkContextprepareCopyInsetReturnStatuscheckBadConnclearOutssimpleProcessRespsendCommitRequestsendRollbackRequestsendBeginRequestprocessBeginResponseprepareContextmsgTextnotifSubSetQueryNotificationsendQuerymakeRPCParamsprocessQueryResponseprocessExecmakeParammakeParamExtraProcNameLineNoSQLErrorNumberSQLErrorStateSQLErrorClassSQLErrorMessageSQLErrorServerNameSQLErrorProcNameSQLErrorLineNotokchannextColsnotifyIdssbDeploymentnotifyTimeoutCurCmdRowCountbulkColumnscolumnsNametablenamenumRowssendBulkCommandmakeRowDatacreateColMetadatagetMetadatadlogfPacketSizeClientProgVerClientPIDConnectionIDOptionFlags1OptionFlags2TypeFlagsOptionFlags3ClientTimeZoneClientLCIDHostNameOffsetHostNameLengthUserNameOffsetUserNameLengthPasswordOffsetPasswordLengthAppNameOffsetAppNameLengthServerNameOffsetServerNameLengthExtensionOffsetExtensionLengthCtlIntNameOffsetCtlIntNameLengthLanguageOffsetLanguageLengthDatabaseOffsetDatabaseLengthSSPIOffsetSSPILengthAtchDBFileOffsetAtchDBFileLengthChangePasswordOffsetChangePasswordLengthSSPILongLengthPacketTypeSpidPacketNoPadtransDescroutstandingReqCntisErrorgetErrorWorkstationInitialBytesNextBytescancelErrordlogAppNameCtlIntNameSSPIAtchDBFileFeatureExtFedAuthEchoFedAuthTokenaccessTokenProviderbulkcopypacketPendingcontinueReadtvpcolumnTypesColumnsNamemssqlgithub.com/denisenkom/go-mssqldbDecodeSegmentEncodeSegmentErrECDSAVerificationErrHashUnavailableErrInvalidKeyErrInvalidKeyTypeErrKeyMustBePEMEncodedErrNotECPrivateKeyErrNotECPublicKeyErrNotRSAPrivateKeyErrNotRSAPublicKeyErrSignatureInvalidGetSigningMethodNewValidationErrorNoneSignatureTypeDisallowedErrorParseECPrivateKeyFromPEMParseECPublicKeyFromPEMParseRSAPrivateKeyFromPEMParseRSAPrivateKeyFromPEMWithPasswordParseRSAPublicKeyFromPEMRegisterSigningMethodSigningMethodECDSASigningMethodES256SigningMethodES384SigningMethodES512SigningMethodHS384SigningMethodNoneSigningMethodPS256SigningMethodPS384SigningMethodPS512SigningMethodRS256SigningMethodRS384SigningMethodRS512SigningMethodRSAPSSUnsafeAllowNoneSignatureTypeValidationErrorAudienceValidationErrorClaimsInvalidValidationErrorExpiredValidationErrorIdValidationErrorIssuedAtValidationErrorIssuerValidationErrorNotValidYetValidationErrorSignatureInvalidValidationErrorUnverifiablesigningMethodLocksigningMethodNonesigningMethodsunsafeNoneMagicConstantverifyAudverifyExpverifyIatverifyIssverifyNbfKeySizeCurveBitsSkipOnlyUTF16BigEndianUTF16LittleEndianUTF32BigEndianUTF32LittleEndianUTF8detectUtfisUTF16BigEndianBOM2isUTF16LittleEndianBOM2isUTF32BigEndianBOM4isUTF32LittleEndianBOM4isUTF8BOM3nilIfEmptyreadBOMutfbomgithub.com/dimchansky/utfbomBigByteBigBytesBigCommaBigCommafBigEByteBigEiByteBigGByteBigGiByteBigIBytesBigKByteBigKiByteBigMByteBigMiByteBigPByteBigPiByteBigSIByteBigTByteBigTiByteBigYByteBigYiByteBigZByteBigZiByteCommafCommafWithDigitsComputeSICustomRelTimeEByteEiByteFormatIntegerFtoaFtoaWithDigitsGByteGiByteIByteKByteKiByteLongTimeMByteMiBytePByteParseBigBytesParseSIPiByteRelTimeRelTimeMagnitudeSISIWithDigitsTByteTiBytebigBytesSizeTablebigIECExpbigSIExpbytesSizeTabledefaultMagnitudeserrInvalidhumanateBigByteshumanateByteslognoomoommrenderFloatPrecisionMultipliersrenderFloatPrecisionRoundersrevSIPrefixTablerevfmapriParseRegexsiPrefixTablestripTrailingDigitsstripTrailingZerostenRoundingModeformmantMinPrecAccMantExpsetExpAndRoundSetMantExpSignbitsetBits64SetRatSetInfuaddusubumuluquoucmpordpow5fmtBfmtXfmtPsqrtInverseDivByAcceptedConnErrorsConnackConnackPacketConnackReturnCodesConnectPacketControlPacketDisconnectDisconnectPacketErrNetworkErrorErrProtocolViolationErrRefusedBadProtocolVersionErrRefusedBadUsernameOrPasswordErrRefusedIDRejectedErrRefusedNotAuthorisedErrRefusedServerUnavailableFixedHeaderNewControlPacketNewControlPacketWithHeaderPacketNamesPingreqPingreqPacketPingrespPingrespPacketPubackPubackPacketPubcompPubcompPacketPublishPacketPubrecPubrecPacketPubrelPubrelPacketReadPacketSubackSubackPacketSubscribePacketUnsubackUnsubackPacketUnsubscribePacketboolToBytedecodeBytedecodeLengthdecodeUint16encodeBytesencodeLengthencodeUint16MessageTypeQosRemainingLengthMessageIDReturnCodesUnpackQossTopicNameProtocolNameCleanSessionWillFlagWillQosWillRetainUsernameFlagPasswordFlagReservedBitKeepaliveClientIdentifierWillTopicWillMessageSessionPresentReturnCodepacketsgithub.com/eclipse/paho.mqtt.golang/packetsCLICRITICALClientOptionsReaderConnectTokenCredentialsProviderDEBUGDECDefaultConnectionLostHandlerDisconnectTokenDummyTokenERRErrInvalidQosErrInvalidTopicEmptyStringErrInvalidTopicMultilevelErrNotConnectedMESMIDMIdMessageHandlerNOOPLoggerNewClientOptionsNewMemoryStoreOnConnectHandlerPNGPacketAndTokenPublishTokenSTASTRSubscribeTokenTSTTokenErrorSetterUnsubscribeTokenalllogicbaseTokenchkerrconnectedconnectingcorruptExtcorruptpathdisconnectederrorWatchfileInfosfullpathinboundKeyFromMIDinboundPrefixincomingisKeyInboundisKeyOutboundmIDFromKeymessageFromPublishmessageIdsmidMaxmidMinmsgExtnewConnectMsgFromOptionsnewRouteropenConnectionoutboundKeyFromMIDoutboundPrefixoutgoingpersistInboundpersistOutboundreconnectingrouteIncludesTopicrouteSplitsignalErrortmppathtokenCompletorvalidateSubscribeMapvalidateTopicAndQosDuplicateRetainedWaitTimeoutResumeSubsWillEnabledWillPayloadWillRetainedPingTimeoutMaxReconnectIntervalAutoReconnectMessageChannelDepthAddRouteIsConnectionOpenOptionsReaderSubscribeMultipleprotocolVersionExplicitDefaultPublishHandlerOnConnectOnConnectionLostAddBrokerSetResumeSubsSetClientIDSetUsernameSetCredentialsProviderSetCleanSessionSetOrderMattersSetStoreSetKeepAliveSetPingTimeoutSetProtocolVersionUnsetWillSetWillSetBinaryWillSetDefaultPublishHandlerSetOnConnectHandlerSetConnectionLostHandlerSetWriteTimeoutSetConnectTimeoutSetMaxReconnectIntervalSetAutoReconnectSetMessageChannelDepthSetHTTPHeadersflowCompletecleanUpmidsfreeIDclaimIDdefaultHandlerdeleteRoutesetDefaultHandlermatchAndDispatchlastReceivedpingOutstandingiboundoboundoboundPmsgRouterstopRouterincomingPubChanpersistworkersconnectionStatussetConnectedreconnectforceDisconnectinternalConnLostcloseStopcloseStopRouterdisconnectresumeackFuncdirectorysubResultreturnCodesessionPresentduplicateqosretainedtopicmessageIDmqttgithub.com/eclipse/paho.mqtt.golangAssetDirectoryAssetFileFakeFileNewAssetDirectoryNewAssetFiledefaultFileTimestampChildrenReadBgBlackBgBlueBgCyanBgGreenBgHiBlackBgHiBlueBgHiCyanBgHiGreenBgHiMagentaBgHiRedBgHiWhiteBgHiYellowBgMagentaBgRedBgWhiteBgYellowBlackBlackStringBlinkRapidBlinkSlowBlueBlueStringConcealedCrossedOutCyanCyanStringFaintFgBlackFgBlueFgCyanFgGreenFgHiBlackFgHiBlueFgHiCyanFgHiMagentaFgHiRedFgHiWhiteFgHiYellowFgMagentaFgWhiteGreenGreenStringHiBlackHiBlackStringHiBlueHiBlueStringHiCyanHiCyanStringHiGreenHiGreenStringHiMagentaHiMagentaStringHiRedHiRedStringHiWhiteHiWhiteStringHiYellowHiYellowStringItalicMagentaMagentaStringRedRedStringReverseVideoUnderlineUnsetWhiteWhiteStringYellowYellowStringcolorPrintcolorStringcolorsCachecolorsCacheMugetCachedColorErrEventOverflowNewWatcherWatcherdurationToTimespeckeventWaitTimekqueuenewCreateEventnewEventnoteAllEventsopenModepathInfokqwatchesexternalWatchesdirFlagsfileExistsaddWatchreadEventswatchDirectoryFilessendDirectoryChangeEventssendFileCreatedEventIfNewinternalWatchKevent_tFflagsUdatafsnotifygithub.com/fsnotify/fsnotifyNewWriterWithContextJSONToYAMLconvertToJSONableObjectfillFieldyamlToJSONCHUNK_MAXFixedSizeRingBufIntMinNewFixedSizeRingBufReadSnappyStreamCompressedFileSnappyFileSnappyStreamHeaderMagicUnsnapOneFrameUnsnappy_COMPRESSED_CHUNK_COMPRESSION_THRESHOLD_IDENTIFIER_CHUNK_RESERVED_SKIPPABLE0_RESERVED_SKIPPABLE1_RESERVED_UNSKIPPABLE0_RESERVED_UNSKIPPABLE1_STREAM_IDENTIFIER_STREAM_TO_STREAM_BLOCK_SIZE_UNCOMPRESSED_CHUNKcrctabintMaxintMinmasked_crc32cBegReadableOneMadeMake2ndBufferContigLenReadWithoutAdvanceReadAndMaybeAdvanceAdoptGetEndmostWritableGetEndmostWritableSliceFnameSnappyEncodeDecodeOffEncBufDecBufHeaderChunkWrittenWritingunsnapAllowContentEncodingCompressContentCharsetDefaultLogFormatterDefaultLoggerEncoderFuncGetLogEntryGetReqIDHeartbeatIsTTYLogEntryCtxKeyLogFormatterLoggerInterfaceNewCompressorNewWrapResponseWriterNextRequestIDNoCachePrintPrettyStackRedirectSlashesRequestIDHeaderRequestIDKeyRequestLoggerThrottleThrottleBacklogThrottleOptsThrottleWithOptsURLFormatURLFormatCtxKeyWithLogEntryWrapResponseWriterbBlackbBluebCyanbGreenbMagentabRedbWhitebYellowbasicAuthFailedbasicWritercWcompressFlushercompressResponseWriterctxKeyRequestIDdefaultBacklogTimeoutdefaultCompressibleContentTypesdefaultLogEntryencoderDeflateencoderGziperrCapacityExceedederrContextCancelederrTimedOutetagHeadersexpVarsflushWriterhttp2FancyWriterhttpFancyWriterioResetterWritermatchAcceptEncodingnBlacknBluenCyannGreennMagentanRednWhitenYellownoCacheHeadersprettyStackrealIPreqidthrottlerxForwardedForxRealIPdecorateLinedecorateFuncCallLinedecorateSourceLineteemaybeWriteHeaderTeeBacklogLimitBacklogTimeoutRetryAfterFnencoderspooledEncodersallowedTypesallowedWildcardsencodingPrecedenceSetEncoderselectEncoderNewLogEntrycontentWildcardscompressableisCompressablebacklogTokensbacklogTimeoutretryAfterFnsetRetryAfterHeaderIfNeededuseColorChainChainHandlerNewRouteContextRegisterMethodRouteCtxKeyServerBaseContextURLParamFromCtxmALLmCONNECTmDELETEmGETmHEADmOPTIONSmPATCHmPOSTmPUTmSTUBmTRACEmethodMapmethodTypStringntCatchAllntParamntRegexpntStaticpatNextSegmentpatParamKeysDeregisterLocalFileDeregisterReaderHandlerDeregisterServerPubKeyDeregisterTLSConfigDialContextFuncDialFuncErrBusyBufferErrCleartextPasswordErrInvalidConnErrMalformPktErrNativePasswordErrNoTLSErrOldPasswordErrOldProtocolErrPktSyncErrPktSyncMulErrPktTooLargeErrUnknownPluginMySQLDriverMySQLErrorParseDSNRegisterDialRegisterDialContextRegisterLocalFileRegisterReaderHandlerRegisterServerPubKeyRegisterTLSConfigappendLengthEncodedIntegerappendMicrosecsatomicBoolatomicErrorbinaryCollationbinaryRowscachingSha2PasswordFastAuthSuccesscachingSha2PasswordPerformFullAuthenticationcachingSha2PasswordRequestPublicKeyclientCanHandleExpiredPasswordsclientCompressclientConnectAttrsclientConnectWithDBclientDeprecateEOFclientFoundRowsclientIgnoreSIGPIPEclientIgnoreSpaceclientInteractiveclientLocalFilesclientLongFlagclientLongPasswordclientMultiResultsclientMultiStatementsclientNoSchemaclientODBCclientPSMultiResultsclientPluginAuthclientPluginAuthLenEncClientDataclientProtocol41clientReservedclientSSLclientSecureConnclientSessionTrackclientTransactionscollationscomBinlogDumpcomChangeUsercomConnectcomConnectOutcomCreateDBcomDebugcomDelayedInsertcomDropDBcomFieldListcomInitDBcomPingcomProcessInfocomProcessKillcomQuerycomQuitcomRefreshcomRegisterSlavecomSetOptioncomShutdowncomStatisticscomStmtClosecomStmtExecutecomStmtFetchcomStmtPreparecomStmtResetcomStmtSendLongDatacomTableDumpcomTimeconnCheckdefaultAuthPlugindefaultCollationdefaultMaxAllowedPacketdeferredClosedialsdialsLockdigits01ensureHavePorterrBadConnNoWriteerrInvalidDSNAddrerrInvalidDSNNoSlasherrInvalidDSNUnescapederrInvalidDSNUnsafeCollationerrLogerrUnexpectedReadescapeBytesBackslashescapeBytesQuotesescapeStringBackslashescapeStringQuotesfieldFlagfieldTypeBLOBfieldTypeBitfieldTypeDatefieldTypeDateTimefieldTypeDecimalfieldTypeDoublefieldTypeEnumfieldTypeFloatfieldTypeGeometryfieldTypeInt24fieldTypeJSONfieldTypeLongfieldTypeLongBLOBfieldTypeLongLongfieldTypeMediumBLOBfieldTypeNULLfieldTypeNewDatefieldTypeNewDecimalfieldTypeSetfieldTypeShortfieldTypeStringfieldTypeTimefieldTypeTimestampfieldTypeTinyfieldTypeTinyBLOBfieldTypeVarCharfieldTypeVarStringfieldTypeYearfileRegisterfileRegisterLockflagAutoIncrementflagBLOBflagBinaryflagEnumflagMultipleKeyflagNotNULLflagPriKeyflagSetflagTimestampflagUniqueKeyflagUnknown1flagUnknown2flagUnknown3flagUnknown4flagUnsignedflagZeroFillformatBinaryDateTimeformatBinaryTimegetServerPubKeygetTLSConfigCloneiAuthMoreDataiEOFiERRiLocalInFileiOKmapIsolationLevelmaxCachedBufSizemaxPacketSizeminProtocolVersionmyRndmyRndMaxValmysqlConnmysqlFieldmysqlResultmysqlRowsmysqlStmtmysqlTxnewMyRndparseBinaryDateTimeparseDSNParamsparseDateTimepwHashreadBoolreadLengthEncodedIntegerreadLengthEncodedStringreadStatusreaderRegisterreaderRegisterLockreserveBufferscanTypeFloat32scanTypeFloat64scanTypeInt16scanTypeInt32scanTypeInt64scanTypeInt8scanTypeNullFloatscanTypeNullIntscanTypeNullTimescanTypeRawBytesscanTypeUint16scanTypeUint32scanTypeUint64scanTypeUint8scrambleOldPasswordscramblePasswordscrambleSHA256PasswordserverPubKeyLockserverPubKeyRegistryskipLengthEncodedStringstatusCursorExistsstatusDbDroppedstatusFlagstatusInAutocommitstatusInTransstatusInTransReadonlystatusLastRowSentstatusMetadataChangedstatusMoreResultsExistsstatusNoBackslashEscapesstatusNoGoodIndexUsedstatusNoIndexUsedstatusPsOutParamsstatusQueryWasSlowstatusReservedstatusSessionStateChangedstringToInttextRowstlsConfigLocktlsConfigRegistryuint64ToBytesuint64ToStringunsafeCollationswriteDSNParamzeroDateTimeaffectedRowsinsertIddbufflipcntflipreadNexttakeBuffertakeSmallBuffertakeCompleteBufferNetMaxAllowedPacketServerPubKeypubKeyAllowAllFilesAllowCleartextPasswordsAllowNativePasswordsAllowOldPasswordsCheckConnLivenessClientFoundRowsColumnsWithAliasInterpolateParamsMultiStatementsRejectReadOnlyFormatDSN_noCopyTrySetrawConnmaxAllowedPacketwatchingsendEncryptedPasswordhandleAuthResulthandleParamsmarkBadConninterpolateParamsgetSystemVarwatchCancelstartWatcherhandleInFileRequestreadPacketwritePacketreadHandshakePacketwriteHandshakeResponsePacketwriteAuthSwitchPacketwriteCommandPacketwriteCommandPacketStrwriteCommandPacketUint32readAuthResultreadResultOKreadResultSetHeaderPackethandleErrorPackethandleOkPacketreadColumnsreadUntilEOFdiscardResultsdecimalscharSettypeDatabaseNamenextResultSetnextNotEmptyResultSetreadRowseed1seed2NextBytereadPrepareResultPacketwriteCommandLongDatawriteExecutePacketmysqlgithub.com/go-sql-driver/mysqlErrNoFuncinGorootpathSuffixpkgFilePathpkgIndexpkgPrefixruntimePathDefaultGeneratorDomainGroupDomainOrgDomainPersonFromBytesFromBytesOrNilFromStringOrNilHWAddrFuncNamespaceDNSNamespaceOIDNamespaceURLNamespaceX500NewGenNewGenWithHWAFNewV1NewV2NewV3NewV5NullUUIDTimestampFromV1V1V3V4V5VariantFutureVariantMicrosoftVariantNCSVariantRFC4122_100nsPerSecondbyteGroupsdefaultHWAddrFuncepochFuncepochStartnewFromHashposixGIDposixUIDtoCapitalHexDigitsurnPrefixclockSequenceOncehardwareAddrOncestorageMutexlastTimeclockSequencegetClockSequencegetHardwareAddrgetEpochgithub.com/gofrs/uuidE_BenchgenE_BenchgenAllE_CastkeyE_CasttypeE_CastvalueE_CompareE_CompareAllE_CustomnameE_CustomtypeE_DescriptionE_DescriptionAllE_EmbedE_EnumCustomnameE_EnumStringerE_EnumStringerAllE_EnumdeclE_EnumdeclAllE_EnumvalueCustomnameE_EqualE_EqualAllE_FaceE_FaceAllE_GogoprotoImportE_GoprotoEnumPrefixE_GoprotoEnumPrefixAllE_GoprotoEnumStringerE_GoprotoEnumStringerAllE_GoprotoExtensionsMapE_GoprotoExtensionsMapAllE_GoprotoGettersE_GoprotoGettersAllE_GoprotoRegistrationE_GoprotoSizecacheE_GoprotoSizecacheAllE_GoprotoStringerE_GoprotoStringerAllE_GoprotoUnkeyedE_GoprotoUnkeyedAllE_GoprotoUnrecognizedE_GoprotoUnrecognizedAllE_GostringE_GostringAllE_JsontagE_MarshalerE_MarshalerAllE_MessagenameE_MessagenameAllE_MoretagsE_NullableE_OnlyoneE_OnlyoneAllE_PopulateE_PopulateAllE_ProtosizerE_ProtosizerAllE_SizerE_SizerAllE_StableMarshalerE_StableMarshalerAllE_StddurationE_StdtimeE_StringerE_StringerAllE_TestgenE_TestgenAllE_TypedeclE_TypedeclAllE_UnmarshalerE_UnmarshalerAllE_UnsafeMarshalerE_UnsafeMarshalerAllE_UnsafeUnmarshalerE_UnsafeUnmarshalerAllE_VerboseEqualE_VerboseEqualAllE_WktpointerEnableFuncEnabledGoEnumPrefixEnabledGoStringerGetCastKeyGetCastTypeGetCastValueGetCustomNameGetCustomTypeGetEnumCustomNameGetEnumValueCustomNameGetJsonTagGetMoreTagsHasBenchGenHasCompareHasDescriptionHasEnumDeclHasEqualHasExtensionsMapHasGoGettersHasGoStringHasMessageNameHasPopulateHasSizecacheHasTestGenHasTypeDeclHasUnkeyedHasUnrecognizedHasVerboseEqualImportsGoGoProtoIsCastKeyIsCastTypeIsCastValueIsCustomNameIsCustomTypeIsEmbedIsEnumCustomNameIsEnumStringerIsEnumValueCustomNameIsFaceIsGoEnumStringerIsMarshalerIsNullableIsProto3IsProtoSizerIsSizerIsStableMarshalerIsStdBoolIsStdBytesIsStdDoubleIsStdDurationIsStdFloatIsStdInt32IsStdInt64IsStdStringIsStdTimeIsStdTypeIsStdUInt32IsStdUInt64IsStringerIsUnionIsUnmarshalerIsUnsafeMarshalerIsUnsafeUnmarshalerIsWktPtrNeedsNilCheckRegistersGolangProtofileDescriptor_592445b5231bc2b9FieldDescriptorProtoFieldDescriptorProto_LabelFieldDescriptorProto_TypeFieldOptions_CTypeFieldOptions_JSTypeUninterpretedOptionUninterpretedOption_NamePartNamePartIsExtensionGetNamePartGetIsExtensionIdentifierValuePositiveIntValueNegativeIntValueDoubleValueAggregateValueGetIdentifierValueGetPositiveIntValueGetNegativeIntValueGetDoubleValueGetAggregateValueCtypePackedJstypeLazyWeakGetCtypeGetPackedGetJstypeGetLazyGetDeprecatedGetWeakGetUninterpretedOptionExtendeeOneofIndexJsonNameIsScalarGetTypeNameGetExtendeeGetDefaultValueGetOneofIndexGetJsonNameWireTypeGetKeyUint64GetKey3Uint64GetKey3IsEnumIsMessageIsBytesIsRepeatedIsStringIsBoolIsRequiredIsPackedIsPacked3FileDescriptorProtoDescriptorProtoEnumDescriptorProtoEnumValueDescriptorProtoEnumValueOptionsEnumOptionsAllowAliasGetAllowAliasEnumDescriptorProto_EnumReservedRangeGetEndReservedRangeReservedNameGetReservedRangeGetReservedNameDescriptorProto_ExtensionRangeExtensionRangeOptionsOneofDescriptorProtoOneofOptionsMessageOptionsMessageSetWireFormatNoStandardDescriptorAccessorMapEntryGetMessageSetWireFormatGetNoStandardDescriptorAccessorGetMapEntryDescriptorProto_ReservedRangeNestedTypeEnumTypeOneofDeclGetFieldGetExtensionGetNestedTypeGetEnumTypeGetExtensionRangeGetOneofDeclGetMapFieldsIsExtendableGetFieldDescriptorHasExtensionServiceDescriptorProtoMethodDescriptorProtoMethodOptions_IdempotencyLevelIdempotencyLevelGetIdempotencyLevelInputTypeOutputTypeClientStreamingServerStreamingGetInputTypeGetOutputTypeGetClientStreamingGetServerStreamingServiceOptionsGetMethodFileOptionsFileOptions_OptimizeModeJavaPackageJavaOuterClassnameJavaMultipleFilesJavaGenerateEqualsAndHashJavaStringCheckUtf8OptimizeForGoPackageCcGenericServicesJavaGenericServicesPyGenericServicesPhpGenericServicesCcEnableArenasObjcClassPrefixCsharpNamespaceSwiftPrefixPhpClassPrefixPhpNamespacePhpMetadataNamespaceRubyPackageGetJavaPackageGetJavaOuterClassnameGetJavaMultipleFilesGetJavaGenerateEqualsAndHashGetJavaStringCheckUtf8GetOptimizeForGetGoPackageGetCcGenericServicesGetJavaGenericServicesGetPyGenericServicesGetPhpGenericServicesGetCcEnableArenasGetObjcClassPrefixGetCsharpNamespaceGetSwiftPrefixGetPhpClassPrefixGetPhpNamespaceGetPhpMetadataNamespaceGetRubyPackageSourceCodeInfoSourceCodeInfo_LocationLeadingCommentsTrailingCommentsLeadingDetachedCommentsGetSpanGetLeadingCommentsGetTrailingCommentsGetLeadingDetachedCommentsPublicDependencyWeakDependencySyntaxGetPackageGetDependencyGetPublicDependencyGetWeakDependencyGetMessageTypeGetServiceGetSourceCodeInfoGetSyntaxGetNestedMessagegogoprotoAppendExtensionBytesToExtensionsMapClearAllExtensionsClearExtensionCompactTextEncodeExtensionMapEncodeExtensionMapBackwardsEncodeInternalExtensionEncodeInternalExtensionBackwardsErrMissingExtensionErrNilExtensionDescsFileDescriptorGetBoolExtensionGetExtensionsGetPropertiesGetRawExtensionGetUnsafeExtensionGetUnsafeExtensionsMapGoGoProtoPackageIsVersion1MarshalJSONEnumMarshalMessageSetMarshalMessageSetJSONMarshalTextStringMergerMessageNameNewExtensionNewRequiredNotSetErrorNewUnsafeXXX_InternalExtensionsOneofPropertiesProtoSizerRegisterMessageSetTypeRegisteredExtensionsRequiredNotSetErrorSetExtensionSetRawExtensionSetUnsafeExtensionSizeOfInternalExtensionSizerStringFromExtensionsBytesStringFromExtensionsMapStringFromInternalExtensionStructPropertiesUnmarshalMergeUnmarshalMessageSetUnmarshalMessageSetJSONWireEndGroupWireFixed32WireStartGroupWireVarint_MessageSet_ItemanyRepeatedlyUnpackedappendBoolPackedSliceappendBoolPtrappendBoolSliceappendBoolValueappendBoolValueNoZeroappendBytesappendBytes3appendBytesOneofappendBytesSliceappendFixed32appendFixed32PackedSliceappendFixed32PtrappendFixed32SliceappendFixed32ValueappendFixed32ValueNoZeroappendFixed64appendFixed64PackedSliceappendFixed64PtrappendFixed64SliceappendFixed64ValueappendFixed64ValueNoZeroappendFixedS32PackedSliceappendFixedS32PtrappendFixedS32SliceappendFixedS32ValueappendFixedS32ValueNoZeroappendFixedS64PackedSliceappendFixedS64PtrappendFixedS64SliceappendFixedS64ValueappendFixedS64ValueNoZeroappendFloat32PackedSliceappendFloat32PtrappendFloat32SliceappendFloat32ValueappendFloat32ValueNoZeroappendFloat64PackedSliceappendFloat64PtrappendFloat64SliceappendFloat64ValueappendFloat64ValueNoZeroappendStringPtrappendStringSliceappendStringValueappendStringValueNoZeroappendUTF8StringPtrappendUTF8StringSliceappendUTF8StringValueappendUTF8StringValueNoZeroappendVarintappendVarint32PackedSliceappendVarint32PtrappendVarint32SliceappendVarint32ValueappendVarint32ValueNoZeroappendVarint64PackedSliceappendVarint64PtrappendVarint64SliceappendVarint64ValueappendVarint64ValueNoZeroappendVarintS32PackedSliceappendVarintS32PtrappendVarintS32SliceappendVarintS32ValueappendVarintS32ValueNoZeroappendVarintS64PackedSliceappendVarintS64PtrappendVarintS64SliceappendVarintS64ValueappendVarintS64ValueNoZeroappendZigzag32PackedSliceappendZigzag32PtrappendZigzag32SliceappendZigzag32ValueappendZigzag32ValueNoZeroappendZigzag64PackedSliceappendZigzag64PtrappendZigzag64SliceappendZigzag64ValueappendZigzag64ValueNoZeroatomicLoadDiscardInfoatomicLoadMarshalInfoatomicLoadMergeInfoatomicLoadUnmarshalInfoatomicStoreDiscardInfoatomicStoreMarshalInfoatomicStoreMergeInfoatomicStoreUnmarshalInfobackslashBSbackslashDQbackslashNbackslashRbackslashTbuildDefaultMessagebyTagbytesValuecheckExtensionTypesclearExtensioncompactTextMarshalercustomTypedecodeExtensiondecodeExtensionFromBytesdecodeVarintdefaultExtensionValuedefaultMessagedefaultMudefaultTextMarshalerdeleteExtensiondiscardInfoLockdiscardInfoMapdiscardLegacydurationFromProtodurationProtoemptyBufencodeExtensionencodeVarintendBraceNewlineenumStringMapsenumValueMapsequalAnyequalExtMapequalExtensionsequalStructerrBadUTF8errInternalBadWireTypeerrInvalidUTF8errNoMessageTypeIDerrNotExtendableerrOneofHasNilerrOverflowerrRepeatedHasNilextPropextPropKeyextendableextendableProtoextendableProtoV1extensionAdapterextensionMapsextensionPropertiesextensionsBytesfieldUnmarshalerfindEndGroupfloat32ValuegeneratedDiscardergeneratedMergergetDiscardInfogetMarshalInfogetMergeInfogetMessageMarshalInfogetPropertiesLockedgetUnmarshalInfoint32PtrTypeint32Sliceint32ValueinvalidFieldinvalidUTF8ErrorisAnyisExtensionFieldisIdentOrNumberCharisNilPtrisNonFatalisProto3ZeroisQuoteisprintmakeCustomMarshalermakeCustomPtrMarshalermakeDurationMarshalermakeDurationPtrMarshalermakeDurationPtrSliceMarshalermakeDurationSliceMarshalermakeGroupMarshalermakeGroupSliceMarshalermakeMapMarshalermakeMessageMarshalermakeMessageRefMarshalermakeMessageRefSliceMarshalermakeMessageSliceMarshalermakeOneOfMarshalermakeStdBoolValueMarshalermakeStdBoolValuePtrMarshalermakeStdBoolValuePtrSliceMarshalermakeStdBoolValuePtrSliceUnmarshalermakeStdBoolValuePtrUnmarshalermakeStdBoolValueSliceMarshalermakeStdBoolValueSliceUnmarshalermakeStdBoolValueUnmarshalermakeStdBytesValueMarshalermakeStdBytesValuePtrMarshalermakeStdBytesValuePtrSliceMarshalermakeStdBytesValuePtrSliceUnmarshalermakeStdBytesValuePtrUnmarshalermakeStdBytesValueSliceMarshalermakeStdBytesValueSliceUnmarshalermakeStdBytesValueUnmarshalermakeStdDoubleValueMarshalermakeStdDoubleValuePtrMarshalermakeStdDoubleValuePtrSliceMarshalermakeStdDoubleValuePtrSliceUnmarshalermakeStdDoubleValuePtrUnmarshalermakeStdDoubleValueSliceMarshalermakeStdDoubleValueSliceUnmarshalermakeStdDoubleValueUnmarshalermakeStdFloatValueMarshalermakeStdFloatValuePtrMarshalermakeStdFloatValuePtrSliceMarshalermakeStdFloatValuePtrSliceUnmarshalermakeStdFloatValuePtrUnmarshalermakeStdFloatValueSliceMarshalermakeStdFloatValueSliceUnmarshalermakeStdFloatValueUnmarshalermakeStdInt32ValueMarshalermakeStdInt32ValuePtrMarshalermakeStdInt32ValuePtrSliceMarshalermakeStdInt32ValuePtrSliceUnmarshalermakeStdInt32ValuePtrUnmarshalermakeStdInt32ValueSliceMarshalermakeStdInt32ValueSliceUnmarshalermakeStdInt32ValueUnmarshalermakeStdInt64ValueMarshalermakeStdInt64ValuePtrMarshalermakeStdInt64ValuePtrSliceMarshalermakeStdInt64ValuePtrSliceUnmarshalermakeStdInt64ValuePtrUnmarshalermakeStdInt64ValueSliceMarshalermakeStdInt64ValueSliceUnmarshalermakeStdInt64ValueUnmarshalermakeStdStringValueMarshalermakeStdStringValuePtrMarshalermakeStdStringValuePtrSliceMarshalermakeStdStringValuePtrSliceUnmarshalermakeStdStringValuePtrUnmarshalermakeStdStringValueSliceMarshalermakeStdStringValueSliceUnmarshalermakeStdStringValueUnmarshalermakeStdUInt32ValueMarshalermakeStdUInt32ValuePtrMarshalermakeStdUInt32ValuePtrSliceMarshalermakeStdUInt32ValuePtrSliceUnmarshalermakeStdUInt32ValuePtrUnmarshalermakeStdUInt32ValueSliceMarshalermakeStdUInt32ValueSliceUnmarshalermakeStdUInt32ValueUnmarshalermakeStdUInt64ValueMarshalermakeStdUInt64ValuePtrMarshalermakeStdUInt64ValuePtrSliceMarshalermakeStdUInt64ValuePtrSliceUnmarshalermakeStdUInt64ValuePtrUnmarshalermakeStdUInt64ValueSliceMarshalermakeStdUInt64ValueSliceUnmarshalermakeStdUInt64ValueUnmarshalermakeTimeMarshalermakeTimePtrMarshalermakeTimePtrSliceMarshalermakeTimeSliceMarshalermakeUnmarshalCustommakeUnmarshalCustomPtrmakeUnmarshalCustomSlicemakeUnmarshalDurationmakeUnmarshalDurationPtrmakeUnmarshalDurationPtrSlicemakeUnmarshalDurationSlicemakeUnmarshalGroupPtrmakeUnmarshalGroupSlicePtrmakeUnmarshalMapmakeUnmarshalMessagemakeUnmarshalMessagePtrmakeUnmarshalMessageSlicemakeUnmarshalMessageSlicePtrmakeUnmarshalOneofmakeUnmarshalTimemakeUnmarshalTimePtrmakeUnmarshalTimePtrSlicemakeUnmarshalTimeSlicemapKeySortermarshalInfoLockmarshalInfoMapmaxSecondsmaxValidSecondsmaxVarintBytesmergeAnymergeExtensionmergeInfoLockmergeInfoMapmergeStructmessageSetmessageTypeIderminSecondsminValidSecondsnannegInfnewMarshalernewSortableExtensionsFromMapnewTextParsernewUnmarshalernonFatalnotLockeroneofFuncsIfaceoneofWrappersIfaceposInfpropertiesMappropertiesMuprotoFilesprotoMapTypesprotoMessageTypeprotoTypedNilsprotosizerTyperequiresQuotesrevProtoTypesscalarFieldsetDefaultssizeBoolPackedSlicesizeBoolPtrsizeBoolSlicesizeBoolValuesizeBoolValueNoZerosizeBytessizeBytes3sizeBytesOneofsizeBytesSlicesizeFixed32PackedSlicesizeFixed32PtrsizeFixed32SlicesizeFixed32ValuesizeFixed32ValueNoZerosizeFixed64PackedSlicesizeFixed64PtrsizeFixed64SlicesizeFixed64ValuesizeFixed64ValueNoZerosizeFixedS32PackedSlicesizeFixedS32PtrsizeFixedS32SlicesizeFixedS32ValuesizeFixedS32ValueNoZerosizeFixedS64PackedSlicesizeFixedS64PtrsizeFixedS64SlicesizeFixedS64ValuesizeFixedS64ValueNoZerosizeFloat32PackedSlicesizeFloat32PtrsizeFloat32SlicesizeFloat32ValuesizeFloat32ValueNoZerosizeFloat64PackedSlicesizeFloat64PtrsizeFloat64SlicesizeFloat64ValuesizeFloat64ValueNoZerosizeStringPtrsizeStringSlicesizeStringValuesizeStringValueNoZerosizeVarint32PackedSlicesizeVarint32PtrsizeVarint32SlicesizeVarint32ValuesizeVarint32ValueNoZerosizeVarint64PackedSlicesizeVarint64PtrsizeVarint64SlicesizeVarint64ValuesizeVarint64ValueNoZerosizeVarintS32PackedSlicesizeVarintS32PtrsizeVarintS32SlicesizeVarintS32ValuesizeVarintS32ValueNoZerosizeVarintS64PackedSlicesizeVarintS64PtrsizeVarintS64SlicesizeVarintS64ValuesizeVarintS64ValueNoZerosizeZigzag32PackedSlicesizeZigzag32PtrsizeZigzag32SlicesizeZigzag32ValuesizeZigzag32ValueNoZerosizeZigzag64PackedSlicesizeZigzag64PtrsizeZigzag64SlicesizeZigzag64ValuesizeZigzag64ValueNoZerosizerTypeskipFieldskipVarintslowExtensionAdaptersortableExtensionssortableMapElemspacesstructFieldByNametagMaptagMapFastLimittextParsertextWritertimestampFromPrototimestampPrototoAddrPointertoFieldtoPointertypeMarshalertypeUnmarshaleruint32Valueuint8SliceTypeunescapeunmarshalBoolPtrunmarshalBoolSliceunmarshalBoolValueunmarshalBytesSliceunmarshalBytesValueunmarshalFixed32PtrunmarshalFixed32SliceunmarshalFixed32ValueunmarshalFixed64PtrunmarshalFixed64SliceunmarshalFixed64ValueunmarshalFixedS32PtrunmarshalFixedS32SliceunmarshalFixedS32ValueunmarshalFixedS64PtrunmarshalFixedS64SliceunmarshalFixedS64ValueunmarshalFloat32PtrunmarshalFloat32SliceunmarshalFloat32ValueunmarshalFloat64PtrunmarshalFloat64SliceunmarshalFloat64ValueunmarshalInfoLockunmarshalInfoMapunmarshalInt32PtrunmarshalInt32SliceunmarshalInt32ValueunmarshalInt64PtrunmarshalInt64SliceunmarshalInt64ValueunmarshalMessageSetunmarshalSint32PtrunmarshalSint32SliceunmarshalSint32ValueunmarshalSint64PtrunmarshalSint64SliceunmarshalSint64ValueunmarshalStringPtrunmarshalStringSliceunmarshalStringValueunmarshalUTF8StringPtrunmarshalUTF8StringSliceunmarshalUTF8StringValueunmarshalUint32PtrunmarshalUint32SliceunmarshalUint32ValueunmarshalUint64PtrunmarshalUint64SliceunmarshalUint64ValueunquoteCunsafeAllowedvalToPointervalidateDurationvalidateTimestampwiretypewriteNamewriteUnknownIntwriteUnknownStructzeroFieldExtensionMapindunindentOrigNameJSONNameWireproto3oneofHasDefaultCustomTypeCastTypeStdTimeStdDurationWktPointerstypectypespropmtypeMapKeyPropMapValPropsetFieldPropsfastTagsslowTagsreqCountdecoderTagsdecoderOrigNamesOneofTypesExpandAnywriteProto3AnywriteAnywriteExtensionswriteExtensionwriteEnumEmallocDmallocChitCmissunquotedbackedbackconsumeTokenmissingRequiredFieldErrorcheckForColonreadStructconsumeExtNameconsumeOptionalSeparatorreadAnyscalarsRequiredNotSetMessageTypeIdInvalidUTF8Default_EnumOptions_DeprecatedDefault_EnumValueOptions_DeprecatedDefault_FieldOptions_CtypeDefault_FieldOptions_DeprecatedDefault_FieldOptions_JstypeDefault_FieldOptions_LazyDefault_FieldOptions_WeakDefault_FileOptions_CcEnableArenasDefault_FileOptions_CcGenericServicesDefault_FileOptions_DeprecatedDefault_FileOptions_JavaGenericServicesDefault_FileOptions_JavaMultipleFilesDefault_FileOptions_JavaStringCheckUtf8Default_FileOptions_OptimizeForDefault_FileOptions_PhpGenericServicesDefault_FileOptions_PyGenericServicesDefault_MessageOptions_DeprecatedDefault_MessageOptions_MessageSetWireFormatDefault_MessageOptions_NoStandardDescriptorAccessorDefault_MethodDescriptorProto_ClientStreamingDefault_MethodDescriptorProto_ServerStreamingDefault_MethodOptions_DeprecatedDefault_MethodOptions_IdempotencyLevelDefault_ServiceOptions_DeprecatedFieldDescriptorProto_LABEL_OPTIONALFieldDescriptorProto_LABEL_REPEATEDFieldDescriptorProto_LABEL_REQUIREDFieldDescriptorProto_Label_nameFieldDescriptorProto_Label_valueFieldDescriptorProto_TYPE_BOOLFieldDescriptorProto_TYPE_BYTESFieldDescriptorProto_TYPE_DOUBLEFieldDescriptorProto_TYPE_ENUMFieldDescriptorProto_TYPE_FIXED32FieldDescriptorProto_TYPE_FIXED64FieldDescriptorProto_TYPE_FLOATFieldDescriptorProto_TYPE_GROUPFieldDescriptorProto_TYPE_INT32FieldDescriptorProto_TYPE_INT64FieldDescriptorProto_TYPE_MESSAGEFieldDescriptorProto_TYPE_SFIXED32FieldDescriptorProto_TYPE_SFIXED64FieldDescriptorProto_TYPE_SINT32FieldDescriptorProto_TYPE_SINT64FieldDescriptorProto_TYPE_STRINGFieldDescriptorProto_TYPE_UINT32FieldDescriptorProto_TYPE_UINT64FieldDescriptorProto_Type_nameFieldDescriptorProto_Type_valueFieldOptions_CORDFieldOptions_CType_nameFieldOptions_CType_valueFieldOptions_JSType_nameFieldOptions_JSType_valueFieldOptions_JS_NORMALFieldOptions_JS_NUMBERFieldOptions_JS_STRINGFieldOptions_STRINGFieldOptions_STRING_PIECEFileDescriptorSetFileOptions_CODE_SIZEFileOptions_LITE_RUNTIMEFileOptions_OptimizeMode_nameFileOptions_OptimizeMode_valueFileOptions_SPEEDForMessageGeneratedCodeInfoGeneratedCodeInfo_AnnotationMethodOptions_IDEMPOTENCY_UNKNOWNMethodOptions_IDEMPOTENTMethodOptions_IdempotencyLevel_nameMethodOptions_IdempotencyLevel_valueMethodOptions_NO_SIDE_EFFECTSdotToUnderscoreextRange_EnumOptionsextRange_EnumValueOptionsextRange_ExtensionRangeOptionsextRange_FieldOptionsextRange_FileOptionsextRange_MessageOptionsextRange_MethodOptionsextRange_OneofOptionsextRange_ServiceOptionsextensionToGoStringDescriptorfileDescriptor_308767df5ffe18afvalueToGoStringDescriptorxxx_messageInfo_DescriptorProtoxxx_messageInfo_DescriptorProto_ExtensionRangexxx_messageInfo_DescriptorProto_ReservedRangexxx_messageInfo_EnumDescriptorProtoxxx_messageInfo_EnumDescriptorProto_EnumReservedRangexxx_messageInfo_EnumOptionsxxx_messageInfo_EnumValueDescriptorProtoxxx_messageInfo_EnumValueOptionsxxx_messageInfo_ExtensionRangeOptionsxxx_messageInfo_FieldDescriptorProtoxxx_messageInfo_FieldOptionsxxx_messageInfo_FileDescriptorProtoxxx_messageInfo_FileDescriptorSetxxx_messageInfo_FileOptionsxxx_messageInfo_GeneratedCodeInfoxxx_messageInfo_GeneratedCodeInfo_Annotationxxx_messageInfo_MessageOptionsxxx_messageInfo_MethodDescriptorProtoxxx_messageInfo_MethodOptionsxxx_messageInfo_OneofDescriptorProtoxxx_messageInfo_OneofOptionsxxx_messageInfo_ServiceDescriptorProtoxxx_messageInfo_ServiceOptionsxxx_messageInfo_SourceCodeInfoxxx_messageInfo_SourceCodeInfo_Locationxxx_messageInfo_UninterpretedOptionxxx_messageInfo_UninterpretedOption_NamePartGetFileFindExtensionFindExtensionByFieldNumberFindMessageGetEnumSourceFileGetSourceFileGetBeginGetAnnotationdescriptorgithub.com/gogo/protobuf/protoc-gen-gogo/descriptorFloat32sFloat64sInt32sUint32sUint64ssortkeysgithub.com/gogo/protobuf/sortkeysAnyMessageNameApiDurationFromProtoDurationProtoDynamicAnyEmptyAnyEnumValueErrIntOverflowAnyErrIntOverflowApiErrIntOverflowDurationErrIntOverflowEmptyErrIntOverflowFieldMaskErrIntOverflowSourceContextErrIntOverflowStructErrIntOverflowTimestampErrIntOverflowTypeErrIntOverflowWrappersErrInvalidLengthAnyErrInvalidLengthApiErrInvalidLengthDurationErrInvalidLengthEmptyErrInvalidLengthFieldMaskErrInvalidLengthSourceContextErrInvalidLengthStructErrInvalidLengthTimestampErrInvalidLengthTypeErrInvalidLengthWrappersErrUnexpectedEndOfGroupAnyErrUnexpectedEndOfGroupApiErrUnexpectedEndOfGroupDurationErrUnexpectedEndOfGroupEmptyErrUnexpectedEndOfGroupFieldMaskErrUnexpectedEndOfGroupSourceContextErrUnexpectedEndOfGroupStructErrUnexpectedEndOfGroupTimestampErrUnexpectedEndOfGroupTypeErrUnexpectedEndOfGroupWrappersField_CARDINALITY_OPTIONALField_CARDINALITY_REPEATEDField_CARDINALITY_REQUIREDField_CARDINALITY_UNKNOWNField_CardinalityField_Cardinality_nameField_Cardinality_valueField_KindField_Kind_nameField_Kind_valueField_TYPE_BOOLField_TYPE_BYTESField_TYPE_DOUBLEField_TYPE_ENUMField_TYPE_FIXED32Field_TYPE_FIXED64Field_TYPE_FLOATField_TYPE_GROUPField_TYPE_INT32Field_TYPE_INT64Field_TYPE_MESSAGEField_TYPE_SFIXED32Field_TYPE_SFIXED64Field_TYPE_SINT32Field_TYPE_SINT64Field_TYPE_STRINGField_TYPE_UINT32Field_TYPE_UINT64Field_TYPE_UNKNOWNListValueMixinNewPopulatedAnyNewPopulatedApiNewPopulatedBoolValueNewPopulatedBytesValueNewPopulatedDoubleValueNewPopulatedDurationNewPopulatedEmptyNewPopulatedEnumNewPopulatedEnumValueNewPopulatedFieldNewPopulatedFieldMaskNewPopulatedFloatValueNewPopulatedInt32ValueNewPopulatedInt64ValueNewPopulatedListValueNewPopulatedMethodNewPopulatedMixinNewPopulatedOptionNewPopulatedSourceContextNewPopulatedStdBoolNewPopulatedStdBytesNewPopulatedStdDoubleNewPopulatedStdDurationNewPopulatedStdFloatNewPopulatedStdInt32NewPopulatedStdInt64NewPopulatedStdStringNewPopulatedStdTimeNewPopulatedStdUInt32NewPopulatedStdUInt64NewPopulatedStringValueNewPopulatedStructNewPopulatedTimestampNewPopulatedTypeNewPopulatedUInt32ValueNewPopulatedUInt64ValueNewPopulatedValueNewPopulatedValue_BoolValueNewPopulatedValue_ListValueNewPopulatedValue_NullValueNewPopulatedValue_NumberValueNewPopulatedValue_StringValueNewPopulatedValue_StructValueNullValueNullValue_NULL_VALUENullValue_nameNullValue_valueSizeOfStdBoolSizeOfStdBytesSizeOfStdDoubleSizeOfStdDurationSizeOfStdFloatSizeOfStdInt32SizeOfStdInt64SizeOfStdStringSizeOfStdUInt32SizeOfStdUInt64SourceContextStdBoolMarshalStdBoolMarshalToStdBoolUnmarshalStdBytesMarshalStdBytesMarshalToStdBytesUnmarshalStdDoubleMarshalStdDoubleMarshalToStdDoubleUnmarshalStdDurationMarshalStdDurationMarshalToStdDurationUnmarshalStdFloatMarshalStdFloatMarshalToStdFloatUnmarshalStdInt32MarshalStdInt32MarshalToStdInt32UnmarshalStdInt64MarshalStdInt64MarshalToStdInt64UnmarshalStdStringMarshalStdStringMarshalToStdStringUnmarshalStdTimeMarshalStdUInt32MarshalStdUInt32MarshalToStdUInt32UnmarshalStdUInt64MarshalStdUInt64MarshalToStdUInt64UnmarshalSyntax_SYNTAX_PROTO2Syntax_SYNTAX_PROTO3Syntax_nameSyntax_valueTimestampFromProtoTimestampNowTimestampProtoTimestampStringUInt32ValueUInt64ValueValue_BoolValueValue_ListValueValue_NullValueValue_NumberValueValue_StringValueValue_StructValueencodeVarintAnyencodeVarintApiencodeVarintDurationencodeVarintEmptyencodeVarintFieldMaskencodeVarintPopulateAnyencodeVarintPopulateApiencodeVarintPopulateEmptyencodeVarintPopulateFieldMaskencodeVarintPopulateSourceContextencodeVarintPopulateStructencodeVarintPopulateTypeencodeVarintPopulateWrappersencodeVarintSourceContextencodeVarintStructencodeVarintTimestampencodeVarintTypeencodeVarintWrappersfileDescriptor_23597b2ebd7ac6c5fileDescriptor_292007bbfe81227efileDescriptor_5158202634f0da48fileDescriptor_5377b62bda767935fileDescriptor_900544acb223d5b8fileDescriptor_a2ec32096296c143fileDescriptor_b53526c13ae22eb4fileDescriptor_b686cdb126d509dbfileDescriptor_dd271cc1e348c538fileDescriptor_df322afd6c9fb402googleApisisValue_KindrandFieldAnyrandFieldApirandFieldEmptyrandFieldFieldMaskrandFieldSourceContextrandFieldStructrandFieldTyperandFieldWrappersrandStringAnyrandStringApirandStringEmptyrandStringFieldMaskrandStringSourceContextrandStringStructrandStringTyperandStringWrappersrandUTF8RuneAnyrandUTF8RuneApirandUTF8RuneEmptyrandUTF8RuneFieldMaskrandUTF8RuneSourceContextrandUTF8RuneStructrandUTF8RuneTyperandUTF8RuneWrappersrandUnrecognizedAnyrandUnrecognizedApirandUnrecognizedEmptyrandUnrecognizedFieldMaskrandUnrecognizedSourceContextrandUnrecognizedStructrandUnrecognizedTyperandUnrecognizedWrappersrandyAnyrandyApirandyEmptyrandyFieldMaskrandySourceContextrandyStructrandyTyperandyWrappersskipAnyskipApiskipDurationskipEmptyskipFieldMaskskipSourceContextskipStructskipTimestampskipTypeskipWrapperssovAnysovApisovDurationsovEmptysovFieldMasksovSourceContextsovStructsovTimestampsovTypesovWrapperssozAnysozApisozDurationsozEmptysozFieldMasksozSourceContextsozStructsozTimestampsozTypesozWrappersvalueToGoStringAnyvalueToGoStringApivalueToGoStringDurationvalueToGoStringEmptyvalueToGoStringFieldMaskvalueToGoStringSourceContextvalueToGoStringStructvalueToGoStringTimestampvalueToGoStringTypevalueToGoStringWrappersvalueToStringAnyvalueToStringApivalueToStringEmptyvalueToStringFieldMaskvalueToStringSourceContextvalueToStringStructvalueToStringTypevalueToStringWrappersxxx_messageInfo_Anyxxx_messageInfo_Apixxx_messageInfo_BoolValuexxx_messageInfo_BytesValuexxx_messageInfo_DoubleValuexxx_messageInfo_Emptyxxx_messageInfo_Enumxxx_messageInfo_EnumValuexxx_messageInfo_FieldMaskxxx_messageInfo_FloatValuexxx_messageInfo_Int32Valuexxx_messageInfo_Int64Valuexxx_messageInfo_ListValuexxx_messageInfo_Methodxxx_messageInfo_Mixinxxx_messageInfo_Optionxxx_messageInfo_SourceContextxxx_messageInfo_StringValuexxx_messageInfo_Structxxx_messageInfo_Timestampxxx_messageInfo_Typexxx_messageInfo_UInt32Valuexxx_messageInfo_UInt64Valuexxx_messageInfo_ValueRequestTypeUrlRequestStreamingResponseTypeUrlResponseStreamingGetRequestTypeUrlGetRequestStreamingGetResponseTypeUrlGetResponseStreamingGetKindOneofsGetOneofsGetSourceContextNumberValueGetNullValueGetNumberValueGetBoolValueGetStructValueGetListValueEnumvalueGetEnumvalueStructValueMixinsGetMethodsGetMixinsAcceptSpecParseAcceptParseListParseValueAndParamsexpectQualityexpectTokenexpectTokenOrQuotedexpectTokenSlashisTokenoctetTypeoctetTypesskipSpacetimeLayoutsgithub.com/golang/gddo/httputil/headerAuthTransportCacheBustersNegotiateContentEncodingResponseBufferStaticServerStripPortbusterWritercopyRequestsanitizeTokenRunestaticHandlerMIMETypesetagsFileHandlerDirectoryHandlerFilesHandlerheaderMapGithubTokenAppendQueryParamEmptyIntervalIntervalFromPointdblEpsilonCenterContainsIntervalInteriorContainsInteriorContainsIntervalInteriorIntersectsIntersectionClampPointExpandedApproxEqualDirectedHausdorffDistancer1github.com/golang/geo/r1EmptyRectRectRectFromCenterSizeRectFromPointsOrthoDotCrossNormVerticesVertexIJContainsPointInteriorContainsPointAddRectExpandedByMarginr2github.com/golang/geo/r2NewPreciseVectorPreciseVectorPreciseVectorFromVectorXAxisYAxisZAxisprecAddprecFloatprecIntprecMulprecStrprecSubprecise0precise1Norm2IsUnitDistanceAngleLargestComponentSmallestComponentMulByFloat64RadiansDegreesisInfE5E6E7Normalizedr3github.com/golang/geo/r3ChordAngleChordAngleFromAngleChordAngleFromSquaredLengthDegreeFullIntervalInfAngleInfChordAngleIntervalFromEndpointsIntervalFromPointPairNegativeChordAngleRadianRightChordAngleStraightChordAnglemaxLength2positiveDistanceisSpecialSuccessorPredecessorMaxPointErrorMaxAngleErrorSin2IsFullIsInvertedfastContainsComplementComplementCentergithub.com/golang/geo/s1AvgAngleSpanMetricAvgAreaMetricAvgDiagMetricAvgEdgeMetricAvgWidthMetricCapFromCenterAngleCapFromCenterAreaCapFromCenterChordAngleCapFromCenterHeightCapFromPointCellFromCellIDCellFromLatLngCellFromPointCellIDCellIDFromFaceCellIDFromFacePosLevelCellIDFromLatLngCellIDFromTokenCellRelationCellUnionCellUnionFromDifferenceCellUnionFromIntersectionCellUnionFromIntersectionWithCellIDCellUnionFromRangeCellUnionFromUnionChainPositionChordAngleBetweenPointsClipEdgeClipToFaceClipToPaddedFaceClockwiseCompareDistanceCompareDistancesContainsPointQueryContainsVertexQueryConvexHullQueryCounterClockwiseCrossingCrossingEdgeQueryCrossingSignCrossingTypeCrossingTypeAllCrossingTypeInteriorCrossingTypeNonAdjacentDisjointDistanceFractionDistanceFromSegmentDoNotCrossEdgeCrosserEdgeMapEdgeOrVertexCrossingEdgePairClosestPointsEdgeQueryEdgeQueryOptionsEdgeQueryResultEdgeTessellatorEdgeTrueCentroidEmptyCapEmptyLoopExpandForSubregionsFaceSegmentFaceSegmentsFloodFillRegionCoveringFullCapFullLoopFullPolygonFullRectGirardAreaIndeterminateIndexedInterpolateInterpolateAtDistanceIsDistanceLessIsInteriorDistanceLessIteratorBeginIteratorEndLatLngLatLngFromDegreesLatLngFromPointLoopFromCellLoopFromPointsMaxAngleSpanMetricMaxAreaMetricMaxDiagAspectMaxDiagMetricMaxDistanceToCellTargetMaxDistanceToEdgeTargetMaxDistanceToPointTargetMaxDistanceToShapeIndexTargetMaxEdgeAspectMaxEdgeMetricMaxWidthMetricMaybeCrossMercatorProjectionMinAngleSpanMetricMinAreaMetricMinDiagMetricMinDistanceToCellTargetMinDistanceToEdgeTargetMinDistanceToPointTargetMinDistanceToShapeIndexTargetMinEdgeMetricMinTessellationToleranceMinWidthMetricNewChainEdgeCrosserNewClosestEdgeQueryNewClosestEdgeQueryOptionsNewContainsPointQueryNewContainsVertexQueryNewConvexHullQueryNewCrossingEdgeQueryNewEdgeCrosserNewEdgeTessellatorNewFurthestEdgeQueryNewFurthestEdgeQueryOptionsNewMaxDistanceToCellTargetNewMaxDistanceToEdgeTargetNewMaxDistanceToPointTargetNewMaxDistanceToShapeIndexTargetNewMercatorProjectionNewMinDistanceToCellTargetNewMinDistanceToEdgeTargetNewMinDistanceToPointTargetNewMinDistanceToShapeIndexTargetNewPlateCarreeProjectionNewRectBounderNewShapeIndexNewShapeIndexCellNewShapeIndexIteratorOrderedCCWOriginPointOriginReferencePointPaddedCellPaddedCellFromCellIDPaddedCellFromParentIJPlanarCentroidPlateCarreeProjectionPointAreaPointFromCoordsPointFromLatLngPointVectorPolygonPolygonFromCellPolygonFromLoopsPolygonFromOrientedLoopsPolylinePolylineFromLatLngsRectBounderRectFromLatLngReferencePointRegionCovererRegularLoopRegularLoopForFrameRobustSignSentinelCellIDShapeEdgeShapeEdgeIDShapeIndexShapeIndexCellShapeIndexIteratorShapeIndexIteratorPosSignedAreaSimpleRegionCoveringSubdividedTrueCentroidTurnAngleUpdateMaxDistanceUpdateMinDistanceUpdateMinInteriorDistanceVertexCrossingVertexModelVertexModelClosedVertexModelOpenVertexModelSemiOpenWedgeContainsWedgeEqualsWedgeIntersectsWedgeIsDisjointWedgeIsProperlyContainedWedgeProperlyContainsWedgeProperlyOverlapsWedgeRelWedgeRelationappendFaceareSiblingsasByteReaderaxisUaxisVbigHalfbisectorIntersectionboundEncodedbyteReaderbyteReaderAdapterca45DegreescandidatecellIDFromFaceIJcellIDFromFaceIJSamecellIDFromFaceIJWrapcellIDFromPointcellIDFromStringcellIDscellPaddingcellSizeToLongEdgeRatiocenterPointclampIntclipBoundAxisclipDestinationclipEdgeBoundclippedEdgeclippedEdgeBoundclippedShapecompareBoundaryRelationcompareEdgescompareLoopscontainsBruteForcecontainsCenterMatchescontainsRelationcosDistancecoverercrossingTargetcrossingTargetCrosscrossingTargetDontCarecrossingTargetDontCrossdblErrordecodeFaceRundecodeFacesdecodeFirstPointFixedLengthdecodePointCompresseddecodePointsCompresseddefaultShapeIsEmptydefaultShapeIsFulldeinterleaveLookupdeinterleaveUint32derivativeEncodingOrderdetErrorMultiplierdirectedHausdorffDistancedistancedistanceTargetedgeClipErrorUVCoordedgeClipErrorUVDistedgeDistanceedgeIntersectsRectemptyLoopPointencodeFaceRunencodeFacesencodeFirstPointFixedLengthencodePointCompressedencodePointsCompressedencodingCompressedVersionencodingVersionexactCompareDistanceexactCompareDistancesexactSignexpandEndpointexpandedByDistanceUVexpensiveSignfacefaceBitsfaceClipErrorRadiansfaceClipErrorUVCoordfaceClipErrorUVDistfaceEdgefacePiQitoXYZfaceRunfaceSiTiToXYZfaceUVToXYZfaceUVWAxesfaceUVWFacesfaceXYZToUVfaceXYZtoUVWfacesIteratorfindEndVertexfindLSBSetNonZero64findMSBSetNonZero64fromFramefullLoopPointgetFramehasCrossingRelationijLevelToBoundUVijToPosijToSTMininitLookupCellinteriorDistinteriorMaxDistanceinterleaveLookupinterleaveUint32interpolateFloat64intersectionErrorintersectionExactintersectionMergeRadiusintersectionStableintersectionStableSortedintersectsLatEdgeintersectsLngEdgeintersectsRectErrorUVDistintersectsRelationinvertMasklatitudelongitudelookupBitslookupIJlookupPosloopCrosserloopMaploopRelationloopStacklsbForLevelmatrix3x3maxAnglemaxChordAnglemaxDeterminantErrormaxDistancemaxEncodedLoopsmaxEncodedVerticesmaxFloat64maxLevelForEdgemaxQueryResultsmaxSiTiminAngleminChordAngleminDistanceminFloat64minUpdateDistanceMaxErrorminUpdateInteriorDistanceMaxErrormoveOriginToValidFacenewBigFloatnewClippedShapenewCompareBoundaryRelationnewEdgeQueryResultnewLoopCrossernewNthDerivativeCodernewQueryOptionsnewRangeIteratornewTrackernextFacenorthPoleLatnthDerivativeCodernumFacesoppositeFaceoriginInsidepiQiToSTpointUVWpoleMinLatpolylineCentroidpolylineLengthposBitsposToIJposToOrientationprojectionqueryOptionsradiusToHeightrangeIteratorreferencePointAtVertexreferencePointForShaperegularPointsregularPointsForFrameremovedShaperobustNormalWithLengthroundAngleshapePointVisitorFuncshapeVisitorFuncsiTiToSTsiTitoPiQisin2DistancesingleEdgeLoopsinglePointLoopsizeIJsortAndUniqueResultssortCellIDssortEdgessortPointssouthPoleLatsplitBoundstToIJstToPiQistToSiTistToUVstableSignstalesumEqualswapMasksymbolicCompareDistancessymbolicallyPerturbedSigntoFrametrackerOrigintriageCompareCosDistancetriageCompareCosDistancestriageCompareSin2DistancetriageCompareSin2DistancestriageSigntypeTagtypeTagLaxPolygontypeTagLaxPolylinetypeTagMinUsertypeTagNonetypeTagPointVectortypeTagPolygontypeTagPolylineuAxisuNormuniqueIntsunitNormupdateEdgePairMaxDistanceupdateEdgePairMinDistanceupdateEndpointupdateMinDistanceuvToSTuvwAxisuvwFacevAxisvNormvalidFaceXYZToUVvalidRectLatRangevalidRectLngRangewedgeContainsSemiwedgewrapOffsetxyzFaceSiTixyzToFaceSiTixyzToFaceUVzigzagDecodezigzagEncodePointCrossapproxEqualCapBoundRectBoundContainsCellIntersectsCellCellUnionBoundV0ChainIDContainedChainEdgeNumChainsNumEdgesprivateInterfaceToTokenFaceIsLeafChildPositionimmediateParentisFacelsbEdgeNeighborsVertexNeighborsAllNeighborsRangeMinRangeMaxChildBeginChildBeginAtLevelChildEndChildEndAtLevelNextWrapPrevWrapAdvanceWrapdistanceFromBeginrawPointfaceSiTifaceIJOrientationCommonAncestorLevelcenterSTsizeSTboundSTcenterUVboundUVMaxTilecenterFaceSiTishapeIDcontainsCenternumEdgescontainsEdgeshapesfindByShapeIDhasInteriorcontainsTrackerOriginmaxEdgesPerCellcellMappendingAdditionsPospendingRemovalsNumEdgesUpToidForShapeIsFreshisFirstUpdateisShapeBeingRemovedmaybeApplyUpdatesapplyUpdatesInternaladdShapeInternaladdFaceEdgeupdateFaceEdgesshrinkToFitskipCellRangeupdateEdgesmakeIndexCellupdateBoundclipUBoundclipVBoundclipVAxisabsorbIndexCelltestAllEdgescountShapesremoveShapeInternalIndexCellrefreshLocatePointLocateCellIDboundmiddleiLojLoMiddleBoundChildIJEntryVertexExitVertexShrinkToFituvSizeIJSizeSTVertexBoundUVExactAreaApproxAreaAverageAreavertexChordDist2uEdgeIsClosestvEdgeIsClosestdistanceInternalMaxDistanceBoundaryDistanceDistanceToEdgeMaxDistanceToEdgeDistanceToCellMaxDistanceToCellLatLngIsPointAreaPolarClosureContainsLatLngDistanceToLatLngHausdorffDistanceverticessubregionBoundinitOriginAndBoundinitBoundfindValidationErrorNoIndexBoundaryEqualcompareBoundaryContainsOriginisEmptyOrFullOrientedVertexNumVerticesbruteForceContainsPointboundaryApproxIntersectsiteratorContainsPointCanonicalFirstVertexTurningAngleturningAngleMaxErrorIsHoleIsNormalizedfindVertexContainsNestedsurfaceIntegralFloat64surfaceIntegralPointCentroidxyzFaceSiTiVerticesencodeCompressedcompressedEncodingPropertiesdecodeCompressedcontainsNonCrossingBoundarySubsampleVerticesIsOnRightedgeIDchordAnglechordAngleBoundfromChordAngleinfinityupdateDistancecapBoundupdateDistanceToPointupdateDistanceToEdgeupdateDistanceToCellvisitContainingShapessetMaxErrormaxBruteForceIndexSizeradiusRadiusAddCapwriteUvarintwriteBoolwriteInt8writeInt16writeInt32writeInt64writeUint8writeFloat32writeFloat64aCrossingTargetbCrossingTargetwedgesCrossaXbaTangentbTangentacbRestartAtChainCrossingSignEdgeOrVertexChainCrossingcrossingSignCrossingsCrossingsEdgeMapcandidatescandidatesEdgeMapgetCellsgetCellsForEdgecomputeCellsIntersectedsplitUBoundsplitVBoundrelationswappedcrosserajbjPrevbQuerybCellsstartEdgeedgeCrossesCellcellCrossesCellcellCrossesAnySubcellhasCrossingrangeMinrangeMaxindexCellseekToseekBeyondmaxResultsdistanceLimitmaxErrorincludeInteriorsuseBruteForceUseBruteForceIncludeInteriorsMaxErrorDistanceLimitClosestInclusiveDistanceLimitFurthestInclusiveDistanceLimitClosestConservativeDistanceLimitFurthestConservativeDistanceLimitShapeIDEdgeIDIsInterioruseConservativeCellDistanceindexNumEdgesindexNumEdgesLimitavoidDuplicatestestedEdgesFindEdgesIsDistanceGreaterIsConservativeDistanceLessOrEqualIsConservativeDistanceGreaterOrEqualfindEdgesfindEdgesInternaladdResultmaybeAddResultfindEdgesBruteForcesetIncludeInteriorssetUseBruteForceedgeMapAddEdgeContainsVertexloopshasHolesnumVerticescumulativeEdgesinitNestedinitLoopsinitOneLoopinitLoopPropertiesinitEdgesAndIndexfindLoopNestingErrorNumLoopsLoopsLastDescendantcontainsBoundaryexcludesBoundaryexcludesNonCrossingShellsexcludesNonCrossingComplementShellsanyLoopContainsanyLoopIntersectsencodeLosslessreadInt8readInt16readInt32readInt64readUint8readFloat32readFloat64readUvarintintersectsFaceintersectsOppositeEdgesexitAxisexitPointfacesnumCurrentFaceShowncurFacefoundSharedVertexDimDerivMinLevelMaxLevelClosestLevelFromLatLngToLatLngUnprojectWrapDistanceisActivenextCellIDshapeIDssavedIDsfocusaddShapemoveTodrawTotestEdgesetNextCellIDatCellIDtoggleShapesaveAndClearStateBeforerestoreStateBeforelowerBoundIntersectsCellIDContainsCellIDDenormalizeLeafCellsCoveredcellUnionDifferenceInternalExpandAtLevelExpandByRadiusterminallevelModmaxCellsinteriorCoveringnewCandidateexpandChildrenaddCandidateadjustLeveladjustCellLevelsinitialCandidatescoveringInternalnormalizeCoveringreverseexcludesEdgeshapeContainsShapeContainsContainingShapessetColsetRowdettransposetolerancewrapDistanceAppendProjectedappendProjectedAppendUnprojectedappendUnprojectedwrapDestinationAddPolylineAddLoopAddPolygonConvexHullmonotoneChainaLLmaxErrorForTestsxWraptoRadiansfromRadiansinsertLoopLevelModMaxCellsnewCovererCoveringInteriorCoveringInteriorCellUnionFastCoveringgithub.com/golang/geo/s2MaxEntriesOnEvictedgithub.com/golang/groupcache/lruAssignableToTypeOfEqGotFormatterGotFormatterAdapterGotFormatterFuncInOrderNewControllerStringerFuncWantFormatterallMatcherassignableToTypeOfMatchercallerInfocancelReportereqMatcherlenMatchernewCallSetnilMatchernopTestHelpernotMatchersetSliceGotmatchersamtargetTypeProtoPackageIsVersion1ProtoPackageIsVersion2ProtoPackageIsVersion3extensionAsLegacyTypeextensionAsStorageTypedereffileDescriptor_e5baabe45344a177github.com/golang/protobuf/protoc-gen-go/descriptorgithub.com/golang/protobuf/ptypes/anygithub.com/golang/protobuf/ptypes/durationgithub.com/golang/protobuf/ptypes/emptygithub.com/golang/protobuf/ptypes/timestampwrappersgithub.com/golang/protobuf/ptypes/wrappersptypesgithub.com/golang/protobuf/ptypesErrCorruptErrUnsupportedchecksumSizechunkHeaderSizechunkTypeCompressedDatachunkTypePaddingchunkTypeStreamIdentifierchunkTypeUncompressedDatacrcTabledecodeErrCodeCorruptdecodeErrCodeUnsupportedLiteralLengthdecodedLenemitCopyencodeBlockerrUnsupportedLiteralLengthextendMatchmagicBodymagicChunkmaxBlockSizemaxEncodedLenOfMaxBlockSizeobufHeaderLenobufLentagCopy1tagCopy2tagCopy4tagLiteralgithub.com/golang-sql/civilDefaultFreeListSizeNewFreeListNewWithFreeListftFreelistFullftNotOwnedftStorednilChildrennilItemsremoveItemremoveMaxremoveMinCodecFlatBufferFlatbuffersCodecGetRootAsSizeBoolSizeByteSizeFloat32SizeFloat64SizeInt16SizeInt32SizeInt64SizeInt8SizeSOffsetTSizeUOffsetTSizeUint16SizeUint32SizeUint64SizeUint8SizeVOffsetTVtableMetadataFieldsWriteSOffsetTWriteUOffsetTWriteVOffsetTbyteSliceToStringfileIdentifierLengthflatbuffersInitvtableEqualminalignvtableobjectEndvtablesFinishedBytesStartObjectWriteVtableEndObjectgrowByteBufferPrepPrependSOffsetTPrependUOffsetTStartVectorEndVectorCreateStringCreateByteStringCreateByteVectorassertNestedassertNotNestedassertFinishedPrependBoolSlotPrependByteSlotPrependUint8SlotPrependUint16SlotPrependUint32SlotPrependUint64SlotPrependInt8SlotPrependInt16SlotPrependInt32SlotPrependInt64SlotPrependFloat32SlotPrependFloat64SlotPrependUOffsetTSlotPrependStructSlotSlotFinishWithFileIdentifierPrependBoolPrependUint8PrependUint16PrependUint32PrependUint64PrependInt8PrependInt16PrependInt32PrependInt64PrependFloat32PrependFloat64PrependBytePrependVOffsetTPlaceBoolPlaceUint8PlaceUint16PlaceUint32PlaceUint64PlaceInt8PlaceInt16PlaceInt32PlaceInt64PlaceFloat32PlaceFloat64PlaceBytePlaceVOffsetTPlaceSOffsetTPlaceUOffsetTflatbuffersgithub.com/google/flatbuffers/goAcyclicTransformerAnyErrorEquateApproxEquateApproxTimeEquateErrorsEquateNaNsIgnoreInterfacesIgnoreMapEntriesIgnoreSliceElementsIgnoreTypesSortMapsSortSlicesanyErrorapproximatorareConcreteErrorsareNaNsF32sareNaNsF64sareNonZeroTimesareRealF32sareRealF64scanonicalNamecompareErrorsequateAlwaysfieldTreefilterFieldifaceFiltermapSorternewIfaceFilternewStructFilternewTypeFilternewUnexportedFiltersliceSorterstructFiltertimeApproximatortypeFilterunexportedFilterxformFiltermatchPrefixcheckSortxformxffracmargcompareF64compareF32marginBoolResultEditScriptEditTypeEqualFuncIdentityUniqueXUniqueYdebuggerrandIntzigzagLenXLenYNINXNYNMgithub.com/google/go-cmp/cmp/internal/diffAtLeastGo110Deterministicgithub.com/google/go-cmp/cmp/internal/flagsEqualAssignableIsTypeKeyValuePredicateNameOfValuePredicatefuncTypelastIdentRxtbFunctibFunctrFunctrbFuncttbFuncgithub.com/google/go-cmp/cmp/internal/functionPointerOfTypeStringappendTypeNamegithub.com/google/go-cmp/cmp/internal/valueAllowUnexportedExporterFilterPathFilterValuesIgnoreSliceIndexTypeAssertionautoTypecoalesceAdjacentEditscoalesceAdjacentRecordscoalesceInterveningIdenticalcommentStringcomparercoreOptiondefaultReporterdetectRacesdiffIdenticaldiffInserteddiffModediffRemoveddiffStatsdiffUnknownelideTypeemitTypeflattenOptionsformatASCIIformatHexformatMapKeyformatOptionsformatPointerformatReferenceformatValueOptionsidentRxidentsRxindentModeleafReferencemakeAddressablemakeLeafReferencemapIndexmaxColumnLengthmaxVerbosityPresetnormalizeOptionnumContextRecordspathFilterpointerDelimPrefixpointerDelimSuffixpointerReferencesrandBoolrepeatCountreportByCyclereportByFuncreportByIgnorereportByMethodreportEqualreportRecordreportUnequalresolveReferencesretrieveUnexportedFieldrootStepsanitizeValuesliceIndexsupportExporterstextEllipsistextLinetextListtextNiltextNodetextRecordtextWraptrunkReferencetrunkReferencestypeAssertiontypeModeupdateReferencePrefixvalidatorvalueNodevaluesFilterverbosityPresetwrapParenswrapTrunkReferencewrapTrunkReferencesunexportedmayForcepaddrpvxpvyxkeyykeyisSliceNumIgnoredNumIdenticalNumRemovedNumInsertedNumModifiedappendIndentformatCompactToformatExpandedToElideCommaAppendEllipsisalignLensAvoidStringerPrintAddressesQualifiedNamesVerbosityLevelLimitVerbosityDiffModeTypeModeWithDiffModeWithTypeModeWithVerbosityFormatDiffformatDiffListFormatTypeFormatValueCanFormatDiffSliceFormatDiffSliceformatDiffSliceValueXValueYNumComparedNumTransformedNumChildrenRecordsTransformerNametaappendCharPushPairAbuseRateLimitErrorAcceptedErrorBasicAuthTransportCheckResponseCheckRunEventCheckSuiteEventCommitCommentEventCreateEventDeleteEventDeliveryIDDeploymentEventDeploymentStatusEventEditChangeErrorResponseForkEventGollumEventInstallationEventInstallationRepositoriesEventIssueCommentEventIssuesEventLabelEventMarketplacePurchaseEventMemberEventMembershipEventMilestoneEventNewEnterpriseClientOrgBlockEventOrganizationEventPageBuildEventParseWebHookPingEventProjectCardChangeProjectCardEventProjectChangeProjectColumnChangeProjectColumnEventProjectEventPublicEventPullRequestEventPullRequestReviewCommentEventPullRequestReviewEventPushEventPushEventCommitPushEventRepoOwnerPushEventRepositoryReleaseEventRepositoryEventScopeAdminGPGKeyScopeAdminOrgScopeAdminOrgHookScopeAdminPublicKeyScopeAdminRepoHookScopeDeleteRepoScopeGistScopeNoneScopeNotificationsScopePublicRepoScopeReadGPGKeyScopeReadOrgScopeReadPublicKeyScopeReadRepoHookScopeRepoScopeRepoDeploymentScopeRepoStatusScopeUserScopeUserEmailScopeUserFollowScopeWriteGPGKeyScopeWriteOrgScopeWritePublicKeyScopeWriteRepoHookStatusEventStringifyTarballTeamAddEventTeamChangeTeamEventTwoFactorAuthErrorUnauthenticatedRateLimitedTransportValidatePayloadWatchEventWebHookAuthorWebHookCommitWebHookPayloadWebHookTypeZipballaddOptionscategoriescheckMACcoreCategorycreateCommitcreateRefRequestcreateTagRequestcreateTreedefaultBaseURLdefaultMediaTypedeliveryIDHeadereventTypeHeadereventTypeMappingformatRateResetgenMACheaderOTPheaderRateLimitheaderRateRemainingheaderRateResetmarkReadOptionsmarkdownRequestmediaTypeBlockUsersPreviewmediaTypeCheckRunsPreviewmediaTypeCodesOfConductPreviewmediaTypeCommitSearchPreviewmediaTypeDeploymentStatusPreviewmediaTypeGitSigningPreviewmediaTypeHovercardPreviewmediaTypeImportPreviewmediaTypeIntegrationPreviewmediaTypeLabelDescriptionSearchPreviewmediaTypeLockReasonPreviewmediaTypeMigrationsPreviewmediaTypeNestedTeamsPreviewmediaTypeOrgPermissionRepomediaTypeOrganizationInvitationPreviewmediaTypePagesPreviewmediaTypePreReceiveHooksPreviewmediaTypeProjectsPreviewmediaTypeReactionsPreviewmediaTypeRepositoryCommunityHealthMetricsPreviewmediaTypeRepositoryInvitationsPreviewmediaTypeRepositoryTransferPreviewmediaTypeRequiredApprovingReviewsPreviewmediaTypeStarringPreviewmediaTypeTeamDiscussionsPreviewmediaTypeTimelinePreviewmediaTypeTopicsPreviewmediaTypeV3mediaTypeV3DiffmediaTypeV3PatchmediaTypeV3SHAmessageMACnewResponseparseBoolResponseparseRatepullRequestMergeRequestpullRequestUpdaterepositoryTopicssanitizeURLsearchCategorysha1Prefixsha256Prefixsha512PrefixsignatureHeaderstartMigrationstartUserMigrationstringifyValuetimestampTypeupdateRefRequestuploadBaseURLvalidateSignaturewithContextRetryAfterGetActionGetMemberGetScopeGetSenderGetPageNameBranchesBaseTreeGetInvitationGetMembershipReviewGetPullRequestRequestedReviewerGetRequestedReviewerEffectiveDatePreviousMarketplacePurchaseGetEffectiveDateGetPreviousMarketplacePurchaseGetBuildAfterIDGetAfterIDPullAddedRemovedGetDistinctForcedHeadCommitGetAfterGetBeforeGetCompareGetCreatedGetDeletedGetForcedGetHeadCommitRepositoriesAddedRepositoriesRemovedRefTypePusherTypeGetPusherTypeGetRefTypeTreeIDGetTreeIDPushIDDistinctSizeBaseRefGetBaseRefGetDistinctSizeGetPushIDDocumentationURLHookIDGetHookIDGetZenForkeeGetForkeeBlockedUserGetBlockedUserApplyBraceArrayCompBinaryOpBopAndBopBitwiseAndBopBitwiseOrBopBitwiseXorBopDivBopGreaterBopGreaterEqBopInBopLessBopLessEqBopManifestEqualBopManifestUnequalBopMapBopMinusBopMultBopOrBopPercentBopPlusBopShiftLBopShiftRBuildSourceConditionalDesugaredObjectDesugaredObjectFieldDesugaredObjectFieldsDollarFodderFodderAppendFodderConcatFodderCountNewlinesFodderElementFodderElementCountNewlinesFodderEnsureCleanNewlineFodderHasCleanEndlineFodderInterstitialFodderKindFodderLineEndFodderMoveFrontFodderParagraphForSpecIdentifierSetIfSpecImportStrInSuperLineBeginningLiteralBooleanLiteralNullLiteralNumberLiteralStringLiteralStringKindLocalBindLocalBindsLocationRangeBetweenMakeFodderElementMakeLocationRangeMakeLocationRangeMessageNamedArgumentNamedParameterNewIdentifierSetNewNodeBaseNewNodeBaseLocNodeBaseObjectAssertObjectCompObjectFieldObjectFieldExprObjectFieldHiddenObjectFieldHideObjectFieldIDObjectFieldInheritObjectFieldKindObjectFieldLocalNoMethodObjectFieldStrObjectFieldVisibleObjectFieldsObjectLocalObjectNullExprObjectNullIDObjectNullStrParensSourceProviderStringBlockStringDoubleStringSingleSuperIndexUnaryUnaryOpUopBitwiseNotUopMapUopMinusUopNotUopPlusVerbatimStringDoubleVerbatimStringSinglebopStringscloneDesugaredFieldcloneFieldcloneForSpeccloneNodeBasecloneParametersidentifierSorterlocationBeforetrimToLineuopStringsBlanksLocRangeFreeVarsDefaultArgFunFullyEscapedSuperSugarMethodSugarExpr1Expr2Expr3PositionalPlusSuperAssertsLocalsBeginIndexEndIndexStepGetSnippetVarNameOuterBranchTrueBranchFalseRestBindsToSliceContainsAllIsSubsetIsSupersetSymmetricDifferenceAddIdentifiersToOrderedSliceBlockIndentTailStrictOriginalStringgithub.com/google/go-jsonnet/astStdAst_StdAstp1p10001p10001Varp10007p10007Varp10020p10020Varp1003p1003Varp10042p10042Varp10048p10048Varp10052p10052Varp10063p10063Varp1007p10071p10071Varp1007Varp10080p10080Varp10093p10093Varp10107p10107Varp10117p10117Varp10131p10131Varp10165p10165Varp1017p10174p10174Varp1017Varp10198p10198Varp10203p10203Varp10220p10220Varp10231p10231Varp10235p10235Varp10256p10256Varp10262p10262Varp10276p10276Varp10283p10283Varp10287p10287Varp10300p10300Varp10312p10312Varp10316p10316Varp10325p10325Varp10335p10335Varp10339p10339Varp10354p10354Varp1037p10376p10376Varp1037Varp10382p10382Varp10425p10425Varp10456p10456Varp10462p10462Varp1047p1047Varp10543p10543Varp10560p10560Varp10673p10673Varp10690p10690Varp10699p10699Varp107p10702p10702Varp10728p10728Varp10737p10737Varp10758p10758Varp10779p10779Varp1078p10783p10783Varp1078Varp10798p10798Varp107Varp10806p10806Varp10810p10810Varp10845p10845Varp10864p10864Varp1088p1088Varp10905p10905Varp10924p10924Varp10964p10964Varp10991p10991Varp11000p11000Varp11004p11004Varp11013p11013Varp1102p11024p11024Varp1102Varp11034p11034Varp11038p11038Varp11047p11047Varp11056p11056Varp11060p11060Varp11069p11069Varp11080p11080Varp11086p11086Varp11105p11105Varp1111p11111p11111Varp11116p11116Varp1111Varp11122p11122Varp11131p11131Varp11146p11146Varp11155p11155Varp11160p11160Varp11166p11166Varp11176p11176Varp11185p11185Varp11194p11194Varp11221p11221Varp11230p11230Varp11234p11234Varp11242p11242Varp11252p11252Varp11261p11261Varp11265p11265Varp11273p11273Varp11289p11289Varp11297p11297Varp11309p11309Varp1131p11317p11317Varp1131Varp11321p11321Varp11325p11325Varp11334p11334Varp11337p11337Varp11346p11346Varp11352p11352Varp11356p11356Varp11418p11418Varp11430p11430Varp11442p11442Varp1145p11457p11457Varp1145Varp11469p11469Varp11484p11484Varp11496p11496Varp11503p11503Varp11512p11512Varp11527p11527Varp11536p11536Varp11545p11545Varp11554p11554Varp1156p1156Varp11571p11571Varp11586p11586Varp11598p11598Varp11607p11607Varp11621p11621Varp11632p11632Varp11638p11638Varp11642p11642Varp11655p11655Varp11662p11662Varp11674p11674Varp11689p11689Varp11698p11698Varp11710p11710Varp1172p11721p11721Varp1172Varp11732p11732Varp11743p11743Varp11752p11752Varp11764p11764Varp11777p11777Varp11786p11786Varp11790p11790Varp118p11803p11803Varp11809p11809Varp11813p11813Varp11828p11828Varp11863p11863Varp1188p11887p11887Varp1188Varp11893p11893Varp118Varp11903p11903Varp11909p11909Varp11929p11929Varp11951p11951Varp11971p11971Varp11990p11990Varp12p12002p12002Varp1202p12021p12021Varp1202Varp12033p12033Varp12046p12046Varp12052p12052Varp12056p12056Varp12073p12073Varp12088p12088Varp12102p12102Varp1211p12114p12114Varp1211Varp12126p12126Varp12148p12148Varp12164p12164Varp1217p12176p12176Varp1217Varp12188p12188Varp12207p12207Varp12226p12226Varp12239p12239Varp12245p12245Varp12249p12249Varp1226p12264p12264Varp1226Varp12283p12283Varp12313p12313Varp1232p12325p12325Varp1232Varp12337p12337Varp1236p12363p12363Varp1236Varp12375p12375Varp12387p12387Varp12406p12406Varp12418p12418Varp12437p12437Varp12449p12449Varp12462p12462Varp12469p12469Varp12482p12482Varp12492p12492Varp12505p12505Varp12517p12517Varp12537p12537Varp12550p12550Varp12562p12562Varp12568p12568Varp12577p12577Varp12588p12588Varp126p12618p12618Varp1263p12631p12631Varp1263Varp12654p12654Varp12667p12667Varp1267p1267Varp12683p12683Varp126Varp12704p12704Varp12715p12715Varp12724p12724Varp12732p12732Varp12741p12741Varp12749p12749Varp12758p12758Varp12768p12768Varp12777p12777Varp12787p12787Varp12791p12791Varp12800p12800Varp12806p12806Varp12815p12815Varp12830p12830Varp12846p12846Varp12853p12853Varp12862p12862Varp12877p12877Varp12888p12888Varp12895p12895Varp12899p12899Varp12931p12931Varp12946p12946Varp12962p12962Varp12969p12969Varp12978p12978Varp12984p12984Varp12993p12993Varp12Varp13010p13010Varp13017p13017Varp1302p13021p13021Varp1302Varp13036p13036Varp13063p13063Varp13078p13078Varp1308p1308Varp13092p13092Varp13101p13101Varp13105p13105Varp13114p13114Varp13126p13126Varp13138p13138Varp13149p13149Varp13156p13156Varp13166p13166Varp13173p13173Varp13177p13177Varp13181p13181Varp13185p13185Varp13194p13194Varp1321p1321Varp13222p13222Varp13243p13243Varp13251p13251Varp13260p13260Varp13288p13288Varp1329p13297p13297Varp1329Varp13303p13303Varp13312p13312Varp13351p13351Varp13360p13360Varp13375p13375Varp13384p13384Varp13400p13400Varp13409p13409Varp13422p13422Varp13439p13439Varp13454p13454Varp13471p13471Varp13477p13477Varp13486p13486Varp13492p13492Varp13501p13501Varp13535p13535Varp13539p13539Varp13571p13571Varp13583p13583Varp13596p13596Varp13613p13613Varp1362p13624p13624Varp13628p13628Varp1362Varp13647p13647Varp13659p13659Varp13664p13664Varp13666p13666Varp13669p13669Varp13672p13672Varp13678p13678Varp13681p13681Varp13684p13684Varp13687p13687Varp13689p13689Varp13692p13692Varp13695p13695Varp13697p13697Varp13701p13701Varp13704p13704Varp13711p13711Varp13713p13713Varp13717p13717Varp13720p13720Varp13727p13727Varp13729p13729Varp13733p13733Varp13736p13736Varp13743p13743Varp13745p13745Varp13749p13749Varp13753p13753Varp13757p13757Varp13760p13760Varp13767p13767Varp13770p13770Varp13774p13774Varp13777p13777Varp13780p13780Varp13783p13783Varp13786p13786Varp13789p13789Varp13792p13792Varp13795p13795Varp13798p13798Varp13800p13800Varp13803p13803Varp13806p13806Varp13809p13809Varp13812p13812Varp13815p13815Varp13818p13818Varp13821p13821Varp13824p13824Varp13826p13826Varp13829p13829Varp13832p13832Varp13837p13837Varp13840p13840Varp13843p13843Varp13846p13846Varp13848p13848Varp13851p13851Varp13854p13854Varp13857p13857Varp13860p13860Varp13863p13863Varp13866p13866Varp13869p13869Varp1387p1387Varp13882p13882Varp13884p13884Varp13888p13888Varp13891p13891Varp13894p13894Varp13897p13897Varp139p13904p13904Varp13907p13907Varp13910p13910Varp13917p13917Varp13920p13920Varp13932p13932Varp13934p13934Varp13937p13937Varp13961p13961Varp13965p13965Varp13968p13968Varp13971p13971Varp13974p13974Varp13977p13977Varp1398p13980p13980Varp13987p13987Varp13989p13989Varp1398Varp139Varp1408p1408Varp1424p1424Varp1428p1428Varp1437p1437Varp1441p1441Varp1453p1453Varp1464p1464Varp1476p1476Varp1484p1484Varp1498p1498Varp1508p1508Varp1519p1519Varp1526p1526Varp1530p1530Varp1539p1539Varp154p1543p1543Varp154Varp1555p1555Varp1566p1566Varp1578p1578Varp1586p1586Varp1600p1600Varp1610p1610Varp1621p1621Varp1628p1628Varp1637p1637Varp1650p1650Varp1661p1661Varp1665p1665Varp167p1670p1670Varp167Varp1700p1700Varp1725p1725Varp1737p1737Varp1784p1784Varp1834p1834Varp184p1848p1848Varp184Varp1871p1871Varp1877p1877Varp1881p1881Varp1911p1911Varp1942p1942Varp1966p1966Varp1988p1988Varp199p1997p1997Varp199Varp2006p2006Varp2010p2010Varp2023p2023Varp2038p2038Varp2052p2052Varp2064p2064Varp2081p2081Varp2093p2093Varp2117p2117Varp2129p2129Varp2137p2137Varp2150p2150Varp216p2167p2167Varp216Varp2184p2184Varp2198p2198Varp2215p2215Varp2226p2226Varp2235p2235Varp2241p2241Varp2247p2247Varp2258p2258Varp2271p2271Varp2288p2288Varp23p2305p2305Varp231p2319p2319Varp231Varp2336p2336Varp2347p2347Varp2356p2356Varp2362p2362Varp2368p2368Varp2381p2381Varp2394p2394Varp23Varp2411p2411Varp2426p2426Varp2443p2443Varp2473p2473Varp2479p2479Varp248p248Varp2496p2496Varp2503p2503Varp2507p2507Varp2511p2511Varp2526p2526Varp2547p2547Varp2571p2571Varp2586p2586Varp2603p2603Varp2612p2612Varp2625p2625Varp2641p2641Varp2665p2665Varp2700p2700Varp2717p2717Varp273p2732p2732Varp273Varp2741p2741Varp2759p2759Varp2768p2768Varp2787p2787Varp2794p2794Varp2803p2803Varp2811p2811Varp2817p2817Varp2828p2828Varp283p283Varp2843p2843Varp2854p2854Varp2870p2870Varp2879p2879Varp2903p2903Varp2910p2910Varp2914p2914Varp2918p2918Varp292p292Varp2933p2933Varp2941p2941Varp2958p2958Varp2962p2962Varp2977p2977Varp2985p2985Varp3p3004p3004Varp302p3022p3022Varp302Varp3036p3036Varp3049p3049Varp3057p3057Varp3061p3061Varp3065p3065Varp3069p3069Varp3084p3084Varp3092p3092Varp31p3111p3111Varp3126p3126Varp3140p3140Varp315p3155p3155Varp315Varp3169p3169Varp3184p3184Varp3198p3198Varp31Varp3213p3213Varp3227p3227Varp323p323Varp3242p3242Varp3248p3248Varp3259p3259Varp3267p3267Varp3280p3280Varp3284p3284Varp3301p3301Varp3317p3317Varp3328p3328Varp3332p3332Varp3347p3347Varp3355p3355Varp3374p3374Varp338p338Varp3402p3402Varp3430p3430Varp3458p3458Varp3486p3486Varp351p3514p3514Varp351Varp3542p3542Varp3570p3570Varp3598p3598Varp362p3626p3626Varp362Varp3646p3646Varp3657p3657Varp3666p3666Varp3670p3670Varp3685p3685Varp3693p3693Varp3712p3712Varp3724p3724Varp3732p3732Varp3736p3736Varp3751p3751Varp3759p3759Varp376p376Varp3797p3797Varp3801p3801Varp3816p3816Varp3824p3824Varp3856p3856Varp3877p3877Varp388p388Varp3898p3898Varp3919p3919Varp3940p3940Varp3961p3961Varp397p397Varp3982p3982Varp3Varp4003p4003Varp4024p4024Varp4045p4045Varp4066p4066Varp4087p4087Varp410p4108p4108Varp410Varp4128p4128Varp4132p4132Varp4147p4147Varp4155p4155Varp4161p4161Varp4169p4169Varp4175p4175Varp4186p4186Varp4192p4192Varp42p4203p4203Varp4209p4209Varp421p421Varp4220p4220Varp4226p4226Varp4237p4237Varp4243p4243Varp4252p4252Varp4263p4263Varp42Varp4302p4302Varp4306p4306Varp4321p4321Varp4331p4331Varp4337p4337Varp435p4354p4354Varp435Varp4360p4360Varp4373p4373Varp4388p4388Varp4402p4402Varp4421p4421Varp4427p4427Varp4436p4436Varp4439p4439Varp4443p4443Varp4447p4447Varp4462p4462Varp4479p4479Varp448p4486p4486Varp448Varp4490p4490Varp4498p4498Varp4511p4511Varp4521p4521Varp4525p4525Varp4535p4535Varp4548p4548Varp4556p4556Varp4560p4560Varp4564p4564Varp4573p4573Varp4579p4579Varp4583p4583Varp459p459Varp4600p4600Varp4609p4609Varp4631p4631Varp4644p4644Varp4654p4654Varp4663p4663Varp4669p4669Varp4678p4678Varp470p4700p4700Varp4709p4709Varp470Varp4717p4717Varp4723p4723Varp4752p4752Varp4756p4756Varp4760p4760Varp4765p4765Varp4782p4782Varp479p4791p4791Varp479Varp4801p4801Varp4810p4810Varp4816p4816Varp4820p4820Varp4836p4836Varp4845p4845Varp4869p4869Varp488p4882p4882Varp488Varp4892p4892Varp4901p4901Varp4907p4907Varp4916p4916Varp4946p4946Varp4955p4955Varp4963p4963Varp497p497Varp4982p4982Varp50p5011p5011Varp5015p5015Varp5019p5019Varp5023p5023Varp503p503Varp5048p5048Varp5064p5064Varp5078p5078Varp5091p5091Varp5098p5098Varp50Varp5102p5102Varp5106p5106Varp5115p5115Varp5121p5121Varp5130p5130Varp5136p5136Varp5155p5155Varp516p5169p5169Varp516Varp5175p5175Varp5186p5186Varp5220p5220Varp5229p5229Varp5248p5248Varp525p525Varp5267p5267Varp5273p5273Varp5301p5301Varp5311p5311Varp5315p5315Varp5319p5319Varp5336p5336Varp5347p5347Varp5356p5356Varp5367p5367Varp5372p5372Varp5386p5386Varp5398p5398Varp542p5422p5422Varp542Varp5441p5441Varp5448p5448Varp5461p5461Varp5471p5471Varp5491p5491Varp5495p5495Varp5499p5499Varp5508p5508Varp5522p5522Varp5536p5536Varp5574p5574Varp5599p5599Varp5623p5623Varp5631p5631Varp5672p5672Varp5696p5696Varp5702p5702Varp5717p5717Varp5759p5759Varp576p576Varp5783p5783Varp5791p5791Varp5840p5840Varp5864p5864Varp5872p5872Varp588p588Varp5917p5917Varp5941p5941Varp5949p5949Varp5999p5999Varp6023p6023Varp6029p6029Varp6038p6038Varp6049p6049Varp6058p6058Varp606p6069p6069Varp606Varp6092p6092Varp61p6130p6130Varp6139p6139Varp6151p6151Varp61Varp6204p6204Varp6216p6216Varp622p622Varp6231p6231Varp6247p6247Varp6266p6266Varp6282p6282Varp6298p6298Varp6302p6302Varp631p6317p6317Varp631Varp6334p6334Varp6354p6354Varp6365p6365Varp6384p6384Varp6393p6393Varp640p640Varp6414p6414Varp6429p6429Varp6450p6450Varp6470p6470Varp6486p6486Varp6498p6498Varp6513p6513Varp6540p6540Varp655p655Varp6560p6560Varp6582p6582Varp6597p6597Varp6606p6606Varp6621p6621Varp664p6647p6647Varp664Varp6656p6656Varp6673p6673Varp6693p6693Varp6709p6709Varp6723p6723Varp6735p6735Varp6758p6758Varp6779p6779Varp6783p6783Varp6798p6798Varp6806p6806Varp6825p6825Varp683p6834p6834Varp683Varp6853p6853Varp6874p6874Varp6895p6895Varp69p6916p6916Varp692p6927p6927Varp692Varp6948p6948Varp6965p6965Varp6979p6979Varp6995p6995Varp69Varp7006p7006Varp701p7017p7017Varp701Varp7045p7045Varp7054p7054Varp7074p7074Varp7083p7083Varp7095p7095Varp7101p7101Varp7111p7111Varp7115p7115Varp7119p7119Varp7134p7134Varp7144p7144Varp716p7163p7163Varp716Varp7180p7180Varp7188p7188Varp7192p7192Varp7196p7196Varp7211p7211Varp7221p7221Varp7231p7231Varp725p7250p7250Varp725Varp7262p7262Varp7275p7275Varp7292p7292Varp7307p7307Varp7324p7324Varp7339p7339Varp7356p7356Varp7367p7367Varp7378p7378Varp7386p7386Varp7414p7414Varp7427p7427Varp744p7444p7444Varp744Varp7464p7464Varp7477p7477Varp7494p7494Varp751p7519p7519Varp751Varp7532p7532Varp7549p7549Varp7564p7564Varp7581p7581Varp7600p7600Varp7613p7613Varp7630p7630Varp764p7645p7645Varp764Varp7662p7662Varp7681p7681Varp7690p7690Varp7693p7693Varp7707p7707Varp7711p7711Varp7715p7715Varp7724p7724Varp7740p7740Varp7744p7744Varp7763p7763Varp7789p7789Varp7794p7794Varp781p7812p7812Varp7817p7817Varp781Varp7830p7830Varp7836p7836Varp7840p7840Varp7854p7854Varp7859p7859Varp7867p7867Varp7870p7870Varp7881p7881Varp7890p7890Varp7911p7911Varp7917p7917Varp7929p7929Varp7938p7938Varp7952p7952Varp796p7968p7968Varp796Varp7973p7973Varp7979p7979Varp7983p7983Varp7992p7992Varp7998p7998Varp8p80p8002p8002Varp8062p8062Varp8071p8071Varp80Varp8107p8107Varp8129p8129Varp813p813Varp8145p8145Varp8151p8151Varp8162p8162Varp8169p8169Varp8178p8178Varp8185p8185Varp8189p8189Varp8198p8198Varp8204p8204Varp8207p8207Varp8235p8235Varp8251p8251Varp8257p8257Varp8268p8268Varp8275p8275Varp8279p8279Varp828p8288p8288Varp828Varp8294p8294Varp8297p8297Varp8316p8316Varp8320p8320Varp8330p8330Varp8341p8341Varp8349p8349Varp8358p8358Varp8366p8366Varp8370p8370Varp8374p8374Varp8411p8411Varp8432p8432Varp8444p8444Varp845p8459p8459Varp845Varp8482p8482Varp8489p8489Varp8498p8498Varp8510p8510Varp8517p8517Varp8527p8527Varp8534p8534Varp8544p8544Varp8547p8547Varp856p8563p8563Varp8567p8567Varp856Varp8577p8577Varp8591p8591Varp8601p8601Varp8618p8618Varp8634p8634Varp8641p8641Varp8648p8648Varp8658p8658Varp8661p8661Varp867p8677p8677Varp867Varp8681p8681Varp8702p8702Varp8711p8711Varp8725p8725Varp8742p8742Varp8748p8748Varp8765p8765Varp8775p8775Varp8785p8785Varp8790p8790Varp8794p8794Varp88p880p880Varp8831p8831Varp8852p8852Varp8859p8859Varp8868p8868Varp8896p8896Varp88Varp8905p8905Varp8917p8917Varp8930p8930Varp8953p8953Varp8966p8966Varp897p897Varp8981p8981Varp8Varp9004p9004Varp9020p9020Varp9028p9028Varp9032p9032Varp9045p9045Varp9058p9058Varp9066p9066Varp9090p9090Varp9103p9103Varp9111p9111Varp912p9123p9123Varp912Varp9131p9131Varp9140p9140Varp9152p9152Varp9184p9184Varp9200p9200Varp9214p9214Varp9222p9222Varp9226p9226Varp9232p9232Varp9249p9249Varp9269p9269Varp9285p9285Varp929p9293p9293Varp9297p9297Varp929Varp9310p9310Varp9323p9323Varp9331p9331Varp9361p9361Varp9374p9374Varp9382p9382Varp9398p9398Varp9431p9431Varp944p9446p9446Varp944Varp9460p9460Varp9474p9474Varp9482p9482Varp9486p9486Varp9492p9492Varp9507p9507Varp9518p9518Varp9532p9532Varp9542p9542Varp9557p9557Varp9574p9574Varp9590p9590Varp9606p9606Varp961p9615p9615Varp961Varp9632p9632Varp9645p9645Varp9674p9674Varp9679p9679Varp9688p9688Varp9699p9699Varp9706p9706Varp9715p9715Varp9730p9730Varp9739p9739Varp9755p9755Varp976p976Varp9771p9771Varp9780p9780Varp9796p9796Varp9805p9805Varp9822p9822Varp9838p9838Varp9847p9847Varp9862p9862Varp9880p9880Varp9892p9892Varp99p9924p9924Varp993p993Varp9950p9950Varp9955p9955Varp9966p9966Varp9973p9973Varp9982p9982Varp999p9993p9993Varp999Varp99Varastgengithub.com/google/go-jsonnet/astgenMakeStaticErrorMakeStaticErrorMsgStaticErrorgithub.com/google/go-jsonnet/internal/errorsLexLiteralFieldLiteralFieldSetNewLiteralFieldSetSnippetToRawASTaddContextanonymousapplyPrecedenceastVarToIdentifierbopPrecedencecheckWhitespacedesugaredObjectDirectChildrendirectChildrenfunctionContextinDesugaredObjectSpecialChildreninObjectFieldsChildrenisHorizontalWhitespaceisIdentifierisIdentifierFirstisLowerisNumberisSymbolisUpperlexEOFlineSplitlocFromTokenASTlocFromTokenslocationFromPositionmakeLexermakeParsermakeUnexpectedErrormaxPrecedenceobjectContextobjectFieldsDirectChildrenprecedencespecialChildrenstripWhitespacethunkChildrentokenAsserttokenBraceLtokenBraceRtokenBracketLtokenBracketRtokenCommatokenDollartokenDottokenElsetokenEndOfFiletokenFalsetokenFortokenFunctiontokenIdentifiertokenIftokenImporttokenImportStrtokenIntokenKindtokenKindStringstokenLocaltokenNullLittokenNumbertokenOperatortokenParenLtokenParenRtokenSelftokenSemicolontokenStringBlocktokenStringDoubletokenStringSingletokenStringToAsttokenSupertokenTailStricttokenThentokenTruetokenVerbatimStringDoubletokenVerbatimStringSingletopLevelContextunaryPrecedencefodderstringBlockIndentstringBlockTermIndentbyteNolineNolineStarttokenStartLocfreshLineacceptNprevLocationresetTokenStartemitFullTokenemitTokenaddFoddermakeStaticErrorPointlexWhitespacelexUntilNewlinelexIdentifierlexSymbolcurrTunexpectedTokenErrorpopExpectpopExpectOpdoublePeekparseArgumentparseArgumentsparseParametersparseBindparseObjectAssignmentOpparseObjectRemainderparseComprehensionSpecsparseArrayparseTerminalparsingFailuregithub.com/google/go-jsonnet/internal/parserSnippetToASTanalysisStateanalyzeVisitbuildAndbuildLiteralStringbuildSimpleIndexbuildStdCalldesugardesugarASTdesugarArrayCompdesugarFieldsdesugarForSpecdesugarLocalBindsdesugarObjectCompdesugaredBopenterLocalmakeStrsimpleLambdastringUnescapevisitNextwrapInArrayfreeVarsgithub.com/google/go-jsonnet/internal/programFileImporterMakeContentsMemoryImporterRuntimeErroraddBindingsarrayFromThunksastMakeArrayElementbinaryBuiltinbinaryBuiltinFuncbindingsUnboundFieldbopBuiltinsbugURLbuildBuiltinMapbuildInterpreterbuildObjectbuildStdObjectbuiltinAcosbuiltinAsinbuiltinAtanbuiltinBitNegbuiltinBitwiseAndbuiltinBitwiseOrbuiltinBitwiseXorbuiltinCeilbuiltinCharbuiltinCodepointbuiltinCosbuiltinDecodeUTF8builtinDivbuiltinEncodeUTF8builtinEqualsbuiltinExpbuiltinExponentbuiltinExtVarbuiltinFilterbuiltinFlatMapbuiltinFloorbuiltinGreaterbuiltinGreaterEqbuiltinIDbuiltinIdentitybuiltinJoinbuiltinLengthbuiltinLessbuiltinLessEqbuiltinLogbuiltinMakeArraybuiltinMantissabuiltinMd5builtinMinusbuiltinModulobuiltinMultbuiltinNativebuiltinNegationbuiltinNotEqualsbuiltinObjectFieldsExbuiltinObjectHasExbuiltinParseJSONbuiltinPlusbuiltinPowbuiltinRangebuiltinShiftLbuiltinShiftRbuiltinSinbuiltinSortbuiltinSplitLimitbuiltinSqrtbuiltinStrReplacebuiltinTanbuiltinToStringbuiltinTracebuiltinTypebuiltinUglyObjectFlatMergebuiltinUnaryMinuscheckArgumentscheckAssertionscheckAssertionsHelperclosurecodeToPVcodeUnboundFieldcodepointMaxconcatArraysconcatStringsdefaultArgumentdumpCallFramedumpCallStackduplicateFieldNameErrMsgerrNoErrorInObjectInvariantsevalKindMultievalKindRegularevalKindStreamevaluableevaluateAuxevaluateMultievaluateStdevaluateStreamextendedObjectfieldHideMapfindFieldflattenArgsforceThunksfsCacheEntryfuncBuiltinsfunctionIDgeneralBuiltingeneralBuiltinFuncgetBuiltinTraceint64ToValueintToValuejoinArraysjoinStringsjsonToValueliftBitwiseliftNumericmakeCallStackmakeClosuremakeDoubleCheckmakeEnvironmentmakeImportCachemakeInitialEnvmakeRuntimeErrormakeUnboundSelfBindingmakeValueArraymakeValueBooleanmakeValueExtendedObjectmakeValueNullmakeValueNumbermakeValueSimpleObjectmakeValueStringnonTailCallnullTypenullValueobjectBindingobjectFieldsobjectFieldsVisibilityobjectHasFieldobjectIndexobjectLocalobjectTypeplusSuperUnboundFieldpotentialValueInEnvprepareClosureParametersprepareExtVarsprepareFieldUpvaluesprimitiveEqualsrawEqualsreadyThunkreadyValueserializeJSONsimpleObjectsimpleObjectFieldsimpleObjectFieldMapsortDatastringEqualstringLessThantailCalltermErrorFormatterternaryBuiltinternaryBuiltinFunctraceElementToTraceFrameunaryBuiltinunaryBuiltinFuncunboundFielduncachedObjectFieldsVisibilityunparseNumberunparseStringuopBuiltinsvalueLessvalueNullwithHiddenwithHiddenFromBoolwithoutHiddencontentsJPathsfsCachetryPathhidethunksmaxStackTraceSizeprettyefformatRuntimeformatStaticformatInternalshowCodebuildStackTraceassertslocalsStackTracetotalInheritanceSizervbindingsinEnvdefaultValuesdaencoderTypeEncodeValuesgithub.com/google/go-querystring/queryClockSequenceMicrosoftMustParseNameSpaceDNSNameSpaceOIDNameSpaceURLNameSpaceX500NewDCEGroupNewDCEPersonNewDCESecurityNewHashNewMD5NewRandomNewUUIDNodeInterfacePersonRFC4122SetClockSequenceSetNodeIDSetNodeInterfaceSetRandg1582g1582ns100getHardwareInterfacegetTimeifnameinterfaceslasttimelilliannodeMuranderrandomBitssetClockSequencesetNodeInterfacetimeMuxtobxvalueszeroIDURNgithub.com/google/uuidAPICallOnCodesWithGRPCOptionsWithRetryXGoogHeaderboRetryergrpcOptinvokeretryerOptiongaxgithub.com/googleapis/gax-go/v2ContainsTypeGetAllTypeWrapperwrappedErrorWrappedErrorserrwrapgithub.com/hashicorp/errwrapDefaultPooledClientDefaultPooledTransportHandlerInputPrintablePathCheckHandlerErrStatuscleanhttpgithub.com/hashicorp/go-cleanhttpconcatdefaultModifiedCacheiradixgithub.com/hashicorp/go-immutable-radixAsSymbolAllAsSymbolDefaultAsSymbolMapStringKeysFlagAsSymbolNoneAsSymbolStructFieldNameFlagBincHandleGoRpcMapBySliceMsgpackSpecRpcMsgpackSpecRpcMultiArgsNewDecoderBytesNewEncoderBytesRpcRpcCodecBufferedSimpleHandlebigenbinaryMarshalerbinaryMarshalerTypbinaryMarshalerTypIdbinaryUnmarshalerbinaryUnmarshalerTypbinaryUnmarshalerTypIdbincDecDriverbincDoPrunebincEncDriverbincFlBin16bincFlBin32bincFlBin64bincSpFalsebincSpNanbincSpNegInfbincSpNegOnebincSpNilbincSpPosInfbincSpTruebincSpZerobincSpZeroFloatbincVdArraybincVdByteArraybincVdCustomExtbincVdDecimalbincVdFloatbincVdMapbincVdNegIntbincVdPosIntbincVdSmallIntbincVdSpecialbincVdStringbincVdSymbolbincVdTimestampbincVdUnicodeOtherboolSliceTypboolSliceTypIdbsAll0x00bsAll0xffbytesDecReaderbytesEncWriterc_RAWc_UTF16BEc_UTF16LEc_UTF32BEc_UTF32LEc_UTF8cachedTypeInfocachedTypeInfoMutexcheckOverflowcheckOverflowFloat32debugfdebuggingdecContLensdefEncByteBufSizefloat32SliceTypfloat32SliceTypIdfloat64SliceTypfloat64SliceTypIdgoRpcgoRpcCodecimplementsIntfint16SliceTypint16SliceTypIdint32SliceTypint32SliceTypIdint64SliceTypint64SliceTypIdint8SliceTypint8SliceTypIdintBitsizeintSliceTypintSliceTypIdintfSliceTypintfSliceTypIdintfTypintfTypIdioDecReaderioEncStringWriterioEncWriterioEncWriterWriterisEmptyValueDerefmapBySliceTypmapInt64IntfTypmapInt64IntfTypIdmapIntIntfTypmapIntIntfTypIdmapIntfIntfTypmapIntfIntfTypIdmapStrIntfTypmapStrIntfTypIdmapStrStrTypmapStrStrTypIdmapUint64IntfTypmapUint64IntfTypIdmapUintIntfTypmapUintIntfTypIdmpArray16mpArray32mpBin16mpBin32mpBin8mpDoublempExt16mpExt32mpExt8mpFalsempFixArrayMaxmpFixArrayMinmpFixExt1mpFixExt16mpFixExt2mpFixExt4mpFixExt8mpFixMapMaxmpFixMapMinmpFixStrMaxmpFixStrMinmpFloatmpInt16mpInt32mpInt64mpInt8mpMap16mpMap32mpNegFixNumMaxmpNegFixNumMinmpNilmpPosFixNumMaxmpPosFixNumMinmpStr16mpStr32mpStr8mpTruempUint16mpUint32mpUint64mpUint8msgBadDescmsgDecCannotExpandArrmsgTagDecmsgTagEncmsgpackContainerBinmsgpackContainerListmsgpackContainerMapmsgpackContainerStrmsgpackContainerTypemsgpackDecDrivermsgpackEncDrivermsgpackSpecRpcmsgpackSpecRpcCodecnewRPCCodecpanicToErrpanicValToErrparseStructFieldInfopruneSignExtraisePanicAfterRecoverrawExtTyprawExtTypIdrecoverPanicToErrrgetTypeInforpcCodecsfiSortedByEncNameshortCircuitReflectToFastPathsimpleDecDriversimpleEncDriversimpleIoEncWriterWritersimpleVdArraysimpleVdByteArraysimpleVdExtsimpleVdFalsesimpleVdFloat32simpleVdFloat64simpleVdMapsimpleVdNegIntsimpleVdNilsimpleVdPosIntsimpleVdStringsimpleVdTruestrSliceTypstrSliceTypIdstringTypstructInfoFieldNamestructTagNamesupportBinaryMarshaltimeDigitstimeLocUTCNametimeTyptimeTypIduint16SliceTypuint16SliceTypIduint32SliceTypuint32SliceTypIduint64SliceTypuint64SliceTypIduint8SliceTypuint8SliceTypIduintBitsizeuintSliceTypuintSliceTypIduseMapForCodecCachevalueTypeArrayvalueTypeBoolvalueTypeBytesvalueTypeExtvalueTypeFloatvalueTypeIntvalueTypeInvalidvalueTypeMapvalueTypeNilvalueTypeSymbolvalueTypeTimestampvalueTypeUintvalueTypeUnsetwriteContainerLenclsBufferedReaderBufferedWriterReadResponseBodyServerCodecClientCodecfixCutoffbFixMinb8b16b32hasFixMinhas8has8AlwaysReadResponseHeaderReadRequestHeaderReadRequestBodybdReadbdTypevddecFloatPredecFloatdecUintdecIntAnyreadContainerLenreadExtLenServiceMethodSeqparseCustomHeaderencIntegerPruneencBytesLenencLenNumbergithub.com/hashicorp/go-msgpack/codecErrorFormatFuncFlattenListFormatFuncflattenErrorFormatErrorOrNilmultierrorgithub.com/hashicorp/go-multierrorCheckRetryDefaultBackoffDefaultRetryPolicyFromRequestLenReaderLeveledLoggerLinearJitterBackoffPassthroughErrorHandlerRequestLogHookResponseLogHookdefaultRetryMaxdefaultRetryWaitMaxdefaultRetryWaitMingetBodyReaderAndContentLengthhookLoggerredirectsErrorRerespReadLimitschemeErrorReRetryWaitMinRetryWaitMaxRetryMaxloggerInitdrainBodyretryablehttpLoadCACertsLoadCAFileLoadCAPathLoadSystemCAsaddCertsFromKeychaincertKeychainsCAFilerootcertsgithub.com/hashicorp/go-rootcertsAscAddressAscIfAddressAscIfDefaultAscIfNameAscIfNetworkSizeAscIfPortAscIfPrivateAscIfTypeAscNetworkSizeAscPortAscPrivateAscTypeAttrNameCmpAddrFuncCmpIfAddrFuncDescIfAddressDescIfDefaultDescIfNameDescIfNetworkSizeDescIfPortDescIfPrivateDescIfTypeExcludeIfsFilterIfByTypeForwardingBlacklistForwardingBlacklistRFCGetAllInterfacesGetDefaultInterfacesGetInterfaceIPGetInterfaceIPsGetPrivateIPGetPrivateIPsGetPrivateInterfacesGetPublicIPGetPublicIPsGetPublicInterfacesIPAddrAttrIPAddrsIPAttrsIPPortIPPrefixLenIPv3lenIPv4AddrIPv4AddrAttrIPv4AddressIPv4AttrsIPv4HostMaskIPv4MaskIPv4NetworkIPv4lenIPv6AddrIPv6AddrAttrIPv6AddressIPv6AttrsIPv6HostPrefixIPv6MaskIPv6NetworkIPv6lenIfAddrIfAddrAttrIfAddrAttrsIfAddrMathIfAddrsIfAddrsMathIfAttrIfAttrsIfByAddressIfByFlagIfByMaskSizeIfByNameIfByNetworkIfByPortIfByRFCIfByRFCsIfByTypeIncludeIfsIsRFCJoinIfAddrsKnownRFCsLimitIfAddrsMustIPAddrMustIPv4AddrMustIPv6AddrMustUnixSockNewIPAddrNewIPv4AddrNewIPv6AddrNewRouteInfoNewSockAddrNewUnixSockOffsetIfAddrsOrderedAddrByOrderedIfAddrByRouteInterfaceSockAddrSockAddrAttrSockAddrAttrsSockAddrMarshalerSockAddrTypeSockAddrsSortIPAddrsByBroadMaskLenSortIPAddrsByNetworkSizeSortIPAddrsBySpecificMaskLenSortIfByToIPAddrToIPv4AddrToIPv6AddrToUnixSockTypeIPTypeIPv4TypeIPv6TypeUnixTypeUnknownUniqueIfAddrsByUnixSockUnixSockAttrUnixSockAttrsUnixSocksVisitAllRFCsbigIntToNetIPv6cmdsifAddrAttrInitifAddrAttrMapifAddrAttrsifNameREipAddrAttrMapipAddrAttrsipAddrInitipAddrREipv4AddrAttrMapipv4AddrAttrsipv4AddrInitipv6AddrAttrMapipv6AddrAttrsipv6AddrInitipv6HostMaskmultiAddrSortermultiIfAddrSorterparseDefaultIPAddrWindowsRouteparseDefaultIfNameFromIPCmdparseDefaultIfNameFromIPCmdAndroidparseDefaultIfNameFromRouteparseDefaultIfNameWindowsparseDefaultIfNameWindowsIPConfigparseIfNameFromIPCmdsignREsockAddrAttrMapsockAddrAttrssockAddrInitsortArgBeforeReceiversortDeferDecisionsortReceiverBeforeArgtrailingHexNetmaskREunixAttrInitunixAttrMapunixAttrswhitespaceREsatCmpRFCDialPacketArgsDialStreamArgsListenPacketArgsListenStreamArgsifAddrifsAddressBinStringAddressHexStringCmpAddressCmpPortFirstUsableLastUsableMaskbitsNetIPNetIPMaskNetIPNetOctetsFilterByTypeipv4BroadcastAddressContainsAddressContainsNetworkNetworkAddressipv6ipv6aGetDefaultInterfaceNameVisitCommandsifAddrsgithub.com/hashicorp/go-sockaddrLRUCacheNewLRUsimplelrugithub.com/hashicorp/golang-lru/simplelruCommentGroupLiteralTypeObjectItemObjectKeyObjectListLbrackRbrackIsIdentifierIsLiteralIsOperatorLeadCommentAssignLbraceRbracegithub.com/hashicorp/hcl/hcl/astPosErrorerrEofTokensrcPosprevPoslastCharLenlastLineLentokStarttokEndErrorCounttokPosscanCommentscanMantissascanFractionscanExponentscanHeredocscanEscapescanIdentifierrecentPositioncommaPrevleadCommentlineCommentenableTraceobjectListconsumeCommentconsumeCommentGroupobjectItemobjectKeylistTypeprintTracegithub.com/hashicorp/hcl/hcl/parserByPositionblanktabSpacesWidthstandaloneCommentsindentTracecollectCommentsalignedItemsisSingleLineListsingleLineListheredocIndentisSingleLineObjectgithub.com/hashicorp/hcl/hcl/printerdigitValisDecimalisLettergithub.com/hashicorp/hcl/hcl/scannerErrSyntaxunhexunquoteChargithub.com/hashicorp/hcl/hcl/strconvASSIGNCOMMACOMMENTHEREDOCILLEGALLBRACELBRACKPERIODRBRACERBRACKidentifier_begidentifier_endliteral_begliteral_endoperator_begoperator_endunindentHeredocgithub.com/hashicorp/hcl/hcl/tokenflattenListTypeflattenObjectTypeflattenObjectsHCLTokenobjectValuegithub.com/hashicorp/hcl/json/parsergithub.com/hashicorp/hcl/json/scannerCOLONgithub.com/hashicorp/hcl/json/tokenDecodeObjectexpandObjectfindNodeTypelexModelexModeHcllexModeJsonlexModeUnknownlexModeValuetagNamedecodePtrhclgithub.com/hashicorp/hclAddStagingDefaultTimeoutScaleDiscardSnapshotSinkDiscardSnapshotStoreErrAbortedByRestoreErrCantBootstrapErrEnqueueTimeoutErrLeaderErrLeadershipLostErrLogNotFoundErrNotLeaderErrNothingNewToSnapshotErrPipelineReplicationNotSupportedErrPipelineShutdownErrRaftShutdownErrTransportShutdownErrUnsupportedProtocolFileSnapshotSinkFileSnapshotStoreFollowerHasExistingStateInmemSnapshotSinkInmemSnapshotStoreInmemStoreInmemTransportLogAddPeerDeprecatedLogBarrierLogCacheLogCommandLogConfigurationLogNoopLogRemovePeerDeprecatedLoopbackTransportNetworkTransportConfigNewDiscardSnapshotStoreNewFileSnapshotStoreNewFileSnapshotStoreWithLoggerNewInmemAddrNewInmemSnapshotStoreNewInmemStoreNewInmemTransportNewLogCacheNewNetworkTransportNewNetworkTransportWithConfigNewNetworkTransportWithLoggerNewObserverNewRaftNewTCPTransportNewTCPTransportWithConfigNewTCPTransportWithLoggerNonvoterPromoteProtocolVersionMaxProtocolVersionMinReadConfigJSONReadPeersJSONRecoverClusterSnapshotVersionMaxSnapshotVersionMinStagingTCPStreamLayerValidateConfigVoterWithCloseWithPeersWithRPCHeaderappendFutureappendStatsasyncNotifyBoolasyncNotifyChbufferedFilecheckConfigurationcommitTupleconfigEntrydecodeConfigurationdecodeMsgPackdecodePeersdecodeResponsedrainNotifyChencodeConfigurationencodeMsgPackencodePeerserrNotAdvertisableerrNotTCPerrorFuturefailureWaitfileSnapshotMetagenerateUUIDgetSnapshotVersionhasVoteinmemPipelineinmemPipelineInflightkeyCurrentTermkeyLastVoteCandkeyLastVoteTermmaxFailureScalemetaFilePathminCheckIntervalnetPipelinenewCommitmentnewInmemPipelinenewNetPipelinenewSeednewTCPTransportnextConfigurationnextObserverIDrandomTimeoutrestoreFuturerpcAppendEntriesrpcInstallSnapshotrpcMaxPipelinerpcRequestVotesendRPCshutdownFuturesnapMetaSlicesnapPathsnapshotNamestateFilePathtestPathtmpSuffixupdateLastAppendedfuturerespChpeerAddrinprogressChdecodeResponsesconsumerChpipelinesmakeRPCDisconnectAlltestPermissionsgetSnapshotsreadMetaReapSnapshotsparentDirstateFilestateHashfinalizelowIndexhighIndexkvInthasSnapshotTCPListenernetFDsotypeladdrraddrsetAddrreadFromInet4readFromInet6readMsgreadMsgInet4readMsgInet6writeToInet4writeToInet6writeMsgwriteMsgInet4writeMsgInet6dupctrlNetworkaddrFunclistenStreamlistenDatagramwriteBuffersListenConfiglcListenPacketAcceptTCPadvertiseNonVoterMaxPoolbhTCPConnSetReadBufferSetWriteBufferCloseReadSetLingerSetKeepAlivePeriodSetNoDelayPacketConnAuthConfigInputAuthConfigOutputAuthMountDefaultRenewerRenewBufferDefaultWrappingLookupFuncDefaultWrappingTTLEnableAuthOptionsEnvRateLimitEnvVaultAddressEnvVaultAgentAddrEnvVaultCACertEnvVaultCAPathEnvVaultClientCertEnvVaultClientKeyEnvVaultClientTimeoutEnvVaultMFAEnvVaultMaxRetriesEnvVaultNamespaceEnvVaultSkipVerifyEnvVaultTLSServerNameEnvVaultTokenEnvVaultWrapTTLErrOutputStringRequestErrRenewerMissingInputErrRenewerMissingSecretErrRenewerNoSecretDataErrRenewerNotRenewableInitStatusResponseLastOutputStringErrorLoadSSHHelperConfigOutputStringErrorParseSSHHelperConfigParseSecretPluginAPIClientMetaPluginMetadataModeEnvPluginUnwrapTokenEnvSSHHelperConfigSSHHelperDefaultMountPointVaultPluginTLSProviderVerifyEchoRequestVerifyEchoResponsecatalogPathByTypegetPoliciesResplistPoliciesRespparseRateLimitsealStatusRequestwrappedResponseLocationparsingErrorparsedCurlStringparseRequestCurlStringVaultAddrSSHMountPointAllowedCidrListAllowedRolesTLSSkipVerifySetTLSParametersshouldSetTLSParametersflagCACertflagCAPathflagClientCertflagClientKeyflagInsecureGetTLSConfigCompressUtilReadCloserCompressionCanaryGzipCompressionCanaryLZ4CompressionCanaryLZWCompressionCanarySnappyCompressionConfigCompressionTypeGzipCompressionTypeLZ4CompressionTypeLZWCompressionTypeSnappyDecompressGzipCompressionLevelcompressutilgithub.com/hashicorp/vault/sdk/helper/compressutilAgentPathCacheClearAuthHeaderNameCoreReplicatedClusterInfoPathCoreReplicatedClusterInfoPathDRCoreReplicatedClusterPrefixCoreReplicatedClusterPrefixDRCoreReplicatedClusterSecondariesPrefixCoreReplicatedClusterSecondariesPrefixDRCurrentReplicatedSecondaryIdentifierErrPathContainsParentReferencesErrSealedErrStandbyExpirationRestoreWorkerCountNamespaceHeaderNameOldReplicationBootstrappingOldReplicationPrimaryOldReplicationSecondaryOldSplitReplicationBootstrappingParsePluginTypePluginTypeCredentialPluginTypeDatabasePluginTypeSecretsPluginTypeUnknownPluginTypesReplicationDRBootstrappingReplicationDRDisabledReplicationDRPrimaryReplicationDRSecondaryReplicationPerformanceBootstrappingReplicationPerformanceDisabledReplicationPerformancePrimaryReplicationPerformanceSecondaryReplicationPerformanceStandbyReplicationStaleReadTimeoutReplicationUnknownStateStringsGetDRStringGetPerformanceStringHasStateAddStateClearStateToggleStateconstsgithub.com/hashicorp/vault/sdk/helper/constsCheckHCLKeyshclutilgithub.com/hashicorp/vault/sdk/helper/hclutilDecodeJSONFromReaderEncodeJSONAndCompressgithub.com/hashicorp/vault/sdk/helper/jsonutilParseAddrsParseCommaStringSliceParseDurationSecondparseutilgithub.com/hashicorp/vault/sdk/helper/parseutilAppendIfMissingEqualStringMapsEquivalentSlicesGlobbedStringsMatchMergeSlicesParseArbitraryKeyValuesParseArbitraryStringSliceParseDedupAndSortStringsParseDedupLowercaseAndSortStringsParseKeyValuesParseStringSliceRemoveDuplicatesRemoveEmptyStrListContainsStrListContainsGlobStrListDeleteStrListSubsetTrimStringsstrutilgithub.com/hashicorp/vault/sdk/helper/strutilisLeapmagicDOW2DOMmask12mask24mask31mask60mask7maxMonthLengthsparse_en_atMacroparse_en_durationMacroparse_en_fivePosparse_en_mainparse_en_sevenPosparse_en_sixPosparse_errorparse_first_finalparse_startsixyPositionstwentyFourPositionsFloatSliceNewAllocatorDeleteOptionHasDuplicateOptionsOptionObjectFnOptionValueFnexactMatchingStrategyfuzzyMatchingStrategymatchArrayExpressionmatchBinaryExpressionmatchBlockmatchBooleanLiteralmatchCallExpressionmatchConditionalExpressionmatchDateTimeLiteralmatchDurationLiteralmatchExpressionStatementmatchFilematchFloatLiteralmatchFunctionExpressionmatchIdentifiermatchImportDeclarationmatchIndexExpressionmatchIntegerLiteralmatchLogicalExpressionmatchLogicalOperatormatchMemberAssignmentmatchMemberExpressionmatchObjectExpressionmatchOperatormatchOptionStatementmatchPackagematchPackageClausematchPipeExpressionmatchPipeLiteralmatchPropertymatchRegexpLiteralmatchReturnStatementmatchStringmatchStringLiteralmatchTestStatementmatchUnaryExpressionmatchUnsignedIntegerLiteralmatchVariableAssignmentmatchVisitoroptionEditorsliceMatchingStrategymatchExpressionsmatchFilesmatchImportDeclarationsmatchPropertiesmatchStatementsUnsignedIntegerLiteralfmssmsMemberAssignmentidentifieroptionFnTestStatementIndexExpressionPipeLiteralemsMutateArrayTypeMutateIndexTypeArrayExpressionAddBaseNodeArrayExpressionAddElementsArrayExpressionEndArrayExpressionStartArrayExpressionStartElementsVectorArrayTypeAddBaseNodeArrayTypeAddElementArrayTypeAddElementTypeArrayTypeEndArrayTypeStartAssignmentMemberAssignmentAssignmentNONEAssignmentVariableAssignmentBadExpressionBadExpressionAddBaseNodeBadExpressionAddExpressionBadExpressionAddExpressionTypeBadExpressionAddTextBadExpressionEndBadExpressionStartBadStatementBadStatementAddBaseNodeBadStatementAddTextBadStatementEndBadStatementStartBaseNodeAddErrorsBaseNodeAddLocBaseNodeEndBaseNodeStartBaseNodeStartErrorsVectorBinaryExpressionAddBaseNodeBinaryExpressionAddLeftBinaryExpressionAddLeftTypeBinaryExpressionAddOperatorBinaryExpressionAddRightBinaryExpressionAddRightTypeBinaryExpressionEndBinaryExpressionStartBlockAddBaseNodeBlockAddBodyBlockEndBlockStartBlockStartBodyVectorBooleanLiteralAddBaseNodeBooleanLiteralAddValueBooleanLiteralEndBooleanLiteralStartBuiltinStatementBuiltinStatementAddBaseNodeBuiltinStatementAddIdBuiltinStatementEndBuiltinStatementStartCallExpressionAddArgumentsCallExpressionAddBaseNodeCallExpressionAddCalleeCallExpressionAddCalleeTypeCallExpressionEndCallExpressionStartConditionalExpressionAddAlternateConditionalExpressionAddAlternateTypeConditionalExpressionAddBaseNodeConditionalExpressionAddConsequentConditionalExpressionAddConsequentTypeConditionalExpressionAddTestConditionalExpressionAddTestTypeConditionalExpressionEndConditionalExpressionStartCreatePositionDateTimeLiteralAddBaseNodeDateTimeLiteralAddNsecsDateTimeLiteralAddOffsetDateTimeLiteralAddSecsDateTimeLiteralEndDateTimeLiteralStartDurationAddMagnitudeDurationAddUnitDurationEndDurationLiteralAddBaseNodeDurationLiteralAddValuesDurationLiteralEndDurationLiteralStartDurationLiteralStartValuesVectorDurationStartEnumNamesAssignmentEnumNamesExpressionEnumNamesExpressionOrBlockEnumNamesLogicalOperatorEnumNamesMonoTypeEnumNamesOperatorEnumNamesParameterKindEnumNamesPropertyKeyEnumNamesStatementEnumNamesTimeUnitExpressionArrayExpressionExpressionBadExpressionExpressionBinaryExpressionExpressionBooleanLiteralExpressionCallExpressionExpressionConditionalExpressionExpressionDateTimeLiteralExpressionDurationLiteralExpressionFloatLiteralExpressionFunctionExpressionExpressionIdentifierExpressionIndexExpressionExpressionIntegerLiteralExpressionLogicalExpressionExpressionMemberExpressionExpressionNONEExpressionObjectExpressionExpressionOrBlockExpressionOrBlockBlockExpressionOrBlockNONEExpressionOrBlockWrappedExpressionExpressionParenExpressionExpressionPipeExpressionExpressionPipeLiteralExpressionRegexpLiteralExpressionStatementAddBaseNodeExpressionStatementAddExpressionExpressionStatementAddExpressionTypeExpressionStatementEndExpressionStatementStartExpressionStringExpressionExpressionStringLiteralExpressionUnaryExpressionExpressionUnsignedIntegerLiteralFileAddBaseNodeFileAddBodyFileAddImportsFileAddMetadataFileAddNameFileAddPackageFileEndFileStartFileStartBodyVectorFileStartImportsVectorFloatLiteralAddBaseNodeFloatLiteralAddValueFloatLiteralEndFloatLiteralStartFunctionExpressionAddBaseNodeFunctionExpressionAddBodyFunctionExpressionAddBodyTypeFunctionExpressionAddParamsFunctionExpressionEndFunctionExpressionStartFunctionExpressionStartParamsVectorFunctionTypeFunctionTypeAddBaseNodeFunctionTypeAddMonotypeFunctionTypeAddMonotypeTypeFunctionTypeAddParametersFunctionTypeEndFunctionTypeStartFunctionTypeStartParametersVectorGetRootAsArrayExpressionGetRootAsArrayTypeGetRootAsBadExpressionGetRootAsBadStatementGetRootAsBaseNodeGetRootAsBinaryExpressionGetRootAsBlockGetRootAsBooleanLiteralGetRootAsBuiltinStatementGetRootAsCallExpressionGetRootAsConditionalExpressionGetRootAsDateTimeLiteralGetRootAsDurationGetRootAsDurationLiteralGetRootAsExpressionStatementGetRootAsFileGetRootAsFloatLiteralGetRootAsFunctionExpressionGetRootAsFunctionTypeGetRootAsIdentifierGetRootAsImportDeclarationGetRootAsIndexExpressionGetRootAsIntegerLiteralGetRootAsLogicalExpressionGetRootAsMemberAssignmentGetRootAsMemberExpressionGetRootAsNamedTypeGetRootAsObjectExpressionGetRootAsOptionStatementGetRootAsPackageGetRootAsPackageClauseGetRootAsParameterTypeGetRootAsParenExpressionGetRootAsPipeExpressionGetRootAsPipeLiteralGetRootAsPropertyGetRootAsPropertyTypeGetRootAsRecordTypeGetRootAsRegexpLiteralGetRootAsReturnStatementGetRootAsSourceLocationGetRootAsStringExpressionGetRootAsStringExpressionPartGetRootAsStringLiteralGetRootAsTestStatementGetRootAsTvarTypeGetRootAsTypeConstraintGetRootAsTypeExpressionGetRootAsUnaryExpressionGetRootAsUnsignedIntegerLiteralGetRootAsVariableAssignmentGetRootAsWrappedExpressionGetRootAsWrappedStatementIdentifierAddBaseNodeIdentifierAddNameIdentifierEndImportDeclarationAddAsImportDeclarationAddBaseNodeImportDeclarationAddPathImportDeclarationEndImportDeclarationStartIndexExpressionAddArrayIndexExpressionAddArrayTypeIndexExpressionAddBaseNodeIndexExpressionAddIndexIndexExpressionAddIndexTypeIndexExpressionEndIndexExpressionStartIntegerLiteralAddBaseNodeIntegerLiteralAddValueIntegerLiteralEndIntegerLiteralStartLogicalExpressionAddBaseNodeLogicalExpressionAddLeftLogicalExpressionAddLeftTypeLogicalExpressionAddOperatorLogicalExpressionAddRightLogicalExpressionAddRightTypeLogicalExpressionEndLogicalExpressionStartLogicalOperatorAndOperatorLogicalOperatorOrOperatorMemberAssignmentAddBaseNodeMemberAssignmentAddInit_MemberAssignmentAddInit_typeMemberAssignmentAddMemberMemberAssignmentEndMemberAssignmentStartMemberExpressionAddBaseNodeMemberExpressionAddObjectMemberExpressionAddObjectTypeMemberExpressionAddPropertyMemberExpressionAddPropertyTypeMemberExpressionEndMemberExpressionStartMonoTypeArrayTypeMonoTypeFunctionTypeMonoTypeNONEMonoTypeNamedTypeMonoTypeRecordTypeMonoTypeTvarTypeNamedTypeNamedTypeAddBaseNodeNamedTypeAddIdNamedTypeEndNamedTypeStartObjectExpressionAddBaseNodeObjectExpressionAddPropertiesObjectExpressionAddWithObjectExpressionEndObjectExpressionStartObjectExpressionStartPropertiesVectorOperatorAdditionOperatorOperatorDivisionOperatorOperatorEmptyOperatorOperatorEqualOperatorOperatorExistsOperatorOperatorGreaterThanEqualOperatorOperatorGreaterThanOperatorOperatorInOperatorOperatorInvalidOperatorOperatorLessThanEqualOperatorOperatorLessThanOperatorOperatorModuloOperatorOperatorMultiplicationOperatorOperatorNotEmptyOperatorOperatorNotEqualOperatorOperatorNotOperatorOperatorNotRegexpMatchOperatorOperatorPowerOperatorOperatorRegexpMatchOperatorOperatorStartsWithOperatorOperatorSubtractionOperatorOptionStatementAddAssignmentOptionStatementAddAssignmentTypeOptionStatementAddBaseNodeOptionStatementEndOptionStatementStartPackageAddBaseNodePackageAddFilesPackageAddPackagePackageAddPathPackageClauseAddBaseNodePackageClauseAddNamePackageClauseEndPackageClauseStartPackageEndPackageStartPackageStartFilesVectorParameterKindParameterKindOptionalParameterKindPipeParameterKindRequiredParameterTypeAddBaseNodeParameterTypeAddIdParameterTypeAddKindParameterTypeAddMonotypeParameterTypeAddMonotypeTypeParameterTypeEndParameterTypeStartParenExpressionParenExpressionAddBaseNodeParenExpressionAddExpressionParenExpressionAddExpressionTypeParenExpressionEndParenExpressionStartPipeExpressionAddArgumentPipeExpressionAddArgumentTypePipeExpressionAddBaseNodePipeExpressionAddCallPipeExpressionEndPipeExpressionStartPipeLiteralAddBaseNodePipeLiteralEndPipeLiteralStartPropertyAddBaseNodePropertyAddKeyPropertyAddKeyTypePropertyAddValuePropertyAddValueTypePropertyEndPropertyKeyIdentifierPropertyKeyNONEPropertyKeyStringLiteralPropertyStartPropertyTypeAddBaseNodePropertyTypeAddIdPropertyTypeAddMonotypePropertyTypeAddMonotypeTypePropertyTypeEndPropertyTypeStartRecordTypeRecordTypeAddBaseNodeRecordTypeAddPropertiesRecordTypeAddTvarRecordTypeEndRecordTypeStartRecordTypeStartPropertiesVectorRegexpLiteralAddBaseNodeRegexpLiteralAddValueRegexpLiteralEndRegexpLiteralStartReturnStatementAddArgumentReturnStatementAddArgumentTypeReturnStatementAddBaseNodeReturnStatementEndReturnStatementStartSourceLocationAddEndSourceLocationAddFileSourceLocationAddSourceSourceLocationAddStartSourceLocationEndSourceLocationStartStatementBadStatementStatementBuiltinStatementStatementExpressionStatementStatementMemberAssignmentStatementNONEStatementOptionStatementStatementReturnStatementStatementTestStatementStatementVariableAssignmentStringExpressionStringExpressionAddBaseNodeStringExpressionAddPartsStringExpressionEndStringExpressionPartStringExpressionPartAddBaseNodeStringExpressionPartAddInterpolatedExpressionStringExpressionPartAddInterpolatedExpressionTypeStringExpressionPartAddTextValueStringExpressionPartEndStringExpressionPartStartStringExpressionStartStringExpressionStartPartsVectorStringLiteralAddBaseNodeStringLiteralAddValueStringLiteralEndStringLiteralStartTestStatementAddAssignmentTestStatementAddAssignmentTypeTestStatementAddBaseNodeTestStatementEndTestStatementStartTimeUnitdTimeUnithTimeUnitmTimeUnitmoTimeUnitmsTimeUnitnsTimeUnitsTimeUnitusTimeUnitwTimeUnityTvarTypeTvarTypeAddBaseNodeTvarTypeAddIdTvarTypeEndTvarTypeStartTypeConstraintTypeConstraintAddBaseNodeTypeConstraintAddKindsTypeConstraintAddTvarTypeConstraintEndTypeConstraintStartTypeConstraintStartKindsVectorTypeExpressionTypeExpressionAddBaseNodeTypeExpressionAddConstraintsTypeExpressionAddMonotypeTypeExpressionAddMonotypeTypeTypeExpressionEndTypeExpressionStartTypeExpressionStartConstraintsVectorUnaryExpressionAddArgumentUnaryExpressionAddArgumentTypeUnaryExpressionAddBaseNodeUnaryExpressionAddOperatorUnaryExpressionEndUnaryExpressionStartUnsignedIntegerLiteralAddBaseNodeUnsignedIntegerLiteralAddValueUnsignedIntegerLiteralEndUnsignedIntegerLiteralStartVariableAssignmentAddBaseNodeVariableAssignmentAddIdVariableAssignmentAddInit_VariableAssignmentAddInit_typeVariableAssignmentEndVariableAssignmentStartWrappedExpressionAddExprWrappedExpressionAddExprTypeWrappedExpressionEndWrappedExpressionStartWrappedStatementAddStatementWrappedStatementAddStatementTypeWrappedStatementEndWrappedStatementStartMonotypeTypeMutateMonotypeTypeMonotypeMutateKindTvarElementTypeMutateElementTypeParametersLengthPartsPartsLengthTextValueInterpolatedExpressionTypeMutateInterpolatedExpressionTypeInterpolatedExpressionConstraintsConstraintsLengthKindsLengthfbastgithub.com/influxdata/flux/ast/internal/fbastBooleanFromLiteralDayUnitDecodeMonoTypeDeserializeFromFlatBufferDivisionOperatorEmptyOperatorHourUnitInOperatorInterpolatedPartLogicalOperatorLookupLogicalOperatorTokensMicrosecondUnitMillisecondUnitMinuteUnitModuloOperatorMonthUnitMultiplicationOperatorNanosecondUnitNotEmptyOperatorOperatorLookupOperatorTokensPowerOperatorPrintErrorsRegexpFromLiteralRegexpLiteralFromValueSecondUnitTextPartUnmarshalNodeUnsignedIntegerFromLiteralUnsignedIntegerLiteralFromValueWeekUnitYearUnitassignmentFromBufcheckNullMsgerrorVisitorescapeStrexprArrayFromBufexprFromBufexprFromBufTableexprGetterFnfunctionCallgetIntForLOpgetIntForOpgetPrecedencegetPrecedenceForLOpgetPrecedenceForOpgetPrecedenceslogOpBeginlogOpEndlogOpMaplogOperatorslopOffsetneedsParenthesisnewFBTableopBeginopEndopMapopOffsetopPrecedenceoperatorsparamKindMappropertyKeyFromBufstatementArrayFromBufstatementFromBufstmtGetterFntoDurationunionTableWriterFnunmarshalAssignmentunmarshalExpressionunmarshalMonotypeunmarshalNodeunmarshalPropertyKeyunmarshalStatementunmarshalStringPartunmarshalTypeExpressionmonotypeTytyp_contyp_exprindentationunIndentsetIndentwriteCommentformatChildWithParensformatLeftChildWithParensformatRightChildWithParensformatNodeWithParensformatPackageformatFileformatBlockformatPackageClauseformatImportDeclarationformatExpressionStatementformatReturnStatementformatOptionStatementformatTestStatementformatVariableAssignmentformatMemberAssignmentformatArrayExpressionformatFunctionExpressionformatUnaryExpressionformatBinaryExpressionformatLogicalExpressionformatBinaryformatCallExpressionformatPipeExpressionformatConditionalExpressionformatMemberExpressionformatIndexExpressionformatObjectExpressionformatObjectExpressionAsFunctionArgumentformatObjectExpressionBracesformatPropertyformatFunctionArgumentformatIdentifierformatStringExpressionformatStringExpressionPartformatTextPartformatInterpolatedPartformatParenExpressionformatStringLiteralformatBooleanLiteralformatDateTimeLiteralformatDurationLiteralformatFloatLiteralformatIntegerLiteralformatUnsignedIntegerLiteralformatPipeLiteralformatRegexpLiteralformatNodestringPartAlreadyExistsstrToCodeEvaluatorNewScopearrayEvaluatorarrayIndexEvaluatorbinaryEvaluatorblockEvaluatorbooleanEvaluatorcallEvaluatorcompiledFnconditionalEvaluatorcontainsStrdeclarationEvaluatordurationEvaluatorfindPropertyfloatEvaluatorfunctionEvaluatorfunctionParamfunctionValueidentifierEvaluatorintegerEvaluatorinterpolatedEvaluatorlogicalEvaluatormemberEvaluatornestScopeobjEvaluatorregexpEvaluatorreturnEvaluatorruntimeScopestringEvaluatorstringExpressionEvaluatorsubstituteTypestextEvaluatortimeEvaluatorunaryEvaluatorunsignedIntegerEvaluatorwithBinaryFunctioncalleeinputScopebuildScopepropertyisFunctionNewResultDecoderResultDecoderannotationIdxcommentPrefixcopyLinecsvEncoderErrordatatypeAnnotationdecodeTypedecodeValueIntodefaultAnnotationdefaultMaxBufferCountencodeValueFromequalColsfloatDatatypegroupAnnotationintDatatypenewCSVReadernewResultDecoderrecordStartIdxresultDecoderresultIteratorresultLabelserializedFluxErrortableDecodertableIdxtableLabeltableMetadatatimeDataTypeWithFmttimeDatatypeuintDatatypewrapEncodingErrorwriteAnnotationswriteDatatypeswriteDefaultswriteGroupswriteSchemaResultIDNumFieldsextraLineappendRecordextraMetaAbortIsEncoderErrorsfeSystemFSsystemFSLimitHTTPBodyNewDefaultClientNewLimitedDefaultClientlimitReadCloserlimitedReadClosermaxResponseBodyroundTripLimiterHttpProviderPredicateSetUnimplementedProviderfilteredHttpReaderhandleErrorhandleErrorCodereaderKeyseriesCardinalityHttpReaderappendFromArgsappendRangeArgsnewRequestBodyprocessResultfunctionToASTincludeImportKeepEmptyReaderForSeriesCardinalityReaderForclientForvalidateHostgithub.com/influxdata/flux/dependencies/influxdbEmptySecretServiceEnvironmentSecretServiceessPassValidatorPrivateIPValidatorisPrivateIPprivateIPBlocksDiffContextDiffOptiondiffDefaultOptionsdiffOptionFndiffergetSortedIndicesnewDifferstringifyKeystringifyRowsAccumulatingModeAddNewTableColsAddTableKeyColsAllProfilersAllTimeAppendColAppendColsAppendKeyValuesAppendKeyValuesNAppendMappedColsAppendMappedRecordExplicitAppendMappedRecordWithNullsAppendMappedTableAppendTableBoolValueFuncBuilderColsMatchReaderColListTableConvertFromKindConvertToKindCreateProfilerFuncCreateSourceFromIteratorDatasetIDFromNodeIDDefaultAggregateConfigDefaultExecutionDependenciesDefaultFormatOptionsDefaultSelectorConfigDiscardingModeDispatcherDoBoolAggDoBoolIndexSelectorDoBoolRowSelectorDoFloatAggDoFloatIndexSelectorDoFloatRowSelectorDoIntAggDoIntIndexSelectorDoIntRowSelectorDoStringAggDoStringIndexSelectorDoStringRowSelectorDoTimeIndexSelectorDoTimeRowSelectorDoUIntAggDoUIntIndexSelectorDoUIntRowSelectorExecutionDependenciesFinishMsgFinishTypeFloatValueFuncFormatOptionsFormatResultGetExecutionDependenciesGroupKeyForRowOnHaveExecutionDependenciesIndexSelectorIntValueFuncMessageQueueMetadataNodeNewAggregateTransformationNewAggregateTransformationAndDatasetNewExecutionDependenciesNewGroupLookupNewIndexSelectorTransformationNewIndexSelectorTransformationAndDatasetNewPassthroughDatasetNewRandomAccessGroupLookupNewRowPredicateFnNewRowReduceFnNewRowSelectorTransformationNewRowSelectorTransformationAndDatasetNewTablePredicateFnNewTriggerFromSpecNewWindowOperatorProfilerContextKeyOperatorProfilingSpanPanicUnknownTypePassthroughDatasetProcessMsgProcessTypeQueryProfilerRandomAccessGroupLookupRegisterProfilerFactoriesReplaceTransformationRetractTableMsgRetractTableTypeRowPredicateFnRowPredicatePreparedFnRowReaderRowReduceFnRowReducePreparedFnRowSelectorRowerScheduleFuncSourceIteratorStringValueFuncTablePredicateFnTablePredicatePreparedFnTablesEqualUIntValueFuncUpdateProcessingTimeMsgUpdateProcessingTimeTypeUpdateWatermarkMsgUpdateWatermarkTypeValueFuncZeroDatasetIDafterAtLeastCountafterProcessingTimeTriggerafterWatermarkTriggeraggregateTransformationboolColumnboolColumnBuilderboolSizecolListTableSortercolsMatchcolumnBuilderBaseconsecutiveTransportcreateExecutionNodeVisitorcreateOperatorProfilercreateQueryProfilerexecutionContextexecutionDependenciesKeyexecutionStatefinishMsgfloat64SizefloatColumnfloatColumnBuildergroupKeyEqualgroupKeyLessgroupLookupElementidleindexSelectorTransformationint64SizeintColumnintColumnBuildernarrowTransformationTriggernewConsecutiveTransportnewDynamicFnnewGroupKeynewMessageQueuenewPoolDispatchernewResultnewSelectorTransformationnonYieldPredecessorsoperatorProfilerLabelGroupoperatorProfilerTypeGrouporFinallypoolDispatcherprocedureToSourceprocedureToTransformationprocessMessagerepeatedlyForeverresolveTimeresultMessageretractTableMsgrowSelectorTransformationselectorTransformationskipYieldssourceDecodersourceIteratorsrcMessagestreamContextstringColumnstringColumnBuilderstringSizetableBuffertimeColumnBuildertimeSizetruncateByMonthstruncateByNsecsuintColumnuintColumnBuilderunboundedMessageQueueupdateProcessingTimeMsgupdateWatermarkMsgvalidatePlanallowedLatenessDoFloatSrcDatasetIDWatermarkTimemetaChtransportsdispatcherDoBoolDoIntDoStringDoUIntNewBoolAggNewFloatAggNewIntAggNewStringAggNewUIntAggRepeatHeaderCountNullRepresentationDoTimeGetNextRowSetColumnssetupBuildererrMuerrValueschedulerStatesourceInfopushMsgtryTransitiontransitionprocessMessagesNewBoolSelectorNewFloatSelectorNewIntSelectorNewStringSelectorNewTimeSelectorNewUIntSelectorappendSelectedInferredInputTypeEvalRowidForKeyatLeastnilsLessFuncfinallyRefCountProcessingTimeappendRowstriggerTimeSettriggerTimecreateExecutionStategithub.com/influxdata/flux/influxqlBooleanArrayValueFilterBooleansFilterFloat64sFilterInt64sFilterStringsFilterUint64sFloat64ArrayValueFloat64IteratorInt64ArrayValueInt64IteratorIterateBooleansIterateFloat64sIterateInt64sIterateStringsIterateUint64sNewArrayValueNewBooleanArrayValueNewFloat64ArrayValueNewInt64ArrayValueNewStringArrayValueNewUint64ArrayValueStringArrayValueUint64ArrayValueUint64Iteratorarrowutilgithub.com/influxdata/flux/internal/arrowutilNewfgithub.com/influxdata/flux/internal/errorsArrowBuilderBufferedBuilderBuilderCacheGetArrowBuilderGetBufferedBuilderKeyLookupNewArrowBuilderNewBufferedBuilderSendFuncStreamWithContextmaskTablemaskTableViewstreamBufferstreamTableAppendBufferappendBuffernormalizeTableSchemanewNullColumngetAllocatorIsDoneBuildersCheckColUnsafeWriteUnsafeWriteBuffergithub.com/influxdata/flux/internal/execute/tableArgumentAddNameArgumentAddOptionalArgumentAddPipeArgumentAddTArgumentAddTTypeArgumentEndArgumentStartArrArrAddTArrAddTTypeArrEndArrStartArrayExpressionAddLocArrayExpressionAddTypArrayExpressionAddTypTypeAssignmentNativeVariableAssignmentBasicAddTBasicEndBasicStartBinaryExpressionAddLocBinaryExpressionAddTypBinaryExpressionAddTypTypeBlockAddLocBooleanLiteralAddLocBuiltinStatementAddLocCallExpressionAddLocCallExpressionAddPipeCallExpressionAddPipeTypeCallExpressionAddTypCallExpressionAddTypTypeCallExpressionStartArgumentsVectorConditionalExpressionAddLocConstraintConstraintAddKindConstraintAddTvarConstraintEndConstraintStartDateTimeLiteralAddLocDateTimeLiteralAddValueDurationAddMonthsDurationAddNanosecondsDurationAddNegativeDurationLiteralAddLocDurationLiteralAddValueDurationLiteralStartValueVectorEnumNamesKindEnumNamesTypeExpressionIdentifierExpressionExpressionStatementAddLocFileAddLocFloatLiteralAddLocFresherFresherAddUFresherEndFresherStartFunAddArgsFunAddRetnFunAddRetnTypeFunEndFunStartFunStartArgsVectorFunctionExpressionAddLocFunctionExpressionAddTypFunctionExpressionAddTypTypeFunctionParameterAddDefaultFunctionParameterAddDefaultTypeFunctionParameterAddIsPipeFunctionParameterAddKeyFunctionParameterAddLocFunctionParameterEndFunctionParameterStartGetRootAsArgumentGetRootAsArrGetRootAsBasicGetRootAsConstraintGetRootAsFresherGetRootAsFunGetRootAsFunctionParameterGetRootAsIdentifierExpressionGetRootAsMonoTypeHolderGetRootAsNativeVariableAssignmentGetRootAsPolyTypeGetRootAsPropGetRootAsRecordGetRootAsTimeGetRootAsTypeAssignmentGetRootAsTypeEnvironmentGetRootAsVarIdentifierAddLocIdentifierExpressionAddLocIdentifierExpressionAddNameIdentifierExpressionAddTypIdentifierExpressionAddTypTypeIdentifierExpressionEndIdentifierExpressionStartImportDeclarationAddAliasImportDeclarationAddLocIndexExpressionAddLocIndexExpressionAddTypIndexExpressionAddTypTypeIntegerLiteralAddLocKindAddableKindComparableKindDivisibleKindEquatableKindNegatableKindNullableKindNumericKindRecordKindSubtractableKindTimeableLogicalExpressionAddLocMemberAssignmentAddLocMemberExpressionAddLocMemberExpressionAddTypMemberExpressionAddTypTypeMonoTypeArrMonoTypeBasicMonoTypeFunMonoTypeHolderMonoTypeHolderAddTypMonoTypeHolderAddTypTypeMonoTypeHolderEndMonoTypeHolderStartMonoTypeRecordMonoTypeVarNativeVariableAssignmentNativeVariableAssignmentAddIdentifierNativeVariableAssignmentAddInit_NativeVariableAssignmentAddInit_typeNativeVariableAssignmentAddLocNativeVariableAssignmentAddTypNativeVariableAssignmentEndNativeVariableAssignmentStartObjectExpressionAddLocObjectExpressionAddTypObjectExpressionAddTypTypeOptionStatementAddLocPackageAddLocPackageClauseAddLocPolyTypePolyTypeAddConsPolyTypeAddExprPolyTypeAddExprTypePolyTypeAddVarsPolyTypeEndPolyTypeStartPolyTypeStartConsVectorPolyTypeStartVarsVectorPropAddKPropAddVPropAddVTypePropEndPropStartPropertyAddLocRecordAddExtendsRecordAddPropsRecordEndRecordStartRecordStartPropsVectorRegexpLiteralAddLocReturnStatementAddLocStatementNativeVariableAssignmentStringExpressionAddLocStringExpressionPartAddLocStringLiteralAddLocTestStatementAddLocTimeAddNsecsTimeAddOffsetTimeAddSecsTimeEndTimeStartTypeAssignmentTypeAssignmentAddIdTypeAssignmentAddTyTypeAssignmentEndTypeAssignmentStartTypeBoolTypeBytesTypeDurationTypeEnvironmentTypeEnvironmentAddAssignmentsTypeEnvironmentEndTypeEnvironmentStartTypeEnvironmentStartAssignmentsVectorTypeFloatTypeIntTypeRegexpTypeTimeTypeUintUnaryExpressionAddLocUnaryExpressionAddTypUnaryExpressionAddTypTypeUnsignedIntegerLiteralAddLocVarAddIVarEndVarStartWrappedExpressionAddExpressionWrappedExpressionAddExpressionTypeMutateIArgumentsLengthPipeTypeMutatePipeTypeArgsLengthRetnTypeMutateRetnTypeRetnMutateUPropsLengthVarsVarsLengthConsConsLengthAssignmentsAssignmentsLengthMutateTfbsemanticgithub.com/influxdata/flux/internal/fbsemanticCsvInputDefaultNumPointsDefaultPeriodappendTagKeydataGeneratorgenSeriesKeysgenTagValuegenTagValuesseriesGroupseriesGroupsNumPointsNullsTypeInfodggenerateBufferTimesgenerateBufferValuesgithub.com/influxdata/flux/internal/genArrayContainerExponentialMovingAverageNewArrayContainerperiodReachedlastValPassThroughDoNumericPassThroughTimeGetEMALastValAddNullOrigValuegithub.com/influxdata/flux/internal/moving_averageFloat64ArrayInt64ArrayUint64Arraygithub.com/influxdata/flux/internal/mutableMustParseTimeParseRegexpParseTextfromHexCharwriteNextUnescapedRunegithub.com/influxdata/flux/internal/parsersyncutilgithub.com/influxdata/flux/internal/pkg/syncutilFromEvaluationFromTableObjectbuildSpecbuildSpecWithTraceiderisDuplicateTableObjectgithub.com/influxdata/flux/internal/specARROWBUILTINDOTELSEEMPTYEXISTSIFIMPORTININTNOTNewFileOPTIONPACKAGEPIPE_FORWARDPIPE_RECEIVEPOWQUOTEREGEXEQREGEXNEQRETURNSTRINGEXPRTESTTEXTTHENTIMEWITHsearchIntstokenStringsAddLineAddFilegithub.com/influxdata/flux/internal/tokenDoFunctionCallDoFunctionCallContextInterpreterNewArgumentsNewInterpreterNewPackageNewPackageWithValuesNowOptionNowPkgPackageMainResolveIdsInFunctionToFloatArrayToStringArrayargumentscallStackKeyconvertdefExecOptsConfigfunctionNameisValidFunctionBlocknewArgumentsresolveValuestackElementwithStackEntryexecOptsConfigPackageNameitrpdoRootdoPackagedoFiledoPackageClausedoImportdoStatementevaluateNowOptionirtpevaluateProfilerOptiondoOptionStatementdoTestStatementdoVariableAssignmentdoMemberAssignmentdoAssignmentdoExpressiondoStringExpressiondoStringPartdoArraydoObjectcheckForDuplicatesdoLiteraldoArgumentsNumVarsNumConstraintsSortedConstraintssortedConstraintsSortedVarssortedVarsASTCompilerTypeCompileTableObjectIsNonNullJSONTableObjectCompilerWithExternWithPhysPlanOptsbuildPlandefaultOptionsgetOptionValuesgetPackageFromScopegetPlanOptionswrapFileJSONInPkgeocASTPkgAnalyzerEnvStdlibFindVarTypeNewAnalyzerParseJSONSemanticPkgfreeablesourceHashesMarshalFBlibfluxgithub.com/influxdata/flux/libflux/go/libflux/Users/austinjaybecker/projects/abeck-go-testing/-/Users/austinjaybecker/go/Users/austinjaybecker/go/pkg/Users/austinjaybecker/go/pkg/mod/Users/austinjaybecker/go/pkg/mod/github.com/Users/austinjaybecker/go/pkg/mod/github.com/influxdata/Users/austinjaybecker/go/pkg/mod/github.com/influxdata/flux@v0.95.0/Users/austinjaybecker/go/pkg/mod/github.com/influxdata/flux@v0.95.0/libflux/Users/austinjaybecker/go/pkg/mod/github.com/influxdata/flux@v0.95.0/libflux/go/Users/austinjaybecker/go/pkg/mod/github.com/influxdata/flux@v0.95.0/libflux/go/libflux/Users/austinjaybecker/go/pkg/mod/github.com/influxdata/flux@v0.95.0/libflux/go/libflux/analyze.go/Users/austinjaybecker/go/pkg/mod/github.com/influxdata/flux@v0.95.0/libflux/go/libflux/analyze.go:5:8TimeProvidergithub.com/influxdata/flux/lineLimitExceededErrorWantedHandleToJSONParseDirParseToHandledefaultPackageNamepackageNameparseFileAfterAtLeastCountAfterAtLeastCountTriggerSpecAfterProcessingTimeAfterProcessingTimeTriggerSpecAfterWatermarkAfterWatermarkTriggerSpecAnyKindAnyPatternBoundsAwareProcedureSpecClearRegisteredRulesComputeBoundsCreatePhysicalNodeDefaultTriggerSpecDefaultYieldNameDetailerDisableIntegrityChecksDisableValidationEmptyBoundsFormatOptionFormattedGeneratedYieldProcedureSpecLogicalPlannerMergeToLogicalNodeNarrowTransformationNarrowTransformationTriggerSpecNewLogicalPlannerNewPhysicalPlannerNewPlanSpecNextPlanNodeIDKeyOnlyLogicalRulesOnlyPhysicalRulesOrFinallyOrFinallyTriggerSpecPhysPatPhysicalOneKindPatternPhysicalPlannerPlannerPlannerBuilderPostPhysicalValidatorRegisterProcedureSpecRemoveLogicalRulesRemovePhysicalRulesRepeatedTriggerSpecSwapPlanNodesTriggerAwareProcedureSpecUnionKindPatternWalkPredecessorsWalkSuccessorsWithDefaultMemoryLimitYieldProcedureSpecadministrationcreateLogicalPlancreateProcedureFnscreateProcedureFnsFromKindfluxSpecVisitorgenerateYieldNodegeneratedYieldKindisNodeInNodeslogicalOptionmergeIDsmergePlanNodesnewHeuristicPlannernewTopologicalWalkphysicalConverterRulephysicalOptionregisterRuleruleNameToLogicalRuleruleNameToPhysicalRulesymmetryChecktopologicalWalkupdateSuccessorsvalidatePhysicalPlansideEffectKindsideEffectOperationAllowedLatenessMainFinallyokpwithDetailsnavigationFntemporaryMarkspermanentMarkspushVisitYieldNameloptspoptsAddLogicalOptionsAddPhysicalOptionsyieldNamesaddYieldNamevisitOperationAnalyzePackageAnalyzeSourceEvalOptionsStdLibTypeEnvMapnewObjectFromScopepreludestdlibTypeEnvironmentvalidatePackageBuiltinsimpBasicBytesBasicDurationBasicRegexpBasicTimeConjunctionsToExprSliceExprsToConjunctionExtendObjectTypeNestingVisitorNewArrayTypeNewFunctionTypeNewMonoTypeNewPolyTypeNewScopedVisitorScopedVisitorToASTanalyzeArrayExpressionanalyzeAssignmentanalyzeBinaryExpressionanalyzeBlockanalyzeBooleanLiteralanalyzeBuiltinStatementanalyzeCallExpressionanalyzeConditionalExpressionanalyzeDateTimeLiteralanalyzeDurationLiteralanalyzeExpressionanalyzeExpressionStatementanalyzeFileanalyzeFloatLiteralanalyzeFunctionExpressionanalyzeIdentifieranalyzeIdentifierExpressionanalyzeImportDeclarationanalyzeIndexExpressionanalyzeIntegerLiteralanalyzeInterpolatedPartanalyzeLiteralanalyzeLogicalExpressionanalyzeMemberAssignmentanalyzeMemberExpressionanalyzeNodeanalyzeObjectExpressionanalyzeOptionStatementanalyzePackageanalyzePackageClauseanalyzePipeExpressionanalyzePropertyanalyzePropertyKeyanalyzeRegexpLiteralanalyzeReturnStatementanalyzeStatementanalyzeStringExpressionanalyzeStringExpressionPartanalyzeStringLiteralanalyzeTestStatementanalyzeTextPartanalyzeUnaryExpressionanalyzeUnsignedIntegerLiteralanalyzeVariableAssignmentbuildArrayTypebuildBasicTypebuildFunctionTypebuildObjectTypebuildVarTypecopyMonoTypefbTyperformattingVisitorfromAssignmentTablefromExpressionTablefromExpressionTableOptionalfromFBDurationVectorfromFBLogicalOperatorfromFBOperatorfromFBRegexpLiteralfromFBStringExpressionPartVectorfromFBTimefromWrappedExpressionfromWrappedStatementgetArrgetBasicgetFungetMonoTypegetPolyTypegetRecordgetTableFngetVarmonoTypeFromFuncmonoTypeFromVarnatureNamesnewArgumentnewBasicTypeobjectExprFromPropertiesoptionDependenciesoptionExprVisitoroptionNameoptionReAssignmentsoptionStatementsoptionStmtVisitorposFromBufpropertyKeyFromFBIdentifierrunChecksupdateTVarMapvarReAssignmentsvarStmtVisitorvarFnFluxTestPackagespkgASTnaiveBayesClassifiergithub.com/influxdata/flux/stdlib/contrib/RohanSreerama5/naiveBayesClassifieranomalydetectiongithub.com/influxdata/flux/stdlib/contrib/anaisdg/anomalydetectionstatsmodelsgithub.com/influxdata/flux/stdlib/contrib/anaisdg/statsmodelsdiscordgithub.com/influxdata/flux/stdlib/contrib/chobbs/discordNewTableTransformationNewWindowTransformationTableColumnTableKindTableOpSpecTableProcedureSpecWindowOpSpeccolumnStatecreateTableOpSpeccreateTableTransformationcreateWindowOpSpeccreateWindowTransformationfillNonefillNullnewTableProcedurenewWindowProceduretableColumnsFromObjecttableTransformationwindowStatewindowTableStatewindowTransformationComputecompileInitFunccompileReduceFunccompileComputeFuncvalidateInputTablevalidateGroupKeyvalidateInputColumnsbuildTableTimeSrcgetTimeColumnEvalPendingWindowsProcessTableEvalWindowcreateWindowgithub.com/influxdata/flux/stdlib/contrib/jsternberg/aggregatecreateMaskOpSpeccreateMaskTransformationmaskKindmaskOpSpecmaskProcedureSpecmaskTransformationnewMaskProcedurenewMaskTransformationgithub.com/influxdata/flux/stdlib/contrib/jsternberg/influxdbMaxIndexMinIndexfloatMaxIndexfloatMinIndexfloatSumintMaxIndexintMinIndexintSumuintMaxIndexuintMinIndexuintSumgithub.com/influxdata/flux/stdlib/contrib/jsternberg/mathMapKindMapOpSpecMapProcedureSpecNewMapTransformationcreateMapOpSpeccreateMapTransformationmapTablemapTransformationnewMapProceduremapValuescreateSchemagithub.com/influxdata/flux/stdlib/contrib/jsternberg/rowsRespondersToJSONinputArgnamedResponderrespondersToJSONFuncuserResponderhasSideEffectopsgeniegithub.com/influxdata/flux/stdlib/contrib/sranka/opsgenieToSensuNameToSensuNameGoreplacedCharacterstoSensuNameFuncsensugithub.com/influxdata/flux/stdlib/contrib/sranka/sensuteamsgithub.com/influxdata/flux/stdlib/contrib/sranka/teamsgithub.com/influxdata/flux/stdlib/contrib/sranka/telegramDurationOpSpecDurationProcedureSpecNewDurationTransformationcreateDurationOpSpeccreateDurationTransformationdurationTransformationnewDurationOpnewDurationProcedureIsStopisStopgithub.com/influxdata/flux/stdlib/contrib/tomhollingworth/eventsCSVSourceFromCSVKindFromCSVOpSpecFromCSVProcedureSpeccreateFromCSVOpSpeccreateFromCSVSourcenewFromCSVOpnewFromCSVProceduregithub.com/influxdata/flux/stdlib/csvSpecialFnsgithub.com/influxdata/flux/stdlib/dategithub.com/influxdata/flux/stdlib/experimental/aggregatecreateFromOpSpeccreateFromSourcenewFromProceduretableSourcegithub.com/influxdata/flux/stdlib/experimental/arrayAddFilterToNodeAddLimitToNodeBigtableDecoderBigtableFilterRewriteRuleBigtableLimitRewriteRuleBigtableRowReaderFamilyRowFromBigtableKindFromBigtableOpSpecFromBigtableProcedureSpecNewBigtableRowReadercreateFromBigtableOpSpeccreateFromBigtableSourcegetPrefixgetRangegetTimeRangeisRFamilyisRRowKeyisRTimenewFromBigtableOpnewFromBigtableProcedurerowsByFamilyfamilyIndexcolumnIndicesrowIndexnextFamilycurrentFamilygithub.com/influxdata/flux/stdlib/experimental/bigtablegithub.com/influxdata/flux/stdlib/experimental/csvAbsoluteMaxSizeboxboxToPolygoncircleearthRadiusKmearthRadiusesgenerateGetGridFuncgenerateGetLevelFuncgenerateS2CellIDTokenFuncgenerateS2CellLatLonFuncgenerateSTContainsFuncgenerateSTDistanceFuncgenerateSTLengthFuncgetGridgetLatLnggetLevelgetParentFromLatLongetParentFromTokengetS2CapRegiongetS2LoopRegiongetS2PointgetS2RectRegiongetSpecGridgridminDistanceToPointminDistanceToShapeIndexparseGeometryArgumentparseUnitsArgumentpolygonpolylineshapeToIndexminLatminLonmaxLatmaxLonlatlongetSetearthRadiusdistanceToRaddistanceToUserlatlngsgeogithub.com/influxdata/flux/stdlib/experimental/geoheaderToObjectgithub.com/influxdata/flux/stdlib/experimental/httptoValueunmarshalToValuegithub.com/influxdata/flux/stdlib/experimental/jsonDefaultToMQTTTimeoutDefaultToMQTTUserAgentNewToMQTTTransformationToMQTTKindToMQTTOpSpecToMQTTProcedureSpecToMQTTTransformationcreateToMQTTOpSpeccreateToMQTTTransformationidxTypeinnerToMQTTOpSpecnewToMQTTProceduretoMqttMetricBrokerQoSNoKeepAliveTagListFieldListtruncateTagsAndFieldscreateTopicgithub.com/influxdata/flux/stdlib/experimental/mqttPrometheusIteratorScrapePrometheusKindScrapePrometheusOpSpecScrapePrometheusProcedureSpeccreateScrapePrometheusOpSpeccreateScrapePrometheusSourcenewScrapePrometheusOpnewScrapePrometheusProcedureTypeValNowFngithub.com/influxdata/flux/stdlib/experimental/prometheusgithub.com/influxdata/flux/stdlib/experimental/queryExperimentalGroupKindJoinOpSpecMakeChainFunctionMergeJoinProcedureSpecNewGroupTransformationNewMergeJoinCacheNewMergeJoinTransformationNewRowIteratorNewSetTransformationSetKindSetOpSpecSetProcedureSpecToSignatureaddDurationaddDurationToappendRowToBuilderbuildSchemachainCallcreateGroupOpSpeccreateGroupTransformationcreateJoinOpSpeccreateMergeJoinTransformationcreateSetOpSpeccreateSetTransformationgroupModeExtendgroupTransformationjoinKindmergeJoinCachemergeJoinTransformationnewGroupOpnewGroupProcedurenewJoinOpnewMergeJoinProcedurenewRowJoinFnnewSetOpnewSetProcedureobjContainsKeyrowJoinFnsetTransformationsubtractDurationFromcleantimeColrecordFromGeneratorKindFromGeneratorOpSpecFromGeneratorProcedureSpecGeneratorSourceNewGeneratorSourcecreateFromGeneratorOpSpeccreateFromGeneratorSourcenewFromGeneratorOpnewFromGeneratorProceduregithub.com/influxdata/flux/stdlib/generatePathEncodebasicAuthFuncbasicAuthPasswordArgbasicAuthUsernameArginputStringArgpathEscapeFuncgithub.com/influxdata/flux/stdlib/httpgithub.com/influxdata/flux/stdlib/influxdata/influxdb/monitorgithub.com/influxdata/flux/stdlib/influxdata/influxdb/schemaGetFuncmakeGetFuncgithub.com/influxdata/flux/stdlib/influxdata/influxdb/secretsLastSuccessLastSuccessFunctionlastSuccessFuncNamemakeLastSuccessFunczeroTimeNamegithub.com/influxdata/flux/stdlib/influxdata/influxdb/tasksDatabasesOpSpecDatabasesRemoteKindDatabasesRemoteProcedureSpecDatabasesRemoteRuleFromInfluxJSONKindFromInfluxJSONOpSpecJSONSourcecreateDatabasesOpSpeccreateFromInfluxJSONOpSpeccreateFromInfluxJSONSourcenewDatabasesOpnewDatabasesProcedurenewFromInfluxJSONOpnewFromInfluxJSONProcedureBuildQueryBucketsOpSpecBucketsRemoteKindBucketsRemoteProcedureSpecBucketsRemoteRuleCardinalityFuncNameCardinalityKindCardinalityOpSpecCardinalityProcedureSpecDefaultFromAttributesFromRemoteKindFromRemoteProcedureSpecFromRemoteRuleGetNameOrIDMergeRemoteFilterRuleMergeRemoteRangeRuleRemoteProcedureSpeccreateBucketsOpSpeccreateCardinalityOpSpeccreateCardinalitySourcenewBucketsOpnewBucketsProcedurenewCardinalityProcedurenewFromOpNewPassTransformationPassKindPassOpSpecPassProcedureSpeccreatePassOpSpeccreatePassTransformationnewPassOpnewPassProcedurepassTransformationgithub.com/influxdata/flux/stdlib/internal/debugTablesKindTablesOpSpecTablesProcedureSpeccreateTablesOpSpeccreateTablesSourcenewTablesOpnewTablesProceduregithub.com/influxdata/flux/stdlib/internal/gengithub.com/influxdata/flux/stdlib/internal/influxqlChangesKindChangesOpSpecChangesProcedureSpecDefaultUpperBoundColumnLabelEmptyTableKindEmptyTableOpSpecEmptyTableProcedureSpecEmptyTableSourceExtrapolatedRateKindExtrapolatedRateOpSpecExtrapolatedRateProcedureSpecHistogramQuantileKindHistogramQuantileOpSpecHistogramQuantileProcedureSpecHoltWintersKindHoltWintersOpSpecHoltWintersProcedureSpecInstantRateKindInstantRateOpSpecInstantRateProcedureSpecLabelReplaceKindLabelReplaceOpSpecLabelReplaceProcedureSpecLinearRegressionKindLinearRegressionOpSpecLinearRegressionProcedureSpecNewChangesTransformationNewExtrapolatedRateTransformationNewHistogramQuantileTransformationNewHoltWintersTransformationNewInstantRateTransformationNewLabelReplaceTransformationNewLinearRegressionTransformationNewResetsTransformationNewTimestampTransformationResetsKindResetsOpSpecResetsProcedureSpecTimestampKindTimestampOpSpecTimestampProcedureSpecbucketQuantilecalcTrendValuechangesTransformationcoalesceBucketscreateChangesOpSpeccreateChangesTransformationcreateEmptyTableOpSpeccreateEmptyTableSourcecreateExtrapolatedRateOpSpeccreateExtrapolatedRateTransformationcreateHistogramQuantileOpSpeccreateHistogramQuantileTransformationcreateHoltWintersOpSpeccreateHoltWintersTransformationcreateInstantRateOpSpeccreateInstantRateTransformationcreateLabelReplaceOpSpeccreateLabelReplaceTransformationcreateLinearRegressionOpSpeccreateLinearRegressionTransformationcreateResetsOpSpeccreateResetsTransformationcreateTimestampOpSpeccreateTimestampTransformationdayOfMonthFndayOfWeekFndaysInMonthFnensureMonotonicextrapolatedRateTransformationgenerateDateFunctionhistogramQuantileTransformationholtWintersTransformationhourFninstantRateTransformationlabelReplaceTransformationlinearRegressionTransformationminuteFnmonthFnnewChangesOpnewChangesProcedurenewEmptyTableOpnewEmptyTableProcedurenewExtrapolatedRateOpnewExtrapolatedRateProcedurenewHistogramQuantileOpnewHistogramQuantileProcedurenewHoltWintersOpnewHoltWintersProcedurenewInstantRateOpnewInstantRateProcedurenewLabelReplaceProcedurenewLinearRegressionOpnewLinearRegressionProcedurenewResetsOpnewResetsProcedurenewTimestampProcedureresetsTransformationtimestampTransformationyearFnCountColumnUpperBoundColumnPredictFromNowIsRateisRatepredictfromNowSmoothingFactorTrendFactorsmoothingFactortrendFactorisCounterdestinationreplacementIsCounterupperBoundDestinationReplacementgithub.com/influxdata/flux/stdlib/internal/promqlgithub.com/influxdata/flux/stdlib/internal/testutilLinearInterpolateKindLinearInterpolateOpSpecLinearInterpolateProcedureSpecNewInterpolateTransformationappendFncreateInterpolateOpSpeccreateInterpolateTransformationinterpolateTransformationnewInterpolateProcedureinterpolategithub.com/influxdata/flux/stdlib/interpolategithub.com/influxdata/flux/stdlib/jsonDefaultKafkaWriterFactoryKafkaWriterNewToKafkaTransformationToKafkaKindToKafkaProcedureSpecToKafkaTransformationcreateToKafkaOpSpeccreateToKafkaTransformationnewToKafkaProceduretoKafkaMetricCompressionCodecWriteMessagesBalanceWriterConfigDialLeaderLookupLeaderLookupPartitionsconnectTLSdialContextwriterStatsobserveDurationsnapshotDurationrebalancesdialTimewaitTimeretriespartitionWriterwriterMessageQueueCapacityBatchSizeBatchTimeoutRebalanceIntervalRequiredAcksAsyncnewPartitionWriterconnDeadlinerconnwconnsetConnReadDeadlinesetConnWriteDeadlineunsetConnReadDeadlineunsetConnWriteDeadlinerlockwlockwdeadlinerdeadlinefetchMaxBytesfetchMinSizecorrelationIDrequiredAcksDeleteTopicsdescribeGroupsfindCoordinatorjoinGroupleaveGrouplistGroupsoffsetCommitoffsetFetchsyncGroupsReadMessageReadBatchReadOffsetReadFirstOffsetReadLastOffsetReadOffsetsreadOffsetReadPartitionsSetRequiredAckswriteRequestHeaderpeekResponseSizeAndIDskipResponseSizeAndIDreadDeadlinewriteDeadlinereadOperationwriteOperationwaitResponserequestHeadercreateTopicsCreateTopicsdeleteTopicsmessageSetItemMagicByteMessageSizeDurationStatsAvgSummaryStatsReplicasIsroffsetFetchRequestV3offsetFetchRequestV3TopicGroupIDoffsetFetchResponseV3offsetFetchResponseV3ResponseoffsetFetchResponseV3PartitionResponsePartitionResponsesThrottleTimeMSdescribeGroupsRequestV1GroupIDsdescribeGroupsResponseV1describeGroupsResponseGroupV1describeGroupsResponseMemberV1ClientHostMemberMetadataMemberAssignmentsProtocolTypecreateTopicsRequestV2createTopicsRequestV2TopiccreateTopicsRequestV2ReplicaAssignmentcreateTopicsRequestV2ConfigEntryConfigNameConfigValueNumPartitionsReplicationFactorReplicaAssignmentsConfigEntriesValidateOnlycreateTopicsResponseV2createTopicsResponseV2TopicErrorTopicErrorslistGroupsRequestV1listGroupsResponseV1ListGroupsResponseGroupV1heartbeatRequestV1GenerationIDheartbeatResponseV1leaveGroupRequestV1leaveGroupResponseV1offsetCommitRequestV3offsetCommitRequestV3TopicoffsetCommitRequestV3PartitionRetentionTimeoffsetCommitResponseV3offsetCommitResponseV3ResponseoffsetCommitResponseV3PartitionResponseTopicConfigReplicaAssignmenttoCreateTopicsRequestV2ReplicaAssignmentConfigEntrytoCreateTopicsRequestV2ConfigEntrytoCreateTopicsRequestV2TopicdeleteTopicsRequestV1deleteTopicsResponseV1deleteTopicsResponseV1TopicErrorCodeTopicErrorCodesapiKeyApiVersionCorrelationIDjoinGroupRequestV2joinGroupRequestGroupProtocolV2ProtocolMetadataSessionTimeoutRebalanceTimeoutGroupProtocolsjoinGroupResponseV2joinGroupResponseMemberV2GroupProtocolLeaderIDsyncGroupRequestV1syncGroupRequestGroupAssignmentV1GroupAssignmentssyncGroupResponseV1findCoordinatorRequestV1CoordinatorKeyCoordinatorTypefindCoordinatorResponseV1findCoordinatorResponseCoordinatorV1highWaterMarkHighWaterMarkgenerateMathFunctionXgenerateMathFunctionXYgithub.com/influxdata/flux/stdlib/mathDedupKeyKindDedupKeyOpSpecDedupKeyTransformationDedupProcedureSpecNewDedupKeyTransformationcreateDedupKeyOpSpeccreateDedupKeyTransformationdedupKeyColNamenewDedupKeyOpnewDedupKeyProceduregithub.com/influxdata/flux/stdlib/pagerdutygithub.com/influxdata/flux/stdlib/plannergithub.com/influxdata/flux/stdlib/profilerpushbulletgithub.com/influxdata/flux/stdlib/pushbulletgithub.com/influxdata/flux/stdlib/regexperrBuildInfoNotPresentmodulePathreadBuildInfoversionFuncNameBuildSettinggithub.com/influxdata/flux/stdlib/runtimedefaultColorserrColorParsevalidateColorStringvalidateColorStringFluxFngithub.com/influxdata/flux/stdlib/slackFromSocketKindFromSocketOpSpecFromSocketProcedureSpecNewSocketSourcecreateFromSocketOpSpeccreateFromSocketSourcedecodersnewFromSocketOpnewFromSocketProcedurenowTimeProviderschemessocketSourcesocketgithub.com/influxdata/flux/stdlib/socketAwsAthenaRowReaderAzureConfigBigQueryColumnTranslateFuncBigQueryRowReaderCreateInsertComponentsDefaultBatchSizeExecuteQueriesFromSQLKindFromSQLOpSpecFromSQLProcedureSpecHdbColumnTranslateFuncHdbRowReaderMssqlColumnTranslateFuncMssqlRowReaderMySQLRowReaderMysqlColumnTranslateFuncNewAwsAthenaRowReaderNewBigQueryRowReaderNewHdbRowReaderNewMssqlRowReaderNewMySQLRowReaderNewPostgresRowReaderNewSnowflakeRowReaderNewSqliteRowReaderNewToSQLTransformationPostgresColumnTranslateFuncPostgresRowReaderSnowflakeColumnTranslateFuncSnowflakeRowReaderSqliteColumnTranslateFuncSqliteRowReaderToSQLKindToSQLOpSpecToSQLProcedureSpecToSQLTransformationUInt8ToFloatUInt8ToInt64correctBatchSizecreateFromSQLOpSpeccreateFromSQLSourcecreateToSQLOpSpeccreateToSQLTransformationdefaultOpenFunctionfluxToBigQueryfluxToHdbfluxToSQLServerfluxToSnowflakegetOpenFuncgetTranslationFunchdbAddIfNotExisthdbDoIfTableNotExistsTemplatehdbEscapeNameisMssqlDriverlayoutDatelayoutTimelayoutTimeStampNtzmssqlAzureAuthConfigmssqlAzureAuthEnvmssqlAzureAuthFilemssqlAzureAuthKeymssqlAzureAuthMsimssqlAzureAuthTokenmssqlAzureClientCertificatePasswordKeymssqlAzureClientCertificatePathKeymssqlAzureClientIdKeymssqlAzureClientSecretKeymssqlAzureMSITokenmssqlAzurePasswordKeymssqlAzureResourcemssqlAzureTenantIdKeymssqlAzureUsernameKeymssqlCheckParametermssqlConfigmssqlIdentityInsertEnabledmssqlOpenFunctionmssqlParseDSNmssqlSetAzureConfignewFromSQLOpnewFromSQLProcedurenewToSQLProcedureopenFuncsqlIteratorsupportsTxtranslationFuncvalidateDataSourcesqlTypesNextFuncCloseFuncInitColumnNamesInitColumnTypesSetColumnTypesDataSourceNameTenantIdClientIdAzureAuthgithub.com/influxdata/flux/stdlib/sqlcutsetgenerateDualArgStringFunctiongenerateDualArgStringFunctionReturnBoolgenerateDualArgStringFunctionReturnIntgenerateRepeatgenerateReplacegenerateReplaceAllgenerateSingleArgStringFunctiongenerateSplitgenerateSplitNgenerateUnicodeIsFunctionstringArgTstringArgUstringArgVstrlensubstrsubstringgithub.com/influxdata/flux/stdlib/stringssystemTimeFuncNamegithub.com/influxdata/flux/stdlib/systemgithub.com/influxdata/flux/stdlib/testing/chronografgithub.com/influxdata/flux/stdlib/testing/influxqltestdatagithub.com/influxdata/flux/stdlib/testing/kapacitorpandasgithub.com/influxdata/flux/stdlib/testing/pandasgithub.com/influxdata/flux/stdlib/testing/prometheusgithub.com/influxdata/flux/stdlib/testing/promqlgithub.com/influxdata/flux/stdlib/testing/usageAssertEmptyKindAssertEmptyOpSpecAssertEmptyProcedureSpecAssertEmptyTransformationAssertEqualsErrorAssertEqualsKindAssertEqualsOpSpecAssertEqualsProcedureSpecAssertEqualsTransformationDefaultEpsilonDiffKindDiffOpSpecDiffProcedureSpecDiffTransformationNewAssertEmptyTransformationNewAssertEqualsTransformationNewDiffTransformationassertEqualsParentStatecopyTablecreateAssertEmptyOpSpeccreateAssertEmptyTransformationcreateAssertEqualsOpSpeccreateAssertEqualsTransformationcreateDiffOpSpeccreateDiffTransformationdiffParentStatenewAssertEmptyOpnewAssertEmptyProcedurenewAssertEqualsOpnewAssertEqualsProcedurenewDiffOpnewDiffProceduretableColumnprocessingntablesgotParentwantParentkeysMatchedunequalAssertionwantIDparentStateinputCacherowEqualappendRowEpsilongithub.com/influxdata/flux/stdlib/testingHoltWintersNewOptimizergithub.com/influxdata/flux/stdlib/universe/holt_wintersChandeMomentumOscillatorKindChandeMomentumOscillatorOpSpecChandeMomentumOscillatorProcedureSpecColumnsKindColumnsOpSpecColumnsProcedureSpecCountAggCovarianceKindCovarianceOpSpecCovarianceProcedureSpecCovarianceTransformationCumulativeSumKindCumulativeSumOpSpecCumulativeSumProcedureSpecDerivativeKindDerivativeOpSpecDerivativeProcedureSpecDieDieKindDifferenceKindDifferenceOpSpecDifferenceProcedureSpecDistinctOpSpecDropKeepMutatorDropKindDropOpSpecDuplicateKindDuplicateMutatorElapsedKindElapsedOpSpecElapsedProcedureSpecExactQuantileAggExactQuantileAggKindExactQuantileAggProcedureSpecExactQuantileSelectKindExactQuantileSelectProcedureSpecExactQuantileSelectorTransformationExponentialMovingAverageKindExponentialMovingAverageOpSpecExponentialMovingAverageProcedureSpecFillOpSpecFillProcedureSpecFirstOpSpecFirstSelectorHistogramKindHistogramOpSpecHistogramProcedureSpecHourSelectionKindHourSelectionOpSpecHourSelectionProcedureSpecIntegralKindIntegralOpSpecIntegralProcedureSpecJoinKindKamaOpSpecKamaProcedureSpecKeepKindKeyValuesKindKeyValuesOpSpecKeyValuesProcedureSpecKeysOpSpecLastOpSpecLastSelectorLimitKindLimitOpSpecLimitProcedureSpecMakeContainsFuncMakeLengthFuncMaxFloatSelectorMaxIntSelectorMaxOpSpecMaxSelectorMaxTimeSelectorMaxUIntSelectorMeanAggMeanOpSpecMergeGroupRuleMergeJoinCacheMergeJoinKindMinFloatSelectorMinIntSelectorMinOpSpecMinSelectorMinTimeSelectorMinUIntSelectorModeKindModeOpSpecModeProcedureSpecMovingAverageKindMovingAverageOpSpecMovingAverageProcedureSpecMutationRegistrarNewBuilderContextNewChandeMomentumOscillatorTransformationNewColumnsTransformationNewCovarianceTransformationNewCumulativeSumTransformationNewDeprecatedFillTransformationNewDeprecatedSchemaMutationTransformationNewDerivativeTransformationNewDifferenceTransformationNewDistinctTransformationNewDropKeepMutatorNewDuplicateMutatorNewElapsedTransformationNewExactQuantileSelectorTransformationNewExponentialMovingAverageTransformationNewFillTransformationNewFilterTransformationNewFindColumnFunctionNewFindRecordFunctionNewFixedWindowTransformationNewGetColumnFunctionNewGetRecordFunctionNewHistogramTransformationNewHistorgramQuantileTransformationNewHourSelectionTransformationNewIntegralTransformationNewKeyValuesTransformationNewKeysTransformationNewLimitTransformationNewModeTransformationNewMovingAverageTransformationNewPivotTransformationNewRangeTransformationNewReduceTransformationNewRelativeStrengthIndexTransformationNewRenameMutatorNewSchemaMutationTransformationNewShiftTransformationNewSortTransformationNewStateTrackingTransformationNewTableFindFunctionNewTailTransformationNewTripleExponentialDerivativeTransformationNewUnionTransformationNewUniqueTransformationNewkamaTransformationPivotOpSpecQuantileAggQuantileOpSpecReduceKindReduceOpSpecReduceProcedureSpecRegistrarsRelativeStrengthIndexKindRelativeStrengthIndexOpSpecRelativeStrengthIndexProcedureSpecRemoveTrivialFilterRuleRenameKindRenameMutatorRenameOpSpecSampleKindSampleOpSpecSampleProcedureSpecSampleSelectorSchemaMutationOpsShiftKindShiftOpSpecShiftProcedureSpecSkewAggSkewKindSkewOpSpecSkewProcedureSpecSortKindSortOpSpecSortProcedureSpecSpreadAggSpreadFloatAggSpreadIntAggSpreadKindSpreadOpSpecSpreadProcedureSpecSpreadUIntAggStateTrackingKindStateTrackingOpSpecStateTrackingProcedureSpecStddevAggStddevKindStddevOpSpecStddevProcedureSpecSumAggSumFloatAggSumIntAggSumUIntAggTDigestQuantileProcedureSpecTailKindTailOpSpecTailProcedureSpecTripleExponentialDerivativeKindTripleExponentialDerivativeOpSpecTripleExponentialDerivativeProcedureSpecUnionKindUnionOpSpecUnionProcedureSpecUniqueKindUniqueOpSpecUniqueProcedureSpecWindowTriggerPhysicalRuleYieldKindYieldOpSpecaddColumnsToSchemaappendSlicedColsarrayFromColumnarrayToFloatArrowboolConvchandeMomentumOscillatorTransformationcolumnsTransformationconvBoolTypeconvBytesTypeconvDurationTypeconvFloatTypeconvIntTypeconvStringTypeconvTimeTypeconvUintTypeconversionArgcreateChandeMomentumOscillatorOpSpeccreateChandeMomentumOscillatorTransformationcreateColumnsOpSpeccreateColumnsTransformationcreateCountOpSpeccreateCountTransformationcreateCovarianceOpSpeccreateCovarianceTransformationcreateCumulativeSumOpSpeccreateCumulativeSumTransformationcreateDeprecatedFillTransformationcreateDeprecatedSchemaMutationTransformationcreateDerivativeOpSpeccreateDerivativeTransformationcreateDifferenceOpSpeccreateDifferenceTransformationcreateDistinctOpSpeccreateDistinctTransformationcreateDropOpSpeccreateDualImplTfcreateDuplicateOpSpeccreateElapsedOpSpeccreateElapsedTransformationcreateExactQuantileAggTransformationcreateExactQuantileSelectTransformationcreateExponentialMovingAverageOpSpeccreateExponentialMovingAverageTransformationcreateFillOpSpeccreateFillTransformationcreateFilterOpSpeccreateFilterTransformationcreateFirstOpSpeccreateFirstTransformationcreateHistogramOpSpeccreateHistogramTransformationcreateHourSelectionOpSpeccreateHourSelectionTransformationcreateIntegralOpSpeccreateIntegralTransformationcreateKeepOpSpeccreateKeyValuesOpSpeccreateKeyValuesTransformationcreateKeysOpSpeccreateKeysTransformationcreateLastOpSpeccreateLastTransformationcreateLimitOpSpeccreateLimitTransformationcreateMaxOpSpeccreateMaxTransformationcreateMeanOpSpeccreateMeanTransformationcreateMinOpSpeccreateMinTransformationcreateModeOpSpeccreateModeTransformationcreateMovingAverageOpSpeccreateMovingAverageTransformationcreatePivotOpSpeccreatePivotTransformationcreateQuantileOpSpeccreateQuantileTransformationcreateRangeOpSpeccreateRangeTransformationcreateReduceOpSpeccreateReduceTransformationcreateRelativeStrengthIndexOpSpeccreateRelativeStrengthIndexTransformationcreateRenameOpSpeccreateSampleOpSpeccreateSampleTransformationcreateSchemaMutationTransformationcreateShiftOpSpeccreateShiftTransformationcreateSkewOpSpeccreateSkewTransformationcreateSortOpSpeccreateSortTransformationcreateSpreadOpSpeccreateSpreadTransformationcreateStateTrackingOpSpeccreateStateTrackingTransformationcreateStddevOpSpeccreateStddevTransformationcreateSumOpSpeccreateSumTransformationcreateTailOpSpeccreateTailTransformationcreateTripleExponentialDerivativeOpSpeccreateTripleExponentialDerivativeTransformationcreateUnionOpSpeccreateUnionTransformationcreateUniqueOpSpeccreateUniqueTransformationcreateYieldOpSpeccreatekamaOpSpeccreatekamaTransformationcumulativeSumcumulativeSumTransformationdefaultMethoddeprecatedFillTransformationdeprecatedSchemaMutationTransformationderivativeTransformationderivativeUnsortedTimeErrdifferenceTransformationdistinctTransformationdurationArgdurationConvelapsedTransformationemptyArrayemptyObjectequalJoinkeysequalRowKeyserrMissingArgexponentialMovingAverageTransformationfillTransformationfilterTransformationfindColumnCallfindRecordCallfixedWindowTransformationfloatConvgetColumnCallgetColumnColumnArggetColumnTableArggetQuantileIndexgetRecordCallgetRecordIndexArggetRecordTableArggroupKeyForObjectgroupModeBygroupModeExceptgrowColumnhasValidPredecessorshistogramTransformationhourSelectionTransformationinfinityVarintConvintegralTransformationjoinParamskamaKindkamaTransformationkeyValuesTransformationkeysTransformationlimitTransformationlinearBinslinearBinsTypelogarithmicBinslogarithmicBinsTypemergeJoinParentStatemethodEstimateTdigestmethodExactMeanmethodExactSelectormodePopulationmodeSamplemodeTransformationmovingAverageTransformationmutateTablenewChandeMomentumOscillatorOpnewChandeMomentumOscillatorProcedurenewColumnsOpnewColumnsProcedurenewCountOpnewCountProcedurenewCovarianceOpnewCovarianceProcedurenewCumulativeSumOpnewCumulativeSumProcedurenewDerivativeOpnewDerivativeProcedurenewDifferencenewDifferenceOpnewDifferenceProcedurenewDistinctOpnewDistinctProcedurenewDropOpnewDualImplSpecnewDuplicateOpnewElapsedOpnewElapsedProcedurenewExponentialMovingAverageOpnewExponentialMovingAverageProcedurenewFillOpnewFillProcedurenewFilterOpnewFilterProcedurenewFirstOpnewFirstProcedurenewHistogramOpnewHistogramProcedurenewHourSelectionOpnewHourSelectionProcedurenewIntegralnewIntegralOpnewIntegralProcedurenewJoinParamsnewKeepOpnewKeyValuesOpnewKeyValuesProcedurenewKeysOpnewKeysProcedurenewLastOpnewLastProcedurenewLimitOpnewLimitProcedurenewMapOpnewMaxOpnewMaxProcedurenewMeanOpnewMeanProcedurenewMinOpnewMinProcedurenewModeOpnewModeProcedurenewMovingAverageOpnewMovingAverageProcedurenewPivotOpnewPivotProcedurenewPivotTransformation2newQuantileOpnewQuantileProcedurenewRangeOpnewRangeProcedurenewReduceOpnewReduceProcedurenewRelativeStrengthIndexOpnewRelativeStrengthIndexProcedurenewRenameOpnewSampleOpnewSampleProcedurenewSchemaMutationProcedurenewShiftOpnewShiftProcedurenewSkewOpnewSkewProcedurenewSortOpnewSortProcedurenewSpreadOpnewSpreadProcedurenewStateTrackingOpnewStateTrackingProcedurenewStddevOpnewStddevProcedurenewStreamBuffernewSumOpnewSumProcedurenewTailOpnewTailProcedurenewTripleExponentialDerivativeOpnewTripleExponentialDerivativeProcedurenewUnionOpnewUnionProcedurenewUniqueOpnewUniqueProcedurenewWindowOpnewYieldOpnewYieldProcedurenewkamaOpnewkamaProcedurenextCMOnullValueLabelobjectFromRowpivotTableBufferpivotTableGrouppivotTransformationpivotTransformation2preJoinGroupKeysrangeTransformationreduceTransformationrelativeStrengthIndexTransformationrenameColumnrowColschemaFnMutatorschemaFnMutatorParamNameschemaMutationTransformationshiftTransformationsleepsleepFuncsortTransformationstateTrackingTransformationstringConvsubsettableColtableFindtableFindCalltableFindFunctionArgtableFindFunctionGroupKeyArgtableFindStreamArgtailTransformationtimeConvtoStringSettripleExponentialDerivativeTransformationuintConvunionParentStateunionTransformationuniqueTransformationvArgvalidateGroupModevalueToStrpassThroughpassThroughTimedoFirstEMAdoRestselectRowPearsonCorrelationValueLabelxm1ym1xm2ym2xym2allBoundsstartColstopColnewWindowGroupKeyclipBoundsgetWindowBoundsgenerateWindowsWithinBoundsselectFirstcomputeGroupKeyDurationColumnDurationUnitTimeColnotEmptydoNumericBinsKeyColumnsWithFitvalidateTablegetColumnKeepColsDropColsFlipPredicateshouldDropshouldDropColkeepToDropColsNonNegativeKeepFirstminSetmaxSetcountColumndurationColumndurationUnitlimitTableUsePreviouscreateRangeGroupKeynextColnextRowcolKeyMapsrowKeyMapsnextRowColmutatorsm2consumedSetParentsmergeKeygroupOndoStringdoBooldoIntdoUIntdoFloatdoTimeselectLasthwtgetCleanDataoperationsTableNamesupdateFloatinterpolateStartinterpolateStopnonNegativekeepFirstpIntValuepUIntValuepFloatValueupdateIntupdateUIntcheckKeepFirstMergeKeyrenameColcheckColumnssortedKeyuintValfloatValsumIntsumUIntsumFloatleftIDrightIDschemasintersectioncolIndexschemaMappostJoinKeysreverseLookupcanEvictTablesinsertIntoBufferregisterKeyisBufferEmptypostJoinSchemaBuiltbuildPostJoinSchemapostJoinGroupKeyleftNamerightNamefillColumnfillIntColumnfillUintColumnfillFloatColumnfillBooleanColumnfillStringColumnfillTimeColumnfillTablepassthroughdoIntsdoUintsdoFloatsdoStringsdoBoolsdoTimesnextKERmergeKeysbuildColumnmergeIntKeysforEachIntbuildColumnFromIntsbuildIntColumnFromIntsbuildUintColumnFromIntsbuildFloatColumnFromIntsbuildBooleanColumnFromIntsbuildStringColumnFromIntsbuildTimeColumnFromIntsmergeUintKeysforEachUintbuildColumnFromUintsbuildIntColumnFromUintsbuildUintColumnFromUintsbuildFloatColumnFromUintsbuildBooleanColumnFromUintsbuildStringColumnFromUintsbuildTimeColumnFromUintsmergeFloatKeysforEachFloatbuildColumnFromFloatsbuildIntColumnFromFloatsbuildUintColumnFromFloatsbuildFloatColumnFromFloatsbuildBooleanColumnFromFloatsbuildStringColumnFromFloatsbuildTimeColumnFromFloatsbuildIntColumnFromBooleansbuildUintColumnFromBooleansbuildFloatColumnFromBooleansbuildBooleanColumnFromBooleansbuildStringColumnFromBooleansbuildTimeColumnFromBooleansmergeStringKeysforEachStringbuildColumnFrombuildIntColumnFrombuildUintColumnFrombuildFloatColumnFrombuildBooleanColumnFrombuildStringColumnFrombuildTimeColumnFrommergeTimeKeysforEachTimebuildColumnFromTimesbuildIntColumnFromTimesbuildUintColumnFromTimesbuildFloatColumnFromTimesbuildBooleanColumnFromTimesbuildStringColumnFromTimesbuildTimeColumnFromTimesdoPivotTDigestCentroidListmaxProcessedmaxUnprocessedprocessedcumulativeprocessedWeightunprocessedWeightAddCentroidListAddCentroidupdateCumulativeCDFintegratedQintegratedLocationkeepEmptyTablescanFilterByKeyfilterByKeyfilterTablem1m3getTableKeyappendTablegroupByRowappendValueFromRowselectSampleValueDstcomputeQuantileTestingBenchmarkCallsTestingInspectCallsTestingRunCallsgenCallstestStmtVisitorSchemaMonoTypeTableMonoTypegithub.com/influxdata/flux/values/objectsAllLabelsBinaryFuncSignatureBuildObjectBuildObjectWithSizeCheckKindConvertDurationMonthsFormattedScopeFromDurationValuesIsTimeableLookupBinaryFunctionNewArrayWithBackingNewDurationNewFunctionNewNestedScopeNewObjectWithValuesNewRegexpNewUIntObjectSetterUnexpectedKindValueStringerallLabelsbinaryFuncLookupbinaryFuncNullCheckfalseValueisLeapYearlabelSetlastDayOfMonthslessThanHalfparseSignedDurationscopeFormattertrueValuereturnValueisAllLabelsDelimitedMultiResultEncoderEncoderErrorErrorDocURLFmtJSONFunctionValueGetDependenciesGroupModeExceptIDerOpSpecNewEmptyDependenciesNewMapResultIteratorNumberOfOperationsOperationSpecNewFnScopeMutatorSemanticTypeSetNowOptionTablesParameterToQueryTimegenerateNowFuncisEncoderErrorkindToOpmapResultIteratornewAdministrationqueryResultIteratorqueryTracingContextKeysliceResultIteratortableKindKeytableParentsKeytableSpecKeyunmarshalOpSpeccreateOpSpecformatJSONformatDAGreleasedMatchedRouteKeyParamsKeyfindWildcardhandleWithFullPathlongestCommonPrefixmatchKeyparamsKeygithub.com/influxdata/influxdb/v2/chronograf/cmd/chronoctlgithub.com/influxdata/influxdb/v2/chronograf/cmd/chronografgithub.com/influxdata/influxdb/v2/chronograf/integrationsgithub.com/influxdata/influxdb/v2/cmd/chronograf-migratorgithub.com/influxdata/influxdb/v2/cmd/influxgithub.com/influxdata/influxdb/v2/cmd/influx_inspect/buildtsigithub.com/influxdata/influxdb/v2/cmd/influx_inspect/verify/seriesfilegithub.com/influxdata/influxdb/v2/cmd/influx_inspect/verify/tombstonegithub.com/influxdata/influxdb/v2/cmd/influx_inspect/verify/tsmgithub.com/influxdata/influxdb/v2/cmd/influxd/internal/profilegithub.com/influxdata/influxdb/v2/cmd/influxdgithub.com/influxdata/influxdb/v2/cmd/telemetrydgithub.com/influxdata/influxdb/v2/dashboards/testinggithub.com/influxdata/influxdb/v2/dbrp/mocksgithub.com/influxdata/influxdb/v2/fluxinit/staticgithub.com/influxdata/influxdb/v2/http/mockgithub.com/influxdata/influxdb/v2/http/mocksgithub.com/influxdata/influxdb/v2/influxql/mockgithub.com/influxdata/influxdb/v2/influxql/query/mocksgithub.com/influxdata/influxdb/v2/internal/cmd/kvmigrategithub.com/influxdata/influxdb/v2/internal/testutilgithub.com/influxdata/influxdb/v2/internalgithub.com/influxdata/influxdb/v2/kit/prom/promtestgithub.com/influxdata/influxdb/v2/kit/tracing/testinggithub.com/influxdata/influxdb/v2/notification/endpoint/service/testinggithub.com/influxdata/influxdb/v2/notification/endpoint/testinggithub.com/influxdata/influxdb/v2/pkg/deepgithub.com/influxdata/influxdb/v2/pkg/lifecyclegithub.com/influxdata/influxdb/v2/pkg/mincoregithub.com/influxdata/influxdb/v2/pkg/testttpgithub.com/influxdata/influxdb/v2/query/influxql/spectestsgithub.com/influxdata/influxdb/v2/query/mockgithub.com/influxdata/influxdb/v2/query/promqlgithub.com/influxdata/influxdb/v2/query/querytestgithub.com/influxdata/influxdb/v2/storage/mocksgithub.com/influxdata/influxdb/v2/task/backend/executor/mockgithub.com/influxdata/influxdb/v2/task/mockgithub.com/influxdata/influxdb/v2/task/servicetestgithub.com/influxdata/influxdb/v2/telegraf/service/testinggithub.com/influxdata/influxdb/v2/tests/pipelinegithub.com/influxdata/influxdb/v2/tools/tmplgithub.com/influxdata/influxdb/v2/tsdb/cursors/mockgithub.com/influxdata/influxdb/v2/uuidgithub.com/influxdata/influxdb/v2/v1/authorization/mocksgithub.com/influxdata/influxdb/v2/v1/monitorgithub.com/influxdata/influxql/internalALTERANALYZEASASCBADESCAPEBADREGEXBADSTRINGBEGINBOUNDPARAMBYBinaryExprNameBoundParameterCARDINALITYCONTINUOUSCREATECloneRegexLiteralContainsVarRefDATABASEDATABASESDESCDESTINATIONSDIAGNOSTICSDISTINCTDOUBLECOLONDROPDataTypeFromStringDateFormatDeleteStatementENDEVERYEXACTEXPLAINErrInvalidTimeFIELDFORGRANTSGROUPGROUPSINFINSERTINTOIdentNeedsQuotesInspectDataTypeKEYKEYSKILLLIMITMEASUREMENTMEASUREMENTSMustParseExprMustParseStatementNilLiteralOFFSETONORDERPOLICIESPOLICYPRIVILEGESParseTreeQUERIESQUERYQuoteIdentQuoteStringREPLICATIONRESAMPLERETENTIONRewriterSELECTSEMICOLONSETSHARDSHARDSSHOWSLIMITSOFFSETSTATSSUBSCRIPTIONSUBSCRIPTIONSSanitizeScanBareIdentScanDelimitedScanStringTAGTypeErrorUSERSVALUESZoneValuerasLiteralbinaryExprNameVisitorcloneSourcecloneSourcesconditionExprcontainsVarRefVisitordateStringRegexpdateTimeStringRegexperrBadEscapeerrBadStringisDateStringisDateTimeStringisFalseLiteralisIdentCharisIdentFirstCharisTrueLiteralkeywordBegkeywordEndkeywordsliteralBegliteralEndmatchExactRegexmatchRegexmultiTypeMappermultiValuernewBufScannernewParseErrornilTypeMapperoperatorBegoperatorEndqiReplacerqsReplacerreduceBinaryExprreduceBinaryExprBooleanLHSreduceBinaryExprDurationLHSreduceBinaryExprIntegerLHSreduceBinaryExprNilLHSreduceBinaryExprNumberLHSreduceBinaryExprStringLHSreduceBinaryExprTimeLHSreduceBinaryExprUnsignedLHSreduceCallreduceParenExprreduceVarRefrewriterFuncsanitizeCreatePasswordsanitizeSetPasswordstringSetSlicetargetNotRequiredtargetRequiredtargetSubquerytokstrvalidateFieldwalkFuncVisitorwalkNameswalkRefszeroBooleanzeroDurationzeroFloat64zeroInt64zeroStringzeroUint64FoundfoundInvalidbadTokenSub0Rune0MaxCapCapNamescapNamesSimplifyErrNeedMoreSpaceErrNoFieldsFieldErrorFieldSortOrderFieldTypeSupportMetricErrorNoSortFieldsUintSupportescapesnameEscapenameEscapernameEscapesstringFieldEscapestringFieldEscaperstringFieldEscapesfieldSortOrderfieldTypeSupportfailOnFieldErrormaxLineBytesfieldListfooterSetMaxLineBytesSetFieldSortOrderSetFieldTypeSupportFailOnFieldErrbuildFieldPairbuildFootergithub.com/influxdata/line-protocolErrWeightLessThanZeroNewCentroidListNewWithCompressionprocessedSizeunprocessedSizeweightedAverageweightedAverageSortedtdigestgithub.com/influxdata/tdigestSimpleErrorStatsDataValidationErrorsErrCommandRequiredErrDuplicatedFlagErrExpectedArgumentErrInvalidChoiceErrInvalidTagErrMarshalErrNoArgumentForBoolErrNotPointerToStructErrRequiredErrShortNameTooLongErrTagErrUnknownErrUnknownCommandErrUnknownFlagErrUnknownGroupHelpFlagIgnoreUnknownIniCommentDefaultsIniDefaultIniErrorIniIncludeCommentsIniIncludeDefaultsIniNoneIniOptionsIniParseIniParserNewIniParserNewNamedParserPassAfterNonOptionPassDoubleDashargumentIsOptionargumentStartsOptionclosestChoicecommandListcompletioncompletionscompletionsWithoutDescriptionsconvertMarshalconvertToStringconvertUnmarshaldefaultLongOptDelimiterdefaultNameArgDelimiterdefaultShortOptDelimiterdistanceBetweenOptionAndDescriptionformatForMangetTerminalColumnsiniSectioniniValueisPrintisStringFalsylevenshteinmanQuotemaxCommandLengthnewErrornewErrorfnewMultiTagoptionIniNamepaddingBeforeOptionquoteIfNeededquoteIfNeededVquoteVreadFullLinereadInireadIniFromFilesplitOptionstrArgumentstripOptionPrefixtIOCGWINSZunquoteIfPossiblewinsizewrapTextwriteCommandIniwriteGroupIniwriteIniwriteIniToFilewriteManPageCommandwriteManPageOptionswriteManPageSubcommandswriteOptionxpixelypixelParseAsDefaultsmatchingGroupsMarshalFlagskipPositionalcompleteOptionNamescompleteNamesForLongPrefixcompleteNamesForShortPrefixcompleteCommandscompleteValueASTAndExpressionASTComparatorASTCurrentNodeASTEmptyASTExpRefASTFieldASTFilterProjectionASTFlattenASTFunctionExpressionASTIdentityASTIndexASTIndexExpressionASTKeyValPairASTLiteralASTMultiSelectHashASTMultiSelectListASTNodeASTNotExpressionASTOrExpressionASTPipeASTProjectionASTSliceASTSubexpressionASTValueProjectionJMESPathLexerNewLexer_astNodeType_index_astNodeType_name_tokType_index_tokType_nameargSpecastNodeTypebasicTokensbindingPowersbyExprFloatbyExprStringcapSlicecomputeSliceParamsexpReffunctionCallerfunctionEntryidentifierStartBitsidentifierTrailingBitsisFalseisSliceTypejpAnyjpArrayjpArrayNumberjpArrayStringjpExprefjpFunctionjpNumberjpObjectjpStringjpTypejpUnknownjpfAbsjpfAvgjpfCeiljpfContainsjpfEndsWithjpfFloorjpfJoinjpfKeysjpfLengthjpfMapjpfMaxjpfMaxByjpfMergejpfMinjpfMinByjpfNotNulljpfReversejpfSortjpfSortByjpfStartsWithjpfSumjpfToArrayjpfToNumberjpfToStringjpfTypejpfValuesnewFunctionCallernewInterpreterobjsEqualsliceParamtAndtColontCommatCurrenttDottEOFtEQtExpreftFiltertFlattentGTtGTEtJSONLiteraltLTtLTEtLbracetLbrackettLparentNEtNottNumbertOrtPipetQuotedIdentifiertRbracetRbrackettRparentStartStringLiteraltUnknowntUnquotedIdentifiertoArrayNumtoArrayStrtokTypetokensOneOftreeInterpreterwhiteSpacetokenTypevariadictypeCheckhasExpRefresolveArgsfunctionTableCallFunctionPrettyPrintfCallintrfieldFromStructflattenWithReflectionsliceWithReflectionfilterProjectionWithReflectionprojectWithReflectionjphasErrorHighlightLocationSpecifiedcurrentPoslastWidthconsumeUntilconsumeLiteralconsumeRawStringLiteralmatchOrElseconsumeLBracketconsumeQuotedIdentifierconsumeUnquotedIdentifierconsumeNumberparseExpressionparseIndexExpressionparseSliceExpressionlednudparseMultiSelectListparseMultiSelectHashprojectIfSliceparseFilterparseDotRHSparseProjectionRHSlookaheadlookaheadTokensyntaxErrorTokenjmespathgithub.com/jmespath/go-jmespathErrUnsupportedValueType_hex_logfmtPooladdFieldsbufferpoolgetEncoderliteralEncoderlogfmtEncoderneedsQuotedValueRuneputEncoderaddSeparatorappendFloatsafeAddStringsafeAddByteStringtryAddRuneSelftryAddRuneErrorOidT__abstimeT__aclitemT__bitT__boolT__boxT__bpcharT__byteaT__charT__cidT__cidrT__circleT__cstringT__dateT__daterangeT__float4T__float8T__gtsvectorT__inetT__int2T__int2vectorT__int4T__int4rangeT__int8T__int8rangeT__intervalT__jsonT__jsonbT__lineT__lsegT__macaddrT__moneyT__nameT__numericT__numrangeT__oidT__oidvectorT__pathT__pg_lsnT__pointT__polygonT__recordT__refcursorT__regclassT__regconfigT__regdictionaryT__regnamespaceT__regoperT__regoperatorT__regprocT__regprocedureT__regroleT__regtypeT__reltimeT__textT__tidT__timeT__timestampT__timestamptzT__timetzT__tintervalT__tsqueryT__tsrangeT__tstzrangeT__tsvectorT__txid_snapshotT__uuidT__varbitT__varcharT__xidT__xmlT_abstimeT_aclitemT_anyT_anyarrayT_anyelementT_anyenumT_anynonarrayT_anyrangeT_bitT_boolT_boxT_bpcharT_byteaT_charT_cidT_cidrT_circleT_cstringT_dateT_daterangeT_event_triggerT_fdw_handlerT_float4T_float8T_gtsvectorT_index_am_handlerT_inetT_int2T_int2vectorT_int4T_int4rangeT_int8T_int8rangeT_internalT_intervalT_jsonT_jsonbT_language_handlerT_lineT_lsegT_macaddrT_moneyT_nameT_numericT_numrangeT_oidT_oidvectorT_opaqueT_pathT_pg_attributeT_pg_auth_membersT_pg_authidT_pg_classT_pg_databaseT_pg_ddl_commandT_pg_lsnT_pg_node_treeT_pg_procT_pg_shseclabelT_pg_typeT_pointT_polygonT_recordT_refcursorT_regclassT_regconfigT_regdictionaryT_regnamespaceT_regoperT_regoperatorT_regprocT_regprocedureT_regroleT_regtypeT_reltimeT_smgrT_textT_tidT_timeT_timestampT_timestamptzT_timetzT_tintervalT_triggerT_tsm_handlerT_tsqueryT_tsrangeT_tstzrangeT_tsvectorT_txid_snapshotT_unknownT_uuidT_varbitT_varcharT_voidT_xidT_xmlgithub.com/lib/pq/oidb64newHashclientNonceserverNoncesaltedPassauthMsgSetNoncestep1step2step3saltPasswordserverSignaturescramgithub.com/lib/pq/scramArrayDelimiterBoolArrayByteaArrayCopyInSchemaDialOpenDialerContextEdebugEfatalEinfoElogEnableInfinityTsEnoticeEpanicErrChannelAlreadyOpenErrChannelNotOpenErrCouldNotDetectUsernameErrInFailedTransactionErrNotSupportedErrSSLKeyHasWorldPermissionsErrSSLNotSupportedErrorClassEventCallbackTypeEwarningFormatTimestampGenericArrayListenerConnListenerEventConnectedListenerEventConnectionAttemptFailedListenerEventDisconnectedListenerEventReconnectedListenerEventTypeNewDialListenerNewListenerConnPGErrorParseTimestampParseURLQuoteIdentifierQuoteLiteralalnumLowerASCIIappendArrayappendArrayElementappendArrayQuotedBytesappendEncodedTextappendEscapedTextbinaryDecodebinaryEncodeciBufferFlushSizeciBufferSizecolFmtDataAllBinarycolFmtDataAllTextconnStateExpectReadyForQueryconnStateExpectResponseconnStateIdledecideColumnFormatsdecodeUUIDBinarydefaultDialerdisableInfinityTsemptyRowsencodeByteaerrBinaryCopyNotSupportederrCopyInClosederrCopyInProgresserrCopyNotSupportedOutsideTxnerrCopyToNotSupportederrInvalidTimestamperrListenerClosederrListenerConnClosederrNoLastInsertIDerrNoRowsAffectederrRecoverNoErrBadConnerrUnexpectedReadyerrorCodeNamesfieldDescfmterrorfformatTextformatTsglobalLocationCacheinfinityTsEnabledinfinityTsEnabledAlreadyinfinityTsNegativeinfinityTsNegativeMustBeSmallerinfinityTsPositiveisDriverSettingisUTF8locationCachemd5smustParsenewDialListenerConnnewLocationCacheparameterStatusparseByteaparseEnvironparseOptsparsePortalRowDescribeparseStatementRowDescribeparseTsrecvNotificationrowsHeaderscanLinearArraysslsslCertificateAuthoritysslClientCertificatessslKeyPermissionssslVerifyCertificateAuthoritytextDecodetimestampParsertransactionStatustxnStatusIdletxnStatusIdleInTransactiontxnStatusInFailedTransactiontypeByteSlicetypeDriverValuertypeSQLScanneruserCurrentBePidDialTimeoutserverVersioncurrentLocationnameitxnStatustxnFinishdialerprocessIDsaveMessageTypesaveMessageBufferdisablePreparedBinaryResultbinaryParametersinCopyhandleDriverSettingshandlePgpassisInTransactioncheckIsInTransactioncloseTxngnamesimpleExecsimpleQueryprepareTosendStartupPacketsendSimpleMessagesaveMessagerecv1Bufrecv1startupparseCompletesendBinaryParameterssendBinaryModeQueryprocessParameterStatusprocessReadyForQueryreadReadyForQueryprocessBackendKeyDatareadParseResponsereadStatementDescribeResponsereadPortalDescribeResponsereadBindResponsepostExecuteWorkaroundreadExecuteResponseerrRecoverconnectionLockconnStatesenderLocknotificationChanreplyChanacquireSenderLockreleaseSenderLocklistenerConnLooplistenerConnMainUnlistenUnlistenAllsendSimpleQueryExecSimpleQueryscanBytesrowDataresploopsetBadisErrorSetPrecisionScalecolNamescolTypscolFmtscolFmtDataparamTypsSeverityHintInternalPositionInternalQueryWhereDataTypeNamegetLocationminReconnectIntervalmaxReconnectIntervaleventCallbackreconnectCondconnNotificationChanNotificationChanneldisconnectCleanupresyncemitEventlistenerMainevaluateDestinationSkipSpacesmustAtoiErrorHandlerFuncLoadAllLoadFileLoadFilesLoadMapLoadStringLoadURLLoadURLsLogFatalHandlerLogHandlerFuncLogPrintfMustLoadAllMustLoadFileMustLoadFilesMustLoadStringMustLoadURLMustLoadURLsNewPropertiesatUnicodeLiteraldecodeEscapedCharacterencodeIsoencodeUtf8expandNameintRangeCheckinvalidKeyErroris32BitisArrayisEOFisEOLisEndOfKeyisEscapeisEscapedCharacterisFloatisMapisPtrisStructisUintitemCommentitemKeyitemValuelexBeforeKeylexBeforeValuelexKeymaxExpansionDepthuintRangeCheckutf8DefaultlastPosappendRuneacceptRunUntilisNotEmptyscanEscapeSequencescanUnicodeLiteralIgnoreMissingLoadBytesloadBytesgithub.com/magiconair/propertiesNewColorableNewColorableStderrNewColorableStdoutNewNonColorableNonColorablecolorablegithub.com/mattn/go-colorableIsCygwinTerminalioctlReadTermiosDefaultConditionEastAsianWidthFillLeftFillRightIsAmbiguousWidthIsEastAsianIsNeutralWidthNewConditionRuneWidthStringWidthZeroWidthJoinerambiguouscombiningdoublewidthemojihandleEnvinTableinTablesisEastAsianmblenTableneutralnonprintnotassignedreLocstringWidthstringWidthZeroJoinerrunewidthgithub.com/mattn/go-runewidthWriteDelimitederrInvalidVarintAndroidChromeFacebookExternalHitFirefoxGooglebotIOSInternetExplorerLinuxMacOSOperaOperaMiniOperaTouchSafariTwitterbotVivaldiWindowsWindowsPhonecheckVerfindVersionrxMacOSVerexistsAnyfindMacOSVersionfindBestMatchDisableCachecacheLockdirUnixdirWindowshomedirCachehomedirgithub.com/mitchellh/go-homedirComposeDecodeHookFuncDecodeHookExecDecodeHookFuncKindDecodeHookFuncTypeDecodeMetadataStringToIPHookFuncStringToIPNetHookFuncStringToSliceHookFuncStringToTimeDurationHookFuncStringToTimeHookFuncWeakDecodeWeakDecodeMetadataWeaklyTypedHookappendErrorsgetKindtypedDecodeHookdecodeBasicdecodeMapFromSlicedecodeMapFromMapdecodeMapFromStructdecodeFuncdecodeStructFromMapmapstructuregithub.com/mitchellh/mapstructurebackupNamebackupTimeFormatbyFormatTimechowncompressLogFilecompressSuffixdefaultMaxSizemegabyteos_StatMaxBackupsLocalTimemillChstartMillopenNewopenExistingOrNewmillRunOncemillRunmilloldLogFilestimeFromNameprefixAndExtlumberjackgithub.com/natefinch/lumberjackarrayValTermbcryptPrefixcommentHashStartcommentSlashStartdqStringEnddqStringStartescapeSpecialisKeySeparatorisNumberSuffixitemArrayStartitemIncludeitemMapEnditemMapStartitemVariablekeySepColonkeySepEquallexBlocklexConvenientNumberlexDateAfterYearlexDubQuotedKeylexDubQuotedStringlexFloatStartlexIPAddrlexIncludelexIncludeDubQuotedStringlexIncludeQuotedStringlexIncludeStartlexIncludeStringlexMapDubQuotedKeylexMapEndlexMapKeylexMapKeyEndlexMapKeyStartlexMapQuotedKeylexMapValuelexMapValueEndlexNegNumberlexNegNumberStartlexNumberOrDateOrIPlexNumberOrDateOrIPStartlexQuotedKeylexQuotedStringlexStringBinarylexTopValueEndmapEndmapStartmapValTermoptValTermpkeysqStringEndsqStringStarttopOptStarttopOptTermtopOptValTermstringPartsstringStateFnemitStringaddCurrentStringPartaddStringParthasEscapedPartskeyCheckKeywordisVariablectxspushContextpopContextpushKeypopKeyprocessItemlookupVariablegithub.com/nats-io/gnatsd/confGetSysLoggerTagNewFileLoggerNewRemoteSysLoggerNewStdLoggerNewSysLoggerSetSyslogNameSysLoggergetNetworkAndAddrpidPrefixsetColoredLabelFormatssetPlainLabelFormatsserverConnEmergAlertCritWarningNoticewriteAndRetryinfoLabelerrorLabelfatalLabeldebugLabeltraceLabelgithub.com/nats-io/gnatsd/loggerProcUsagepsegithub.com/nats-io/gnatsd/server/pseACCEPT_MAX_SLEEPACCEPT_MIN_SLEEPAUTH_TIMEOUTAuthenticationTimeoutAuthenticationViolationBadClientProtocolVersionByCidByIdleByInBytesByInMsgsByLastByOutBytesByOutMsgsByPendingByReasonByStartByStopBySubsByUptimeCLIENTCONNECT_ARGCR_LFClientClosedClientProtoInfoClientProtoZeroCommandQuitCommandReloadCommandReopenCommandStopConProtoConfigureOptionsConnAllConnClosedConnInfosConnOpenConnzPathDEFAULT_FLUSH_DEADLINEDEFAULT_HOSTDEFAULT_HTTP_PORTDEFAULT_MAX_CLOSED_CLIENTSDEFAULT_MAX_CONNECTIONSDEFAULT_PING_INTERVALDEFAULT_PING_MAX_OUTDEFAULT_PORTDEFAULT_REMOTE_QSUBS_SWEEPERDEFAULT_ROUTE_CONNECTDEFAULT_ROUTE_DIALDEFAULT_ROUTE_RECONNECTDefaultConnListSizeDefaultSubListSizeDuplicateRouteErrAuthTimeoutErrAuthorizationErrBadClientProtocolErrClientConnectedToRoutePortErrConnectionClosedErrInvalidSubjectErrMaxControlLineErrMaxPayloadErrReservedPublishSubjectErrTooManyConnectionsErrTooManySubsExplicitFlagSnapshotGenTLSConfigINFO_ARGImplicitInfoProtoIsValidLiteralSubjectIsValidSubjectLEN_CR_LFMAX_CONTROL_LINE_SIZEMAX_MSG_ARGSMAX_PAYLOAD_SIZEMAX_PENDING_SIZEMAX_PUB_ARGSMINUS_ERR_ARGMSG_ARGMSG_ENDMSG_PAYLOADMaxConnectionsExceededMaxControlLineExceededMaxPayloadExceededMergeOptionsNewSublistOP_COP_COOP_CONOP_CONNOP_CONNEOP_CONNECOP_CONNECTOP_IOP_INOP_INFOP_INFOOP_MOP_MINUSOP_MINUS_EOP_MINUS_EROP_MINUS_ERROP_MINUS_ERR_SPCOP_MSOP_MSGOP_MSG_SPCOP_POP_PIOP_PINOP_PINGOP_PLUSOP_PLUS_OOP_PLUS_OKOP_POOP_PONOP_PONGOP_PUOP_PUBOP_PUB_SPCOP_SOP_STARTOP_SUOP_SUBOP_SUB_SPCOP_UOP_UNOP_UNSOP_UNSUOP_UNSUBOP_UNSUB_SPCPROTOPROTO_SNIPPET_SIZEPUB_ARGPrintAndDiePrintServerAndExitPrintTLSHelpAndDieProcessCommandLineArgsProcessSignalProtocolViolationQRSIDQRSID_LENROUTERRSIDRemoveSelfReferenceResponseHandlerRootPathRouteRemovedRoutesFromStrRoutezPathSUB_ARGServerShutdownSetProcessNameSlowConsumerPendingBytesSlowConsumerWriteDeadlineStackszPathStaleConnectionSubszPathTLSConfigOptsTLSHandshakeErrorTLS_TIMEOUTUNSUB_ARGVERSIONVarzPathWrongPort_CRLF__EMPTY_addLocalSubaddNodeToResultsasciiNineasciiZeroauthOptionauthTimeoutOptionauthorizationOptionbtsepbyCidbyIdlebyInBytesbyInMsgsbyLastbyOutBytesbyOutMsgsbyPendingbyReasonbyStopbySubsbyUptimecheckSubjectArraycipherMapcipherMapByIDclientAdvertiseOptionclusterOptioncomparePasswordsconnectInfoconnectReceivedcopyResultcurvePreferenceMapdebugOptiondecodeUint64defaultStackBufSizediffRoutesfindQSliceForSubfirstPongSentformatURLgetInterfaceIPsgetURLIPgitCommithandshakeCompletehostPortisBcryptisIPInListisWindowsServicelntlogfileOptionloggingOptionlogtimeOptionmatchLevelmatchLiteralmaxBufSizemaxConnOptionmaxControlLineOptionmaxPayloadOptionmaxPermCacheSizemaxPingsOutOptionmaxResultCacheSizemergeRoutesminBufSizemsgHeadProtomsgHeadProtoLenmsgScratchSizemyUptimeneedFlushnewClosedRingBuffernewLevelnoopOptionnumCoresoverrideClusteroverrideTLSparseAuthorizationparseCipherparseClusterparseCurvePreferencesparseHostPortparseListenparseOldPermissionStyleparseRouteQueueSidparseSizeparseSubjectPermissionparseSubjectsparseTLSparseUserPermissionsparseUsersparseVariablePermissionspasswordOptionpgreppidFileOptionpingIntervalOptionplistMinportsFileDirOptionprocessNameprocessOptionsprocessSignalprotoSnippetpruneSizeremoteSyslogOptionresolveHostPortsresolvePidsrouteSeenrouteSidroutesOptionsecondsToDurationshortsToShrinkslCacheMaxslCacheSweepsplitArgsubProtosubjectIsLiteralsyslogOptiontlsCiphertlsOptiontlsTimeouttlsTimeoutOptiontlsUsagetlsVersiontraceOptiontsepunsubProtoupdateUsageurlsAreEqualusernameOptionusersOptionvalidateClusterOptsvisitLevelwriteDeadlineOptionCertFileKeyFileCaFileCiphersdefaultPermissionsoldValueCloneTLSConfigutilgithub.com/nats-io/gnatsd/utilDefaultEncoderJsonEncoderfalseBnilBtrueBjegegithub.com/nats-io/go-nats/encoders/builtingithub.com/nats-io/go-nats/utilCloseResponseConnectResponseErrIntOverflowProtocolErrInvalidLengthProtocolPingResponseStartPosition_FirstStartPosition_LastReceivedStartPosition_NewOnlyStartPosition_SequenceStartStartPosition_TimeDeltaStartStartPosition_nameStartPosition_valueSubscriptionResponseencodeVarintProtocolfileDescriptorProtocolskipProtocolsovProtocolsozProtocolPubPrefixSubRequestsUnsubRequestsCloseRequestsSubCloseRequestsPingRequestsgithub.com/nats-io/go-nats-streaming/pbConnectWaitDefaultACKPrefixDefaultAckWaitDefaultConnectWaitDefaultDiscoverPrefixDefaultMaxInflightDefaultMaxPubAcksInflightDefaultNatsURLDefaultPingIntervalDefaultPingMaxOutDefaultSubscriptionOptionsDeliverAllAvailableErrBadAckErrBadConnectionErrBadSubscriptionErrCloseReqTimeoutErrConnectReqTimeoutErrManualAckErrMaxPingsErrNilMsgErrNoServerSupportErrSubReqTimeoutErrUnsubReqTimeoutPingsPubAckWaitStartAtSequenceStartAtTimeStartAtTimeDeltaStartWithLastReceivedprotocolOnetestAllowMillisecInPingsNUIDpreincresetSequentialRandomizePrefixconnIDpubPrefixsubRequestsunsubRequestssubCloseRequestscloseRequestsackSubjectackSubscriptionhbSubscriptionsubMappubAckMappubAckChanncOwnedpubNUIDconnLostCBpingMupingSubpingTimerpingBytespingRequestspingInboxpingIntervalpingMaxOutpingOutpingServerprocessPingResponsecloseDueToPingcleanupOnCloseprocessHeartBeatpublishAsyncremoveAckqgroupinboxackInboxcloseOrUnsubscribeAUTHORIZATION_ERRAsyncSubscriptionCLOSEDCONNECTEDCONNECTINGChanSubscriptionClosedHandlerDEFAULT_ENCODERDISCONNECTEDDRAINING_PUBSDRAINING_SUBSDefaultDrainTimeoutDefaultMaxChanLenDefaultMaxPingOutDefaultMaxReconnectDefaultReconnectBufSizeDefaultReconnectWaitDefaultSubPendingBytesLimitDefaultSubPendingMsgsLimitDefaultURLDisconnectHandlerDiscoveredServersHandlerDontRandomizeEncodedConnEncoderForTypeErrBadSubjectErrBadTimeoutErrChanArgErrClientIDNotSupportedErrConnectionDrainingErrConnectionReconnectingErrDrainTimeoutErrInvalidArgErrInvalidConnectionErrInvalidContextErrInvalidMsgErrJsonParseErrMaxMessagesErrMultipleTLSConfigsErrNkeyAndUserErrNkeyButNoSigCBErrNkeysNotSupportedErrNoEchoNotSupportedErrNoInfoReceivedErrNoServersErrNoUserCBErrReconnectBufExceededErrSecureConnRequiredErrSecureConnWantedErrSlowConsumerErrStaleConnectionErrSyncSubRequiredErrTokenAlreadySetErrTypeSubscriptionErrUserButNoSigCBGOB_ENCODERInboxPrefixJSON_ENCODERLangStringMaxReconnectsNewEncodedConnNewInboxNilSubscriptionNkeyOptionFromSeedNoReconnectOP_INFO_SPCPERMISSIONS_ERRRECONNECTINGReconnectHandlerRegisterEncoderRequestChanLenSTALE_CONNECTIONSetCustomDialerSyncSubscriptionUserCredentials_ERR_OP__INFO_OP__OK_OP__PONG_OP__PUB_P__SPC_argInfoargsLenMaxascii_0ascii_9chPublishclientProtoInfoconProtodefaultPortStringemptyMsgTypeencLockencMapflushChanSizeglobalTimerPoolhostIsIPinboxPrefixLennkeyPairFromSeedFilenormalizeErrnscDecoratedRenuidSizeokProtopingProtopongProtoprocessUrlStringrdigitsreplySuffixLenrespInboxPrefixLenrespTokenscratchSizesigHandlersrvPoolSizetimeoutWritertimerPooltlsSchemeuserFromFilewipeSliceBindSendChanBindRecvChanBindRecvQueueChanbindRecvChanKeyPairWipeLogPrefixNewStanLoggergithub.com/nats-io/nats-streaming-server/loggerChannelsPathChannelszChannelzClientsPathClientszClientzDefaultClientHBTimeoutDefaultClosePrefixDefaultClusterIDDefaultHeartBeatIntervalDefaultIOBatchSizeDefaultIOSleepTimeDefaultLogCacheSizeDefaultLogSnapshotsDefaultMaxFailedHeartBeatsDefaultPubPrefixDefaultStoreTypeDefaultSubClosePrefixDefaultSubPrefixDefaultTrailingLogsDefaultUnSubPrefixErrChanDelInProgressErrClusteredRestartErrDupDurableErrInvalidAckWaitErrInvalidClientErrInvalidClientIDErrInvalidCloseReqErrInvalidConnReqErrInvalidDurNameErrInvalidMaxInflightErrInvalidPubReqErrInvalidStartErrInvalidSubErrInvalidSubReqErrInvalidUnsubReqErrMissingClientErrNoChannelErrUnknownClientFTActiveFTStandbyNewNATSOptionsRunServerServerPathServerzStandaloneStorePathStorezSubscriptionzbyChannelNamebyClientIDbyExpirebySeqchannelInterestclientCheckTimeoutclientIDRegExclusterSetupForTestcomputeAckWaitconfBucketconnectRequestProtoconnectResponseProtocreateNATSTransportcreateSubSentAndAckcreateSubscriptionzdefaultAcksPrefixdefaultCheckDupCIDTimeoutdefaultClientCheckTimeoutdefaultJoinRaftGroupTimeoutdefaultLazyReplicationIntervaldefaultMonitorListLimitdefaultRaftCommitTimeoutdefaultRaftElectionTimeoutdefaultRaftHBTimeoutdefaultRaftLeaseTimeoutdefaultRaftPrefixdefaultSnapshotPrefixdefaultSubStartChanLenerrKeyNotFoundfindBestQueueSubforceDeliveryftDefaultHBIntervalftDefaultHBMissedIntervalftGetRandomIntervalftHBPrefixftNoPanicftPauseBeforeFirstAttemptftPauseChftReleasePausegetBytesgetMinMaxOffsetgetMonitorChannelSubsgetMonitorClientgetMonitorClientSubsgetOffsetAndLimithonorMaxInFlightioChannelSizejoinRaftGroupTimeoutlazyReplicationIntervallogsBucketmakeSortedPendingMsgsmakeSortedSequencesnatsAddrnatsConnnatsConnectInboxnatsRequestInboxnatsStreamLayernewChannelStorenewClientStorenewNATSStreamLayernewNATSTransportnewNATSTransportWithConfignewNATSTransportWithLoggernewRaftLognewTimeoutReaderparseChannelLimitsparseFileOptionsparsePerChannelLimitsparseSQLOptionsparseStoreLimitspartitionsDefaultRequestTimeoutpartitionsDefaultWaitOnTopologyChangepartitionsNoPanicpartitionsPrefixpartitionsRequestTimeoutpartitionsWaitOnChangependingMsgraftLogFileraftLoggerreplicateAckreplicateSentreplicatedSubrunningInTestsserverSnapshotshrinkSubListIfNeededsubStateTraceCtxtestAckWaitIsInMillisecondtestDeleteChanneltestPauseAfterNewRaftCalledtimeoutReadertraceSubStateupdateChannelzwaitForReplicationErrResponsecloseFuncoutboxmsgHandlernewNATSConnQueueNameIsOfflinePendingCountIsStalledTotalMsgsTotalBytesHBInboxisRemoveisUnsubscribeisGroupEmptystartTracesnapshotClientssnapshotChannelsFirstSeqLastSeqAddSubscriptionClientDeleteCtrlMsgCtrlMsg_ConnCloseCtrlMsg_FTHeartbeatCtrlMsg_PartitioningCtrlMsg_SubCloseCtrlMsg_SubUnsubscribeCtrlMsg_TypeCtrlMsg_Type_nameCtrlMsg_Type_valueRaftJoinRequestRaftJoinResponseRaftOperationRaftOperation_CloseSubscriptionRaftOperation_ConnectRaftOperation_DeleteChannelRaftOperation_DisconnectRaftOperation_PublishRaftOperation_RemoveSubscriptionRaftOperation_SendAndAckRaftOperation_SubscribeRaftOperation_Type_nameRaftOperation_Type_valueSubStateDeleteSubStateUpdateOpTypePublishBatchUnsubSubSentAckClientConnectClientDisconnectSeqnoNodeAddrMsgTypeRefIDspbgithub.com/nats-io/nats-streaming-server/spbAllOptionsDefaultFileStoreOptionsDefaultSQLStoreOptionsDefaultStoreLimitsErrAlreadyExistsErrTooManyChannelsFileMsgStoreFileStoreOptionFileStoreTestSetBackgroundTaskIntervalFileSubStoreMemoryMsgStoreMemorySubStoreNewRaftStoreNewSQLStoreRaftStoreRaftSubStoreSQLAllOptionsSQLMaxOpenConnsSQLMsgStoreSQLNoCachingSQLStoreSQLStoreOptionSQLSubStoreSliceConfigTypeFileTypeRaftTypeSQLaddClientbakSuffixbeforeFileClosebkgTaskMubkgTaskRefsbkgTasksSleepDurationbufShrinkIntervalbufShrinkThresholdbufferedMsgbufferedWritercacheTTLcachedMsgchannelRecoveryCtxcheckFileVersionclientsFileNamecommonStorecrcSizecreateFilesManagerdatSuffixdefaultBkgTasksSleepDurationdefaultBufShrinkIntervaldefaultCacheTTLdefaultFileFlagsdefaultSliceCloseIntervaldelClientdriverMySQLdriverPostgresdroppingMsgsFmtdumpBytesemptySuberrNeedRewindfileClosedfileClosingfileIDfileInUsefileOpenedfileRemovedfileSlicefileVersionfilesManagerfmClosedgenericMsgStoregenericStoregenericSubStoregetChannelLimitsPrintLinesgetGlobalLimitsPrintLinesgetLimitStrgetTempFileidxSuffixinitParalleRecoveryinitSQLStmtsinitSQLStmtsTableinvalidFileIDlimitByteslimitCountlimitDurationlockFileNamemsgBufMinShrinkSizemsgFilesPrefixmsgIndexmsgIndexRecSizemsgRecordOverheadmsgsCachenewBufferWriteropenFileWithFlagsrecNoTyperecordHeaderSizerecoveredChannelrepeatCharserverFileNamesliceCloseIntervalsqlAddChannelsqlAddClientsqlAddServerInfosqlCachedMsgsqlCheckMaxSubssqlCreateSubsqlDBLocksqlDBLockInsertsqlDBLockSelectsqlDBLockUpdatesqlDecodeSeqssqlDefaultExpirationIntervalOnErrorsqlDefaultLockLostCountsqlDefaultLockUpdateIntervalsqlDefaultMaxPendingAckssqlDefaultSubStoreFlushIdleIntervalsqlDefaultSubStoreFlushIntervalsqlDefaultTimeTickIntervalsqlDeleteChannelDelChannelsqlDeleteChannelDelSomeMessagessqlDeleteChannelDelSubsPendingsqlDeleteChannelDelSubscriptionssqlDeleteChannelFastsqlDeleteChannelGetSomeMessagesSeqsqlDeleteChannelGetSubIdssqlDeleteClientsqlDeleteMessagesqlDeleteSubMarkedAsDeletedsqlDeleteSubPendingMessagessqlDeleteSubscriptionsqlDeletedMsgsWithSeqLowerThansqlEncodeSeqssqlExpirationIntervalOnErrorsqlGetExpiredMessagessqlGetFirstMsgTimestampsqlGetLastSeqsqlGetSequenceFromTimestampsqlGetSizeOfMessagesqlHasServerInfoRowsqlLockLostCountsqlLockUpdateIntervalsqlLookupMsgsqlMarkSubscriptionAsDeletedsqlMaxPendingAckssqlMsgsCachesqlNoPanicsqlRecoverChannelMsgssqlRecoverChannelSubssqlRecoverChannelsListsqlRecoverClientssqlRecoverDoExpireMsgssqlRecoverDoPurgeSubsPendingsqlRecoverGetChannelLimitssqlRecoverGetChannelTotalSizesqlRecoverGetMessagesCountsqlRecoverGetSeqFloorForMaxBytessqlRecoverGetSeqFloorForMaxMsgssqlRecoverMaxChannelIDsqlRecoverMaxSubIDsqlRecoverServerInfosqlRecoverSubPendingsqlRecoverUpdateChannelLimitssqlSeqArrayPoolsqlSeqMapPoolsqlStmtErrorsqlStmtssqlStoreMsgsqlSubAcksPendingsqlSubAcksPendingCachesqlSubAddPendingsqlSubAddPendingRowsqlSubDeletePendingsqlSubDeletePendingRowsqlSubStoreFlushIdleIntervalsqlSubStoreFlushIntervalsqlSubUpdateLastSentsqlSubsPendingRowsqlTimeTickIntervalsqlUpdateChannelMaxSeqsqlUpdateServerInfosqlUpdateSubsqlVersionsubBufMinShrinkSizesubRecAcksubRecDelsubRecMsgsubRecNewsubRecUpdatesubStoresFlushersubsFileNametruncateBadEOFFileNamewriteRecordseqnossublistgssetLimitsgetChannelLimitsdeleteChannelcanAddChannelbeforeCloseopenedFDsrootDircloseUnusedFilescreateFilecloseLockedFilecloseFileIfOpenedcloseLockedOrOpenedFiledoCloselockFilelockFileIfOpenedunlockFiletrySwitchStatesetBeforeCloseCbtruncateFileLockFileserverFileclientsFilecompactItvldelClientReccliFileSizecliDeleteRecscliCompactTSrecoverOneChannelrecoverServerInfoshouldCompactClientFilecompactClientFilehandleUnexpectedEOFnewFileMsgStorenewFileSubStoretransferToFreeListtotalCounttotalByteshitLimitgmsisOwnermsgsRefsacksRefsprevLastSentmsgToRowackToRowneedsFlushmaxSubIDchannelIDsqlStorehasMarkedAsDelsubLastSentgetOrCreateAcksPendingaddSeqackSeqdeleteSubPendingRowrecoverPendingRowsignalednowInNanomaxChannelIDpreparedStmtsssFlusherupdateDBLockacquireDBLockreleaseDBLockIfOwnerscheduleSubStoreFlushnewSQLMsgStorenewSQLSubStorecreatePreparedStmtsapplyLimitsOnRecoverydeepChannelDeletetimeTickexpireTimerwriteCachecreateExpireTimerexpireMsgsidxFilefirstSeqlastSeqrmCountmsgsCountmsgsSizefirstWritelastUsedgssminShrinkSizeshrinkReqcreateNewWritertryShrinkBuffercheckShrinkRequestfstoretmpSubBufdelSubupdateSubfileSizenumRecsdelRecscompactTSshrinkTimerallDoneshrinkBufferrecoverSubscriptionsshouldCompactpoolChrecoverChmsgSizeageTimerremoveFirstMsgtryEvictseqMapscheckSlicestmpMsgBufhasFDsLimitchannelNamefirstFSlSeqlastFSlSeqslCountLimslSizeLimslAgeLimslHasLimitswOffsetfirstMsglastMsgbufferedSeqsbufferedMsgsbkgTasksDonebkgTasksWakebeforeDataFileCloseCbbeforeIndexFileCloseCbsetFiledoLockFileslockFileslockIndexFileunlockIndexFileunlockFilescloseLockedFilesrecoverOneMsgFileensureLastMsgAndIndexMatchsetSliceLimitswriteIndexaddIndexfillGapsprocessBufferedMsgsenforceLimitsgetMsgIndexreadMsgIndexremoveFirstSlicegetFileSliceForSeqbackgroundTasksinitCacheBackoffTimeCheckCloseFileCreateLockFileDecodeChannelsEnsureBufBigEnoughErrUnableToLockNowFriendlyBytesIsChannelNameLiteralIsChannelNameValidNewBackoffTimeCheckRaceEnabledSendChannelsListencodeChannelsRequestencodedChannelLengetSubjectsremoveFromListsfwcshrinkAsNeededspwcfrequencyminFrequencymaxFrequencyfactorgithub.com/nats-io/nats-streaming-server/utilCreateAccountCreateOperatorCreatePairCreateServerDecodeSeedEncodeSeedErrCannotSignErrInvalidChecksumErrInvalidPrefixByteErrInvalidSeedErrInvalidSeedLenErrInvalidSignatureErrPublicKeyOnlyFromPublicKeyFromRawSeedFromSeedIsValidEncodingIsValidPublicAccountKeyIsValidPublicClusterKeyIsValidPublicKeyIsValidPublicOperatorKeyIsValidPublicServerKeyIsValidPublicUserKeyPrefixBytePrefixByteAccountPrefixByteClusterPrefixByteOperatorPrefixBytePrivatePrefixByteSeedPrefixByteServerPrefixByteUknownPrefixByteUserb32EnccheckValidPrefixBytecheckValidPublicPrefixBytecrc16crc16tabrawSeednkeysglobalNUIDlockedNUIDmaxIncmaxSeqminIncpreLenseqLentotalLennuidALIGN_DEFAULTALIGN_RIGHTBgBlackColorBgBlueColorBgCyanColorBgGreenColorBgHiBlackColorBgHiBlueColorBgHiCyanColorBgHiGreenColorBgHiMagentaColorBgHiRedColorBgHiWhiteColorBgHiYellowColorBgMagentaColorBgRedColorBgWhiteColorBgYellowColorCENTERCOLUMNConditionStringDisplayWidthESCFgBlackColorFgBlueColorFgGreenColorFgHiBlackColorFgHiMagentaColorFgHiRedColorFgHiWhiteColorFgHiYellowColorFgMagentaColorFgWhiteColorFgYellowColorMAX_ROW_WIDTHNEWLINENewCSVNewCSVReaderNormalPadLeftPadRightROWSEPSPACEUnderlineSingleWrapStringWrapWordsansidefaultPenaltyfooterRowIdxgetLinesheaderRowIdxisNumOrSpacemakeSequencepercentstartFormatstopFormatComponentDBInstanceDBStatementDBTypeDBUserHTTPUrlMessageBusDestinationPeerAddressPeerHostIPv4PeerHostIPv6PeerHostnamePeerPortPeerServiceSamplingPrioritySpanKindSpanKindConsumerSpanKindConsumerEnumSpanKindEnumSpanKindProducerSpanKindProducerEnumSpanKindRPCClientSpanKindRPCClientEnumSpanKindRPCServerSpanKindRPCServerEnumboolTagNameipv4TagrpcServerOptionspanKindTagNamestringTagNameuint16TagNameuint32TagNameclientContextNoopfloat32TypelazyLoggerTypenoopTypeuint32TypeErrInvalidCarrierErrInvalidSpanContextErrSpanContextCorruptedErrSpanContextNotFoundErrUnsupportedFormatFollowsFromIsGlobalTracerRegisteredNoopTracerStartSpanFromContextWithTracerTextMapCarrieractiveSpanKeydefaultNoopSpandefaultNoopSpanContextdefaultNoopTraceremptyStringglobalTracernoopSpannoopSpanContextregisteredTracerisRegisteredLoadReaderSetOptionsTreeFromMapcallCustomMarshalercleanupNumberTokendateRegexpencOptsDefaultsencodeMultilineTomlStringencodeTomlStringformatErrorhexNumberContainsInvalidUnderscorehexNumberUnderscoreInvalidRegexpisAlphanumericisCustomMarshalerisHexDigitisKeyCharisKeyStartCharisOtherSliceisPrimitiveisTreeisTreeSliceisValidBareCharisValidBinaryRuneisValidHexRuneisValidOctalRunekindToTypelexTomlnewTreenumberContainsInvalidUnderscorenumberUnderscoreInvalidRegexpparseKeyparseTomlsimpleValueCoercionsliceToTreetagKeyMultilinetoTreetokenColontokenCommenttokenDatetokenDotDottokenDoubleLeftBrackettokenDoubleRightBrackettokenEOFtokenEOLtokenEqualtokenFloattokenInftokenIntegertokenIsCommatokenKeytokenKeyGrouptokenKeyGroupArraytokenLeftBrackettokenLeftCurlyBracetokenLeftParentokenNantokenQuestiontokenRightBrackettokenRightCurlyBracetokenRightParentokenStartokenTypeNamestomlLexStateFntomlLexertomlOptionstomlOptstomlParsertomlParserStateFntomlValuetomlValueStringRepresentationtypeForvalidRuneFnwriteStringsMarshalTOMLCommentedMultilineColcommentedHasPathGetPositionPathSetWithOptionsSetPathWithOptionsSetWithCommentSetPathWithCommentcreateSubTreeToTomlStringToMapinputIdxcurrentTokenStartcurrentTokenStopendbufferLineendbufferColfastForwardemitWithValuepeekStringfollowlexVoidlexRvaluelexLeftCurlyBracelexRightCurlyBracelexDatelexTruelexFalselexInflexNanlexEquallexCommalexLeftBracketlexLiteralStringAsStringlexLiteralStringlexStringAsStringlexTableKeylexInsideTableArrayKeylexInsideTableKeylexRightBracketflowIdxcurrentTableseenTableKeysraiseErrorassumeparseStartparseGroupArrayparseGroupparseAssignparseRvalueparseInlineTablequoteMapKeysarraysOneElementPerLinetvalvalueFromTreevalueFromTreeSlicevalueFromOtherSlicevalueFromTomlunwrapPointerQuoteMapKeysArraysWithOneElementPerLinevalueToTreevalueToTreeSlicevalueToOtherSlicevalueToTomlgithub.com/pelletier/go-tomlDefaultReaderSizeDefaultWriterSizeminReaderSizeminWriterSizeunsafestrpushbackfwdChecksumZeroUint32ZeroXXHZeroprime32_1prime32_1plus2prime32_2prime32_3prime32_4prime32_5prime32_minus1rol13rol17bufusedxxh32github.com/pierrec/lz4/internal/xxh32CompressBlockCompressBlockBoundCompressBlockHCErrInvalidSourceShortBufferUncompressBlockblockHashbsMapIDbsMapValuecompressedBlockFlagcompressedBlockMaskdebugFlagframeMagicframeSkipMagichashLoghashShifthashTableSizemfLimitminMatchskipStrengthwinMaskwinSizewinSizeLogBlockChecksumNoChecksumBlockMaxSizezdatahashtablecompressBlocklz4github.com/pierrec/lz4OpenURLopenBrowsersetFlagsbrowsergithub.com/pkg/browserWithMessageWithMessagefWithStackfuncnamefundamentalwithMessagewithStackformatSliceContextDiffGetContextDiffStringGetUnifiedDiffStringNewMatcherWithJunkSequenceMatcherSplitLinesUnifiedDiffWriteContextDiffWriteUnifiedDiffcalculateRatioformatRangeContextformatRangeUnifiedFromDateToFileToDateEolI1I2J1J2b2jIsJunkautoJunkbJunkmatchingBlocksfullBCountbPopularopCodesSetSeqsSetSeq1SetSeq2chainBisBJunkfindLongestMatchGetMatchingBlocksGetOpCodesGetGroupedOpCodesRatioQuickRatioRealQuickRatiodifflibgithub.com/pmezard/go-difflib/difflibNormalizeMetricFamiliesmetricSortergithub.com/prometheus/client_golang/prometheus/internalHTTPErrorOnErrorInstrumentHandlerCounterInstrumentHandlerDurationInstrumentHandlerInFlightInstrumentHandlerRequestSizeInstrumentHandlerResponseSizeInstrumentHandlerTimeToWriteHeaderInstrumentMetricHandlerInstrumentRoundTripperCounterInstrumentRoundTripperDurationInstrumentRoundTripperInFlightInstrumentRoundTripperTraceInstrumentTraceRoundTripperFuncacceptEncodingHeadercheckLabelscloseNotifiercloseNotifierDelegatorcomputeApproximateRequestSizecontentEncodingHeadercontentTypeHeaderdelegatoremptyLabelsflusherDelegatorgzipAcceptedgzipPoolhijackerhijackerDelegatorhttpErrorisLabelCurriedmagicStringnewDelegatorpickDelegatorpusherpusherDelegatorreaderFromreaderFromDelegatorresponseWriterDelegatorsanitizeCodesanitizeMethodobserveWriteHeaderWrittenAlreadyRegisteredErrorBuildFQNameCounterFuncDefAgeBucketsDefBucketsDefBufCapDefMaxAgeDefaultGathererDefaultRegistererDescribeByCollectExemplarAdderExemplarMaxRunesExemplarObserverGathererFuncGatherersGaugeFuncLinearBucketsMustNewConstHistogramMustNewConstSummaryNewBuildInfoCollectorNewConstHistogramNewConstMetricNewConstSummaryNewCounterFuncNewExpvarCollectorNewGaugeNewGaugeFuncNewHistogramNewInvalidDescNewInvalidMetricNewMetricWithTimestampNewPedanticRegistryNewProcessCollectorNewUntypedFuncObserverFuncProcessCollectorOptsUntypedFuncUntypedOptsUntypedValueWrapRegistererWithWrapRegistererWithPrefixWriteToTextfilebuckSortbucketLabelcanCollectProcesscapDescChancapMetricChancheckDescConsistencycheckLabelNamecheckMetricConsistencycheckSuffixCollisionsconstHistogramconstMetricconstSummaryerrBucketLabelNotAllowederrInconsistentCardinalityerrQuantileLabelNotAllowedexpvarCollectorextractLabelValuesfindMetricWithLabelValuesfindMetricWithLabelsgoCollectorhashNewhistogramCountsinlineLabelValuesinvalidMetricmakeInconsistentCardinalityErrormakeLabelPairsmatchLabelValuesmatchLabelsmemStatsMetricsmemstatNamespacenewExemplarnewHistogramnewMetricVecnewSummarynewValueFuncnoObjectivesSummarypopulateMetricprocessCollectorprocessMetricquantSortquantileLabelreservedLabelPrefixselfCollectorseparatorByteSlicesummaryCountstimestampedMetricvalidateLabelValuesvalidateValuesInLabelsvalueFuncwrapDescwrappingCollectorwrappingMetricwrappingRegistererwrappedMetricExistingCollectorNewCollectorvalBitsvalIntexemplarAddWithExemplarupdateExemplarsumBitscountAndHotIdxwriteMtxMaybeUnwrapwrappedCollectorunwrapRecursivelyObserveWithExemplarPidFnReportErrorsObserveDurationbufMtxobjectivessortedObjectiveshotBufcoldBufstreamsstreamDurationheadStreamheadStreamIdxheadStreamExpTimehotBufExpTimeasyncFlushmaybeRotateStreamsflushColdBufswapBufsexportsupperBoundsexemplarsgoroutinesDescthreadsDescgcDescgoInfoDescmsLastmsLastTimestampmsMtxmsMetricsmsReadmsMaxWaitmsMaxAgemsCollectquantileswrappedRegisterercollectFnpidFnreportErrorscpuTotalopenFDsmaxFDsmaxVsizerssreportErrorprocessCollectMetricType_nameMetricType_valuefileDescriptor_6039342a2ba47b72xxx_messageInfo_Bucketxxx_messageInfo_Counterxxx_messageInfo_Exemplarxxx_messageInfo_Gaugexxx_messageInfo_Histogramxxx_messageInfo_LabelPairxxx_messageInfo_Metricxxx_messageInfo_MetricFamilyxxx_messageInfo_Quantilexxx_messageInfo_Summaryxxx_messageInfo_Untypedio_prometheus_clientExtractSamplesFinalizeOpenMetricsFmtOpenMetricsFmtProtoCompactFmtProtoTextMetricFamilyToOpenMetricsMetricFamilyToTextNegotiateNegotiateIncludingOpenMetricsOpenMetricsTypeOpenMetricsVersionProtoFmtProtoProtocolProtoTypeSampleDecoderTextVersionencoderCloserenhancedWriterextractCounterextractGaugeextractHistogramextractSamplesextractSummaryextractUntypedhdrAccepthdrContentTypehistogramMetricNameinitialNumBufSizeisBlankOrTabisBucketisCountisSumisValidLabelNameContinuationisValidLabelNameStartisValidMetricNameContinuationisValidMetricNameStartnumBufPoolprotoDecoderquotedEscapersummaryMetricNametextDecoderwriteEscapedStringwriteExemplarwriteFloatwriteIntwriteLabelPairswriteOpenMetricsFloatwriteOpenMetricsLabelPairswriteOpenMetricsSamplewriteSamplewriteUintsdFastFingerprintSampleValuefamsaccept_sliceSubTypegoautoneggithub.com/prometheus/common/internal/bitbucket.org/ww/goautonegAddressLabelAlertFiringAlertNameLabelAlertResolvedAlertStatusAlertsBucketLabelExportedLabelPrefixFingerprintFromStringFingerprintSetFingerprintsInstanceLabelIsValidMetricNameJobLabelLabelNameRELabelPairsLabelSetLabelValuesLabelsToSignatureMatrixMetaLabelPrefixMetricNameLabelMetricNameREMetricsPathLabelParamLabelPrefixParseFingerprintQuantileLabelReservedLabelPrefixSamplePairSampleStreamSchemeLabelSeparatorByteSignatureForLabelsSignatureWithoutLabelsSilenceTimeFromUnixTimeFromUnixNanoTmpLabelPrefixValMatrixValNoneValScalarValStringValVectorZeroSampleZeroSamplePairdotPrecisiondurationREemptyLabelSignaturelabelSetToFastFingerprintlabelSetToFingerprintminimumTicknanosPerTickIsRegexMatchersStartsAtEndsAtCreatedByGeneratorURLResolvedResolvedAtMarshalYAMLmatHasFiringgithub.com/prometheus/common/modelDefaultConfigfsMountPointDefaultProcMountPointDefaultSysMountPointNewFSgithub.com/prometheus/procfs/internal/fsNewValueParserParsePInt64sParseUint32sParseUint64sReadFileNoStatReadUintFromFileSysReadFileValueParserPInt64PUInt64github.com/prometheus/procfs/internal/utilARPEntryAllProcsBuddyInfoCPUInfoCPUStatCryptoDefaultMountPointGetMountsGetProcMountsIPVSBackendStatusIPVSStatsInotifyInfoMDStatMeminfoMountInfoMountStatsMountStatsNFSNFSBytesStatsNFSEventsStatsNFSOperationStatsNFSTransportStatsNamespacesNetDevNetDevLineNetSockstatNetSockstatProtocolNetUnixNetUnixFlagsNetUnixLineNetUnixStateNetUnixTypeNewDefaultFSNewNetUnixNewNetUnixByPathNewNetUnixByReaderNewProcNewStatNewXfrmStatPSILinePSIStatsProcProcFDInfoProcFDInfosProcIOProcLimitsProcSchedstatProcStatProcStatusProcsSchedstatSchedstatCPUSoftIRQStatSoftnetEntryXfrmStatZoneinfocpuLineREdeviceEntryLenerrInvalidKernelPtrFmtevalRecoveryLineevalStatusLinefieldBytesLenfieldEventsLenfieldTransport10TCPLenfieldTransport10UDPLenfieldTransport11TCPLenfieldTransport11UDPLenlimitsDelimiterlimitsFieldslimitsUnlimitedlineFormatmountOptionsIsValidFieldmountOptionsParseOptionalFieldsmountOptionsParsernetUnixFlagListennetUnixFlagsIdxnetUnixInodeIdxnetUnixKernelPtrIdxnetUnixRefCountIdxnetUnixStateConnectednetUnixStateConnectingnetUnixStateDisconnectednetUnixStateIdxnetUnixStateUnconnectednetUnixStaticFieldsCntnetUnixTypeDgramnetUnixTypeIdxnetUnixTypeSeqpacketnetUnixTypeStreamnewNetDevnodeZoneREparseARPEntriesparseARPEntryparseBuddyInfoparseCPUInfoparseCPUStatparseCryptoparseIPPortparseIPVSBackendStatusparseIPVSStatsparseInotifyInfoparseMDStatparseMemInfoparseMountparseMountInfoparseMountInfoStringparseMountStatsparseMountStatsNFSparseNFSBytesStatsparseNFSEventsStatsparseNFSOperationStatsparseNFSTransportStatsparsePSIStatsparseProcSchedstatparseSockstatparseSockstatKVsparseSockstatProtocolparseSoftIRQStatparseSoftnetEntriesparseSoftnetEntryparseZoneinfoprocLineRErFlagsrInotifyrMntIDrPosreadSockstatrecoveryLineREstatVersion10statVersion11statusLineREuserHZprocGatherARPEntriesNetSockstat6GatherSoftnetStatsPSIStatsForResourceWDSdevMntIDInotifyInfosInotifyWatchLenCommPPIDPGRPTTYTPGIDMinFltCMinFltMajFltCMajFltUTimeSTimeCUTimeCSTimeNiceNumThreadsStarttimeVSizeRSSVirtualMemoryResidentMemoryCPUTimeMountIdParentIdMajorMinorVerOptionalFieldsFSTypeSuperOptionsIowaitIRQSoftIRQStealGuestGuestNiceHWAddrmountStatsTransmissionsMajorTimeoutsCumulativeQueueMillisecondsCumulativeTotalResponseMillisecondsCumulativeTotalRequestMillisecondsRunningNanosecondsWaitingNanosecondsRunTimeslicesInodeRevalidateDnodeRevalidateDataInvalidateAttributeInvalidateVFSOpenVFSLookupVFSAccessVFSUpdatePageVFSReadPageVFSReadPagesVFSWritePageVFSWritePagesVFSGetdentsVFSSetattrVFSFlushVFSFsyncVFSLockVFSFileReleaseCongestionWaitTruncationSillyRenameShortReadShortWriteJukeboxDelayPNFSReadPNFSWriteNrFreePagesScannedSpannedPresentManagedNrActiveAnonNrInactiveAnonNrIsolatedAnonNrAnonPagesNrAnonTransparentHugepagesNrActiveFileNrInactiveFileNrIsolatedFileNrFilePagesNrSlabReclaimableNrSlabUnreclaimableNrMlockStackNrKernelStackNrMappedNrDirtyNrWritebackNrUnevictableNrShmemNrDirtiedNrWrittenNumaHitNumaMissNumaForeignNumaInterleaveNumaLocalNumaOtherTWStackSizeCoreFileSizeResidentSetOpenFilesLockedMemoryAddressSpaceFileLocksPendingSignalsMsqqueueSizeNicePriorityRealtimePriorityRealtimeTimeoutRxBytesRxPacketsRxErrorsRxDroppedRxFIFORxFrameRxCompressedRxMulticastTxBytesTxPacketsTxErrorsTxDroppedTxFIFOTxCollisionsTxCarrierTxCompressedDirectReadDirectWriteReadTotalWriteTotalReadPagesWritePagesProcessedTimeSqueezedIncomingPacketsOutgoingPacketsIncomingBytesOutgoingBytesKernelPtrInodeActivityStateDisksActiveDisksTotalDisksFailedDisksSpareBlocksTotalBlocksSyncedparseLineparseKernelPtrparseTypeparseFlagsparseInodeCmdLineCwdRootDirFileDescriptorsFileDescriptorTargetsFileDescriptorsLenfileDescriptorsFileDescriptorsInfoFDInfoIONewLimitsNewStatusTGIDVmPeakVmSizeVmLckVmPinVmHWMVmRSSRssAnonRssFileRssShmemVmDataVmStkVmExeVmLibVmPTEVmPMDVmSwapHugetlbPagesVoluntaryCtxtSwitchesNonVoluntaryCtxtSwitchesfillStatusTotalCtxtSwitchesAvg10Avg60Avg300SomeFullNetTxNetRxBlockIoPollTaskletSchedHrtimerRcuAdminReserveKbytesBlockDumpCompactUnevictableAllowedDirtyBackgroundBytesDirtyBackgroundRatioDirtyBytesDirtyExpireCentisecsDirtyRatioDirtytimeExpireSecondsDirtyWritebackCentisecsDropCachesExtfragThresholdHugetlbShmGroupLaptopModeLegacyVaLayoutLowmemReserveRatioMaxMapCountMemoryFailureEarlyKillMemoryFailureRecoveryMinFreeKbytesMinSlabRatioMinUnmappedRatioMmapMinAddrNrHugepagesNrHugepagesMempolicyNrOvercommitHugepagesNumaStatNumaZonelistOrderOomDumpTasksOomKillAllocatingTaskOvercommitKbytesOvercommitMemoryOvercommitRatioPageClusterPanicOnOomPercpuPagelistFractionStatIntervalSwappinessUserReserveKbytesVfsCachePressureWatermarkBoostFactorWatermarkScaleFactorZoneReclaimModeMemTotalMemFreeMemAvailableSwapCachedActiveAnonInactiveAnonActiveFileInactiveFileUnevictableMlockedSwapTotalSwapFreeDirtyWritebackAnonPagesMappedShmemSlabSReclaimableSUnreclaimKernelStackPageTablesNFSUnstableBounceWritebackTmpCommitLimitCommittedASVmallocTotalVmallocUsedVmallocChunkHardwareCorruptedAnonHugePagesShmemHugePagesShmemPmdMappedCmaTotalCmaFreeHugePagesTotalHugePagesFreeHugePagesRsvdHugePagesSurpHugepagesizeDirectMap4kDirectMap2MDirectMap1GnetDevVendorIDCPUFamilyModelNameSteppingMicrocodeCPUMHzCacheSizePhysicalIDSiblingsCoreIDCPUCoresAPICIDInitialAPICIDFPUFPUExceptionCPUIDLevelWPBugsBogoMipsCLFlushSizeCacheAlignmentAddressSizesPowerManagementAlignmaskBlocksizeChunksizeCtxsizeDigestsizeGenivIvsizeMaxauthsizeMaxKeysizeMinKeysizeRefcntSeedsizeSelftestWalksizeLocalAddressRemoteAddressLocalPortRemotePortLocalMarkActiveConnInactConnBindConnectIdleTimeIdleTimeSecondsSendsReceivesBadTransactionIDsCumulativeActiveRequestsCumulativeBacklogMaximumRPCSlotsUsedCumulativeSendingQueueCumulativePendingQueueCPUNumCPUsSizesBootTimeCPUTotalIRQTotalContextSwitchesProcessCreatedProcessesRunningProcessesBlockedSoftIRQTotalXfrmInErrorXfrmInBufferErrorXfrmInHdrErrorXfrmInNoStatesXfrmInStateProtoErrorXfrmInStateModeErrorXfrmInStateSeqErrorXfrmInStateExpiredXfrmInStateMismatchXfrmInStateInvalidXfrmInTmplMismatchXfrmInNoPolsXfrmInPolBlockXfrmInPolErrorXfrmOutErrorXfrmOutBundleGenErrorXfrmOutBundleCheckErrorXfrmOutNoStatesXfrmOutStateProtoErrorXfrmOutStateModeErrorXfrmOutStateSeqErrorXfrmOutStateExpiredXfrmOutPolBlockXfrmOutPolDeadXfrmOutPolErrorXfrmFwdHdrErrorXfrmOutStateInvalidXfrmAcquireErrorStatVersionAgeRCharWCharSyscRSyscWCancelledWriteBytesprocfsgithub.com/prometheus/procfsGLOBglobgithub.com/ryanuber/go-globnewRFC4122Generatorrfc4122GeneratorBalancerFuncBrokerAuthorizationFailedBrokerNotAvailableClusterAuthorizationFailedCompressionNoneCodeConcurrentTransactionsConnConfigDefaultClientIDDefaultCompressionLevelDefaultDialerDuplicateSequenceNumberGroupAuthorizationFailedGroupCoordinatorNotAvailableGroupLoadInProgressIllegalGenerationIllegalSASLStateInconsistentGroupProtocolInvalidCommitOffsetSizeInvalidConfigurationInvalidGroupIdInvalidMessageInvalidMessageSizeInvalidPartitionNumberInvalidProducerEpochInvalidProducerIDMappingInvalidReplicaAssignmentInvalidReplicationFactorInvalidRequestInvalidRequiredAcksInvalidSessionTimeoutInvalidTimestampInvalidTopicInvalidTransactionStateInvalidTransactionTimeoutLeaderNotAvailableLeastBytesMessageSizeTooLargeNewConnWithNotControllerNotCoordinatorForGroupNotEnoughReplicasNotEnoughReplicasAfterAppendNotLeaderForPartitionOffsetMetadataTooLargeOffsetOutOfRangeOutOfOrderSequenceNumberPolicyViolationReaderConfigReaderStatsRebalanceInProgressRecordListTooLargeRegisterCompressionCodecReplicaNotAvailableRequestTimedOutRoundRobinSecurityDisabledSeekAbsoluteStaleControllerEpochTopicAlreadyExistsTopicAuthorizationFailedTransactionCoordinatorFencedTransactionalIDAuthorizationFailedUnknownMemberIdUnknownTopicOrPartitionUnsupportedForMessageFormatUnsupportedSASLMechanismUnsupportedVersionWriterStatsacquireCrc32BufferadjustDeadlineForRTTallStrategiesbrokerMetadataV0coalesceErrorscodecscodecsMutexcommitRequestcompressionCodecMaskcrc32Buffercrc32BufferPoolcrc32OfMessagecrc32UpdatecreateTopicsRequestdeadlineToTimeoutdefaultCommitRetriesdefaultHeartbeatIntervaldefaultProtocolTypedefaultRTTdefaultRebalanceTimeoutdefaultRetentionTimedefaultSessionTimeoutdeleteTopicsRequestdescribeGroupsRequestdiffpdiscardBytesdiscardInt16discardInt32discardInt64discardInt8discardNdiscardStringdontExpectEOFerrInvalidWritePartitionerrInvalidWriteTopicerrNotAvailableWithGrouperrOnlyAvailableWithGrouperrShortReadexpectZeroSizeextractTopicsfetchRequestfetchRequestPartitionV1fetchRequestTopicV1fetchRequestV1fetchResponsePartitionV1fetchResponseTopicV1fetchResponseV1findMembersByTopicfindOffsetfindPartitionsfindStrategyfirstOffsetfnv1aPoolgroupAssignmentgroupCoordinatorRequestgroupMetadataheartbeatRequestisTemporaryisTimeoutjoinGroupRequestlastOffsetleastBytesCounterleaveGroupRequestlistGroupsRequestlistOffsetRequestlistOffsetRequestPartitionV1listOffsetRequestTopicV1listOffsetRequestV1listOffsetResponseTopicV1listOffsetResponseV1makeCommitmakeCommitsmakeInt16makeInt32makeInt64makeInt8makeSummarymaxTimeoutmemberGroupAssignmentsmemberGroupMetadatametadataRequestmetadataResponseV0millisecondsoffsetCommitRequestoffsetCommitteroffsetFetchRequestoffsetStashpartitionMetadataV0partitionOffsetV1partitionReaderpeekReadproduceRequestproduceRequestPartitionV2produceRequestTopicV2produceRequestV2produceResponsePartitionV2produceResponseTopicV2produceResponseV2rangeStrategyreadArrayWithreadBytesWithreadFetchResponseHeaderreadMapStringInt32readMessageHeaderreadNewBytesreadNewStringreadPtrreadStringArrayreadStringWithreadablereaderMessagereaderStatsreleaseCrc32BufferroundrobinStrategyrunGroupshuffledStringsshufflershufflerMutexsilentEOFsizablesizeofArraysizeofBoolsizeofBytessizeofInt16sizeofInt32sizeofInt32ArraysizeofInt64sizeofInt8sizeofStringsizeofStringArraysplitHostPortstrategysyncGroupRequesttimestampToTimetopicMetadataRequestV0topicMetadataV0v0writeArraywriteArrayLenwriteFetchRequestV1writeInt32ArraywriteListOffsetRequestV1writeProduceRequestV2writeStringArraywriterErrorUserDataAssignGroupsGroupMetadataMessageSetSizeMessageSetbatchTimeoutPartitionOffsetsReplicaIDDialsWritesRebalancesDialTimeWaitTimeRetriesQueueLengthFetchOffsetFetchesTimeoutsFetchBytesLagMinBytesMaxWaitThrottleTimeReadLagIntervalHeartbeatIntervalCommitIntervalErrorLoggercommitserrchfetchestimeoutsfetchByteslaggenerationIDstctxuseConsumerGroupuseSyncCommitsmembershiplookupCoordinatorrefreshCoordinatormakeJoinGroupRequestV2makeMemberProtocolMetadataassignTopicPartitionsmakeSyncGroupRequestV1syncGroupfetchOffsetswaitThrottleTimeheartbeatLoopcommitOffsetscommitOffsetsWithRetrycommitLoopImmediatecommitLoopIntervalcommitLoopFetchMessageCommitMessagesReadLagwithLoggerwithErrorLoggeractivateReadLagreadLagerrorLoggerminBytesmaxWaitreadOffsetssendMessagesendErrorcounterOfmakeCountersHighwaterMarkOffsetMaxWaitTimePartitionErrorCodeTopicErrorCodeHashergithub.com/segmentio/kafka-goDiffDeleteDiffEqualDiffInsertDiffMatchPatchblanklineEndRegexblanklineStartRegexcommonPrefixLengthcommonSuffixLengthdiffCleanupSemanticScoreindexOflastIndexOflinebreakRegexnonAlphaNumericRegexrunesEqualrunesIndexrunesIndexOfsplicewhitespaceRegexDiffTimeoutDiffEditCostMatchDistancePatchDeleteThresholdPatchMarginMatchMaxBitsMatchThresholdDiffMaindmpDiffMainRunesdiffMainRunesdiffComputediffLineModeDiffBisectdiffBisectdiffBisectSplitDiffLinesToCharsDiffLinesToRunesdiffLinesToRunesdiffLinesToRunesMungeDiffCharsToLinesDiffCommonPrefixDiffCommonSuffixDiffCommonOverlapDiffHalfMatchdiffHalfMatchdiffHalfMatchIDiffCleanupSemanticDiffCleanupSemanticLosslessDiffCleanupEfficiencyDiffCleanupMergeDiffXIndexDiffPrettyHtmlDiffPrettyTextDiffText1DiffText2DiffLevenshteinDiffToDeltaDiffFromDeltaMatchMainMatchBitapmatchBitapScoreMatchAlphabetPatchAddContextPatchMakepatchMake2PatchDeepCopyPatchApplyPatchAddPaddingPatchSplitMaxPatchToTextPatchFromTextdiffsStart1Start2Length1Length2diffmatchpatchgithub.com/sergi/go-diff/diffmatchpatchAuthTypeExternalBrowserAuthTypeJwtAuthTypeOAuthAuthTypeOktaAuthTypeSnowflakeConfigBoolConfigBoolFalseConfigBoolTrueCustomJSONDecoderEnabledDataTypeArrayDataTypeBinaryDataTypeDateDataTypeFixedDataTypeObjectDataTypeRealDataTypeTextDataTypeTimeDataTypeTimestampLtzDataTypeTimestampNtzDataTypeTimestampTzDataTypeVariantErrCodeEmptyAccountCodeErrCodeEmptyPasswordCodeErrCodeEmptyUsernameCodeErrCodeFailedToConnectErrCodeFailedToParseAuthenticatorErrCodeFailedToParseHostErrCodeFailedToParsePortErrCodeIdpConnectionErrorErrCodePrivateKeyParseErrorErrCodeSSOURLNotMatchErrCodeServiceUnavailableErrEmptyAccountErrEmptyPasswordErrEmptyUsernameErrFailedToAuthErrFailedToAuthOKTAErrFailedToAuthSAMLErrFailedToCancelQueryErrFailedToCloseSessionErrFailedToGetChunkErrFailedToGetExternalBrowserResponseErrFailedToGetSSOErrFailedToHeartbeatErrFailedToParseResponseErrFailedToPostQueryErrFailedToRenewSessionErrInvalidBinaryHexFormErrInvalidOffsetStrErrInvalidTimestampTzErrNoDefaultTransactionIsolationLevelErrNoReadOnlyTransactionErrOCSPInvalidValidityErrOCSPNoOCSPResponderURLErrOCSPStatusRevokedErrOCSPStatusUnknownErrObjectNotExistOrAuthorizedErrRoleNotExistErrSessionGoneLocationWithOffsetStringMaxChunkDownloadWorkersOCSPFailOpenFalseOCSPFailOpenModeOCSPFailOpenTrueSQLStateConnectionFailureSQLStateConnectionRejectedSQLStateConnectionWasNotEstablishedSQLStateFeatureNotSupportedSQLStateInvalidDataTimeFormatSQLStateNumericValueOutOfRangeScanSnowflakeParameterSnowflakeDriverSnowflakeErrorSnowflakeGoDriverVersionSnowflakeParameterSnowflakeTransportSnowflakeTransportTestabortRequestPathauthOKTARequestauthOKTAResponseauthRequestauthRequestClientEnvironmentauthRequestDataauthResponseMainauthResponseSessionInfoauthenticateauthenticateByExternalBrowserauthenticateBySAMLauthenticatorRequestPathbinaryTypebindToPortbuildResponsecaRootcaRootPEMcacheDircacheDirEnvcacheExpirecacheFileBaseNamecacheFileNamecacheServerEnabledEnvcacheServerURLcacheServerURLEnvcacheUpdatedcalcTolerableValiditycanEarlyExitForOCSPcancelQuerycancelQueryResponsecertIDcertIDKeycertPoolcheckOCSPCacheServercheckOCSPResponseCachechunkErrorclientInterfaceclientTypecloseSessionconfigBoolNotSetcreateOCSPCacheDirdataTypeModedateTypedebugPanicfdecodeCertIDKeydecodeLargeChunkdefaultChunkBufferSizedefaultClientTimeoutdefaultDomaindefaultJWTTimeoutdefaultLoginTimeoutdefaultOCSPCacheServerTimeoutdefaultOCSPResponderTimeoutdefaultRequestTimeoutdefaultStringBufferSizedefaultWaitAlgodeleteOCSPCachedeleteOCSPCacheAlldeleteOCSPCacheFiledetermineAuthenticatorTypedownloadChunkdownloadChunkHelperdownloadOCSPCacheServerdurationMaxdurationMinencodeCertIDKeyerrMsgFailedToAutherrMsgFailedToAuthOKTAerrMsgFailedToAuthSAMLerrMsgFailedToCancelQueryerrMsgFailedToCloseSessionerrMsgFailedToConnecterrMsgFailedToGetChunkerrMsgFailedToGetExternalBrowserResponseerrMsgFailedToGetSSOerrMsgFailedToParseAuthenticatorerrMsgFailedToParseHosterrMsgFailedToParsePorterrMsgFailedToParseResponseerrMsgFailedToPostQueryerrMsgFailedToRenewerrMsgIdpConnectionErrorerrMsgInvalidByteArrayerrMsgInvalidOffsetStrerrMsgNoDefaultTransactionIsolationLevelerrMsgNoReadOnlyTransactionerrMsgOCSPInvalidValidityerrMsgOCSPNoOCSPResponderURLerrMsgOCSPStatusRevokederrMsgOCSPStatusUnknownerrMsgSSOURLNotMatcherrMsgServiceUnavailableexecBindParameterexecRequestexecResponseexecResponseChunkexecResponseDataexecResponseRowTypeextractCertIDKeyFromRequestextractOCSPCacheResponseValueextractOCSPCacheResponseValueWithoutSubjectextractTimestampfillMissingConfigParametersfixedTypegenTimezonegetAllRevocationStatusgetChunkgetHashAlgorithmFromOIDgetHeadersgetIdpURLProofKeygetOIDFromHashAlgorithmgetRestfulgetRevocationStatusgetSSOgetTokenFromResponseglogglogWrappergoTypeToSnowflakehashOIDsheaderAcceptTypeApplicationSnowflakeheaderAuthorizationKeyheaderContentTypeApplicationJSONheaderSnowflakeTokenheaderSseCAesheaderSseCAlgorithmheaderSseCKeyheartBeatIntervalheartBeatPathinitOCSPCacheisInValidityRangeisPrefixEqualisTestInvalidValidityisTestNoOCSPURLisTestUnknownStatusisValidOCSPStatuslargeChunkDecoderlargeResultSetReaderloginRequestPathmarshalPKCS8PrivateKeymaxChunkDownloaderErrorCountermaxClockSkewnameValueParameternewRequestGUIDReplacenewRetryHTTPnewRetryUpdateocspCacheExpiredocspFailOpenocspFailOpenNotSetocspFailedComposeRequestocspFailedDecodeResponseocspFailedDecomposeRequestocspFailedExtractResponseocspFailedParseOCSPHostocspFailedParseResponseocspFailedResponseocspFailedSubmitocspInvalidValidityocspMissedCacheocspModeFailClosedocspModeFailOpenocspModeInsecureocspNoServerocspResponseCacheocspResponseCacheLockocspStatusocspStatusCodeocspStatusGoodocspStatusOthersocspStatusRevokedocspStatusUnknownocspSuccessocspTestInjectUnknownStatusEnvocspTestInjectValidityErrorEnvocspTestNoOCSPURLEnvocspTestResponderTimeoutEnvocspTestResponderURLEnvocspTestResponseCacheServerTimeoutEnvoperatingSystemoverrideCacheDirparseAccountHostPortparsePKCS8PrivateKeyparseParamsparseTimeoutparseUserPasswordpopulateSnowflakeParameterpostAuthpostAuthOKTApostAuthSAMLpostBackURLpostRestfulpostRestfulQuerypostRestfulQueryHelperprepareJWTTokenqueryInProgressAsyncCodequeryInProgressCodequeryRequestPathraiseDownloadErrorrandSecondDurationreadCACertsrenewRestfulSessionrenewSessionResponserenewSessionResponseMainrequestFuncrequestGUIDKeyrequestGUIDReplacerequestGUIDReplacerrequestIDKeyretryCounterKeyretryCounterUpdateretryCounterUpdaterretryHTTPretryOCSPreturnOCSPStatussessionClientSessionKeepAlivesessionClientValidateDefaultParameterssessionExpiredCodesessionRequestPathsnowflakeChunkDownloadersnowflakeConnsnowflakeInsecureTransportsnowflakeRestfulsnowflakeResultsnowflakeRowssnowflakeStmtsnowflakeTxsnowflakeTypeToGostatementTypeIDDeletestatementTypeIDDmlstatementTypeIDInsertstatementTypeIDMergestatementTypeIDMultiTableInsertstatementTypeIDUpdatestringToValuesuccessHTMLtextTypetimestampLtzTypetimestampNtzTypetimestampTzTypetimezonestoNamedValuestokenRequestPathtolerableValidityRatiotransformAccountToHosttransientReplacetransientReplaceOrAddupdateTimezoneMutexvalidateOCSPvalidateWithCachevalidateWithCacheForAllCertificatesvalueToStringvariantTypeverifyPeerCertificateverifyPeerCertificateSerialwaitAlgowriteOCSPCacheFileSQLStateMessageArgsIncludeQueryIDWarehouseValidateDefaultParametersPasscodePasscodeInPasswordOktaURLLoginTimeoutRequestTimeoutJWTExpireTimeoutInsecureModeOCSPFailOpenocspModerestfulshutdownChanheartbeatMainByteLengthRowTypeReturnedDatabaseProviderFinalDatabaseNameFinalSchemaNameFinalWarehouseNameFinalRoleNameNumberOfBindsStatementTypeIDQrmkChunkHeadersGetResultURLProgressDescQueryAbortTimeoutDatabaseNameWarehouseNameMasterTokenMasterValidityDisplayUserNameServerVersionFirstLoginRemMeTokenRemMeValidityHealthCheckIntervalNewClientForUpgradeSessionIDSessionInfoSSOURLProofKeyCookieTokenHeartBeatFuncPostQueryFuncPostQueryHelperFuncPostFuncGetFuncRenewSessionFuncPostAuthFuncCloseSessionFuncCancelQueryFuncPostAuthSAMLFuncPostAuthOKTAFuncGetSSOgetURLgetFullURLSequenceCounterisDmlpopulateSessionParametersisClientSessionKeepAliveEnabledstartHeartBeatstopHeartBeatTotalRowIndexCellCountCurrentChunkCurrentChunkIndexCurrentChunkSizeChunksMutexChunkMetasChunksChanChunksErrorChunksErrorCounterChunksFinalErrorsChunkHeaderCurrentIndexFuncDownloadFuncDownloadHelperDoneDownloadCondtotalUncompressedSizescdhasNextResultSetcheckErrorRetrySetByUserSetInJobSetByThreadIDSetByThreadNameSetByClassParameterCommentChunkDownloaderurlPtrreplaceOrAddreplaceOrAdderHashAlgorithmNameHashIssuerKeyHashfullURLraise4XXdoRaise4XXdoPostsetBodyisRetryableErrordecorrOsOsVersionOCSPModeClientAppIDClientAppVersionSvnRevisionAccountNameLoginNameRawSAMLResponseExtAuthnDuoMethodSessionParametersClientEnvironmentBrowserModeRedirectPortProducedAtRevokedAtRevocationReasonTBSResponseDataIssuerHashRawResponderNameResponderKeyHashInfolnInfoDepthValidityInSecondsSTValidityInSecondsMTurlValuesSQLTextAsyncExecSequenceIDIsInternalsbufioErrormkErrorlcddecodeRowdecodeCelldecodeEscapedgetu4WithPrefixnextByteNonWhitespacerewindensureBytesfillBuffergosnowflakegithub.com/snowflakedb/gosnowflakeAddToMemDirChangeFileNameCreateDirDirMapErrDestinationExistsErrFileClosedErrFileExistsErrFileNotFoundErrOutOfRangeFileDataFilePathSeparatorGetFileInfoInitializeDirNewFileHandleNewReadOnlyFileHandleRemoveFromMemDirfilesSortersetModTimememDirmodtimereadDirCountfileDatagithub.com/spf13/afero/memAferoBADFDBasePathFileBasePathFsCacheOnReadFsCopyOnWriteFsDirExistsDirsMergerFileContainsAnyBytesFileContainsBytesFullBaseFsPathGetTempDirHttpFsLstaterMemMapFsNeuterAccentsNewBasePathFsNewCacheOnReadFsNewCopyOnWriteFsNewHttpFsNewMemMapFsNewOsFsNewReadOnlyFsNewRegexpFsOsFsReadOnlyFsRegexpFileRegexpFsSafeWriteReaderUnicodeSanitizeUnionFileWriteReadercacheHitcacheLocalcacheMisscacheStalecacheStatecopyToLayerdefaultUnionMergeDirsFnhasMetahttpDirisMnlstatIfPossiblenextSuffixrandmureadDirNamesreaderContainsAnyreseedvalidateBasePathNamematchesNamedirOrMatcheslayerisBaseFileLstatIfPossibleisNotExistLayerRealPathgetDataunRegisterWithParentfindParentregisterWithParentlockfreeMkdiropenWritelockfreeOpencacheTimecacheStatusaferogithub.com/spf13/aferoStringToDateToBoolEToBoolSliceToDurationEToDurationSliceToDurationSliceEToFloat32ToFloat32EToInt16ToInt16EToInt32ToInt32EToInt64ToInt8ToInt8EToIntEToIntSliceToIntSliceEToSliceEToStringMapToStringMapBoolToStringMapBoolEToStringMapEToStringMapIntToStringMapInt64ToStringMapInt64EToStringMapIntEToStringMapStringToStringMapStringEToStringMapStringSliceToStringMapStringSliceEToStringSliceToTimeEToUint16ToUint16EToUint32ToUint32EToUint64ToUint8ToUint8EToUintEerrNegativeNotAllowedindirectToStringerOrErrorjsonStringToObjectparseDateWithAddTemplateFuncAddTemplateFuncsBashCompCustomBashCompFilenameExtBashCompOneRequiredFlagBashCompSubdirsInDirCompDebugCompDebuglnCompErrorCompErrorlnEnableCommandSortingEnablePrefixMatchingGtMinimumNArgsMousetrapDisplayDurationMousetrapHelpTextOnInitializeOnlyValidArgsRangeArgsShellCompDirectiveDefaultShellCompDirectiveErrorShellCompDirectiveNoFileCompShellCompDirectiveNoSpaceShellCompNoDescRequestCmdShellCompRequestCmdappendIfNotPresentargsMinusFirstXcheckIfFlagCompletioncommandSorterByNameescapeStringForPowerShellfindFlagflagCompletionFunctionsgenFishCompgeneratePowerShellSubcommandCasesgetFlagNameCompletionshasNoOptDefValinitializersisFlagArglegacyArgsminCommandPathPaddingminNamePaddingminUsagePaddingnonCompletableFlagpowerShellCompletionTemplatepreExecHookFnprepareCustomAnnotationsForFlagsrpadshortHasNoOptDefValstringInSlicestripFlagstemplateFuncstrimRightSpacewriteArgAliaseswriteCmdAliaseswriteCommandswriteFlagwriteFlagHandlerwriteLocalNonPersistentFlagwritePostscriptwritePreamblewriteRequiredFlagwriteRequiredNounswriteShortFlagzshCompArgumentAnnotationzshCompArgumentFilenameCompzshCompArgumentWordCompzshCompDirnamezshCompExtractArgumentCompletionHintsForRenderingzshCompExtractFlagzshCompFlagCouldBeSpecifiedMoreThenOncezshCompFuncMapzshCompGenFlagEntryExtraszshCompGenFlagEntryForArgumentszshCompGenFlagEntryForMultiOptionFlagzshCompGenFlagEntryForSingleOptionFlagzshCompGenFuncNamezshCompQuoteFlagDescriptionzshCompRenderZshCompArgHintzshCompletionTextFATALFEEDBACKFeedbackGetLogThresholdGetStdoutThresholdLOGLevelCriticalLevelDebugLevelErrorLevelFatalLevelInfoLevelTraceLevelWarnLogCountForLevelLogCountForLevelsGreaterThanorEqualToLogThresholdNewNotepadNotepadResetLogCountersSetLogOutputSetLogThresholdSetStdoutThresholdStdoutThresholdTRACEdefaultNotepadlogCounterreloadDefaultNotepadincrresetCountergetCountloggerslogHandleoutHandlelogThresholdstdoutThresholdlogCountersjwalterweathermangithub.com/spf13/jwalterweathermanPFlagFromGoFlagParseIPv4MaskSliceValueboolSliceConvboolSliceValuebytesBase64ValuebytesBase64ValueConvbytesHexConvbytesHexValuecountConvcountValuedurationSliceConvdurationSliceValueflagValueWrapperfloat32Convfloat32SliceConvfloat32SliceValuefloat64Convfloat64SliceConvfloat64SliceValuegoBoolFlagint16Convint16Valueint32Convint32SliceConvint32SliceValueint64Convint64SliceConvint64SliceValueint8Convint8ValueintSliceConvintSliceValueipConvipMaskValueipNetConvipNetValueipSliceConvipSliceValueipValuenewBoolSliceValuenewBytesBase64ValuenewBytesHexValuenewCountValuenewDurationSliceValuenewFloat32SliceValuenewFloat32ValuenewFloat64SliceValuenewIPMaskValuenewIPNetValuenewIPSliceValuenewIPValuenewInt16ValuenewInt32SliceValuenewInt32ValuenewInt64SliceValuenewInt8ValuenewIntSliceValuenewStringArrayValuenewStringSliceValuenewStringToInt64ValuenewStringToIntValuenewStringToStringValuenewUint16ValuenewUint32ValuenewUint8ValuenewUintSliceValueparseIPv4MaskreadAsCSVstringArrayConvstringArrayValuestringSliceConvstringSliceValuestringToInt64ConvstringToInt64ValuestringToIntConvstringToIntValuestringToStringConvstringToStringValuestripUnknownFlagValueuint16Convuint16Valueuint32Convuint64Convuint8Convuint8ValueuintSliceConvuintSliceValuewrapFlagValuewrapNwriteAsCSVfromStringGetSliceipnetbytesBase64bytesHexConfigFileAlreadyExistsErrorConfigMarshalErrorConfigParseErrorEnvKeyReplacerGetViperKeyDelimiterNewWithOptionsRemoteConfigRemoteConfigErrorRemoteResponseSupportedExtsSupportedRemoteProvidersUnsupportedConfigErrorUnsupportedRemoteProviderErrorabsPathifycastMapFlagToMapInterfacecastMapStringToMapInterfacecastToMapStringInterfacecopyAndInsensitiviseMapdeepSearchdefaultDecoderConfiginsensitiviseMapkeyExistsmergeMapsoptionFuncparseSizeInBytespflagValuepflagValueSetremoteConfigFactorysafeMultoCaseInsensitiveValueuserHomeDirfaeerceWatchChannelAnErrorAssertionsBoolAssertionFuncCallerInfoComparisonAssertionFuncConditionfContainsfDirExistsfElementsMatchElementsMatchfEmptyfEqualErrorEqualErrorfEqualValuesEqualValuesfEqualfErrorAssertionFuncEventuallyEventuallyfExactlyExactlyfFailNowfFailfFalsefFileExistsFileExistsfGreaterOrEqualGreaterOrEqualfGreaterfHTTPBodyHTTPBodyContainsHTTPBodyContainsfHTTPBodyNotContainsHTTPBodyNotContainsfHTTPErrorHTTPErrorfHTTPRedirectHTTPRedirectfHTTPSuccessHTTPSuccessfImplementsfInDeltaInDeltaMapValuesInDeltaMapValuesfInDeltaSliceInDeltaSlicefInDeltafInEpsilonInEpsilonSliceInEpsilonSlicefInEpsilonfIsTypefJSONEqJSONEqfLenfLessOrEqualLessOrEqualfLessfNeverNeverfNilfNoDirExistsNoDirExistsfNoErrorfNoFileExistsNoFileExistsfNotContainsfNotEmptyNotEmptyfNotEqualfNotNilNotNilfNotPanicsNotPanicsfNotRegexpNotRegexpfNotSameNotSamefNotSubsetNotSubsetfNotZerofObjectsAreEqualObjectsAreEqualValuesPanicsPanicsWithErrorPanicsWithErrorfPanicsWithValuefPanicsfRegexpfSameSamefSubsetfValueAssertionFuncWithinDurationWithinDurationfYAMLEqYAMLEqfZerofcalcRelativeErrorcontainsKindfailNowerformatUnequalValuesgetLenhttpCodeincludeElementindentMessageLinesisTestlabeledContentlabeledOutputmatchRegexpmessageFromMsgAndArgssamePointersspewConfigtHelpertoFloattypeAndKindvalidateEqualArgsOverApplyOverLoadStrictParsecheckFormatlinePatternloadenvparseExportparseValparsetsetenvvarReplacementvariablePatterngotenvgithub.com/subosito/gotenvDefaultUIErrEmptyErrNotNumberLineSepdefaultMaskValdefaultReaderdefaultValidateFuncdefaultWritermaskStringAppendArrayHeaderAppendBytesAppendIntfAppendMapHeaderAppendMapStrIntfAppendMapStrStrAppendStringFromBytesArrayErrorArrayHeaderSizeBinTypeBoolSizeByteSizeBytesPrefixSizeComplex128ExtensionComplex128SizeComplex128TypeComplex64ExtensionComplex64SizeComplex64TypeCopyReplaceCopyToJSONDecodableEncodableEndlessReaderErrShortBytesErrUnsupportedTypeExtensionPrefixSizeExtensionTypeErrorFloat32SizeFloat64SizeGuessSizeHasKeyInt16SizeInt32SizeInt64SizeInt8SizeIntOverflowIntTypeInvalidPrefixErrorInvalidTypeLocateMapHeaderSizeMarshalSizerNewEndlessReaderNilSizeNilTypeNowhereRawExtensionReadArrayHeaderBytesReadBoolBytesReadByteBytesReadBytesBytesReadBytesZCReadComplex128BytesReadComplex64BytesReadExtensionBytesReadFloat32BytesReadFloat64BytesReadInt16BytesReadInt32BytesReadInt64BytesReadInt8BytesReadIntBytesReadIntfBytesReadMapHeaderBytesReadMapKeyZCReadMapStrIntfBytesReadNilBytesReadStringBytesReadStringZCReadTimeBytesReadUint16BytesReadUint32BytesReadUint64BytesReadUint8BytesReadUintBytesRequireResumableStrTypeStringPrefixSizeTimeExtensionTimeSizeTimeTypeUint16SizeUint32SizeUint64SizeUint8SizeUintBelowZeroUintOverflowUintSizeUintTypeUnmarshalAsJSONUnsafeBytesUnsafeStringWrapError_maxtypeaddCtxadviseReadadviseWriteappendNextarray16varray32vbadPrefixbigbtsTypebytespecconstsizecontextErrorctxStringdefunsensureerrExterrFatalerrShorterrWrappedextensionRegextra16extra32extra8fallocatefatalfirst3first4freeRfreeWgetMint16getMint32getMint64getMint8getMuint16getMuint32getMuint64getMuint8getNextSizegetUnixisSupportedisfixarrayisfixintisfixmapisfixstrisnfixintjsWriterlast4last5last7locatelocateKVmap16vmap32vmarray16marray32mbin16mbin32mbin8mext16mext32mext8mfixarraymfixext1mfixext16mfixext2mfixext4mfixext8mfixintmfixmapmfixstrmfloat32mfloat64mint16mint32mint64mint8mmap16mmap32mnfixintmnilmstr16mstr32mstr8muint16muint32muint64muint8nwherepeekExtensionprefixu16prefixu32prefixu64prefixu8putMint16putMint32putMint64putMint8putMuint16putMuint32putMuint64putMuint8putUnixreadBytesBytesreaderPoolresizeMapresumableDefaultrfixarrayrfixintrfixmaprfixstrrnfixintrwArrayrwArrayBytesrwBoolrwBoolBytesrwBytesrwBytesBytesrwExtensionrwExtensionBytesrwFloat32rwFloat32BytesrwFloat64rwFloat64BytesrwFloatBytesrwIntrwIntBytesrwMaprwMapBytesrwMapKeyBytesrwNextrwNilrwNullBytesrwStringrwStringBytesrwTimerwTimeBytesrwUintrwUintBytesrwquotedsizessmallintunfunsvarmodewfixarraywfixintwfixmapwfixstrwnfixintwriteNextwriterPoolFailedBitsizeEncodedAsIntAsUintAsFloat32AsFloat64WantErrNotTCPLimitListenerRunWithErrTLSConfigHasHTTP2EnabledenableHTTP2ForTLSConfigkeepAliveConnkeepAliveListenerlimitListenerlimitListenerConnsendSignalIntsignalNotifykeepAlivePeriodreleaseOnceAthenaResultAthenaTimeColsRowsToCSVColsToCSVDDLQueryTimeoutDMLQueryTimeoutDateUniXFormatDefaultBytesScannedCutoffPerQueryDefaultDBNameDefaultRegionDefaultWGNameDriverTracerDummyAccessIDDummyRegionDummySecretAccessKeyErrAthenaNilAPIErrAthenaNilDatumErrAthenaTransactionUnsupportedErrConfigAccessIDRequiredErrConfigAccessKeyRequiredErrConfigInvalidConfigErrConfigOutputLocationErrConfigRegionErrConfigWGPointerErrQueryBufferOFErrQueryTimeoutErrQueryUnknownTypeErrTestMockFailedByAthenaErrTestMockGenericGetDefaultWGConfigGetFromEnvValLoggerKeyMAXQueryStringLengthMetricsKeyNewDefaultWGNewNoOpsConfigNewWGNewWGConfigNewWGTagsNoopsSQLConnectorPoolIntervalRowsToCSVSQLConnectorSQLDriverTContextKeyTimestampUniXFormatWGConfigWGTagsWarnLevelWorkgroupZeroDateTimeStringcolInFirstPagegenHeaderRowgetWGisInsertStatementisQueryTimeOutisQueryValidisReadOnlyStatementmissingDataRowmockRowsToSQLRowsnewColumnInfonewDefaultObservabilitynewHeaderResultPagenewHeaderlessResultPagenewNoOpsObservabilitynewObservabilityparseAthenaTimeparseAthenaTimeWithLocationprintCostrandDaterandFloat32randFloat64randInt16randInt8randRowrandStrrandStringrandTimeStamprandUInt64randomInt64reAccessIDreSecretAccessKeyreSessionTokenscanNullStringvalueToNamedValueSafeStringifySetOutputBucketSetRegionGetRegionSetUserSetDBSetAccessIDGetAccessIDGetSecretAccessKeyGetOutputBucketGetWorkgroupIsMissingAsEmptyStringIsMissingAsDefaultSetMissingAsEmptyStringSetMissingAsDefaultCheckColumnMaskedSetMaskedColumnValueIsWGRemoteCreationAllowedSetWGRemoteCreationAllowedIsLoggingEnabledSetLoggingIsMetricsEnabledSetMetricsSetReadOnlySetMoneyWiseIsMoneyWiseTaggingAsDurationsStopwatchStopwatchRecorderRecordStopwatchRecordDurationRecordValueSubScopeTaggedSetScopeathenaAPInumInputCreateWGRemotelyqueryIDreachedLastPageResultOutputpageCountfetchNextPageathenaTypeToGoTypegetDefaultValueForColumnTypeconnectionlastInsertedIDrowAffectedwgConfigathenadrivergithub.com/uber/athenadriver/goGen128BitMaxTagValueLengthPoolSpansZipkinSharedRPCSpandefaultSamplingProbabilityenvAgentHostenvAgentPortenvDisabledenvEndpointenvPasswordenvRPCMetricsenvReporterFlushIntervalenvReporterLogSpansenvReporterMaxQueueSizeenvSamplerManagerHostPortenvSamplerMaxOperationsenvSamplerParamenvSamplerRefreshIntervalenvSamplerTypeenvServiceNameenvTagsenvUsernullCloserreporterConfigFromEnvsamplerConfigFromEnvNewRestrictionManagerdefaultHostPortdefaultMaxValueLengthdefaultRefreshIntervalhttpBaggageRestrictionManagerProxynewHTTPBaggageRestrictionManagerProxydenyBaggageOnInitializationFailurerefreshIntervalBaggageRestrictionManagerBaggageRestrictionBaggageKeyGetBaggageKeyGetMaxValueLengthreadField1readField2writeField1writeField2GetBaggageRestrictionsrestrictionsthriftProxypollStoppedstopPollinvalidRestrictionvalidRestrictionisReadypollManagerupdateRestrictionsparseRestrictionsTProtocolTMessageTypeTTransportReadSizeProviderRemainingBytesIsOpenReadBinaryReadDoubleReadFieldBeginReadFieldEndReadI16ReadI32ReadI64ReadListBeginReadListEndReadMapBeginReadMapEndReadMessageBeginReadMessageEndReadSetBeginReadSetEndReadStructBeginReadStructEndWriteBinaryWriteDoubleWriteFieldBeginWriteFieldEndWriteFieldStopWriteI16WriteI32WriteI64WriteListBeginWriteListEndWriteMapBeginWriteMapEndWriteMessageBeginWriteMessageEndWriteSetBeginWriteSetEndWriteStructBeginWriteStructEndgithub.com/uber/jaeger-client-go/internal/baggage/remoteDefaultRestrictionManagerNewDefaultRestrictionManagerNewRestrictiondefaultRestrictiongithub.com/uber/jaeger-client-go/internal/baggageMaterializeWithJSONfieldsAsMapmlspanloggithub.com/uber/jaeger-client-go/internal/spanlogNewThrottlercreditResponseerrorUUIDNotSethttpCreditManagerProxyminimumCreditsnewHTTPCreditManagerProxyoperationBalancesynchronousInitializationFetchCreditscreditManagercreditsSetProcessrefreshCreditsfetchCreditsBalancesgithub.com/uber/jaeger-client-go/internal/throttler/remoteDefaultThrottlergithub.com/uber/jaeger-client-go/internal/throttlerBytesBufferLoggerNullLoggerStdLoggernullLoggerstdLoggergithub.com/uber/jaeger-client-go/logDefaultNameNormalizerInboundMetricsByEndpointNameNormalizerNewSpanObserverOutboundSafeCharacterSetSimpleNameNormalizerdefaultMaxNumberOfEndpointsendpointNameMetricTagnewMetricsByEndpointnewNormalizedEndpointsnormalizedEndpointsotherEndpointsPlaceholderdefaultNamenormalizernormalizeWithLockRequestCountSuccessRequestCountFailuresRequestLatencySuccessRequestLatencyFailuresHTTPStatusCode2xxHTTPStatusCode3xxHTTPStatusCode4xxHTTPStatusCode5xxrecordHTTPStatusCodemetricsFactorymetricsByEndpointgetWithWriteLockIsSafeSafeSetssafeBytehttpStatusCodehandleTagInLockrpcmetricsgithub.com/uber/jaeger-client-go/rpcmetricsAgentAgentClientAgentEmitBatchArgsAgentEmitBatchArgs_Batch_DEFAULTAgentEmitZipkinBatchArgsAgentProcessorGoUnusedProtection__NewAgentClientFactoryNewAgentClientProtocolNewAgentEmitBatchArgsNewAgentEmitZipkinBatchArgsNewAgentProcessoragentProcessorEmitBatchagentProcessorEmitZipkinBatchIpv4Ipv6GetIpv4GetPortGetServiceNameGetIpv6IsSetIpv6readField3readField4writeField3writeField4IsSetHostBinaryAnnotationAnnotationTypeGetAnnotationTypeBinaryAnnotationsTraceIDHighGetParentIDGetAnnotationsGetBinaryAnnotationsGetDebugGetTraceIDHighIsSetParentIDIsSetDebugIsSetTimestampIsSetDurationIsSetTraceIDHighreadField5readField6readField8readField9readField10readField11readField12writeField5writeField6writeField8writeField9writeField10writeField11writeField12TagTypeVStrVDoubleVBoolVLongVBinaryGetVTypeGetVStrGetVDoubleGetVBoolGetVLongGetVBinaryIsSetVStrIsSetVDoubleIsSetVBoolIsSetVLongIsSetVBinaryreadField7writeField7IsSetTagsSpanRefSpanRefTypeTraceIdLowTraceIdHighSpanIdGetTraceIdLowGetTraceIdHighGetSpanIdParentSpanIdLogsGetParentSpanIdGetOperationNameGetReferencesGetFlagsGetLogsIsSetReferencesIsSetLogsGetProcessIsSetProcessEmitBatchEmitZipkinBatchTProcessorFunctionTExceptionprocessorMapAddToProcessorMapGetProcessorFunctionProcessorMapTProtocolFactoryGetProtocolProtocolFactoryInputProtocolOutputProtocolSeqIdsendEmitZipkinBatchsendEmitBatchIsSetBatchagentgithub.com/uber/jaeger-client-go/thrift-gen/agentBaggageRestrictionManagerClientBaggageRestrictionManagerGetBaggageRestrictionsArgsBaggageRestrictionManagerGetBaggageRestrictionsResultBaggageRestrictionManagerGetBaggageRestrictionsResult_Success_DEFAULTBaggageRestrictionManagerProcessorNewBaggageRestrictionNewBaggageRestrictionManagerClientFactoryNewBaggageRestrictionManagerClientProtocolNewBaggageRestrictionManagerGetBaggageRestrictionsArgsNewBaggageRestrictionManagerGetBaggageRestrictionsResultNewBaggageRestrictionManagerProcessorbaggageRestrictionManagerProcessorGetBaggageRestrictionsGetSuccessIsSetSuccessreadField0writeField0sendGetBaggageRestrictionsrecvGetBaggageRestrictionsgithub.com/uber/jaeger-client-go/thrift-gen/baggageBatchSubmitResponseBatch_Process_DEFAULTNewBatchNewBatchSubmitResponseNewLogNewProcessNewSpanNewSpanRefProcess_Tags_DEFAULTSpanRefTypeFromStringSpanRefTypePtrSpanRefType_CHILD_OFSpanRefType_FOLLOWS_FROMSpan_Logs_DEFAULTSpan_References_DEFAULTSpan_Tags_DEFAULTTagTypeFromStringTagTypePtrTagType_BINARYTagType_BOOLTagType_DOUBLETagType_LONGTagType_STRINGTag_VBinary_DEFAULTTag_VBool_DEFAULTTag_VDouble_DEFAULTTag_VLong_DEFAULTTag_VStr_DEFAULTGetOkgithub.com/uber/jaeger-client-go/thrift-gen/jaegerNewOperationSamplingStrategyNewPerOperationSamplingStrategiesNewProbabilisticSamplingStrategyNewRateLimitingSamplingStrategyNewSamplingManagerClientFactoryNewSamplingManagerClientProtocolNewSamplingManagerGetSamplingStrategyArgsNewSamplingManagerGetSamplingStrategyResultNewSamplingManagerProcessorNewSamplingStrategyResponseOperationSamplingStrategyOperationSamplingStrategy_ProbabilisticSampling_DEFAULTPerOperationSamplingStrategiesPerOperationSamplingStrategies_DefaultUpperBoundTracesPerSecond_DEFAULTProbabilisticSamplingStrategyRateLimitingSamplingStrategySamplingManagerSamplingManagerClientSamplingManagerGetSamplingStrategyArgsSamplingManagerGetSamplingStrategyResultSamplingManagerGetSamplingStrategyResult_Success_DEFAULTSamplingManagerProcessorSamplingStrategyResponseSamplingStrategyResponse_OperationSampling_DEFAULTSamplingStrategyResponse_ProbabilisticSampling_DEFAULTSamplingStrategyResponse_RateLimitingSampling_DEFAULTSamplingStrategyTypeSamplingStrategyTypeFromStringSamplingStrategyTypePtrSamplingStrategyType_PROBABILISTICSamplingStrategyType_RATE_LIMITINGsamplingManagerProcessorGetSamplingStrategyMaxTracesPerSecondGetMaxTracesPerSecondSamplingRateGetSamplingRateProbabilisticSamplingGetProbabilisticSamplingIsSetProbabilisticSamplingDefaultSamplingProbabilityDefaultLowerBoundTracesPerSecondPerOperationStrategiesDefaultUpperBoundTracesPerSecondGetDefaultSamplingProbabilityGetDefaultLowerBoundTracesPerSecondGetPerOperationStrategiesGetDefaultUpperBoundTracesPerSecondIsSetDefaultUpperBoundTracesPerSecondStrategyTypeRateLimitingSamplingOperationSamplingGetStrategyTypeGetRateLimitingSamplingGetOperationSamplingIsSetRateLimitingSamplingIsSetOperationSamplingGetSamplingStrategysendGetSamplingStrategyrecvGetSamplingStrategysamplinggithub.com/uber/jaeger-client-go/thrift-gen/samplingAnnotationTypeFromStringAnnotationTypePtrAnnotationType_BOOLAnnotationType_BYTESAnnotationType_DOUBLEAnnotationType_I16AnnotationType_I32AnnotationType_I64AnnotationType_STRINGAnnotation_Host_DEFAULTBinaryAnnotation_Host_DEFAULTCLIENT_ADDRCLIENT_RECVCLIENT_RECV_FRAGMENTCLIENT_SENDCLIENT_SEND_FRAGMENTEndpoint_Ipv6_DEFAULTLOCAL_COMPONENTMESSAGE_ADDRMESSAGE_RECVMESSAGE_SENDNewBinaryAnnotationNewEndpointNewZipkinCollectorClientFactoryNewZipkinCollectorClientProtocolNewZipkinCollectorProcessorNewZipkinCollectorSubmitZipkinBatchArgsNewZipkinCollectorSubmitZipkinBatchResultSERVER_ADDRSERVER_RECVSERVER_RECV_FRAGMENTSERVER_SENDSERVER_SEND_FRAGMENTSpan_Debug_DEFAULTSpan_Duration_DEFAULTSpan_ParentID_DEFAULTSpan_Timestamp_DEFAULTSpan_TraceIDHigh_DEFAULTWIRE_RECVWIRE_SENDZipkinCollectorZipkinCollectorClientZipkinCollectorProcessorZipkinCollectorSubmitZipkinBatchArgsZipkinCollectorSubmitZipkinBatchResultZipkinCollectorSubmitZipkinBatchResult_Success_DEFAULTzipkinCollectorProcessorSubmitZipkinBatchSubmitZipkinBatchsendSubmitZipkinBatchrecvSubmitZipkinBatchzipkincoregithub.com/uber/jaeger-client-go/thrift-gen/zipkincoreALREADY_OPENBAD_SEQUENCE_IDBAD_VERSIONBYTECALLCOMPACT_BINARYCOMPACT_BOOLEAN_FALSECOMPACT_BOOLEAN_TRUECOMPACT_BYTECOMPACT_DOUBLECOMPACT_I16COMPACT_I32COMPACT_I64COMPACT_LISTCOMPACT_MAPCOMPACT_PROTOCOL_IDCOMPACT_SETCOMPACT_STRUCTCOMPACT_TYPE_BITSCOMPACT_TYPE_MASKCOMPACT_TYPE_SHIFT_AMOUNTCOMPACT_VERSIONCOMPACT_VERSION_MASKDEFAULT_RECURSION_DEPTHDEPTH_LIMITDOUBLEEND_OF_FILEEXCEPTIONI08I16I32I64INFINITYINTERNAL_ERRORINVALID_DATAINVALID_MESSAGE_TYPE_EXCEPTIONINVALID_TMESSAGE_TYPEJSON_COLONJSON_COMMAJSON_FALSEJSON_INFINITYJSON_INFINITY_BYTESJSON_LBRACEJSON_LBRACKETJSON_NANJSON_NAN_BYTESJSON_NEGATIVE_INFINITYJSON_NEGATIVE_INFINITY_BYTESJSON_NULLJSON_QUOTEJSON_QUOTE_BYTESJSON_RBRACEJSON_RBRACKETJSON_TRUEMISSING_RESULTNANNEGATIVE_INFINITYNEGATIVE_SIZENOT_IMPLEMENTEDNOT_OPENNUMERIC_NULLNewNullNumericNewNumericFromDoubleNewNumericFromI32NewNumericFromI64NewNumericFromJSONStringNewNumericFromStringNewTApplicationExceptionNewTBinaryProtocolNewTBinaryProtocolFactoryNewTBinaryProtocolFactoryDefaultNewTBinaryProtocolTransportNewTCompactProtocolNewTCompactProtocolFactoryNewTMemoryBufferNewTMemoryBufferLenNewTMemoryBufferTransportFactoryNewTProtocolExceptionNewTProtocolExceptionWithTypeNewTRichTransportNewTSerializerNewTSimpleJSONProtocolNewTSimpleJSONProtocolFactoryNewTTransportExceptionNewTTransportExceptionFromErrorNewTTransportFactoryNumericONEWAYPROTOCOL_ERRORPrependErrorREPLYRichTransportSIZE_LIMITSTOPSkipDefaultDepthTApplicationExceptionTBinaryProtocolTBinaryProtocolFactoryTCompactProtocolTCompactProtocolFactoryTIMED_OUTTMemoryBufferTMemoryBufferTransportFactoryTProcessorTProtocolExceptionTRichTransportTSerializerTSimpleJSONProtocolTSimpleJSONProtocolFactoryTStructTTransportExceptionTTransportFactoryUNKNOWN_APPLICATION_EXCEPTIONUNKNOWN_METHODUNKNOWN_PROTOCOL_EXCEPTIONUNKNOWN_TRANSPORT_EXCEPTIONUTF16UTF7VERSION_1VERSION_MASKVOIDWRONG_METHOD_NAMEZERO_CONTEXT_IN_LIST_CONTEXT_IN_LIST_FIRST_CONTEXT_IN_OBJECT_FIRST_CONTEXT_IN_OBJECT_NEXT_KEY_CONTEXT_IN_OBJECT_NEXT_VALUE_CONTEXT_IN_TOPLEVEL_ParseContexterrTransportInterruptedinvalidDataLengthjsonQuotejsonUnquotejson_nonbase_map_elem_bytesmismatchnumericstringWritertApplicationExceptiontCompactTypetProtocolExceptiontTransportExceptiontTransportFactorytimeoutablettypeToCompactTypetypeNamesstrictReadstrictWriteparseContextStackdumpContextOutputPreValueOutputPostValueOutputBoolOutputNullOutputF64OutputI64OutputStringOutputStringDataOutputObjectBeginOutputObjectEndOutputListBeginOutputListEndOutputElemListBeginParsePreValueParsePostValuereadNonSignificantWhitespaceParseStringBodyParseQuotedStringBodyParseBase64EncodedBodyParseI64ParseF64ParseObjectStartParseObjectEndParseListBeginParseElemListBeginParseListEndreadSingleValuereadIfNullreadQuoteIfNextreadNumericsafePeekContainsresetContextStackisNullGetTransportorigTransportreadStringBodyiValuedValuesValuelastFieldlastFieldIdbooleanFieldNamebooleanFieldIdbooleanFieldPendingboolValueIsNotNullwriteFieldBeginInternalwriteCollectionBeginwriteVarint32writeVarint64int64ToZigzagint32ToZigzagfixedUint64ToBytesfixedInt64ToByteswriteByteDirectwriteIntAsByteDirectreadVarint32readVarint64readByteDirectzigzagToInt32zigzagToInt64bytesToInt64isBoolTypegetTTypegetCompactTypetype_thriftgithub.com/uber/jaeger-client-go/thriftHTTPBasicAuthHTTPBasicAuthCredentialsHTTPBatchSizeHTTPOptionHTTPRoundTripperHTTPTimeoutHTTPTransportNewHTTPTransportdefaultHTTPTimeoutserializeThrifthttpCredentialsgithub.com/uber/jaeger-client-go/transportAgentClientUDPErrEmptyIPErrNotFourOctetsErrNotHostColonPortHostIPNewAgentClientUDPNewRandNewRateLimiterPackIPAsUint32ParseIPToUint32ParsePortRateLimiterReadJSONTimeToMicrosecondsSinceEpochInt64UDPPacketMaxLengthrateLimiterscoreAddrcreditsPerSecondbalancemaxBalancelastTickCheckCreditmutUDPConnReadFromUDPreadFromUDPReadFromUDPAddrPortReadMsgUDPReadMsgUDPAddrPortWriteToUDPWriteToUDPAddrPortWriteMsgUDPWriteMsgUDPAddrPortreadFromAddrPortwriteToAddrPortwriteMsgAddrPortconnUDPthriftBufferUDPAddrutilsgithub.com/uber/jaeger-client-go/utilsBinaryPropagatorBuildJaegerProcessThriftBuildJaegerThriftBuildZipkinThriftConstSamplerContextFromStringConvertLogsToJaegerTagsDefaultMaxTagValueLengthDefaultUDPSpanServerHostDefaultUDPSpanServerPortExtractableZipkinSpanGuaranteedThroughputProbabilisticSamplerInjectableZipkinSpanJaegerClientVersionJaegerClientVersionTagKeyNewAdaptiveSamplerNewBinaryPropagatorNewCompositeReporterNewGuaranteedThroughputProbabilisticSamplerNewHTTPHeaderPropagatorNewLoggingReporterNewNullMetricsNewNullReporterNewProbabilisticSamplerNewRateLimitingSamplerNewRemoteReporterNewRemotelyControlledSamplerNewSpanContextNewTextMapPropagatorNewUDPTransportProbabilisticSamplerProcessSetterRemotelyControlledSamplerReporterOptionReporterOptionsSamplerOptionSamplerOptionsSamplerParamTagKeySamplerTypeConstSamplerTypeLowerBoundSamplerTypeProbabilisticSamplerTypeRateLimitingSamplerTypeRemoteSamplerTypeTagKeySpanContextFormatSpanIDFromStringTextMapPropagatorTraceIDFromStringTracerHostnameTagKeyTracerIPTagKeyTracerOptionsTracerStateHeaderNameTracerUUIDTagKeyZipkinSpanFormatadaptiveSamplerallowPackedNumbersapplySamplerOptionsbuildAnnotationsbuildBinaryAnnotationbuildBinaryAnnotationsbuildJaegerProcessThriftbuildLogsbuildReferencesbuildTagbuildTagscompositeReportercompositeSpanObserverdefaultBufferFlushIntervaldefaultMaxOperationsdefaultQueueSizedefaultSamplingRefreshIntervaldefaultSamplingServerURLemitBatchOverheademptyContexterrEmptyTracerStateStringerrMalformedTracerStateStringerrSpanTooLargeflagDebugflagSampledformatKeygetDefaultHeadersConfighttpSamplingManagerint32ToBytesint64ToBytesjaegerTraceContextPropagatorloggingReportermaxRandomNumbernewAdaptiveSamplernewBaggageSetternewGuaranteedThroughputProbabilisticSamplernewProbabilisticSamplernoopSpanObservernullReporteroldObserverrateLimitingSamplerremoteReporterremoveTagreporterOptionsreporterQueueItemreporterQueueItemClosereporterQueueItemSpanreporterQueueItemTypesamplerOptionssetPeerIPv4setPeerPortsetPeerServicesetSamplingPrioritysetSpanKindspanRefspecialTagHandlerstracerOptionsudpSenderzipkinPropagatorzipkinSpanCustomHeaderKeysTimeNowRandomNumberHostIPv4HighTraceIDGeneratorDebugThrottlerheaderKeysparseCommaSeparatedMapaddBaggageKeyPrefixremoveBaggageKeyPrefixsamplingRatesamplingBoundarybufferFlushIntervalprobabilisticSamplerlowerBoundSamplersetProbabilisticSamplersamplersdefaultSamplermaxOperationssamplingServerURLsamplingRefreshIntervalInitialSamplerserverURLSetParentIDSetSpanIDSetTraceIDspanKindhandleSpecialTagspeerDefinedisRPCisRPCClientqueueLengthsendCloseEventprocessQueuemaxSpanBytesbyteBufferSizespanBufferthriftProtocolprocessByteSizecalcSizeOfSerializedThriftresetBuffersmanagerdoneChanpollControllerpollControllerWithTickergetSamplersetSamplerupdateSamplerupdateAdaptiveSamplerupdateRateLimitingOrProbabilisticSamplerDecisionmaxTracesPerSecondMustInitNullCounterNullFactoryNullGaugeNullHistogramNullTimerStartStopwatchnullCounternullFactorynullGaugenullHistogramnullTimerElapsedTimegithub.com/uber/jaeger-lib/metricsAlphanumericRangeBaseStatsReporterBucketPairBucketPairsCachedCountCachedGaugeCachedHistogramCachedHistogramBucketCachedStatsReporterCachedTimerCounterSnapshotDefaultBucketsDefaultReplacementCharacterDefaultSeparatorDurationBucketsExponentialDurationBucketsExponentialValueBucketsGaugeSnapshotHistogramSnapshotKeyForPrefixedStringMapKeyForStringMapLinearDurationBucketsLinearValueBucketsMustMakeExponentialDurationBucketsMustMakeExponentialValueBucketsMustMakeLinearDurationBucketsMustMakeLinearValueBucketsNewNoOpSanitizerNewObjectPoolNewRootScopeNewSanitizerNewStopwatchNewTestScopeNoOpSanitizeFnNoopScopeNullStatsReporterObjectPoolSanitizeFnSanitizeOptionsSanitizeRangeSanitizerScopeOptionsStatsReporterTestScopeTimerSnapshotUnderscoreCharactersUnderscoreDashCharactersUnderscoreDashDotCharactersValidCharactersValueBuckets_defaultInitialSliceSizebucketPaircapabilitiescapabilitiesNonecapabilitiesReportingNoTaggingcapabilitiesReportingTaggingcopyAndSortDurationscopyAndSortValuescounterSnapshotdefaultScopeBucketsdurationHistogramTypeerrBucketsCountNeedsGreaterThanZeroerrBucketsFactorNeedsGreaterThanOneerrBucketsStartNeedsGreaterThanZerogaugeSnapshotglobalNowhistogramBuckethistogramSnapshothistogramTypekeyGenPoolkeyGenerationPoolkeyNameSplitterkeyPairSplittermergeRightTagsnewCounternewGaugenewHistogramBucketnewKeyGenerationPoolnewRootScopenewSnapshotnewTimernullStatsReporterprefixSplittersanitizerscopeRegistryscopeRegistryKeyscopeStatustimerNoReporterSinktimerSnapshottimerValuesvalueHistogramTypeReportTimerReportCounterReportGaugeReportHistogramDurationSamplesReportHistogramValueSamplesReportCountcachedCountcachedReportReportSamplesvalueLowerBoundvalueUpperBounddurationLowerBounddurationUpperBoundcachedValueBucketcachedDurationBuckethtypespecificationlookupByValuelookupByDurationaddBucketsnapshotValuessnapshotDurationsnameFnkeyFnvalueFnCharacterssanitizeFnNameCharactersKeyCharactersValueCharactersReplacementCharacterDurationsHistogramsTimersDurationBucketValueBucketstringsPoolcachedGaugeAllocateCounterAllocateGaugeAllocateHistogramAllocateTimerCachedReporterreportingtaggingLowerBoundDurationLowerBoundValueUpperBoundDurationUpperBoundValuegaugescachedTimerunreportedlowerBoundValueupperBoundValuelowerBoundDurationupperBoundDurationsubscopescachedReporterbaseReporterdefaultBucketsregistrygmcountersSlicegaugesSlicehistogramsSlicereportLoopreportLoopRunreportRegistryWithLockcachedSubscopesubscopefullyQualifiedNamecopyAndSanitizeMapdurationstallygithub.com/uber-go/tallyEdgeTypeEdgeTypeEndEdgeTypeLinkEdgeTypeMidFmtFuncFromStructFromStructWithMetaReprStructNameTreeStructTagTreeStructTreeOptionStructTypeSizeTreeStructTypeTreeStructValueTreefilterTagsgetMetaisEndedmetaTreenameTreeprintNodesprintValuestagSpectagTreetypeSizeTreetypeTreevalueTreeAssignStmtBadBadDeclBadExprBadStmtBasicLitBlockStmtBranchStmtCallExprCaseClauseChanTypeCommClauseCommentMapCompositeLitConDeclStmtDeferStmtEllipsisEmptyStmtExprStmtFieldFilterFileExportsFilterDeclFilterFileFilterFuncDuplicatesFilterImportDuplicatesFilterPackageFilterUnassociatedCommentsForStmtFuncDeclFuncLitFuncTypeGenDeclGoStmtIfStmtImportSpecIncDecStmtIndexExprIndexListExprInspectInterfaceTypeKeyValueExprLabeledStmtLblMergeModeMergePackageFilesNewCommentMapNewIdentNewObjNotNilFilterObjKindPackageExportsPkgRECVRangeStmtReturnStmtSENDSelectStmtSelectorExprSendStmtSliceExprSortImportsStarExprSwitchStmtTypeAssertExprTypeSpecTypeSwitchStmtUnaryExprValueSpecbyIntervalbyPoscgPoscollapsecommentListReaderexportFilterfilterCompositeLitfilterDeclfilterExprListfilterFieldListfilterFilefilterIdentListfilterPackagefilterParamListfilterSpecListfilterTypefprintimportCommentimportNameinspectorisDirectivelineAtlocalErrornameOfnodeListnodeStackobjKindStringspkgBuilderposSpansortCommentssortSpecsstripTrailingWhitespacewalkDeclListwalkExprListwalkIdentListwalkStmtListlineInfoLineCountMergeLineSetLinesSetLinesForContentLineStartAddLineInfoAddLineColumnInfoPositionForRemoveFileIterateSlashNamePosdeclNodeIsKeywordValuePosDocEndPosspecNodeDeclsUnresolvedstmtNodeColonOpPosOpeningTypeParamsaddCommentcmapEltsIncompleteTokPosTokForLparenRparenDeferSemicolonCaseEltChanArrowLhsRhsptrmapprintfElseErrorListRemoveMultiplesdeclareSelgo/astAndExprIsGoBuildIsPlusBuildNotExprOrExprPlusBuildLinesTagExprandArgappendSplitAndappendSplitOrerrComplexerrNotConstraintexprParserorArgparsePlusBuildExprpushNotsplitGoBuildsplitPlusBuildatomisExprconstraintgo/build/constraintDefaultLookupPackageDocLinkHeadingLinkDefListItemParagraphPlainPrinterautoURLblankBeforecommentPrinterhtmlPrinterimportPathOKindentedisHeadingisHostisIdentASCIIisListisOldHeadingisPathisPunctisSchemeisStdPkgleadingSpacelistMarkermdPrinterparseDocparseLinkparseSpansspanCodespanHeadingspanListspanOldHeadingspanParasplitDocNamestdPkgstextPrintervalidImportPathvalidImportPathElemwrapPenaltywriteNLWordsLookupPackageLookupSymDefaultIDAutoImportPathHeadingLevelHeadingIDDocLinkURLDocLinkBaseURLTextPrefixTextCodePrefixTextWidthHTMLheadingLevelheadingIDdocLinkURLheadingPrefixneedDoctightlookupSymlookupPkgoldHeadingheadingparagraphparseLinkedTextdocLinkparseTextForceBlankBeforeForceBlankBetweenBlankBeforeBlankBetweenrawTextcodePrefixoneLongLinego/doc/commenthasUnsortedImportsparserModeprinterModeprinterNormalizeNumberstabWidthTabwidthPackIndexExprUnpackIndexExprOrigtypeparamsgo/internal/typeparamsAllErrorsDeclarationErrorsImportsOnlyPackageClauseOnlyParseCommentsParseExprFromSkipObjectResolutionSpuriousErrorsbailoutdebugResolvedecNestLevdeclStartexprEndextractNameincNestLevisTypeElemisTypeSwitchAssertlabelOkmaxNestLevmaxScopeDepthparseSpecFunctionrangeOkresolveFileunparenunresolvedrdOffsetlineOffsetinsertSeminlPosupdateLineInfoscanRawStringswitch2switch3switch4syncPossyncCntexprLevinRhsnestLevnext0errorExpectedexpect2expectClosingexpectSemiatCommasafePosparseIdentparseIdentListparseExprListparseQualifiedIdentparseTypeNameparseArrayTypeparseArrayFieldOrTypeInstanceparseFieldDeclparseStructTypeparsePointerTypeparseDotsTypeparseParamDeclparseParameterListparseResultparseFuncTypeparseMethodSpecembeddedElemembeddedTermparseInterfaceTypeparseMapTypeparseChanTypeparseTypeInstancetryIdentOrTypeparseStmtListparseBodyparseBlockStmtparseFuncTypeOrLitparseOperandparseSelectorparseTypeAssertionparseIndexOrSliceOrInstanceparseCallOrConversionparseElementparseElementListparseLiteralValueparsePrimaryExprtokPrecparseBinaryExprparseRhsparseSimpleStmtparseCallExprparseGoStmtparseDeferStmtparseReturnStmtparseBranchStmtmakeExprparseIfHeaderparseIfStmtparseCaseClauseisTypeSwitchGuardparseSwitchStmtparseCommClauseparseSelectStmtparseForStmtparseStmtparseImportSpecparseValueSpecparseGenericTypeparseTypeSpecparseGenDeclparseFuncDeclparseDecldeclErrpkgScopetopScopelabelScopetargetStackopenScopecloseScopeopenLabelScopecloseLabelScopeshortVarDeclwalkExprswalkLHSwalkStmtswalkFuncTyperesolveListdeclareListwalkRecvwalkFieldListwalkTParamswalkBodygo/parserCommentedNodeRawFormatSourcePosTabIndentUseSpacesaNewlineallStarsappendLinescombinesWithNamecommaTermcommentInfodeclTokendiffPrecexprListModefilteredMsgformatDocCommentformfeedfuncParamfuncTParamgetDocgetLastCommentidentListSizeinEscapeinSpaceinTextisBinaryisTypeNamekeepTypeColumnmaxNewlinesmayCombinenlimitnoExtraBlanknoExtraLinebreaknoIndentnormalizeNumbersnormalizedNumberparamModepmodeprinterPoolreduceDepthsanitizeImportPathsizeCounterstripCommonPrefixstripParensstripParensAlwaystrimRighttrimmertypeTParamvtabwalkBinaryresetSpacecindexcommentOffsetcommentNewlineendAlignmentimpliedSemilastTokprevOpenwsbufgoBuildplusBuildlinePtruseNodeCommentsnodeSizescachedPoscachedLinefixGoBuildLinescommentTextAtlinebreaksetCommentidentListexprListisOneLineFieldListsetLineCommentbinaryExprpossibleSelectorExprselectorExprstmtListcontrolClauseindentListvalueSpecgenDeclnodeSizenumLinesbodySizefuncBodydistanceFromfuncDecldeclListcommentsHaveNewlinenextCommentcommentBeforecommentSizeBeforerecordLinelinesFromposForlineForwriteLineDirectivewriteCommentPrefixwriteCommentSuffixcontainsLinebreakintersperseCommentswriteWhitespacesetPosprintNodehasNewlinego/printerPrintErrorScanCommentsdontInsertSemisinvalidSepisHexlitnamestripCRtrailingDigitsgo/scannerADD_ASSIGNAND_ASSIGNAND_NOTAND_NOT_ASSIGNBREAKCASECHANCHARCONSTCONTINUEDEFERDEFINEELLIPSISEQLFALLTHROUGHFUNCGEQGOGOTOGTRHighestPrecIMAGINCINTERFACELANDLEQLORLSSLowestPrecMUL_ASSIGNNoPosOR_ASSIGNQUOQUO_ASSIGNRANGEREMREM_ASSIGNSHLSHL_ASSIGNSHRSHR_ASSIGNSUB_ASSIGNSWITCHTILDETYPEUnaryPrecVARXORXOR_ASSIGNadditional_begadditional_endkeyword_begkeyword_endsearchFilessearchLineInfosserializedFileserializedFileSetInfosgo/tokenFreelistArrayTypeFreelistMapTypeflockRetryTimeoutpgidNoFreelisttxsByIdunsafeAddunsafeByteSliceunsafeIndexunsafeSliceReadIndexgrowIfRequiredWriteValueReadValuetagencodinggo.opencensus.io/internal/tagencodingBucketConfigurationErrorBucketSummaryLatencyBucketSummaryLocalSpanStoreEnabledMonotonicEndTimePerMethodSummarylabelKeySizeLimitsanitizeRuneMinLatencyMaxLatencyLatencyBucketsErrorBucketsMaxRequestsSucceededMaxRequestsErrorsgo.opencensus.io/internalAttachmentKeySpanContextAttachmentsBucketOptionsDistributionLabelKeyNewDistributionPointNewFloat64PointNewInt64PointNewLabelValueNewSummaryPointTypeCumulativeDistributionTypeCumulativeFloat64TypeCumulativeInt64TypeGaugeDistributionTypeGaugeFloat64TypeGaugeInt64TypeSummaryUnitBytesUnitDimensionlessUnitMillisecondsValueVisitorPercentilesHasCountAndSumLabelKeysSumOfSquaredDeviationVisitDistributionValueVisitFloat64ValueVisitInt64ValueVisitSummaryValuemetricdatago.opencensus.io/metric/metricdataGlobalManagerProducerprodMgrproducersAddProducerDeleteProducermetricproducergo.opencensus.io/metric/metricproducerClientCompletedRPCsViewClientHandlerClientReceivedBytesPerRPCClientReceivedBytesPerRPCViewClientReceivedMessagesPerRPCClientReceivedMessagesPerRPCViewClientRoundtripLatencyClientRoundtripLatencyViewClientSentBytesPerRPCClientSentBytesPerRPCViewClientSentMessagesPerRPCClientSentMessagesPerRPCViewClientServerLatencyClientServerLatencyViewDefaultBytesDistributionDefaultClientViewsDefaultMessageCountDistributionDefaultMillisecondsDistributionDefaultServerViewsKeyClientMethodKeyClientStatusKeyServerMethodKeyServerStatusServerCompletedRPCsViewServerHandlerServerLatencyServerLatencyViewServerReceivedBytesPerRPCServerReceivedBytesPerRPCViewServerReceivedMessagesPerRPCServerReceivedMessagesPerRPCViewServerSentBytesPerRPCServerSentBytesPerRPCViewServerSentMessagesPerRPCServerSentMessagesPerRPCViewgetSpanCtxAttachmentgrpcInstrumentationKeyhandleRPCEndhandleRPCInPayloadhandleRPCOutPayloadmethodNamerpcDatarpcDataKeystatsHandleRPCstatusCodeToStringtraceHandleRPCMeasureAggregationAggTypeAggregationDataaddSampleisAggregationDataWithNamesamecanonicalizeOutPayloadWireLengthSentTimeStartOptionsSamplingParametersTraceOptionsTracestatesetIsSampledParentContextHasRemoteParentSamplingDecisionstatsTagRPCtraceTagRPCInt64MeasuremeasureDescriptorsubscribedBeginTimesentCountsentBytesrecvCountrecvBytesIsPublicEndpointextractPropagatedTagsInPayloadRecvTimeFloat64MeasuretagContentmetadatasupsertocgrpcgo.opencensus.io/plugin/ocgrpcHTTPFormatParseSampledParseSpanIDParseTraceIDSampledHeaderSpanIDHeaderTraceIDHeaderSpanContextFromRequestSpanContextToRequestb3go.opencensus.io/plugin/ochttp/propagation/b3ClientCompletedCountClientLatencyClientLatencyViewClientReceivedBytesClientReceivedBytesDistributionClientRequestBytesClientRequestBytesViewClientRequestCountClientRequestCountByMethodClientRequestCountViewClientResponseBytesClientResponseBytesViewClientResponseCountByStatusCodeClientRoundtripLatencyDistributionClientSentBytesClientSentBytesDistributionDefaultLatencyDistributionDefaultSizeDistributionHostAttributeKeyClientHostKeyClientPathKeyServerRouteMethodAttributeNewSpanAnnotatingClientTraceNewSpanAnnotatorPathAttributeServerRequestBytesServerRequestBytesViewServerRequestCountServerRequestCountByMethodServerRequestCountViewServerResponseBytesServerResponseBytesViewServerResponseCountByStatusCodeSetRouteStatusCodeAttributeTraceStatusURLAttributeUserAgentAttributeWithRouteTagaddedTagsaddedTagsKeybodyTrackercodeToStrdefaultFormatisHealthEndpointrequestAttrsresponseAttrsspanAnnotatorspanNameFromURLstatsTransporttaggedHandlerFunctraceTransporttrackingResponseWriterwrappedBodyPropagationGetStartOptionsFormatSpanNameIsHealthEndpointextractSpanContextstartStatsrespContentLengthreqSizeendOnceSpanDataMessageEventMessageEventTypeUncompressedByteSizeCompressedByteSizeLinkTypeMessageEventsDroppedAttributeCountDroppedAnnotationCountDroppedMessageEventCountDroppedLinkCountChildSpanCountlruMapdroppedCountevictedQueuespanStorelatencymaxSpansPerErrorBucketlruAttributesmessageEventsexecutionTracerTaskEndIsRecordingEventsmakeSpanDatainterfaceArrayToLinksArrayinterfaceArrayToMessageEventArrayinterfaceArrayToAnnotationArraylruAttributesToAttributeMapcopyToCappedAttributesAddAttributeslazyPrintfInternalprintStringInternalAddMessageSendEventAddMessageReceiveEventAddLinkstatusLinewrappedResponseWriterNewClientTracegotConnputIdleConngotFirstResponseBytegot100ContinuednsStartdnsDoneconnectStartconnectDonetlsHandshakeStarttlsHandshakeDonewroteHeaderswait100ContinuestartOptionsformatSpanNamenewClientTraceochttpgo.opencensus.io/plugin/ochttpDecodeLabelsDetectorEncodeLabelsEnvVarLabelsEnvVarTypeMultiDetectordetectAlllabelRegexgo.opencensus.io/resourceDefaultRecorderSubscriptionReportergo.opencensus.io/stats/internalAggTypeCountAggTypeDistributionAggTypeLastValueAggTypeNoneAggTypeSumCountDataDistributionDataErrNegativeBucketBoundsLastValueLastValueDataRegisterExporterRetrieveDataSetReportingPeriodSumDataUnregisterExporteraggCountaggSumaggTypeNamecheckViewNamecollectorconvertUnitdefaultReportingDurationdefaultWorkerdropZeroBoundsencodeWithKeysexportersMugetExemplargetLabelKeysgetUnitgetViewByNameReqgetViewByNameRespmaxNameLengthmeasureRefnewDistributionDatanewViewInternalnewWorkerrecordReqregisterViewReqretrieveDataReqretrieveDataResprowToTimeseriessetReportingPeriodReqtoLabelValuesunregisterFromViewReqviewInternalviewToMetricviewToMetricDescriptorExportViewsignaturescollectedRowsclearRowsmetricDescriptorisSubscribedmeasuresgetMeasureReftryRegisterViewunregisterViewreportViewreportUsagetoMetricSumOfSquaredDevCountPerBucketExemplarsPerBucketaddToBucketattachmentsgo.opencensus.io/stats/viewRecordWithOptionsRecordWithTagsUnitNoneUnitSecondsWithAttachmentsWithMeasurementsWithTagscreateRecordOptionrecordOptionsregisterMeasureHandlego.opencensus.io/statsDecodeEachFromContextMustNewKeyNewContextNewKeyTTLNoPropagationTTLUnlimitedPropagationUpsertWithTTLcheckKeyNamecheckValuecreateMetadatasctxKeyencoderGRPCerrInvalidKeyNameerrInvalidValuekeyTypeFalsekeyTypeInt64keyTypeStringkeyTypeTruemapCtxKeynewMaptagsVersionIDvalidKeyValueMaxvalidKeyValueMinvalueTTLNoPropagationvalueTTLUnlimitedPropagationwriteIdxreadIdxwriteTagStringegwriteTagUint64writeTagTruewriteTagFalsewriteBytesWithVarintLenwriteStringWithVarintLenreadBytesWithVarintLenreadStringWithVarintLenreadEndedgo.opencensus.io/tagNewSpanIDNewTraceIDgo.opencensus.io/trace/internalFromBinarypropagationgo.opencensus.io/trace/propagationareEntriesValidcontainsDuplicateKeykeyFormatkeyMaxSizekeyValidationRegExpkeyWithVendorFormatkeyWithoutVendorFormatmaxKeyValuePairsvalueFormatvalueMaxSizevalueValidationRegExptracestatego.opencensus.io/trace/tracestateAlwaysSampleApplyConfigBoolAttributeDefaultMaxAnnotationEventsPerSpanDefaultMaxAttributesPerSpanDefaultMaxLinksPerSpanDefaultMaxMessageEventsPerSpanFloat64AttributeInt64AttributeLinkTypeChildLinkTypeParentLinkTypeUnspecifiedMessageEventTypeRecvMessageEventTypeSentMessageEventTypeUnspecifiedNeverSampleProbabilitySamplerSpanKindClientSpanKindServerSpanKindUnspecifiedStartOptionStartSpanWithRemoteParentStatusCodeAbortedStatusCodeAlreadyExistsStatusCodeCancelledStatusCodeDataLossStatusCodeDeadlineExceededStatusCodeFailedPreconditionStatusCodeInternalStatusCodeInvalidArgumentStatusCodeNotFoundStatusCodeOKStatusCodeOutOfRangeStatusCodePermissionDeniedStatusCodeResourceExhaustedStatusCodeUnauthenticatedStatusCodeUnavailableStatusCodeUnimplementedStatusCodeUnknownStringAttributeWithSamplerWithSpanKindconfigWriteMucopyAttributesdefaultBucketSizedefaultIDGeneratordefaultLatenciesexporterMuexportersMaplatencyBucketlatencyBucketBoundsmakeBucketmaxBucketSizenewEvictedQueuenewLruMapnewSpanStoresamplePeriodspanStoreForNamespanStoreForNameCreateIfNewspanStoreSetSizespanStoresssmustartExecutionTracerTaskDefaultSamplerMaxAnnotationEventsPerSpanMaxMessageEventsPerSpanMaxAttributesPerSpanMaxLinksPerSpanExportSpanReportActiveSpansReportSpansByErrorConfigureBucketSizesReportSpansPerMethodReportSpansByLatencynextSpanIDspanIDInctraceIDAddtraceIDRandgo.opencensus.io/traceopencensusgo.opencensus.ioNewFloat64NewInt32NewInt64NewUint32NewUint64boolToInterrorHolderTogglego.uber.org/atomicAppendInto_bufferPool_multilineIndent_multilinePrefix_multilineSeparator_singlelineSeparatorerrorGroupfromSliceinspectResultwritePrefixLinecopyNeededmerrwriteSinglelinewriteMultilineFirstErrorIdxContainsMultiErrorNewPoolgo.uber.org/zap/buffer_poolgo.uber.org/zap/internal/bufferpoolgo.uber.org/zap/internal/colorStubStubbedExitWithStubrealUnstubexitgo.uber.org/zap/internal/exitDiscarderFailWriterShortWriterSyncer_timeoutScaleCalledStrippedztestgo.uber.org/zap/internal/ztestArrayMarshalerFuncArrayMarshalerTypeByteStringTypeCapitalColorLevelEncoderCapitalLevelEncoderDPanicLevelDefaultLineEndingEpochMillisTimeEncoderEpochNanosTimeEncoderEpochTimeEncoderFatalLevelFullCallerEncoderFullNameEncoderISO8601TimeEncoderLowercaseColorLevelEncoderLowercaseLevelEncoderMapObjectEncoderMillisDurationEncoderNamespaceTypeNanosDurationEncoderNewEntryCallerNewIncreaseLevelCoreNewMapObjectEncoderNewMultiWriteSyncerNewNopCoreNewTeeObjectMarshalerFuncObjectMarshalerTypeOmitKeyPanicLevelRFC3339NanoTimeEncoderRFC3339TimeEncoderReflectTypeRegisterHooksSecondsDurationEncoderShortCallerEncoderSkipTypeStringDurationEncoderStringerTypeUintptrTypeUnknownTypeWriteThenFatalWriteThenNoopWriteThenPanic_cePool_countersPerLevel_errArrayElemPool_jsonPool_levelToCapitalColorString_levelToColor_levelToLowercaseColorString_maxLevel_minLevel_numLevels_sliceEncoderPool_unknownLevelColorconsoleEncoderencodeErrorencodeStringerencodeTimeLayouterrArrayerrArrayElemerrUnmarshalNilLevelfnv32agetCheckedEntrygetJSONEncodergetSliceEncoderhookedioCorejsonEncoderlevelFilterCorelockedWriteSyncermultiCoremultiWriteSyncernewCountersnewErrArrayElemnewJSONEncodernopCorenullLiteralBytesputCheckedEntryputJSONEncoderputSliceEncodersliceArrayEncoderwriterWrapperspacedopenNamespacesreflectBufreflectEncresetReflectBufencodeReflectedAppendTimeLayoutcloseOpenNamespacesaddElementSeparatorresetAtIncCheckResetthereafterwriteContextaddTabIfNecessaryWithDebugfatalfzapgrpcgo.uber.org/zap/zapgrpcWrapOptionsloggerOptionFuncnewTestingWritertestingWritermarkFailedWithMarkFailedAddCallerAddCallerSkipAddStacktraceBoolpByteStringsCombineWriteSyncersComplex128pComplex128sComplex64Complex64pComplex64sDurationpFloat32pFloat64pIncreaseLevelInt16pInt16sInt32pInt64pInt8pInt8sIntpLevelEnablerFuncLevelFlagNamedErrorNewAtomicLevelNewDevelopmentNewDevelopmentConfigNewDevelopmentEncoderConfigNewExampleNewProductionNewStdLogAtRedirectStdLogRedirectStdLogAtRegisterSinkReplaceGlobalsSinkStringpTimepUint16pUint16sUint32pUint64pUint8pUint8sUintpUintptrpUintptrsUintsWrapCore_encoderMutex_encoderNameToConstructor_globalL_globalMu_globalS_loggerWriterDepth_nonStringKeyErrMsg_oddNumberErrMsg_programmerErrorTemplate_sinkFactories_sinkMutex_stacktracePool_stdLogDefaultDepth_zapPackage_zapStacktracePrefixes_zapStacktraceVendorContainsaddPrefixboolsbyteStringsArraycomplex128scomplex64serrNoEncoderNameSpecifiederrSinkNotFoundfloat32sfloat64sint16sint32sint64sint8sintsinvalidPairinvalidPairsisZapFramelevelToFuncloggerWriternewFileSinknewProgramCountersnewSinknilFieldnopCloserSinknormalizeSchemeprogramCountersredirectStdLogAtresetSinkRegistryschemeFiletakeStacktracetimeToMillisuint16suint32suint64suint8suintptrsuintsnumslogFuncErrHashTooShortErrMismatchedHashAndPasswordHashVersionTooNewErrorInvalidCostErrorInvalidHashPrefixErrorMaxCostMinCostalphabetbase64Decodebase64EncodebcEncodingcheckCostencodedHashSizeencodedSaltSizeexpensiveBlowfishSetupmagicCipherDatamajorVersionmaxCryptedHashSizemaxSaltSizeminHashSizeminorVersionnewFromPassworddecodeVersiondecodeCosts0s3ihExpandKeyNewSaltedCipherexpandKeyWithSaltgetNextWordinitCipherblowfishgolang.org/x/crypto/blowfishgolang.org/x/crypto/ed25519_Block_Chunk_Init0_Init1_Init2_Init3shift1shift2shift3xIndex2xIndex3d0md4golang.org/x/crypto/md4AACompromiseAffiliationChangedCACompromiseCertificateHoldCessationOfOperationCreateRequestCreateResponseGoodInternalErrorErrorResponseKeyCompromiseMalformedMalformedRequestErrorResponseParseRequestParseResponseForCertPrivilegeWithdrawnRemoveFromCRLResponseErrorResponseStatusRevokedServerFailedSigRequredErrorResponseSignatureRequiredSupersededTryLaterTryLaterErrorResponseUnauthorizedErrorResponseUnspecifiedbasicResponsegetSignatureAlgorithmFromOIDidPKIXOCSPBasicocspRequestresponseASN1responseDatarevokedInfosingleResponsetbsRequestCertIDSingleExtensionsRawResponderIDResponseTypeRequestorNameRequestListIssuerNameHashTBSRequestocspgolang.org/x/crypto/ocsppbkdf2golang.org/x/crypto/pbkdf2piTablerc2Cipherrotl16golang.org/x/crypto/pkcs12/internal/rc2ErrIncorrectPasswordToPEMbmpStringcertBagcertificateTypecontentInfoconvertAttributeconvertBagdecodeBMPStringdecodeCertBagdecodePkcs8ShroudedKeyBagdecryptabledigestInfoencryptedContentInfoencryptedDataencryptedPrivateKeyInfofillWithRepeatsgetSafeContentsmacDataoidCertBagoidCertTypeX509CertificateoidDataContentTypeoidEncryptedDataContentTypeoidFriendlyNameoidLocalKeyIDoidMicrosoftCSPNameoidPBEWithSHAAnd3KeyTripleDESCBCoidPBEWithSHAAnd40BitRC2CBCoidPKCS8ShroundedKeyBagoidSHA1pbDecryptpbDecrypterForpbeCipherpbeParamspbkdfpfxPdupkcs12AttributeprivateKeyTypesafeBagsha1SumshaWith40BitRC2CBCshaWithTripleDESCBCverifyMacSaltIterationsMacMacSaltAuthSafeMacDataContentEncryptionAlgorithmEncryptedContentEncryptedDataderiveIVEncryptedContentInfopkcs12golang.org/x/crypto/pkcs12ErrPasteIndicatorEscapeCodesMakeRawNewTerminalReadPasswordTerminalbytesToKeycrlferaseUnderCursorioctlWriteTermioskeyAltLeftkeyAltRightkeyBackspacekeyClearScreenkeyCtrlCkeyCtrlDkeyCtrlUkeyDeleteLinekeyDeleteWordkeyDownkeyEndkeyEnterkeyEscapekeyHomekeyLeftkeyPasteEndkeyPasteStartkeyRightkeyUnknownkeyUppasswordReaderpasteEndpasteIndicatorErrorpasteStartreadPasswordLinestRingBuffervisualLengthvt100EscapeCodeswriteWithCRLFNthPreviousEntryAutoCompleteCallbackpasteActivecursorXcursorYmaxLinetermWidthtermHeightoutBufinBufhistoryhistoryIndexhistoryPendingmoveCursorToPosclearLineToRightsetLineeraseNPreviousCharscountToLeftWordcountToRightWordhandleKeyaddKeyToLineSetPromptclearAndRepaintLinePlusNPreviousSetBracketedPasteModeTermiosIflagOflagCflagLflagIspeedOspeedtermiosgolang.org/x/crypto/ssh/terminalctxhttpgolang.org/x/net/context/ctxhttpHeaderValuesContainsTokenIsTokenRunePunycodeHostPortValidHeaderFieldNameValidHeaderFieldValueValidHostHeaderValidTrailerHeaderbadTrailerheaderValueContainsTokenisCTLisLWSisNotTokenisOWSisTokenTablelowerASCIItrimOWSvalidHostBytehttpgutsgolang.org/x/net/http/httpgutsAppendHuffmanStringDecodingErrorErrInvalidHuffmanErrStringLengthHuffmanDecodeHuffmanDecodeToStringHuffmanEncodeLengthInvalidIndexErroraddDecoderNodeappendByteToHuffmanCodeappendHpackStringappendIndexedappendIndexedNameappendNewNameappendTableSizeappendVarIntbuildRootHuffmanNodebuildRootOnceencodeTypeByteerrNeedMoreerrVarintOverflowgetRootHuffmanNodehuffmanCodeLenhuffmanCodeshuffmanDecodeindexedFalseindexedNeverindexedTrueinitialHeaderTableSizelazyRootHuffmanNodenewInternalNodenewStaticTablereadVarIntstaticTablestaticTableEntriesuint32MaxcodeLendehpackgolang.org/x/net/http2/hpackClientConnPoolClientPrefaceConfigureServerConfigureTransportConnectionErrorContinuationFrameDebugGoroutinesErrCodeCancelErrCodeCompressionErrCodeConnectErrCodeEnhanceYourCalmErrCodeFlowControlErrCodeFrameSizeErrCodeHTTP11RequiredErrCodeInadequateSecurityErrCodeInternalErrCodeNoErrCodeProtocolErrCodeRefusedStreamErrCodeSettingsTimeoutErrCodeStreamClosedErrFrameTooLargeErrNoCachedConnErrPushLimitReachedErrRecursivePushFlagContinuationEndHeadersFlagDataEndStreamFlagDataPaddedFlagHeadersEndHeadersFlagHeadersEndStreamFlagHeadersPaddedFlagHeadersPriorityFlagPingAckFlagPushPromiseEndHeadersFlagPushPromisePaddedFlagSettingsAckFrameContinuationFrameDataFrameGoAwayFrameHeadersFramePingFramePriorityFramePushPromiseFrameRSTStreamFrameSettingsFrameWindowUpdateFrameWriteRequestGoAwayErrorNewFramerNewPriorityWriteSchedulerNewRandomWriteSchedulerNextProtoTLSOpenStreamOptionsPriorityFramePriorityWriteSchedulerConfigPushPromiseFrameReadFrameHeaderRoundTripOptServeConnOptsSettingEnablePushSettingHeaderTableSizeSettingInitialWindowSizeSettingMaxConcurrentStreamsSettingMaxFrameSizeSettingMaxHeaderListSizeTrailerPrefixUnknownFrameVerboseLogsWriteScheduleractualContentLengthaddConnCallauthorityAddrawaitRequestCancelbodyAllowedForStatusbodyReadMsgbodyWriterStatebufWriterPoolbufWriterPoolBufferSizebuildCommonHeaderMapsbuildCommonHeaderMapsOncecanRetryErrorcheckConnHeaderscheckPrioritycheckValidHTTP2RequestHeaderscheckWriteHeaderCodecipher_TLS_DHE_DSS_EXPORT_WITH_DES40_CBC_SHAcipher_TLS_DHE_DSS_WITH_3DES_EDE_CBC_SHAcipher_TLS_DHE_DSS_WITH_AES_128_CBC_SHAcipher_TLS_DHE_DSS_WITH_AES_128_CBC_SHA256cipher_TLS_DHE_DSS_WITH_AES_128_GCM_SHA256cipher_TLS_DHE_DSS_WITH_AES_256_CBC_SHAcipher_TLS_DHE_DSS_WITH_AES_256_CBC_SHA256cipher_TLS_DHE_DSS_WITH_AES_256_GCM_SHA384cipher_TLS_DHE_DSS_WITH_ARIA_128_CBC_SHA256cipher_TLS_DHE_DSS_WITH_ARIA_128_GCM_SHA256cipher_TLS_DHE_DSS_WITH_ARIA_256_CBC_SHA384cipher_TLS_DHE_DSS_WITH_ARIA_256_GCM_SHA384cipher_TLS_DHE_DSS_WITH_CAMELLIA_128_CBC_SHAcipher_TLS_DHE_DSS_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_DHE_DSS_WITH_CAMELLIA_128_GCM_SHA256cipher_TLS_DHE_DSS_WITH_CAMELLIA_256_CBC_SHAcipher_TLS_DHE_DSS_WITH_CAMELLIA_256_CBC_SHA256cipher_TLS_DHE_DSS_WITH_CAMELLIA_256_GCM_SHA384cipher_TLS_DHE_DSS_WITH_DES_CBC_SHAcipher_TLS_DHE_DSS_WITH_SEED_CBC_SHAcipher_TLS_DHE_PSK_WITH_3DES_EDE_CBC_SHAcipher_TLS_DHE_PSK_WITH_AES_128_CBC_SHAcipher_TLS_DHE_PSK_WITH_AES_128_CBC_SHA256cipher_TLS_DHE_PSK_WITH_AES_128_CCMcipher_TLS_DHE_PSK_WITH_AES_128_GCM_SHA256cipher_TLS_DHE_PSK_WITH_AES_256_CBC_SHAcipher_TLS_DHE_PSK_WITH_AES_256_CBC_SHA384cipher_TLS_DHE_PSK_WITH_AES_256_CCMcipher_TLS_DHE_PSK_WITH_AES_256_GCM_SHA384cipher_TLS_DHE_PSK_WITH_ARIA_128_CBC_SHA256cipher_TLS_DHE_PSK_WITH_ARIA_128_GCM_SHA256cipher_TLS_DHE_PSK_WITH_ARIA_256_CBC_SHA384cipher_TLS_DHE_PSK_WITH_ARIA_256_GCM_SHA384cipher_TLS_DHE_PSK_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_DHE_PSK_WITH_CAMELLIA_128_GCM_SHA256cipher_TLS_DHE_PSK_WITH_CAMELLIA_256_CBC_SHA384cipher_TLS_DHE_PSK_WITH_CAMELLIA_256_GCM_SHA384cipher_TLS_DHE_PSK_WITH_CHACHA20_POLY1305_SHA256cipher_TLS_DHE_PSK_WITH_NULL_SHAcipher_TLS_DHE_PSK_WITH_NULL_SHA256cipher_TLS_DHE_PSK_WITH_NULL_SHA384cipher_TLS_DHE_PSK_WITH_RC4_128_SHAcipher_TLS_DHE_RSA_EXPORT_WITH_DES40_CBC_SHAcipher_TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHAcipher_TLS_DHE_RSA_WITH_AES_128_CBC_SHAcipher_TLS_DHE_RSA_WITH_AES_128_CBC_SHA256cipher_TLS_DHE_RSA_WITH_AES_128_CCMcipher_TLS_DHE_RSA_WITH_AES_128_CCM_8cipher_TLS_DHE_RSA_WITH_AES_128_GCM_SHA256cipher_TLS_DHE_RSA_WITH_AES_256_CBC_SHAcipher_TLS_DHE_RSA_WITH_AES_256_CBC_SHA256cipher_TLS_DHE_RSA_WITH_AES_256_CCMcipher_TLS_DHE_RSA_WITH_AES_256_CCM_8cipher_TLS_DHE_RSA_WITH_AES_256_GCM_SHA384cipher_TLS_DHE_RSA_WITH_ARIA_128_CBC_SHA256cipher_TLS_DHE_RSA_WITH_ARIA_128_GCM_SHA256cipher_TLS_DHE_RSA_WITH_ARIA_256_CBC_SHA384cipher_TLS_DHE_RSA_WITH_ARIA_256_GCM_SHA384cipher_TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHAcipher_TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_DHE_RSA_WITH_CAMELLIA_128_GCM_SHA256cipher_TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHAcipher_TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA256cipher_TLS_DHE_RSA_WITH_CAMELLIA_256_GCM_SHA384cipher_TLS_DHE_RSA_WITH_CHACHA20_POLY1305_SHA256cipher_TLS_DHE_RSA_WITH_DES_CBC_SHAcipher_TLS_DHE_RSA_WITH_SEED_CBC_SHAcipher_TLS_DH_DSS_EXPORT_WITH_DES40_CBC_SHAcipher_TLS_DH_DSS_WITH_3DES_EDE_CBC_SHAcipher_TLS_DH_DSS_WITH_AES_128_CBC_SHAcipher_TLS_DH_DSS_WITH_AES_128_CBC_SHA256cipher_TLS_DH_DSS_WITH_AES_128_GCM_SHA256cipher_TLS_DH_DSS_WITH_AES_256_CBC_SHAcipher_TLS_DH_DSS_WITH_AES_256_CBC_SHA256cipher_TLS_DH_DSS_WITH_AES_256_GCM_SHA384cipher_TLS_DH_DSS_WITH_ARIA_128_CBC_SHA256cipher_TLS_DH_DSS_WITH_ARIA_128_GCM_SHA256cipher_TLS_DH_DSS_WITH_ARIA_256_CBC_SHA384cipher_TLS_DH_DSS_WITH_ARIA_256_GCM_SHA384cipher_TLS_DH_DSS_WITH_CAMELLIA_128_CBC_SHAcipher_TLS_DH_DSS_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_DH_DSS_WITH_CAMELLIA_128_GCM_SHA256cipher_TLS_DH_DSS_WITH_CAMELLIA_256_CBC_SHAcipher_TLS_DH_DSS_WITH_CAMELLIA_256_CBC_SHA256cipher_TLS_DH_DSS_WITH_CAMELLIA_256_GCM_SHA384cipher_TLS_DH_DSS_WITH_DES_CBC_SHAcipher_TLS_DH_DSS_WITH_SEED_CBC_SHAcipher_TLS_DH_RSA_EXPORT_WITH_DES40_CBC_SHAcipher_TLS_DH_RSA_WITH_3DES_EDE_CBC_SHAcipher_TLS_DH_RSA_WITH_AES_128_CBC_SHAcipher_TLS_DH_RSA_WITH_AES_128_CBC_SHA256cipher_TLS_DH_RSA_WITH_AES_128_GCM_SHA256cipher_TLS_DH_RSA_WITH_AES_256_CBC_SHAcipher_TLS_DH_RSA_WITH_AES_256_CBC_SHA256cipher_TLS_DH_RSA_WITH_AES_256_GCM_SHA384cipher_TLS_DH_RSA_WITH_ARIA_128_CBC_SHA256cipher_TLS_DH_RSA_WITH_ARIA_128_GCM_SHA256cipher_TLS_DH_RSA_WITH_ARIA_256_CBC_SHA384cipher_TLS_DH_RSA_WITH_ARIA_256_GCM_SHA384cipher_TLS_DH_RSA_WITH_CAMELLIA_128_CBC_SHAcipher_TLS_DH_RSA_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_DH_RSA_WITH_CAMELLIA_128_GCM_SHA256cipher_TLS_DH_RSA_WITH_CAMELLIA_256_CBC_SHAcipher_TLS_DH_RSA_WITH_CAMELLIA_256_CBC_SHA256cipher_TLS_DH_RSA_WITH_CAMELLIA_256_GCM_SHA384cipher_TLS_DH_RSA_WITH_DES_CBC_SHAcipher_TLS_DH_RSA_WITH_SEED_CBC_SHAcipher_TLS_DH_anon_EXPORT_WITH_DES40_CBC_SHAcipher_TLS_DH_anon_EXPORT_WITH_RC4_40_MD5cipher_TLS_DH_anon_WITH_3DES_EDE_CBC_SHAcipher_TLS_DH_anon_WITH_AES_128_CBC_SHAcipher_TLS_DH_anon_WITH_AES_128_CBC_SHA256cipher_TLS_DH_anon_WITH_AES_128_GCM_SHA256cipher_TLS_DH_anon_WITH_AES_256_CBC_SHAcipher_TLS_DH_anon_WITH_AES_256_CBC_SHA256cipher_TLS_DH_anon_WITH_AES_256_GCM_SHA384cipher_TLS_DH_anon_WITH_ARIA_128_CBC_SHA256cipher_TLS_DH_anon_WITH_ARIA_128_GCM_SHA256cipher_TLS_DH_anon_WITH_ARIA_256_CBC_SHA384cipher_TLS_DH_anon_WITH_ARIA_256_GCM_SHA384cipher_TLS_DH_anon_WITH_CAMELLIA_128_CBC_SHAcipher_TLS_DH_anon_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_DH_anon_WITH_CAMELLIA_128_GCM_SHA256cipher_TLS_DH_anon_WITH_CAMELLIA_256_CBC_SHAcipher_TLS_DH_anon_WITH_CAMELLIA_256_CBC_SHA256cipher_TLS_DH_anon_WITH_CAMELLIA_256_GCM_SHA384cipher_TLS_DH_anon_WITH_DES_CBC_SHAcipher_TLS_DH_anon_WITH_RC4_128_MD5cipher_TLS_DH_anon_WITH_SEED_CBC_SHAcipher_TLS_ECDHE_ECDSA_WITH_3DES_EDE_CBC_SHAcipher_TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHAcipher_TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256cipher_TLS_ECDHE_ECDSA_WITH_AES_128_CCMcipher_TLS_ECDHE_ECDSA_WITH_AES_128_CCM_8cipher_TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256cipher_TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHAcipher_TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384cipher_TLS_ECDHE_ECDSA_WITH_AES_256_CCMcipher_TLS_ECDHE_ECDSA_WITH_AES_256_CCM_8cipher_TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384cipher_TLS_ECDHE_ECDSA_WITH_ARIA_128_CBC_SHA256cipher_TLS_ECDHE_ECDSA_WITH_ARIA_128_GCM_SHA256cipher_TLS_ECDHE_ECDSA_WITH_ARIA_256_CBC_SHA384cipher_TLS_ECDHE_ECDSA_WITH_ARIA_256_GCM_SHA384cipher_TLS_ECDHE_ECDSA_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_ECDHE_ECDSA_WITH_CAMELLIA_128_GCM_SHA256cipher_TLS_ECDHE_ECDSA_WITH_CAMELLIA_256_CBC_SHA384cipher_TLS_ECDHE_ECDSA_WITH_CAMELLIA_256_GCM_SHA384cipher_TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256cipher_TLS_ECDHE_ECDSA_WITH_NULL_SHAcipher_TLS_ECDHE_ECDSA_WITH_RC4_128_SHAcipher_TLS_ECDHE_PSK_WITH_3DES_EDE_CBC_SHAcipher_TLS_ECDHE_PSK_WITH_AES_128_CBC_SHAcipher_TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA256cipher_TLS_ECDHE_PSK_WITH_AES_256_CBC_SHAcipher_TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA384cipher_TLS_ECDHE_PSK_WITH_ARIA_128_CBC_SHA256cipher_TLS_ECDHE_PSK_WITH_ARIA_256_CBC_SHA384cipher_TLS_ECDHE_PSK_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_ECDHE_PSK_WITH_CAMELLIA_256_CBC_SHA384cipher_TLS_ECDHE_PSK_WITH_CHACHA20_POLY1305_SHA256cipher_TLS_ECDHE_PSK_WITH_NULL_SHAcipher_TLS_ECDHE_PSK_WITH_NULL_SHA256cipher_TLS_ECDHE_PSK_WITH_NULL_SHA384cipher_TLS_ECDHE_PSK_WITH_RC4_128_SHAcipher_TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHAcipher_TLS_ECDHE_RSA_WITH_AES_128_CBC_SHAcipher_TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256cipher_TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256cipher_TLS_ECDHE_RSA_WITH_AES_256_CBC_SHAcipher_TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384cipher_TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384cipher_TLS_ECDHE_RSA_WITH_ARIA_128_CBC_SHA256cipher_TLS_ECDHE_RSA_WITH_ARIA_128_GCM_SHA256cipher_TLS_ECDHE_RSA_WITH_ARIA_256_CBC_SHA384cipher_TLS_ECDHE_RSA_WITH_ARIA_256_GCM_SHA384cipher_TLS_ECDHE_RSA_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_ECDHE_RSA_WITH_CAMELLIA_128_GCM_SHA256cipher_TLS_ECDHE_RSA_WITH_CAMELLIA_256_CBC_SHA384cipher_TLS_ECDHE_RSA_WITH_CAMELLIA_256_GCM_SHA384cipher_TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256cipher_TLS_ECDHE_RSA_WITH_NULL_SHAcipher_TLS_ECDHE_RSA_WITH_RC4_128_SHAcipher_TLS_ECDH_ECDSA_WITH_3DES_EDE_CBC_SHAcipher_TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHAcipher_TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA256cipher_TLS_ECDH_ECDSA_WITH_AES_128_GCM_SHA256cipher_TLS_ECDH_ECDSA_WITH_AES_256_CBC_SHAcipher_TLS_ECDH_ECDSA_WITH_AES_256_CBC_SHA384cipher_TLS_ECDH_ECDSA_WITH_AES_256_GCM_SHA384cipher_TLS_ECDH_ECDSA_WITH_ARIA_128_CBC_SHA256cipher_TLS_ECDH_ECDSA_WITH_ARIA_128_GCM_SHA256cipher_TLS_ECDH_ECDSA_WITH_ARIA_256_CBC_SHA384cipher_TLS_ECDH_ECDSA_WITH_ARIA_256_GCM_SHA384cipher_TLS_ECDH_ECDSA_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_ECDH_ECDSA_WITH_CAMELLIA_128_GCM_SHA256cipher_TLS_ECDH_ECDSA_WITH_CAMELLIA_256_CBC_SHA384cipher_TLS_ECDH_ECDSA_WITH_CAMELLIA_256_GCM_SHA384cipher_TLS_ECDH_ECDSA_WITH_NULL_SHAcipher_TLS_ECDH_ECDSA_WITH_RC4_128_SHAcipher_TLS_ECDH_RSA_WITH_3DES_EDE_CBC_SHAcipher_TLS_ECDH_RSA_WITH_AES_128_CBC_SHAcipher_TLS_ECDH_RSA_WITH_AES_128_CBC_SHA256cipher_TLS_ECDH_RSA_WITH_AES_128_GCM_SHA256cipher_TLS_ECDH_RSA_WITH_AES_256_CBC_SHAcipher_TLS_ECDH_RSA_WITH_AES_256_CBC_SHA384cipher_TLS_ECDH_RSA_WITH_AES_256_GCM_SHA384cipher_TLS_ECDH_RSA_WITH_ARIA_128_CBC_SHA256cipher_TLS_ECDH_RSA_WITH_ARIA_128_GCM_SHA256cipher_TLS_ECDH_RSA_WITH_ARIA_256_CBC_SHA384cipher_TLS_ECDH_RSA_WITH_ARIA_256_GCM_SHA384cipher_TLS_ECDH_RSA_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_ECDH_RSA_WITH_CAMELLIA_128_GCM_SHA256cipher_TLS_ECDH_RSA_WITH_CAMELLIA_256_CBC_SHA384cipher_TLS_ECDH_RSA_WITH_CAMELLIA_256_GCM_SHA384cipher_TLS_ECDH_RSA_WITH_NULL_SHAcipher_TLS_ECDH_RSA_WITH_RC4_128_SHAcipher_TLS_ECDH_anon_WITH_3DES_EDE_CBC_SHAcipher_TLS_ECDH_anon_WITH_AES_128_CBC_SHAcipher_TLS_ECDH_anon_WITH_AES_256_CBC_SHAcipher_TLS_ECDH_anon_WITH_NULL_SHAcipher_TLS_ECDH_anon_WITH_RC4_128_SHAcipher_TLS_EMPTY_RENEGOTIATION_INFO_SCSVcipher_TLS_FALLBACK_SCSVcipher_TLS_KRB5_EXPORT_WITH_DES_CBC_40_MD5cipher_TLS_KRB5_EXPORT_WITH_DES_CBC_40_SHAcipher_TLS_KRB5_EXPORT_WITH_RC2_CBC_40_MD5cipher_TLS_KRB5_EXPORT_WITH_RC2_CBC_40_SHAcipher_TLS_KRB5_EXPORT_WITH_RC4_40_MD5cipher_TLS_KRB5_EXPORT_WITH_RC4_40_SHAcipher_TLS_KRB5_WITH_3DES_EDE_CBC_MD5cipher_TLS_KRB5_WITH_3DES_EDE_CBC_SHAcipher_TLS_KRB5_WITH_DES_CBC_MD5cipher_TLS_KRB5_WITH_DES_CBC_SHAcipher_TLS_KRB5_WITH_IDEA_CBC_MD5cipher_TLS_KRB5_WITH_IDEA_CBC_SHAcipher_TLS_KRB5_WITH_RC4_128_MD5cipher_TLS_KRB5_WITH_RC4_128_SHAcipher_TLS_NULL_WITH_NULL_NULLcipher_TLS_PSK_DHE_WITH_AES_128_CCM_8cipher_TLS_PSK_DHE_WITH_AES_256_CCM_8cipher_TLS_PSK_WITH_3DES_EDE_CBC_SHAcipher_TLS_PSK_WITH_AES_128_CBC_SHAcipher_TLS_PSK_WITH_AES_128_CBC_SHA256cipher_TLS_PSK_WITH_AES_128_CCMcipher_TLS_PSK_WITH_AES_128_CCM_8cipher_TLS_PSK_WITH_AES_128_GCM_SHA256cipher_TLS_PSK_WITH_AES_256_CBC_SHAcipher_TLS_PSK_WITH_AES_256_CBC_SHA384cipher_TLS_PSK_WITH_AES_256_CCMcipher_TLS_PSK_WITH_AES_256_CCM_8cipher_TLS_PSK_WITH_AES_256_GCM_SHA384cipher_TLS_PSK_WITH_ARIA_128_CBC_SHA256cipher_TLS_PSK_WITH_ARIA_128_GCM_SHA256cipher_TLS_PSK_WITH_ARIA_256_CBC_SHA384cipher_TLS_PSK_WITH_ARIA_256_GCM_SHA384cipher_TLS_PSK_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_PSK_WITH_CAMELLIA_128_GCM_SHA256cipher_TLS_PSK_WITH_CAMELLIA_256_CBC_SHA384cipher_TLS_PSK_WITH_CAMELLIA_256_GCM_SHA384cipher_TLS_PSK_WITH_CHACHA20_POLY1305_SHA256cipher_TLS_PSK_WITH_NULL_SHAcipher_TLS_PSK_WITH_NULL_SHA256cipher_TLS_PSK_WITH_NULL_SHA384cipher_TLS_PSK_WITH_RC4_128_SHAcipher_TLS_RSA_EXPORT_WITH_DES40_CBC_SHAcipher_TLS_RSA_EXPORT_WITH_RC2_CBC_40_MD5cipher_TLS_RSA_EXPORT_WITH_RC4_40_MD5cipher_TLS_RSA_PSK_WITH_3DES_EDE_CBC_SHAcipher_TLS_RSA_PSK_WITH_AES_128_CBC_SHAcipher_TLS_RSA_PSK_WITH_AES_128_CBC_SHA256cipher_TLS_RSA_PSK_WITH_AES_128_GCM_SHA256cipher_TLS_RSA_PSK_WITH_AES_256_CBC_SHAcipher_TLS_RSA_PSK_WITH_AES_256_CBC_SHA384cipher_TLS_RSA_PSK_WITH_AES_256_GCM_SHA384cipher_TLS_RSA_PSK_WITH_ARIA_128_CBC_SHA256cipher_TLS_RSA_PSK_WITH_ARIA_128_GCM_SHA256cipher_TLS_RSA_PSK_WITH_ARIA_256_CBC_SHA384cipher_TLS_RSA_PSK_WITH_ARIA_256_GCM_SHA384cipher_TLS_RSA_PSK_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_RSA_PSK_WITH_CAMELLIA_128_GCM_SHA256cipher_TLS_RSA_PSK_WITH_CAMELLIA_256_CBC_SHA384cipher_TLS_RSA_PSK_WITH_CAMELLIA_256_GCM_SHA384cipher_TLS_RSA_PSK_WITH_CHACHA20_POLY1305_SHA256cipher_TLS_RSA_PSK_WITH_NULL_SHAcipher_TLS_RSA_PSK_WITH_NULL_SHA256cipher_TLS_RSA_PSK_WITH_NULL_SHA384cipher_TLS_RSA_PSK_WITH_RC4_128_SHAcipher_TLS_RSA_WITH_3DES_EDE_CBC_SHAcipher_TLS_RSA_WITH_AES_128_CBC_SHAcipher_TLS_RSA_WITH_AES_128_CBC_SHA256cipher_TLS_RSA_WITH_AES_128_CCMcipher_TLS_RSA_WITH_AES_128_CCM_8cipher_TLS_RSA_WITH_AES_128_GCM_SHA256cipher_TLS_RSA_WITH_AES_256_CBC_SHAcipher_TLS_RSA_WITH_AES_256_CBC_SHA256cipher_TLS_RSA_WITH_AES_256_CCMcipher_TLS_RSA_WITH_AES_256_CCM_8cipher_TLS_RSA_WITH_AES_256_GCM_SHA384cipher_TLS_RSA_WITH_ARIA_128_CBC_SHA256cipher_TLS_RSA_WITH_ARIA_128_GCM_SHA256cipher_TLS_RSA_WITH_ARIA_256_CBC_SHA384cipher_TLS_RSA_WITH_ARIA_256_GCM_SHA384cipher_TLS_RSA_WITH_CAMELLIA_128_CBC_SHAcipher_TLS_RSA_WITH_CAMELLIA_128_CBC_SHA256cipher_TLS_RSA_WITH_CAMELLIA_128_GCM_SHA256cipher_TLS_RSA_WITH_CAMELLIA_256_CBC_SHAcipher_TLS_RSA_WITH_CAMELLIA_256_CBC_SHA256cipher_TLS_RSA_WITH_CAMELLIA_256_GCM_SHA384cipher_TLS_RSA_WITH_DES_CBC_SHAcipher_TLS_RSA_WITH_IDEA_CBC_SHAcipher_TLS_RSA_WITH_NULL_MD5cipher_TLS_RSA_WITH_NULL_SHAcipher_TLS_RSA_WITH_NULL_SHA256cipher_TLS_RSA_WITH_RC4_128_MD5cipher_TLS_RSA_WITH_RC4_128_SHAcipher_TLS_RSA_WITH_SEED_CBC_SHAcipher_TLS_SRP_SHA_DSS_WITH_3DES_EDE_CBC_SHAcipher_TLS_SRP_SHA_DSS_WITH_AES_128_CBC_SHAcipher_TLS_SRP_SHA_DSS_WITH_AES_256_CBC_SHAcipher_TLS_SRP_SHA_RSA_WITH_3DES_EDE_CBC_SHAcipher_TLS_SRP_SHA_RSA_WITH_AES_128_CBC_SHAcipher_TLS_SRP_SHA_RSA_WITH_AES_256_CBC_SHAcipher_TLS_SRP_SHA_WITH_3DES_EDE_CBC_SHAcipher_TLS_SRP_SHA_WITH_AES_128_CBC_SHAcipher_TLS_SRP_SHA_WITH_AES_256_CBC_SHAclientConnIdleStateclientConnPoolclientConnPoolIdleCloserclientConnReadLoopclientPrefaceclientStreamcloneHeadercloseWaitercommaSeparatedTrailerscommonBuildOncecommonCanonHeadercommonLowerHeaderconfigureTransportconnHeadersconnectionStatercurGoroutineIDcutoff64dataBufferdataChunkPoolsdataChunkSizeClassesdefaultMaxReadFrameSizedefaultMaxStreamsdefaultUserAgentdialCalldialOnMissduplicatePseudoHeaderErrorencKVencodeHeaderserrChanPoolerrClientConnClosederrClientConnGotGoAwayerrClientConnUnusableerrClientDisconnectederrClosedBodyerrClosedPipeWriteerrClosedResponseBodyerrCodeNameerrDepStreamIDerrHandlerCompleteerrHandlerPanickederrMixPseudoHeaderTypeserrPadByteserrPadLengtherrPrefaceTimeouterrPseudoAfterRegularerrReadEmptyerrReqBodyTooLongerrRequestCancelederrRequestHeaderListSizeerrResponseHeaderListSizeerrStopReqBodyWriteerrStopReqBodyWriteAndCancelerrStreamClosederrStreamIDerrTimeouterringRoundTrippererrnoerrorReaderfhBytesfilterOutClientConnfirstSettingsTimeoutflushFrameWriterforeachHeaderElementframeHeaderLenframeNameframeParserframeParsersframeWriteResultgategetDataBufferChunkgoAwayFlowErrorgoAwayTimeoutgoroutineLockgoroutineSpacegot1xxFuncForTestsgracefulShutdownMsggzipReaderh1ServerKeepAlivesDisabledhandleHeaderListTooLonghandlerChunkWriteSizehandlerPanicRSTheaderFieldNameErrorheaderFieldValueErrorheadersEnderheadersOrContinuationhttpCodeStringidleTimerMsginTestsinitialMaxFrameSizeisBadCipherisClosedConnErrorisConnectionCloseRequestisEOFOrNetReadErrorisNoCachedConnErrorlittleBuflogFrameReadslogFrameWriteslowerHeadermaxAllocFrameSizemaxFrameSizemaxQueuedControlFramesminMaxFrameSizemustUint31new400HandlernewBufferedWriternewGoroutineLocknoCachedConnErrornoDialClientConnPoolnoDialH2RoundTrippernoDialOnMisspadZerosparseContinuationFrameparseDataFrameparseGoAwayFrameparseHeadersFrameparsePingFrameparsePriorityFrameparsePushPromiseparseRSTStreamFrameparseSettingsFrameparseUnknownFrameparseWindowUpdateFramepipeBufferprefaceTimeoutpriorityDefaultWeightpriorityNodepriorityNodeClosedpriorityNodeIdlepriorityNodeOpenpriorityNodeStatepriorityWriteSchedulerpseudoHeaderErrorputDataBufferChunkrandomWriteSchedulerreadFrameHeaderreadFrameResultregisterHTTPSProtocolrequestBodyrequestParamresAndErrorresponseWriterresponseWriterStateresponseWriterStatePoolserverConnBaseContextserverInternalStateserverMessagesettingNamesettingsTimerMsgshouldSendReqContentLengthshutdownEnterWaitStateHookshutdownTimerMsgsortPriorityNodeSiblingssorterPoolsplitHeaderBlockstartPushRequeststateClosedstateHalfClosedLocalstateHalfClosedRemotestateIdlestateNamestateOpenstickyErrWriterstrSliceContainsstreamEnderstreamErrorsummarizeFrameterminalReadFrameErrortestHookGetServerConntestHookOnConntestHookOnPanictestHookOnPanicMutraceFirstResponseBytetraceGetConntraceGot100ContinuetraceGot1xxResponseFunctraceGotConntraceHasWroteHeaderFieldtraceWait100ContinuetraceWroteHeaderFieldtraceWroteHeaderstraceWroteRequesttransportDefaultConnFlowtransportDefaultStreamFlowtransportDefaultStreamMinRefreshtransportResponseBodytypeFrameParservalidPseudoPathvalidStreamIDvalidStreamIDOrZerovalidWireHeaderFieldNamewrite100ContinueHeadersFramewriteDatawriteDataPoolwriteEndsStreamwriteFramerwriteGoAwaywritePingAckwritePushPromisewriteQueuewriteQueuePoolwriteResHeaderswriteSettingswriteSettingsAckwriteWindowUpdateCloseConnHeaderEncoderstaysWithinBufferwriteFramePusherIDAdjustStreamOpenStreamactiveConnsregisterConnunregisterConnstartGracefulShutdownMaxHandlersMaxConcurrentStreamsMaxReadFrameSizePermitProhibitedCipherSuitesMaxUploadBufferPerConnectionMaxUploadBufferPerStreamNewWriteSchedulerinitialConnRecvWindowSizeinitialStreamRecvWindowSizemaxReadFrameSizeServeConnreadMoresetConnFlowtakecheckNotOnbaseCtxdoneServingreadFrameChwantWriteFrameChwroteFrameChbodyReadChserveMsgChinflowremoteAddrStrwriteSchedserveGpushEnabledsawFirstSettingsneedToSendSettingsAckunackedSettingsqueuedControlFramesclientMaxStreamsadvMaxStreamscurClientStreamscurPushedStreamsmaxClientStreamIDmaxPushPromiseIDinitialStreamSendWindowSizeheaderTableSizepeerMaxHeaderListSizecanonHeaderwritingFramewritingFrameAsyncneedsFrameFlushinGoAwayinFrameScheduleLoopneedToSendGoAwaygoAwayCodeshutdownTimerheaderWriteBufhpackEncodershutdownOncerejectConncurOpenStreamssetConnStatevlogfcondlogfcanonicalHeaderreadFrameswriteFrameAsynccloseAllStreamsOnConnClosestopShutdownTimernotePanicawaitGracefulShutdownonSettingsTimeronIdleTimeronShutdownTimersendServeMsgreadPrefacewriteDataFromHandlerwriteFrameFromHandlerstartFrameWritewroteFramescheduleFrameWritestartGracefulShutdownInternalshutDownInresetStreamprocessFrameFromReaderprocessFrameprocessWindowUpdateprocessResetStreamprocessSettingsprocessSettingprocessSettingInitialWindowSizeprocessGoAwayprocessHeadersprocessPrioritynewWriterAndRequestnewWriterAndRequestNoBodyrunHandlerwriteHeaderswrite100ContinueHeadersnoteBodyReadFromHandlernoteBodyReadsendWindowUpdatesendWindowUpdate32startPushbreakErrreadFnBreakWithErrorcloseWithErrorAndCodecloseWithErrorcloseDoneLockedbodyBytesdeclBodyBytesresetQueuedgotTrailerHeaderreqTrailerisPushedcopyTrailersToHandlerRequestonWriteTimeoutprocessTrailerHeadersisControlConsumereplyToWritersubtreeByteskidssetParentaddByteswalküÿ Ý Ð8PV    